<!DOCTYPE html>
<!-- saved from url=(0022)http://localhost:3000/ -->
<html><script type="text/javascript" src="chrome-extension://ibniinmoafhgbifjojidlagmggecmpgf/dist/inject.js"></script><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <link rel="stylesheet" type="text/css" class="__meteor-css__" href="./release-note_files/merged-stylesheets.css">
<title>release-notes</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="./release-note_files/semantic.min.css">
  <link rel="stylesheet" href="./release-note_files/bootstrap.min.css">

</head>
<body><div id="react-target"><div><h1>Camunda 8 version 8.3.0 Release Notes</h1><div><div><h2>Zeebe</h2><div><h3>Broker</h3><div><h4>Enhancements</h4><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14019"><strong>Create `DbTenantAwareKey`</strong></a><p> 
In the previous situation, multi-tenancy was not supported, meaning the system was unable to store a unique tenant id in the state.
 
The system had no provision to hold the tenant id in the state due to the absence of an object that could contain this information and interact with other keys concurrently. 
 
A new object, `DbTenantAwareKey`, was introduced. This key implements the `DbKey` interface and always contains a `DbString` that is used to store the tenantId. Additionally, this key can wrap any other key. 
 
Now, the system supports multi-tenancy, effectively storing unique tenant ids in the state. This id can be interacted with by other keys, thanks to the newly introduced `DbTenantAwareKey`.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13989"><strong>Add auth data to Zeebe records</strong></a><p>Previously, the Zeebe Authentication data (such as the user’s tenant access list) was not being transmitted efficiently from the gateway to the broker, making it challenging for future extensions, like the eventual addition of user permissions. 
This inefficiency was due to the placement of the Authentication Data. It was not incorporated into `RecordMetadata`, making it inaccessible to all types of Zeebe Records and causing potential disturbance to record value outcomes of commands.
The code has been modified to include the Authentication data within the `RecordMetadata`. It is encoded to facilitate its extraction only when necessary and ease any potential extensions in the future without making further changes to the `RecordMetadata` structure. 
Now, the Zeebe Authentication data is appropriately contained in the `RecordMetadata`. It's encoded based on a flag indicator that signifies the mechanism used for encoding/decoding. Also, the Authentication data is incorporated into `ExecuteCommandRequest` to enable more straightforward Gateway-to-Broker requests. This change improves overall extendability and system efficiency.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13988"><strong>Provide an API for data interchange of Zeebe auth data</strong></a><p>Users were previously unable to send Zeebe authentication data from the gateway to the broker in an easily extendable manner. 
The system lacked a viable data interchange protocol, meaning that it was unable to adapt to potential future needs, such as adding user permissions. 
The development team implemented a general API for encoding and decoding authentication data. This feature also includes the functionality to encode and decode this data within a JSON Web Token (JWT) string. 
Users are now equipped with a pluggable, easily extendable tool to send authentication data from the gateway to the broker, significantly enhancing the versatility of the system for potential future changes.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13987"><strong>Provide a `TenantAccessChecker` API for multi-tenancy</strong></a><p>Prior to the update, users could not determine if they had access to data from a selected tenant, and if they had full access to all the data they requested (across multiple tenants).
There was no existing `TenantAccessChecker` class in the `engine` module for checking user access to different tenant data.
We implemented a `TenantAccessChecker` class in the `engine` module which is utilized by `*Processor` classes and other classes that process `Command` records. This class includes methods that check if a user has access to specific tenant data and if a user has full access to all the tenant IDs they requested.
Now, users can easily determine if they have access to the tenant data they requested and if they have full access to all requested tenant IDs by leveraging the `TenantAccessChecker`.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/8263"><strong>Show blacklisting in the Grafana Dashboard</strong></a><p> Users found it difficult to keep track of blacklisted instances and the rate of blacklisting on the Grafana Dashboard as this feature was not incorporated previously.
 The metrics for blacklisting instances had been included in the system, but these were not integrated onto the Grafana Dashboard. Moreover, this count metric was only exported if there had been a new blacklisted instance, and was not exported upon pod restart.
 The system was updated to continuously export the blacklisted instances metric. Now, this metric is exported at least once on restart. Moreover, different visualization methods were explored and a heatmap method was employed.
 On the Grafana Dashboard, users can now see if there are any blacklisted instances and the rate of blacklisting. This feature provides an indication whether there is something problematic happening in the process execution. Users can, therefore, monitor and address potential issues in a timely manner.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13989"><strong>Add auth data to Zeebe records</strong></a><p>Previously, the Zeebe Authentication data (such as the user’s tenant access list) was not being transmitted efficiently from the gateway to the broker, making it challenging for future extensions, like the eventual addition of user permissions. 
This inefficiency was due to the placement of the Authentication Data. It was not incorporated into `RecordMetadata`, making it inaccessible to all types of Zeebe Records and causing potential disturbance to record value outcomes of commands.
The code has been modified to include the Authentication data within the `RecordMetadata`. It is encoded to facilitate its extraction only when necessary and ease any potential extensions in the future without making further changes to the `RecordMetadata` structure. 
Now, the Zeebe Authentication data is appropriately contained in the `RecordMetadata`. It's encoded based on a flag indicator that signifies the mechanism used for encoding/decoding. Also, the Authentication data is incorporated into `ExecuteCommandRequest` to enable more straightforward Gateway-to-Broker requests. This change improves overall extendability and system efficiency.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13988"><strong>Provide an API for data interchange of Zeebe auth data</strong></a><p>Users were previously unable to send Zeebe authentication data from the gateway to the broker in an easily extendable manner. 
The system lacked a viable data interchange protocol, meaning that it was unable to adapt to potential future needs, such as adding user permissions. 
The development team implemented a general API for encoding and decoding authentication data. This feature also includes the functionality to encode and decode this data within a JSON Web Token (JWT) string. 
Users are now equipped with a pluggable, easily extendable tool to send authentication data from the gateway to the broker, significantly enhancing the versatility of the system for potential future changes.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13987"><strong>Provide a `TenantAccessChecker` API for multi-tenancy</strong></a><p>Prior to the update, users could not determine if they had access to data from a selected tenant, and if they had full access to all the data they requested (across multiple tenants).
There was no existing `TenantAccessChecker` class in the `engine` module for checking user access to different tenant data.
We implemented a `TenantAccessChecker` class in the `engine` module which is utilized by `*Processor` classes and other classes that process `Command` records. This class includes methods that check if a user has access to specific tenant data and if a user has full access to all the tenant IDs they requested.
Now, users can easily determine if they have access to the tenant data they requested and if they have full access to all requested tenant IDs by leveraging the `TenantAccessChecker`.</p></div></p></div><div><h4>Bug Fixes</h4><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14509"><strong>Potential inconsistency in raft after restoring from a backup</strong></a><p> 
After restoring from a backup, inconsistencies were reported in some partitions leading to potential data loss for newly processed data. This issue, marked by an "IllegalStateException" error, happened when the system didn't recognize new leadership within the partitions post a restore, leading to an unexpected restart of term count from 1.
 
The issue was rooted in the system's Raft algorithm. After a restore process, the Raft metastore was emptied and restarted the term from 1. However, the last log term maintained its value of 2. This created a disconnect when a new leader was not recognized, and a different process incorrectly initiated a new term, leading to inconsistencies.
 
A corrective measure was implemented to ensure recognition of new leadership and prevent incorrect initiation of new terms post a restore, therefore preventing the IllegalStateException.
 
Now, the system correctly identifies new leadership after a restore from the backup, preventing term inconsistencies and ensuring data integrity for newly processed data. The Raft algorithm can now function correctly in all scenarios after a restore, preserving the continuity of data processes.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14486"><strong>Failure to write snapshot checksum is silently ignored</strong></a><p>In the past, when writing to the checksum file of a snapshot failed, the system was silently ignoring IOExceptions. This error was not visible to users.
This was due to the fact that `PrintWriter#flush` did not throw `IOExceptions` but only set an internal error flag.
The code was modified to throw an exception when encountering an error while writing the checksum file. More specifically, the recommended `PrintWriter#checkError` method was used to flush and check for errors at the same time.
Now, if there's any IO failure while writing the checksum file, the exception is thrown, indicating the snapshot store as a failed attempt to commit a snapshot. This is handled appropriately with the newly implemented error checks. The system is transparent with error handling in such scenarios.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14406"><strong>Migration could result in inconsistent data on rolling update</strong></a><p>
Previously, during the migration from software version 8.2 to 8.3 of Zeebe, potential inconsistencies in states of snapshots across brokers occurred. This happened during a rolling update owing to a scenario where an updated broker replicated a migrated snapshot to a non-updated broker. The latter, replaying events in absence of an essential 'column family', created a different state of events.
This issue was caused because in software version 8.2 the column family `PROCESS_INSTANCE_KEY_BY_DEFINITION_KEY` did not exist. While replaying, the old broker version did not put anything into this column family, leading to a state not consistent with the state of the updated broker.
The problem was resolved by restructuring the migration task. Under the new arrangement, if the deprecated CF still exists and contains data, the migration occurs when a broker gets updated. This process ensures the records get migrated to the new CF, which effectively handles the case of an old broker receiving a snapshot from a new broker.
Now, during a rolling update, all brokers eventually end up having a consistent snapshot state. The updated method guarantees a uniform state regardless of the version of the broker recreating the events, avoiding any risks of inconsistency. The snapshot state will remain consistent across all brokers even after multiple updates.
</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14367"><strong>Duplicate snapshot replication causing to fail raft follower</strong></a><p> Users experienced high impact bugs in recent chaos tests due to the node transitioning to inactive. This was triggered by the system receiving a duplicate snapshot and resulted in the node not participating in the corresponding partition.
 The underlying issue was due to a race condition that occurred because the snapshot listener was invoked asynchronously after persistence.  Thus, the snapshot reference of the follower might not have been updated yet when the leader attempts to send an append request. 
 The issue was addressed by enacting a robust check in the `onInstall` method that stopped snapshot overwrite with a replicated one if it already exists locally. The problem was also eased by aborting pending snapshots.
 The leader should no longer send the same snapshot again; therefore, users should not experience the system transitioning to inactive due to receipt of a duplicate snapshot. This ensures that the node continues participating in the corresponding partition improving the system stability.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14309"><strong>Cannot process record due to process deployed version being null.</strong></a><p> 
The system was unable to process a record as it returned a NullPointerException for the deployed process version, thereby negatively impacting the system's functionality.
 
The system experienced a caching error regarding process versions, where distinct keys in the cache were all attributed to the same value object. Consequently, once the object got updated, it led to modifications for all entries. 
 
The issue was resolved by revising the handling of the state data. Specifically, the data fetched from the state was copied before being stored in the cache, thus eliminating conflicts due to the initial shared object. 
 
The system is now able to handle process versions properly. Each cache entry operates independently, ensuring that updating the object doesn't affect all entries simultaneously. As a result, the system effectively processes records without any Null Pointer Exceptions, improving usability.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14275"><strong>IndexedRaftLogEntry are kept in memory until committed, which can lead to OOM</strong></a><p> Previously, IndexedRaftLogEntry objects were kept in memory until they were committed. This led to instances where the Broker ran out of memory, especially when there was a lot of internal load (e.g., several timers triggered at once, repeatedly).
 The cause was related to the fact that appendFutures and async lambda completion listeners held references to IndexedRaftEntries for their entire lifetime. If a commit was not possible due to network issues, disk issues, etc. for a prolonged period, the system would accumulate more futures and records, eventually leading to a memory overflow situation.
 We have adjusted the system so that it does not keep the reference to the instances. We have minimized the information needed in the listeners, mostly to committing indexes.
 Currently, the system no longer keeps instances in memory until they are committed. Instead, we only store essential information in listeners, preventing out of memory issues.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14055"><strong>No executable process found while processing Deployment:Create command</strong></a><p>Consequence:
In the past, while processing the Deployment:Create command, an error consistently occurred in partitions 2 and 3 stating 'No executable process found'. The deployment was apparently successful in partition 1 but could not be distributed to the other partitions. This caused confusion as users could not create instances on other partitions though the deployment seemed successful.
Cause:
The error stemmed from an issue with the updateInMemoryState function within the DbProcessState class in zeebe-workflow-engine-8.3.0-alpha4.jar. The function was expected to find an executable process in the persisted process with a certain key, however, no such process was found. This may be due to a regression or potentially a race condition between different versions of a process.
Fix:
The underlying technical implementation of the system was rectified, ensuring that an executable process in the persisted process with the appropriate key can be found during the updateInMemoryState function execution. The root cause of the issue was thoroughly investigated and necessary corrections were implemented.
Result:
When a user now tries to process the Deployment:Create command, they will no longer encounter the 'No executable process found' error. The deployment mechanism functions as expected, allowing users to create instances on all partitions without any confusions or hindrances.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14044"><strong>Backpressure queue is not reset when back-to-back role transitions</strong></a><p> 
Users experienced a 100% backpressure on one partition. This occurred during back-to-back role transitions. Specifically, a node being a leader for a particular partition then transitioned to a follower and the transition was cancelled mid-process because it became the leader again.
 
The issue was traced back to an existing fix that failed to notify the command API that it had become a follower. Consequently, the system reused the limiter from the previous leader role since it was not removed, triggering this anomaly.
 
We resolved the issue by implementing a new procedure after the leader-follower-leader transition where, each time a node transitions to be a leader, we reset the partition backpressure queue. This adjustment ensures that the transition and command notification process is controlled and accurate.
 
As a result of the fix, the partition currently operates as intended when a new leader is elected, regardless of previous roles. The backpressure now functions correctly during back-to-back transitions. Users no longer experience the 100% backpressure in any of the partitions.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13936"><strong>Condition ignored on Inclusive Gateway with singular outgoing Sequence Flow</strong></a><p>When users deployed a process with an inclusive gateway that had a singular outgoing sequence flow under the condition `= false`, the sequence flow was still taken, and no incident was raised. This was against the expected functionality as stated in the BMPN specification.
The issue was that the application had not been properly configured to handle conditions in an Inclusive Gateway when only a single outgoing Sequence Flow was present. The software did not correctly evaluate the conditions, resulting in a lack of a runtime exception or incident raised when none of the conditions equated to 'true'.
The inconsistency in the Inclusive Gateway Sequence Flow management logic was identified and amended. Changes were made to ensure it accurately reads and interprets the singular outgoing sequence flow conditions.
Now, when deploying a process with an inclusive gateway containing a singular outgoing sequence flow, the application correctly evaluates and reacts based on the condition. If the condition `= false`, as per the BMPN spec, an incident is raised appropriately, ensuring better executable process management for users.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13233"><strong>Regression in deploying large payloads</strong></a><p> 
A regression in our system resulted in decreased capacity for deploying large payloads on multi-partition clusters. Deployment size was reduced from ~2MB down to ~1.4MB.
 
The regression was created by a pull request that introduced a new event called `CommandDistribution:STARTED` for storing command for distribution. This event was appended only for multi-partition clusters and as it contains the entire deployment (including the resource), when appended, it decreases the maximum payload size because the follow-up events reach the `MAX_BATCH_SIZE` restriction quicker as this event is part of the calculation.
 
To resolve this issue, we have updated the system to not write the resource in the `Deployment:CREATED` event, the resource is now only written in the `Process:CREATED` and `DecisionRequirements:CREATED` events. 
 
As a result, the deployment size is no longer limited by the issue and can support larger payloads. For the users, the resource is available in the `Process:CREATED` and `DecisionRequirements:CREATED` events. We recommend users to review the Update Guide for more information on this behavior change.
</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13093"><strong>`RandomizedRaftTest.consistencyTestWithSnapshot` fails with unexpected exception</strong></a><p>An error occurred while generating a response.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12780"><strong>Failing to write to logstream during stepdown is logged as error</strong></a><p>In the past, when a system leader transitioned to a follower and the logstream closed, the user received an unexpected error message saying they failed to write a CREATE command to the logstream. Additionally, a plethora of error messages could appear in the gateway.
Prior to our system update, the system attempted to write a user request to the leader's logstream during the leader's transition to a follower, resulting in a failure. It happened because the command was tried to write after the logstream was already closed. 
Developers changed the log level from error to either warn/debug to clarify the message. In addition, they enhanced the logstream#tryWrite function to return a specific error code, instead of -1, for more meaningful logging. During leader transition, the system now chooses not to log the error but instead returns a PARTITION_LEADER_MISMATCH code back to the gateway.
Now, when a leader transitions to a follower, the system no longer logs an error message about failure to write to logstream. The system logs a warning or debug message instead. The system also returns a specific error code if it tries to write to a closed logstream. If the system recognizes the action is taking place during a leader transition, it doesn't log the error but returns a PARTITION_LEADER_MISMATCH code to the gateway for command retry with the new leader before sending an error to the client. These changes have improved the clarity and usefulness of system messages, thereby reducing noise in the gateway.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/7855"><strong>Messaging service is not running</strong></a><p>
Users faced errors when the messaging service was shut down while it was still trying to accept or send a response. This was an observable issue in the system, causing messages like the "Messaging service is not running" error to appear. This occurred notably in version `1.2.0-alpha2` and has been observed multiple times.
The issue lies in the system's resource closing order. The messaging service was closed concurrently while it was still processing actions, causing the sending to fail and subsequent errors upon closing the broker. The flaw was traceable back to the `AtomixClientTransportAdapter` within the `Gateway BrokerClient` component, where the messaging service was initialized but never appropriately closed.
The technical resolution involved adjusting the closing sequence within the `BrokerClientImpl` and integrating the closure of `AtomixClientTransportAdapter`. This has been designed to ensure that the messaging service is closed only after the broker client, preventing any unfulfilled requests from being sent after closure.
Currently, users no longer face the issue of "Messaging service is not running" errors during shutdown. This is due to the adjusted sequence of closing resources that ensures the messaging service's successful closure after the broker client completes its tasks, improving the system shutdown process.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/5209"><strong>Startup failure of one raft partition server affects the availability of other healthy partition</strong></a><p> In the past, if one partition server in a node failed to start up, all the services (like processing and snapshoting) were held up until the faulty partition's issue was resolved. This caused other healthy partitions, even if they were serving as leaders, to not perform any processing leading to service unavailability or only partial availability. It also led to a large buildup of events in the log, which resulted in a lengthy replay phase when the unhealthy partition was finally fixed.
 This issue originated from waiting for the successful start-up of all raft servers in a node before kick-starting Zeebe services. The underlying problem was the system design, coded in such a way that all Zeebe services would only be installed after Atomix (a framework for building distributed systems) was completely started, which included the successful initialization of all partitions.
 The startup and bootstrap procedures were revised by decoupling the startup of partitions. The code was adjusted to initiate services for each partition as soon as it was deemed ready, rather than waiting for all partitions to be ready. Sub-steps for installing processors were also integrated into the partition's startup process.
 Now, each partition operates independently from the others during the startup process. The startup failure of one partition server doesn't affect the availability and performance of other partitions in the same node. With the independent starters for each partition, the services can be installed and active, even if some partitions have not completed the startup process. This results in improved service availability.
</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/2890"><strong>I can spawn inner instances for a large input collection</strong></a><p> Earlier, when users tried to spawn a large number of instances simultaneously at a multi-instance activity, the system frequently failed to spawn the inner instances. They were forced to manually reduce the size of the input collection variable to resolve this issue.
 The system was not designed to handle a large number of instances in the input collection and attempted to spawn all instances at once upon activation, leading to failure in the presence of larger collections.
 The spawning of instances has been adjusted to occur in a step-wise manner, creating a specific number of instances at a time until all instances are created. An operation can now be interrupted by an event or a terminate command, providing more flexibility and control over the process.
 Users can now spawn as many instances as defined by the input collection, even for larger input collections. The system handles these larger collections smoothly by spawning instances in steps. The performance has been enhanced, and previously reported issues have been resolved.
</p></div></p></div></div><div><h3>Gateway</h3><div><h4>Enhancements</h4><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14497"><strong>Allow using the default tenant even when no tenant associated to requester</strong></a><p>In the past, all clients, even those not associated with any tenants in Identity, could use the default tenant. This caused a security issue where users from one team could possibly access data from another team if they were using the same cluster and had updated to the newer version.
The issue was due to the system's architecture that allowed access to the default tenant regardless of the client's association to the any tenant in Identity. This practice did not take into consideration multi-tenancy and data privacy-related requirements of different teams using the same cluster.
The access level to the default tenant was modified to not be granted by default to all clients. Implementation support was removed for use cases where the client is not associated with any tenants.
Now, clients cannot use the default tenant if they're not explicitly associated with any tenants in Identity. This ensures that each team's data remains private and is not accessible to other teams on the same cluster.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14278"><strong>Gateway supports multi-tenancy in EvaluateDecision RPC</strong></a><p>Previously, users could not use the EvaluateDecision RPC with multi-tenancy in the gateway.
The issue was caused by the lack of multi-tenancy support in the EvaluateDecision RPC implementation.
We modified the EvaluateDecision RPC in the gateway to include support for multi-tenancy.
Now, users can use the EvaluateDecision RPC in a multi-tenant environment.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14276"><strong>Gateway supports multi-tenancy in PublishMessage RPCs</strong></a><p>Previously, users encountered difficulty when attempting to utilize multi-tenancy in PublishMessage RPCs via Gateway. 
This was due to the fact that the Gateway systems were not correctly configured to support multi-tenant operations. 
We modified the Gateway system’s configurations and allowed them to successfully accommodate multi-tenant operations with PublishMessage RPCs.
Now, users can seamlessly utilize multi-tenancy in PublishMessage RPCs through Gateway, benefiting from a fluid and efficient system operation.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14254"><strong>Job polling/pushing in the Gateway supports multi-tenancy</strong></a><p>Previously, users encountered a PERMISSION_DENIED error when trying to poll/stream jobs for a tenant they were not authorized for when multi-tenancy was enabled, as well as an INVALID_ARGUMENT error for a provided (non-default) tenant id when multi-tenancy was disabled, or if an invalid tenant id was provided when multi-tenancy was enabled. 
The cause of these issues stemmed from the Gateway's support for receiving and forwarding `ActivateJobs` and `StreamActivatedJobs` RPC calls with a list of `tenantId`s. The system was not accurately identifying the validity and the authorization level of the provided tenant id.
The Gateway and the Broker have been revised to correct the validation and authorization procedures when processing `ActivateJobs` and `StreamActivatedJobs` RPC calls with a list of `tenantId`s. 
Users can now successfully poll or stream jobs for their authorized tenants without encountering permission or argument errors. If the tenant ids list is left empty, all of the user's authorized tenants will be used.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14211"><strong>Gateway supports multi-tenancy in CreateProcessInstance* RPCs</strong></a><p>Previously, the Gateway did not support multi-tenancy in `CreateProcessInstance*` RPC calls, essentially ignoring any `tenantId` passed with requests.
This was due to the fact that the relevant gRPC methods, namely `CreateProcessInstanceRequest`, `CreateProcessInstanceWithResultRequest`, and `CreateProcessInstanceResponse`, lacked a `tenantId` property. As a result, even if a `tenantId` was provided, it wouldn't be transferred to the respective `BrokerCreateProcessInstanceRequest` and `BrokerCreateProcessInstanceWithResultRequest`.
Updates were made to include a `tenantId` property in the mentioned gRPC methods. Accordingly, the Gateway endpoints `createProcessInstance(...)` and `createProcessInstanceWithResult(...)` were modified to pass the `tenantId` property to the Broker requests. Additionally, checks were implemented to handle when multi-tenancy is disabled or if the `tenantId` is `null`.
The Gateway now fully supports `tenantId` property handling and forwarding in `CreateProcessInstance*` RPC calls. This ensures that, if provided, every request's `tenantId` will be correctly carried out, facilitating multi-tenancy support.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14041"><strong>I can use a Gateway configuration flag for multi-tenancy</strong></a><p>Previously, users were unable to enable multi-tenancy for the Zeebe cluster using a Gateway configuration flag.
This was because the product did not have the feature `zeebe.gateway.multiTenancy.enabled` in its Gateway configuration.
Developers added a Gateway configuration flag, `zeebe.gateway.multiTenancy.enabled`, which by default is set to FALSE (disabled). 
Now, users can toggle multi-tenancy for the Zeebe cluster by adjusting the Gateway configuration flag, which in turn accommodates the potential integration of additional multi-tenancy-related configuration properties in the future.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13237"><strong>Support multi-tenancy in the Gateway</strong></a><p> Previously, the Zeebe Gateway did not fully support multi-tenancy. Users could not use different tenants, impacting those who required data segmentation and isolation at the tenant level.
 The Zeebe Gateway was not initially designed or configured to handle multiple tenants. Its `IdentityInterceptor` class did not provide a list of tenant IDs that a user has access to, and it did not adequately interact with the Identity SDK for multi-tenancy settings.
 The Zeebe Gateway was reconfigured to support multi-tenancy. Changes included the addition of a Gateway configuration flag to toggle multi-tenancy, the expansion of the `IdentityInterceptor` class to provide a list of tenant IDs the user has access to, and the use of the Identity SDK to fetch these tenant IDs when multi-tenancy is enabled. An API was also provided for the IdentityInterceptor class, allowing an Identity-independent use of multi-tenancy.
 With this fix, users can now fully utilize multi-tenancy in the Zeebe Gateway. The system provides data on the user's authorized tenants and forwards this data to the Zeebe Broker. If multi-tenancy is enabled, the system fetches and provides a list of tenant IDs from the Identity SDK. If disabled, then only the default tenant ID is provided. The system now supports the use of a multi-tenant Zeebe without the need for Camunda Identity.</p></div></p></div><div><h4>Bug Fixes</h4><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14236"><strong>Prevent concurrent StreamObserver calls on the same instance</strong></a><p>
Previously, when calling any of the `StreamObserver` methods concurrently, the system would send garbled messages, sometimes leading to java.lang.IllegalStateException or io.grpc.StatusRuntimeException. This was perceived as operational errors while running applications such as the Camunda 8 Benchmark.
The critical issue was rooted in the `ClientStreamAdapter` implementation, which calls `onNext` using a thread pool executor, making concurrent calls indeed possible. This was effectively a violation of the StreamObserver contract, which clearly stated that separate StreamObservers do not need to be synchronized together, however, if multiple threads will be writing to a StreamObserver concurrently, the application must synchronize calls.
The issue was rectified by employing the `SerializingExecutor` utility of gRPC in the short term. The executor serializes the calls coming into the `StreamObserver`, thereby ensuring they are not concurrent and abiding by the StreamObserver contract.
Now, calls to a `StreamObserver` are serialized, and the system no longer throws errors related to unsynchronized `StreamObserver` calls while running concurrent tasks. No garbled messages are sent, and thus it provides desired operational results without internal errors. More permanent solutions using actors are planned for future releases.
</p></div></p></div></div><div><h3>Java Client</h3><div><h4>Enhancements</h4><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14255"><strong>The JobWorker API of the java client support multi-tenancy</strong></a><p>Previously, the JobWorker API of the Zeebe Java client did not support tenant-aware job workers. This omission limited the scalability and flexibility of the system as it could not poll or stream jobs based on single or multiple tenant identifiers.
The JobWorker API lacked methods to allow for the specification of single or multiple tenant identifiers for job polling or streaming. Also, it failed to pick up tenant IDs from the `defaultJobWorkerTenantIds` client configuration property if nothing was defined.
We integrated the new behavior provided by #13560 within the JobWorker API of the Zeebe Java client to enhance its capabilities. Additionally, we embedded methods to specify single or multiple tenant identifiers for which jobs can be polled or streamed.
Now, the JobWorker API provides methods for specifying single or multiple tenant identifiers for polling or streaming jobs. It also picks up tenant IDs from the `defaultJobWorkerTenantIds` client configuration property if none are defined, which significantly increases system scalability and flexibility.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13752"><strong>job.getVariable ergonomic method in Java client</strong></a><p> 
Users of the Java client previously had to write `job.getVariablesAsMap().get("name")` numerous times when working with variables, making the code more cumbersome than necessary especially for those new to the platform.
 
There was no shortcut method to achieve this functionality, requiring users to fetch a job's variable map and then get the variable from the map. This wasn't as efficient as it could be.
 
A new method, `job.getVariable("name")`, was added. This method calls `job.getVariablesAsMap()`, then caches the map for performance and retrieves the required variable. 
 
Users can now conveniently and quickly access job variables with the shorthand `job.getVariable("name")`. This not only enhances the user experience but also improves the efficiency and speed of deserializing to a map, improving overall client performance.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13560"><strong>Java client supports multi-tenancy for the ActivateJobs/StreamJobs commands</strong></a><p>Previously, the Java client’s ActivateJobsCommandImpl and StreamJobsCommandImpl commands for processing tenant-aware jobs in Zeebe didn’t support multi-tenancy. This caused issues when users wanted to specify a single, or multiple tenant IDs for which the client could poll/stream jobs from the Zeebe Gateway/Broker. 
The underlying issue was that `tenantIds`, the property that could allow for multi-tenancy, had not been exposed in these commands. 
The technical problem was resolved by expanding the `ZeebeClientBuilderImpl` class with a `defaultJobWorkerTenantIds` property. In addition to this, the `ActivateJobsCommandImpl` and `StreamJobsCommandImpl` commands were revised to include a `tenantIds(List&lt;String&gt; tenantIds)` method, allowing for the commands to set the `tenantId`s to the value of `zeebe.client.worker.tenantIds`. 
Now, the Java client’s ActivateJobsCommandImpl and StreamJobsCommandImpl commands can support multi-tenancy. Users can specify a single, or multiple tenant IDs for polling/streaming jobs from the Zeebe Gateway/Broker, increasing usability of the system.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13559"><strong>Java client supports multi-tenancy for PublishMessage RPC</strong></a><p>Previously, users were not able to send multiple messages at the same time due to the lack of support for multi-tenancy in the Java client for PublishMessage RPC.
The underlying issue was in the design of the Java client for PublishMessage RPC. It was not configured to support multi-tenancy, leading to a single-threaded operation where one message had to be sent before another could be started.
We improved the engineering of the Java client by introducing multi-tenancy support in PublishMessage RPC. This change involved updating the configurations and making necessary modifications in the client code to manage multiple tenants.
Now, with the implemented fix, multiple messages can be sent simultaneously using the Java client for PublishMessage RPC. The system manages multiple simultaneous tenants efficiently, enhancing overall productivity and performance.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13557"><strong>Java client supports multi-tenancy for EvaluateDecision RPC</strong></a><p>Previously, the Java client did not support multi-tenancy for the EvaluateDecision RPC function, limiting users' ability to control access to resources. 
This issue was due to a lack of implementation of multi-tenancy support in the Java client's EvaluateDecision RPC functionality
We have updated the evaluation function for the Java client to now support multi-tenancy. This involved careful refactoring and extension of existing code.
Users of the Java client now have the ability to make use of multi-tenant capabilities when utilizing the EvaluateDecision RPC. This improves access control and security in multi-user environments.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13536"><strong>Java client supports multi-tenancy for CreateProcessInstance* RPCs</strong></a><p>Users were unable to create tenant-specific process instances using the `CreateProcessInstanceCommand` and `CreateProcessInstanceWithResultCommand` in the Java client. This inadequacy could result in access permissions denial and invalid argument errors when trying to start a process instance of a tenant for whom they were not authorized, especially when multi-tenancy was enabled.
The underlying issue was that the Java client's commands for creating process instances did not support multi-tenancy because there was no `tenantId` property or method exposed. This meant that the client could not distinguish process instances from different tenants, causing errors when permissions or invalid argument checks were made.
A new feature was introduced that provided support for multi-tenancy on `CreateProcessInstanceCommand` and `CreateProcessInstanceWithResultCommand`. This was realized by exposing an optional `tenantId` property/method. Thus, users were able to create process instances that were tenant-aware in Zeebe.
Now, users can create and start a process instance for any tenant by using the respective command in the Java client. This enhancement has hence eliminated the purported permission denial and invalid argument errors, thereby offering seamless support for multi-tenancy in Zeebe.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13473"><strong>Stream jobs using job worker</strong></a><p>Previously, switching to job streaming was a complex task, as the `StreamJobsCommand` was not integrated into the job worker API by default. This led to potential difficulties and inefficiencies.
The issue was due to the ongoing development of the job streaming feature which was not yet integrated into the job worker API.
We have now integrated the `StreamJobsCommand` into the job worker API and it is an opt-in feature. We have added an additional timeout API for setting the stream timeout. Upon opting in, the worker will now open a long living stream on `open`, handle activated jobs via the stream just as jobs activated via `ActivateJobsCommand` and will reopen closed streams.
Now, users have the opportunity to enable job streaming with the job worker API, enhancing ease of running jobs and improving efficiency. Furthermore, the newly added timeout API gives users the ability to set the streaming timeout.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13460"><strong>newModifyProcessInstanceCommand: complete command with single variable</strong></a><p>Users were unable to complete the `newModifyProcessInstanceCommand` using only a single variable through the Java Client, as it required a map, which was only available with Java 9 or above.
The `newModifyProcessInstanceCommand` method was initially designed to take in a map for variables, which limited the client compatibility to Java 9 and above.
The underlying code of `newModifyProcessInstanceCommand` was updated to receive a single variable in addition to the existing map argument.
Users can now utilize the `client.newModifyProcessInstanceCommand(job).variable("name", value)` function, irrespective of their Java Client version, thereby increasing the method's usability and accessibility.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13458"><strong>newThrowErrorCommand: complete command with single variable</strong></a><p>Users were unable to complete the `.newThrowErrorCommand()` using only a single variable, which was inconvenient for those only wanting to use one variable.
The system was designed to support only the method `client.newThrowErrorCommand(job).variables(Map.of("name", value))`, restricting the use to Java 9 or above.
A modification was implemented to introduce a new method: `client.newThrowErrorCommand(job).variable("name", value)`.
Now users can complete the `.newThrowErrorCommand()` with a single variable regardless of the Java version they are using.
</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13456"><strong>newFailCommand: complete command with single variable</strong></a><p> 
Users were not able to complete the `.newFailCommand()` using a single variable due to the constraint in the implementation.
 
The cause of the issue was that using the map method for variable addition, `Map.of()`, only works with Java 9 or above. This limitation could not support individual variable addition.
 
The application was adjusted to allow for individual (single) variable addition with `.newFailCommand()`. The solution implemented was `client.newFailCommand(job).variable("name", value)`. 
 
Users are now able to conveniently complete the `.newFailCommand()` by using only a single variable. This feature is compatible with all versions of Java.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13451"><strong>newBroadcastingSignalCommand: complete command with single variable</strong></a><p>Previously, users were unable to complete the `newBroadcastingSignalCommand()` using just a single variable with the Java Client due to the requirement of using `Map.of()`, which is only available with Java 9 or above.
The limitation was due to the original design of the `newBroadcastingSignalCommand()` method, which didn't accommodate the use of a single variable without `Map.of()`.
The `newBroadcastingSignalCommand()` method was revised, allowing a user to use a single variable directly.
Users are now able to execute `client.newBroadcastingSignalCommand(job).variable("name", value)`, making it possible to use a single variable directly without necessitating the use of `Map.of()`. This feature is now fully compatible with Java versions below 9.
</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13449"><strong>newEvaluateDecisionCommand: complete command with single variable</strong></a><p>Users were unable to complete `.newEvaluateDecisionCommand()` using only a single variable while working with the Java Client. This restriction posed difficulties for those using the client.
The system was designed to only accept a map of variables using the command `client.newEvaluateDecisionCommand(job).variables(Map.of("name", value))`. However, this function was only available for Java 9 or above, limiting its usability for others.
The development team extended the function to enable completion of `.newEvaluateDecisionCommand()` with a single variable. The new command has been implemented as `client.newEvaluateDecisionCommand(job).variable("name", value)`.
Users can now execute the `.newEvaluateDecisionCommand()` with a single variable in the Java Client. This update has increased the flexibility of command execution and improved the functionality for those using versions below Java 9.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13447"><strong>newPublishMessageCommand: complete command with single variable</strong></a><p>Users previously struggled to complete the `.newPublishMessageCommand()` function with just a single variable using the Java Client, which made the process overly complex.
The issue was due to limitations within the software which necessitated the use of `Map.of()` - a feature available only with Java 9 or above - to handle variables within the command function.
The software was updated to allow the `.newPublishMessageCommand()` function to handle a single variable using `client.newPublishMessageCommand(job).variable("name", value)`.
Now, users can conveniently utilize one variable within the `.newPublishMessageCommand()` function. This improvement simplifies the overall process, reducing reliance on features associated with Java 9 or above.
</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13443"><strong>newCreateInstanceCommand: complete command with single variable</strong></a><p>Users were only able to complete the `newCreateInstanceCommand` by using a map of variables, which required Java 9 or above, limiting backward compatiblity.
The previous design of the system only allowed `newCreateInstanceCommand` to accept a map of variables, which means users had to rely on the `Map.of()` function that is unsupported below Java 9.
The system was reengineered to accept single variables, allowing users to complete `newCreateInstanceCommand` by using a single variable functionality: `client.newCreateInstanceCommand(job).variable("name", value)`.
Now, users can add a single variable when calling the `newCreateInstanceCommand` without the need for Java 9 or above, improving backward compatibility and simplifying the user experience.
</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13428"><strong>Allow custom job worker executors</strong></a><p> Users previously could only configure the number of threads in the job worker's pool, hindering performance analysis and customization. They didn't have the option of utilizing a thread-per-task execution model and tracking the time a job spends waiting before being processed. 
 The java client was designed to support Java 8, which inherently limited user's ability to use more advanced threading models, such as virtual threads, and implement custom job worker executors.
 The software was updated to allow users to provide their own executor for job workers. This enhancement added complexity to testing but achieved the intended flexibility.
 Users can now utilize a thread-per-task execution model with the help of virtual threads. They can also analyze the time a job spends waiting before being processed through custom instrumentation. As a result, more granular tuning and performance measurement is possible.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13321"><strong>Java client supports multi-tenancy for DeployResource RPC</strong></a><p> In the past, users were facing complications while performing resource deployments to Zeebe using the Java client's `DeployResourceCommand`. This was because the command did not support multi-tenancy, hence it did not provide an optional `tenantId` property/method. 
 The `DeployResourceCommand` handling resource deployments in the Zeebe Java client was initially designed without considering multi-tenancy. This led to permission denied errors when a user attempted to access data of a tenant they were not authorized for, and invalid argument errors for provided, missing, or invalid tenant ids when multi-tenancy was enabled or disabled.
 To address this issue, a new `ClientProperties#DEFAULT_TENANT_ID` was defined with a value `zeebe.client.tenantId`. The `ZeebeClientBuilderImpl` class was expanded with a `defaultTenantId` property. This property would be set by the `ZeebeClientBuilderImpl#withProperties(...)` method to a value defined by `zeebe.client.tenantId` in a `.properties` file. Additionally, the `DeployResourceCommand` was now provided with a new `tenantId(String tenantId)` method. The command sets the `tenantId` to the value of `zeebe.client.tenantId` if provided through a `.properties` file. The default value of the `tenantId` property was set to `null`. 
 Now, when users use the `DeployResourceCommand` for performing resource deployments, it supports multi-tenancy and provides the option of `tenantId`. Consequently, these adjustments greatly enhance the usability of the Zeebe Java client, reduce error messages, and allow for the smooth execution of multi-tenant deployments. Multi-tenancy is handled better with less chance of permission denied or invalid argument errors.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12122"><strong>Configure the client's inbound max_message_size</strong></a><p>Before, clients were restricted to a default inbound `MAX_MESSAGE_SIZE` of 4 MB. When creating a process instance with a result where the resulting variables were larger than this default size, errors would be thrown.
The limitation was due to the channel setup on client construction, which had a hardcoded inbound `MAX_MESSAGE_SIZE` of 4 MB.
We introduced a new configuration option in the client to specify an inbound max message size, which applies to the gRPC responses.
Now, larger messages can be processed without error, with a default message size still set at 4 MB. Users can now modify the setting as per their own requirements through the new configuration option.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/4700"><strong>Introduce JobWorker metrics for the Java client</strong></a><p>
Previously, users were unable to properly monitor their zeebe workers or ascertain the number of jobs scheduled by a worker in the Java client. The monitoring of workers was not on par with the Go client. 
This issue was caused by the lack of a metrics facade in the Java client for tracking the queued jobs. 
A solution was implemented to add metrics through a facade to the job worker. This aligned the Java client's functionality with the Go client. Metrics such as count of jobs activated, count of jobs handled, and count of queued jobs were introduced. 
Now, users can monitor the amount of jobs currently enqueued in their zeebe workers effectively. This offers better and more aligned monitoring capabilities with the Go client.</p></div></p></div><div><h4>Bug Fixes</h4><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14176"><strong>ActivateJobsCommandImpl throws NullPointerException when ZeebeClientProperties.getDefaultJobWorkerName is null</strong></a><p> The Zeebe system threw a Null Pointer Exception when the `getDefaultJobWorkerName` in `ZeebeClientProperties` returned a null value. Customers experienced an error at the application level, which occurred when an implementation of the `ZeebeClientProperties` that returned null for the `getDefaultJobWorkerName` was created.
 This was due to a regression from the command builder being used instead of the previously used request builder since this commit: https://github.com/camunda/zeebe/commit/359a402b5bbee7247749385a458c5b3f3aba7e78. In the code where the `workerName` is set during construction, it was expected to be non-null. The underlying builder, however, did not support a null value which caused the error.
 The default worker name was set to only apply during the build method process and not during construction. This action resolved the issue by allowing clients to set the `workerName` using the builder pattern after constructing the command without causing a Null Pointer Exception.
 Now, when the getDefaultJobWorkerName returns a null value, the Zeebe system no longer throws a Null Pointer Exception.  It is possible to create implementations of the `ZeebeClientProperties where `getDefaultJobWorkerName` returns null without experiencing an application-level error.</p></div></p></div></div><div><h3>zbctl</h3><div><h4>Enhancements</h4><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13024"><strong>Remove zbctl from 8.3 images going forward</strong></a><p> Multiple CVEs reported on docker images, which were related to the `zbctl` command-line tool used for debugging and troubleshooting. 
 The `zbctl` tool was included in the 8.2 docker image, which became a source of security vulnerabilities, especially considering it is only used for debugging and not a crucial part of the core functionality.
 Removed the `zbctl` tool from the docker images version 8.3 onwards. This was executed both from the distribution and also from the Dockerfile, giving us more flexibility to revise this decision.
 The docker images from version 8.3 and onwards no longer incorporate the `zbctl` tool. As a consequence, the potential security vulnerabilities related to this tool have been eliminated, making the docker images safer to use. The debugging tool `zbctl` continues to be available as a standalone artifact on every release for those who need it.
</p></div></p></div></div><div><h3>Misc</h3><div><h4>Enhancements</h4><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14555"><strong>Support Multi-tenant Form deployments</strong></a><p> Users were not able to deploy multiple form instances across different tenants simultaneously. 
 The original implementation of forms did not account for multi-tenant deployments, thus them not being stored statefully per tenant.
 The forms have been updated to support multi-tenancy. Each form is now stored in the state specific to their tenant. 
 User can now concurrently deploy and manage multiple form instances for each tenant, offering improved efficiency for managing forms in multi-tenant environments.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14302"><strong>Add tests to verify Incident behaviour when form not found during user task activation</strong></a><p>In the past, if a user task was activated with a formId, yet the form was not yet deployed, no incident was raised. Similarly, if an incident was raised, and a form with the same formId was deployed, users observed that the incident was not resolved. 
The system lacked adequate tests to check the behavior of incidents when forms were not found during user task activation. This led to missed incidents and unresolved issues even when the forms were deployed after the incidents were raised.
We created a set of tests to solve the issue. These tests verify the following: (i) an incident is raised if a user task is activated using a formId and the form is not yet deployed. (ii) an incident is resolved if it was raised and a form with the same formId is deployed subsequently. (iii) no incidents are raised if the form is already deployed before user task activation.
Now, whenever users activate a user task with a formId and if the form is not yet deployed, an incident is effectively raised. If that incident has been raised and a form with the same formId is deployed, the incident gets resolved. Also, no incidents are raised if the form is already deployed before user task activation.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14270"><strong>Receive tenantId in the Form deployment response</strong></a><p>Previously, user interactions with the Form deployment process didn't provide the `tenantId` in the deployment response, making it harder for users to manage and monitor associated tenants. 
This was, technically, due to the lack of `tenantId` field support in the `Form` and `FormImpl` classes of the Java client module.
We have added the `tenantId` field to `Form` and `FormImpl` classes in the Java client module, to support the receipt of `tenantId` in the DeployResource gRPC.
Now, users receive the `tenantId` in the Form deployment response, improving the ease of tenant management and monitoring.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14269"><strong>Add tenantId to the Form deployment gRPC response</strong></a><p> Previously, a tenantId was not included in the Form deployment gRPC response. 
 This issue was caused by the tenantId from the FormRecord not being mapped to the gRPC response on the gateway, and the new form-related Record/RecordValues did not have a `tenantId` property. 
 Resolved this by mapping the tenantId from the FormRecord to the gRPC response on the gateway. In addition, we have updated the code so that all new Form-related Record/RecordValues now include a `tenantId` property.
 Now, the Form deployment gRPC response includes the tenantId, providing more detailed and accurate information in every response.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14268"><strong>Support versioning of the Forms in the state</strong></a><p>Users previously were facing issues with form versioning. Regardless of internal support for versioning, the latest version of the Form was always exported during a user task activation. 
The system lacked a dedicated version manager and version info that could handle form versions. There was also no column family available to retrieve the key with `formId` and `version` values in the database. 
Engineers have implemented a version manager class similar to the `ProcessVersionManager` and a version info class similar to `ProcessVersionInfo`. They have also created a new column family that can retrieve the key using `formId` and `version` values. These changes were integrated into the `DbFormState` class.
Now, versioning for Forms effectively works. This implementation prevents migration issues for users and whenever a user task is activated, the appropriate form version is exported.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14248"><strong>Export Deployment Record with form metadata</strong></a><p>Previously, when users exported their Deployment Record, it did not include form metadata information. 
This was due to the 'zeebe-record-deployment-template.json' file not having form metadata properties included in its structure. 
The 'zeebe-record-deployment-template.json' has been updated to include form metadata properties.
Now, when users export their Deployment Record, it includes form metadata information.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14222"><strong>Export Form Deployment record to Opensearch</strong></a><p> Users and teams were unable to display deployed forms because Zeebe didn't export forms to Opensearch.
  
 Zeebe lacked the capacity to create a new index template with all the form fields, such as `zeebe-record-form-template.json`, and it didn't have `FORM` as a new index `ValueType`. It also didn't enable the creation of the new `form` index. 
 In order to address these limitations, the engineering team enabled form exports from Zeebe to Opensearch. They created a new index template `zeebe-record-form-template.json` which holds all the form fields. `FORM` was also introduced as a new index `ValueType`. Lastly, this fix also allowed for the creation of a new `form` index. 
 Now, the system allows other teams to display deployed forms, as Zeebe exports form data to Opensearch effectively and reliably.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14187"><strong>zeebe running with relatively high idle CPU load</strong></a><p>
Zeebe was previously observed to have a relatively high CPU load even when idle, roughly 10%. Ironically, this resulted in wastage of resources for users who don't run many processes.
The ActorThread in the system was initializing the ActorTaskRunnerIdleStrategy such that it polls for work at 1ms intervals. This high-frequency polling was the underlying cause of the notable CPU load even during periods of idleness. 
Changes were made in the system to increase the default park timeout to 20ms. An examination of the system performance with this modification revealed no significant differences in the functionality at medium and high throughput levels, but a considerable decrease in CPU usage during an idle state was recorded.
The system now, even at idle levels, utilizes less CPU, as seen in the drop in CPU usage from 0.04 to 0.01-0.02. Thus, with this change, system users can expect to see a cut in the CPU load by half during idle periods. Additionally, Zeebe capacities can be managed more efficiently due to the inclusion of variables to customize the polling interval. This development significantly reduces CPU load, power consumption and optimizes system performance.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14139"><strong>Add support to Java client for returning Form metadata in DeployResource gRPC response</strong></a><p>Previously, Form metadata was not returned in the DeployResource gRPC response in the Java client.
This was due to the 'DeploymentEventImpl' class constructor not being designed to map Form metadata and the absence of a 'Form' class to hold this metadata on the client side.
The 'DeploymentEventImpl' class constructor was updated to map Form metadata and a new 'Form' class was created to store this metadata client-side.
Now, the Java client returns Form metadata successfully in the DeployResource gRPC response.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14138"><strong>Export form key of the latest form on user task activation</strong></a><p>Previously, when a user task was activated that referenced a Form by formId, the key of the latest version of the Form wasn't fetched from the state, causing inconsistencies and possible discrepancies in user interfaces using these forms. 
The system did not have an implemented function to fetch the latest form key by the given formId when the user task was activated. This lack of functionality resulted in system performance issues.
The system was updated to fetch the latest form key from the state during the activation of a user task that references a Form by formId. The method added to UserTaskTransformer transforms formId. Following that, BpmnJobBehaviour.encodeHeaders was updated to include the transformed formId into job custom headers.
Now, whenever a user task is activated that references a Form by formId, the system fetches the latest key of the Form from the state and integrates it into the custom headers of the Job record. This results in consistent user interfaces and improved system performance. Additionally, if a form is not found by the given formId upon user task activation, the system raises an incident, providing clear identification of any mishaps for easier troubleshooting and resolution.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14137"><strong>Implement query to retrieve the latest form by formId from the state</strong></a><p> Users were unable to directly retrieve the latest form using formId from the DbFormState class.
 The previous design of the DbFormState class lacked a method to get the most recent form using formId.
 A new method was added to the DbFormState class that allows fetching the latest form by its formId.
 Users can now effortlessly retrieve the most recent form by using formId from the DbFormState class.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14136"><strong>Export Form Deployment record to Elasticsearch</strong></a><p>Other teams were previously unable to display deployed forms as the system did not have a suitable feature to export data to Elasticsearch.
There was a lack of an appropriate index template to export form fields in the previous versions of the system. Additionally, a "FORM" index ValueType was not available, and the creation of a new 'form' index was not enabled.
An index template named 'zeebe-record-form-template.json' was created consisting all the form fields and the 'indexPattern' property set to 'zeebe-record_form_*'. "FORM" was included as a new index ValueType, and the creation of a new 'form' index was enabled.
Now, other teams can display deployed forms as this new version of the system supports exporting forms to Elasticsearch. The application now comes with a dedicated index template and a new "FORM" index ValueType, and allows creation of a new 'form' index.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14135"><strong>Allow binding forms to start events by formId</strong></a><p> Previously, users could not bind forms to start events using the `formId` field.
 This limitation was because the `ZeebeFormDefinition` lacked a `formId` attribute and the `ZeebeElementValidator.validate()` method was not capable of validating instances where only one field between `formKey` and `formId` was present.
 A `formId` attribute was added to `ZeebeFormDefinition` and the `ZeebeElementValidator.validate()` method has been updated to validate for a group of fields, thereby allowing a starting event with an associated form to be valid if only one field between `formKey` and `formId` is present.
 Now, users can connect forms to start events using the `formId` field, allowing more flexibility in form handling.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14134"><strong>Allow binding forms to user tasks by formId</strong></a><p>Previously, users could not bind forms to user tasks using the formId field. 
The limitation was caused by the absence of the formId attribute in the ZeebeFormDefinition, and the ZeebeElementValidator.validate() method was not set up to validate for groups of fields.
We added the new formId attribute into the ZeebeFormDefinition and updated the ZeebeElementValidator.validate() method to enable zeebe to also validate for groups of fields.
Now, users can successfully link a User Task to a deployed form using the new formId field. User tasks with linked forms are considered valid if just one field between formKey and formId is present.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14133"><strong>Save Form to the state</strong></a><p>Prior to this update, the users could not save or retrieve forms within the application due to the absence of a form saving functionality.
The application’s database structure lacked necessary definitions for the form column family and the methods needed to store and retrieve forms.
We developed and implemented database classes for the form, created form column family definitions, and established methods that facilitate storing of the form and retrieving it either by form id or by key.
Users can now save forms and access them later by utilizing form id or key, enhancing user experience and adding functionality to the application.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14132"><strong>Handle Form Deployment in the Engine</strong></a><p> The application lacked the ability to process forms deployment within the engine, having no intent, applier, nor transformer created for Form, and no Form metadata included in the deployment record.
 This was due to the omission of Form deployment processing in the original engineering setup of the engine, causing a failure in the DeploymentCreateProcessor class to transform the deployed form.
 We have implemented Form deployment processing in the engine. This included the creation of the Form intent, Form transformer, Form applier, and also included Form metadata into the deployment record. Additionally, we incorporated the transformation of the deployed Form in the DeploymentCreateProcessor class.
 With the fix now applied, the application is capable to process the form deployment within the engine efficiently. Form intent, transformer, applier now work flawlessly and deployed forms are neatly transformed within the DeploymentCreateProcessor class.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14131"><strong>Implement Form Deployment gRPC endpoint in the Gateway</strong></a><p>Users were unable to deploy Form resources through the `DeployResource` gRPC endpoint in the Gateway.
The `gateway.proto` file in the `DeployResource` gRPC endpoint lacked the proper form metadata response definition. Additionally, the form metadata was not included in the gateway `ResponseMapper` class. 
The form metadata response definition was added to the `DeployResource` gRPC in the `gateway.proto` file, and the form metadata was filled in the gateway `ResponseMapper` class.
Users can now successfully deploy Form resources via the `DeployResource` gRPC endpoint in the Gateway.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13516"><strong>Add integration tests for Job Push</strong></a><p>Previously, the lack of integration tests for the Job Push feature meant that the system was not thoroughly tested, which could have potentially led to unnoticed issues.
This happened because the implementation of the Job Push feature did not include the creation of relevant integration tests, including tests for the `onClose` hook.
Developers added required integration tests for the Job Push feature to cover the successful or 'happy path' scenarios. However, tests for the `onClose` hook were opted out as these can only be triggered by calling `onComplete` or `onError` from the server side, which was not the case for this implementation.
Now, the system is better tested thanks to the inclusion of integration tests for the Job Push feature. This helps ensure that the feature works as intended, providing confidence in its reliability and stability. Note that testing for the `onClose` hook remains absent until such time as `onComplete` is potentially called, for example during server shutdown.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13465"><strong>Validate user input before registering a worker to the job stream</strong></a><p>
Workers were able to register to the job stream with incorrect input parameters, which resulted in no jobs being pushed to them. Validation error messages were not clear and helpful for the user. 
The underlying issue was the absence of sufficient user input validation before registering a worker to the job stream. This was observed when the job type was incorrect, resulting in the system not pushing jobs to the worker.
Implemented a series of validation checks on user input. These validations ensure that the job type is neither null nor an empty string, and that the timeout value is not less than 1. Also clarified the validation error messages returned to the user. 
Now, the system clearly states validation errors with helpful messages for the user. It prevents the registration of workers to the job stream with incorrect parameters, ensuring that jobs are correctly pushed to registered workers.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13429"><strong>Implement ZeebeClient wrapper for the StreamJobs RPC</strong></a><p> Users previously had no ability to stream jobs from a Zeebe cluster because it lacked a wrapper for the StreamJobs RPC in the ZeebeClient.
 The missing wrapper around the StreamJobs RPC command was due to the fact that the `ZeebeClient` did not have an essential command `StreamJobsCommand` and API `ZeebeClient#newStreamJobsCommand`.
 A new command, `StreamJobsCommand`, was introduced to the `ZeebeClient` together with a new API, `ZeebeClient#newStreamJobsCommand`. This implementation allows the Zeebe client to wrap the underlying gRPC call.
 Users are now able to use ZeebeClient's `StreamJobsCommand` to easily stream jobs from a Zeebe cluster. This addition also includes management tools for this long living stream, such as toggling the default request timeout and handling cancellation/termination of the stream.
</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13426"><strong>A ResolveIncident request/command supports tenant validation</strong></a><p>Previously, a ResolveIncident request/command didn't support tenant validation, allowing potentially unauthorized users to resolve incidents.
The 'ResolveIncidentProcessor' didn't verify if the tenant owning the incident was accessible by the user making the request.
Validation checks were included in the 'ResolveIncidentProcessor' to ensure that an incident could only be resolved if the tenant owning the incident was accessible by the user making the request. Also, the 'ResolveIncidentProcessor' was updated to reject a command if the tenant owning the incident was not accessible by the user making the request.
The ResolveIncident request now strictly validates tenant ownership, ensuring that incidents can only be resolved by users who have access rights to the respective tenant, enhancing the security of incident management.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13425"><strong>A ThrowError request/command supports tenant validation</strong></a><p>Consequence:
Previously, when an error was thrown for a job, the system didn't validate the tenant ownership of the job, which could have led to unauthorized access issues.
Cause:
This was because the `JobThrowErrorProcessor` in the underlying system did not check whether the tenant owning the job was accessible by the user making the request or not.
Fix:
A system enhancement was implemented where the `JobThrowErrorProcessor` now verifies tenant ownership of a job when an error is thrown. It will only process the command if the job's tenant is accessible by the user making the request.
Result:
Now, ThrowError command supports tenant validation. Therefore, an error can only be thrown for a job if the tenant owning the job is accessible by the user making the request, ensuring appropriate tenant validation and strengthening the system's security.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13388"><strong>Support multi-tenancy for setting variables on a process instance scope</strong></a><p>Users were unable to support multi-tenancy when setting variables on a specific scope such as process instance or flow element instance.
The system did not allow for tenant identification in the `VariableDocumentRecord` and `VariableRecord` classes. As a result, the `VariableState` could not store or fetch variables per tenant, rendering multi-tenancy unachievable in variable settings.
Added a new `tenantId` property in `VariableDocumentRecord` and `VariableRecord` and redesigned the VariableState' to include a `tenantId`. Changes were also made to Elasticsearch/Opensearch `variable-template.json` and `variable-document-template.json` to include a `tenantId` property. The `UpdateVariableDocumentProcessor` was adjusted to update variables of a given scope only if the tenant was accessible by the user making the request.
Now, users can have multi-tenancy support when setting variables on a specific scope. It's feasible to set and fetch variables by `tenantId` thus enhancing the levels of security, isolation, and convenience for multi-tenant environments.
</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13354"><strong>Delete and recreate signal subscription of previous version</strong></a><p>Previously, if a process containing a signal start event was deleted, the system failed to unsubscribe the signal initially. This caused issues when the deleted process was the latest and the previous version, which also included a signal start event, stood as the new latest version. In such cases, the signal subscription was not correctly recreated, and process instances didn't start as expected.
The underlying problem related to the technical engineering function responsible for managing signal subscriptions. When a process was deleted, the associated signal was not properly unsubscribed, for instances in which the subsequent version also used a signal start event, creating a mismatch.
The issue was corrected by ensuring proper signal unsubscription whenever a process containing a signal event start was deleted. An additional safeguard ensured the successful creation of signal subscription when the previous version, which includes a signal start event, becomes the latest.
Now, when a process that contains a signal start event is deleted, the system correctly unsubscribes the signal. If the deleted process was the latest version and the previous version (which also has a signal start event) becomes the latest, the signal subscription is accurately created, ensuring process instances are initiated as expected.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13349"><strong>Add and handle Process Deleting event</strong></a><p>Previously, when users attempted to delete a process, the process definition would not be marked as pending deletion. 
The issue arose due to the lack of a `DELETING` intent in the `ProcessIntent`, and the absence of an `EventApplier` to manage the new `DELETING` intent. 
We added the `DELETING` intent to the`ProcessIntent`. An `EventApplier` was also introduced to handle the `DELETING` intent and change the state of the `PersistedProcess` to `PENDING_DELETION` in both the `ColumnFamily` and the cache.
Now, when a user deletes a process, the system correctly marks the process definition as pending deletion.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13348"><strong>Add a `state` to `PersistedProcess`</strong></a><p>Before the fix, users were not able to track the process definitions that were scheduled for deletion.
The system was lacking a 'state' property on the 'PersistedProcess', therefore, it couldn't differentiate between active processes and those marked for deletion. 
A 'state' property was added to the 'PersistedProcess'. An enumeration 'PersistedProcessState' was implemented to specify if the state is 'ACTIVE' or 'PENDING_DELETION'. This setup establishes a framework to allow future expansion of possible states like 'SUSPENDED'.
Now users are able to see and track process definitions that are marked as 'PENDING_DELETION'. The system differentiates between 'ACTIVE' and 'PENDING_DELETION' states, providing users with better control and understanding of process definitions.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13347"><strong>An UpdateJobRetries request/command supports tenant validation</strong></a><p> Previously, when a job's retries were updated, the system didn't support tenant validation. Therefore, there were possibilities that unauthorized users could potentially make changes to job retries. 
 The `JobUpdateRetriesProcessor` was not set up to update a job only if the tenant that owned the job was accessible by the user making the request.
 We have made necessary updates to the `JobUpdateRetriesProcessor`. Now it ensures tenant validation by updating a job only if the tenant, owning the job, is accessible by the user making the update request. Also, the `JobUpdateRetriesProcessor` now rejects a command if the tenant that owns the job is not accessible by the user making the request.
 Jobs retries can now be updated with tenant validation support. This means that the system is now secure from unauthorized changes to job retries as the process will validate tenant ownership before granting access for modifications.
</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13346"><strong>A FailJob request/command supports tenant validation</strong></a><p>Previously, the system lack of tenant validation during the process of failing a job was causing data inconsistence and security breaches, enabling users to accomplish tasks without suitable permissions.
This issue was caused by the JobFailProcessor's inability to verify tenant ownership of a job before completing it or rejecting it. Additionally, the IncidentRecord and the Elasticsearch/Opensearch incident-template didn't have a tenantId property that could facilitate tenant-specific job processing.
The JobFailProcessor was updated to only complete a job if the job's owning tenant is accessible by the requesting user, and to reject commands if the job's owner tenant is inaccessible. The IncidentRecord now contains a tenantId property alongside the Elasticsearch/Opensearch incident-template being updated with a tenantId property as well. The JobFailProcessor now creates an IncidentRecord with the tenantId of the JobRecord.
Now, tenant validation is incorporated when a job fails, ensuring that system behaves as expecter and adheres to proper permissions rules. The system now robustly checks ownership before completing or rejecting jobs, while also enabling tenant-specific incident management. Moreover, the user's tenant id list is obtained from the gateway, ensuring the tenant ownership is clear from the JobRecord.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13343"><strong>Migrate running process instance into `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily</strong></a><p>Previously, all running process instances were not contained within the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily. This caused inconsistencies and potential issues in managing the running process instances.
This was caused due to lack of a migration script to populate the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily with all running process instances.
A migration script was created to add all active process instances to the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily. This has been tested to ensure it operates as expected.
Now, the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily contains all running process instances. This has streamlined process instance management and reduced risk of errors.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13342"><strong>Remove from `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily</strong></a><p> Process instances remained in the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily even after they had reached an end state. This could potentially lead to memory bloat and impact the system performance.
 There was no method in place to remove completed or terminated processes from the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily. 
 A new method was introduced to remove data from the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily. When a process is completed or terminated, this method is now invoked to remove it from the ColumnFamily. The change also applied when a call activity is completed or terminated. `ElementInstanceStateTest#shouldNotLeakMemoryOnRemoval` was also updated accordingly to improve the memory management.
 Now, only active processes are stored in `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily. Any process that reaches an end state is immediately removed reducing memory usage and improving the performance of the system.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13341"><strong>Insert into the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily</strong></a><p>Prior to the fix, the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily hadn't been populated with relevant data when initiating a new process instance or when calling a CallActivity.
The methods responsible for managing the ColumnFamily didn't include a feature for inserting data into this ColumnFamily. 
We implemented an insertion method for the ColumnFamily and employed the method when starting a new process instance or calling a CallActivity. Comprehensive tests were created to ensure this function operated correctly.
Currently, with the advancement, the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily is automatically populated whenever a new process instance initiates or a CallActivity is called.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13340"><strong>Add `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily</strong></a><p>Previously, users were unable to quickly determine if there were any running process instances for a specific definition key. When a process instance ended, it caused difficulties in confirming whether other instances were still running, especially if a deployment was awaiting deletion.
The underlying cause was the lack of a `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily in `ZbColumnFamililes`. Additionally, this ColumnFamily was not included in the `DbElementInstanceState`.
The issue was resolved by creating the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily in `ZbColumnFamililes`. The same ColumnFamily was also built in the `DbElementInstanceState`.
Now, users can efficiently identify if there are any active process instances for a given definition key. Facilities to add or delete data in the ColumnFamily will be added in the further issues: #13341, #13342, and #13343.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13337"><strong>Support multi-tenancy for BPMN timer events</strong></a><p>Prior to this fix, users were limited in using the BPMN timer events feature as it did not support multi-tenancy. For those relying on multi-tenant architectures, this resulted in timer events only working correctly for default tenants, while non-default tenants had this feature disabled.
This was due to the `TimerRecord` and `TimerInstance` classes, and Elasticsearch/Opensearch `timer-template.json` files not initially designed to hold or handle a `tenantId` property. The system failed to propagate the `tenantId` from the `TimerRecord` or `TimerInstance` to other process components in the workflow, such as `TriggerTimerProcessor` and `DueDateChecker`. 
New properties were added to the `TimerRecord` and `TimerInstance` classes to incorporate and handle `tenantId`. The Elasticsearch/Opensearch `timer-template.json` files have been updated to contain a `tenantId` property. Additionally, the `TriggerTimerProcessor` and `DueDateChecker` have been enhanced to propagate the `tenantId` from the `TimerRecord` or `TimerInstance` to follow-up records.
With these changes, the BPMN timer events now support multi-tenancy, allowing users to properly utilize timer events across all tenants. In any multi-tenant environment, timers can now be correctly processed in the engine, inheriting the `tenantId` from the process definition or process instance.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13335"><strong>Add Resource Deleting intent</strong></a><p>Before the fix, only a 'DELETED' event record was logged and sent back as a response to the client during a resource deletion process, leaving users without a notification of the starting of the deletion process.
This was because the `ResourceDeletionProcessor` class was only configured to process 'DELETED' event without including a 'DELETING' event before a resource, such as DMN, got removed.
A 'DELETING' event was added to `ResourceDeletionIntent` class and was made to be written into the log as well as sent as a response to the client by modifying the `ResourceDeletionProcessor`.
Now when a resource deletion process starts, a 'DELETING' event is logged and also sent to the client as a response before the final 'DELETED' event happens. This gives users a live feedback about the phase of deletion of a resource.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13320"><strong>Engine can process multi-tenant DeploymentRecord commands</strong></a><p> Previously, our system could not handle multi-tenant DeploymentRecord commands, as it did not support working with multi-tenancy during deployments. 
 This limitation in the system was due to the fact that classes such as `DeploymentRecord`, `ProcessMetadata`, and `DecisionRecord` among others did not implement the `RecordValueWithTenant` interface.
 We implemented the `RecordValueWithTenant` interface in the mentioned classes. We also ensured that `DeploymentRecord` implements the `RecordValueWithTenantPermissions` interface, and updated `DeploymentCreateProcessor`, `BpmnResourceTransformer`, and `DmnResourceTransformer` to propagate the `tenantId` to the BPMN/DMN resources and their appropriate Records. Addition of the `tenantId` key in `DeploymentState`, `ProcessState`, and `DecisionState` was made when an appropriate resource was added.
 Now the system is capable of processing multi-tenant DeploymentRecord commands. It identifies the tenant and applies the appropriate permissions, enhancing the multi-tenancy capabilities of deployments.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13319"><strong>Gateway supports multi-tenancy in deployment RPCs</strong></a><p>Users were unable to make DeployResource RPC calls with a specific `tenantId` because the application was not supporting multi-tenancy in deployment RPCs. 
The Gateway was not passing the `tenantId` from the `DeployResourceRequest` to the `BrokerDeployResourceRequest`. Also, the lack of an option to set the `tenantId` to default when the multi-tenancy feature was disabled resulted in the issue.
The `deployResource(...)` Gateway endpoint was updated to include a new `tenantId` property in various gRPC messages such as `DeployResourceRequest`, `ProcessMetadata`, `DecisionMetadata`, `DecisionRequirementsMetadata`. This `tenantId` is now passed on to the `BrokerDeployResourceRequest` whether or not the multi-tenancy feature is enabled. Moreover, when the multi-tenancy feature is disabled, the `BrokerDeployResourceRequest#tenantId` is auto-set to default. Lastly, if the `tenantId` is found null when the multi-tenancy feature is enabled, the deployment is now rejected.
The system now allows for DeployResource RPC calls with a specific `tenantId`. Multi-tenancy is supported and so, users can now make gRPC calls with a specified `tenantId`. This update makes resource deployment more flexible and secure.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13318"><strong>`JobBatchRecord` and `JobActivationProperties` provide tenant information</strong></a><p> Before the fix, users faced difficulties in supporting multi-tenancy when polling or pushing jobs as it wasn't possible to activate a job based on both `jobType` and `tenantId`. This limitation prevented users from simultaneously handling jobs for multiple tenants.
 The root cause was that the `JobBatchRecord` and `JobActivationProperties` classes did not include a `tenantIds` property, making it impossible to filter jobs by both `jobType` and `tenantId`.
 The issue was addressed by adding a `tenantIds` property, which is a list of `tenantIds`, to the `JobBatchRecord` and `JobActivationProperties` classes.
 Now the system supports multi-tenancy when jobs are polled or pushed. A job can be activated based on `jobType` and `tenantId`, and jobs for multiple tenants can be managed simultaneously.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13317"><strong>Jobs are pushed/polled only for requested tenants</strong></a><p> Users experienced a system behavior where job polling and job pushing would not filter jobs by `jobType` and `tenantId` before the jobs were activated.
 This occurred due to the limitation in `JobBatchCollector` and `JobStreamer` API, as they did not provide support for querying jobs by `jobType` and `tenantIds`.
 Changes were made to `JobBatchCollector` to enable it to query jobs by `jobType` and `tenantIds`. The `JobStreamer` API has been adjusted via a new method `streamFor(final DirectBuffer jobType, final DirectBuffer tenantId)`. This method allows for the return of a `JobStream` for a given `jobType` and `tenantId`. Modifications were also made to `BpmnJobActivationBehavior` to query `JobStream` by `jobType` and `tenantId`.
 Currently, the system is equipped to filter jobs by their `jobType` and `tenantId` before they are activated. This functionality ensures that job polling and pushing takes place only for the requested tenants, improving the user experience with more accurate job activation.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13316"><strong>A CompleteJob request/command supports tenant validation</strong></a><p>Previously, users were able to complete jobs even when the tenant owning the job was not accessible by the user making the request. This could lead to unauthorized job completions. 
The issue was caused by the 'JobCompleteProcessor' not validating tenant ownership during the job completion process. 
Developers introduced a tenant validation step in the 'JobCompleteProcessor'. Now, job completion requests would only be processed if the tenant owning the job is accessible by the user making the request. 
Currently, the system ensures that only authorized users can complete jobs. If a user tries to complete a job owned by a tenant that is not accessible to them, the request gets rejected.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13288"><strong>Support multi-tenancy for process instance operations (modify, cancel, set variables)</strong></a><p> Prior to the fix, users were unable to support multi-tenancy for process instance operations, including modification, cancellation, and variable setting. This limited functionality, as process instance operations could only be executed if the user/token had access to the tenant owning the process instance.
 The system was initially designed without considering multi-tenancy in process instances. Engine validation of tenant access was missing in the 'ProcessInstanceModificationProcessor,' 'ProcessInstanceCommandProcessor,' and 'UpdateVariableDocumentProcessor.' Additionally, the gateway did not propagate tenant data to the broker, which was an underlying cause of the problem.
 We introduced multi-tenancy support for key process instance operations. We ensured the engine validates tenant access in the pertinent processors and modified the gateway to propagate tenant data to the broker. 
 Now, users can execute process instance operations across multiple tenants. These operations now only require the user’s information and the tenant ownership, both of which the Identity SDK in the Gateway retrieves and forwards to the engine through the command record. No additional user-side changes should be necessary to use these operations.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13279"><strong>Support multi-tenancy for core BPMN process operations</strong></a><p> Before this issue was addressed, multi-tenancy was not supported for the core operations of BPMN processes in Zeebe. Thus, key actions did not contain a `tenantId`, making it impossible for process instances and their events to be owner-specific, disrupting tenant-specific workflow management.
 The Zeebe engine was initially designed without considering multi-tenancy within BPMN process operations. Consequently, process instances, process instance results, CallActivity, decision evaluation, and child instance creation did not have tenant-specific identifiers.
 The issue has been rectified by modifying the Zeebe engine to support multi-tenancy. Using tenant-specific identifiers, the engine now ensures that process instances and their events are tenant-specific. Furthermore, there has been careful implementation of handling tenant ids in client requests, ensuring that process instances are started only if the user has the correct access rights.
 With the fix applied, Zeebe now supports multi-tenancy for core BPMN process operations. Process instances and their events adhere to tenant id determinations, making it convenient for tenant-specific process management. Process instances can only be started if a valid tenant id is provided, bolstering access security.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13253"><strong>Notify Zeebe Team in Slack when an issue is labelled as critical</strong></a><p>Prior to the fix, when an issue was labeled as critical, there was no automatic notification sent out, posing a risk that urgent issues could be overlooked if manual notification was omitted.
The issue arose due to the missing workflow in the system which was supposed to trigger an automatic notification in Slack upon assigning a 'critical' label to an issue.
A workflow was added that checks the issue label and sends a notification to the Zeebe Slack channel if the label is 'critical'. 
Now, whenever an issue is labelled as critical, the notification is automatically sent out to the Zeebe Slack channel, drawing the team’s immediate attention to fix the issue promptly.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13238"><strong>Support multi-tenancy for deployments</strong></a><p> Users were unable to effectively implement multi-tenancy in deployments, causing inefficiencies and potential security risks as users could deploy to tenants they didn't have access to. 
 Zeebe did not have a feature to support resource versioning separated by tenant nor did it reject deployments lacking tenant id when multi-tenancy was enabled. 
 The Zeebe Deployment functionality has been updated to enable support for multi-tenancy. Deployments now require tenant ids and restrict access according to user/token permissions, preventing unauthorized deployments to inaccessible tenants. The process definitions and decisions owned by different tenants now maintain separate version counts for each tenant.
 Users can now use and manage multi-tenant deployments more effectively. Each tenant has an associated id and resource versioning separate from other tenants. Unauthorized deployments have been prevented, ensuring the secure distribution of resources across tenants.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13040"><strong>As a Zeebe Java user, I want to complete a Job with a single variable</strong></a><p>Users previously had to bundle a single variable into a Map to complete a job in Zeebe Java, causing unnecessary overhead. 
The `newCompleteCommand` method only had a `.variables()` parameter where users needed to wrap a single variable in a Map. 
We implemented a `.variable("name", value)` method to simply return a single variable value when a job is completed.
Zeebe Java users can now complete jobs using single variable values directly, simplifying the process and reducing overhead.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12975"><strong>Export number of buffered messages as metrics</strong></a><p> Users were unable to monitor the health of their systems effectively due to a lack of metrics related to "buffered" messages. The absence of these metrics made it challenging to configure the async message TTL checker optimally and prevent message accumulation.
 The system was not designed to export specific message-related metrics, which would have allowed tracking the number of buffered messages. This limitation required users to rely on an exported record stream, thereby requiring a separate continuously running application.
 We expanded the system metrics to include the number of buffered messages. This feature maintains accuracy even after broker restarts by restoring the relevant counts upon recovery, akin to the instance ban metrics.
 Users can now continuously monitor the number of buffered messages directly from the system without requiring any separate running application. If this number is increasing persistently, users can optimise the TTL checker efficiently to ensure swift message expiration and thus prevent message buildup. This feature should enhance system health monitoring and configuration capabilities.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12942"><strong>Support S3 backup httpclient custom configuration.</strong></a><p>-  Users experienced failures during backups due to a high request rate and a resultant lack of available connections within the set maximum time. Logs presented an error pointing to "unable to execute HTTP request: Acquire operation took longer than the configured maximum time." The situation was amplified when a partition housed a multitude of log segments, which led to concurrent uploads.
-  The software configuration lacked the ability to limit the number of parallel uploads, causing some uploads to exceed the connection acquisition timeout of 45 seconds. Concurrent uploads were controlled by a static value with no available customization.
-  The option to limit the number of concurrent uploads was introduced. A configurable limit was set to control the number of simultaneous uploads and hence balance the load on the system.
-  Users can now smoothly carry out backup operations. The number of concurrent uploads is controllable, which decreases the likelihood of a connection timeout error during large backup processes. This modification has improved the resilience of the backup system even with a high number of log segments or larger states.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12878"><strong>Add message definition extension elements</strong></a><p> 
Previously, the BPMN model API used `send task` or `message throw event` without a reference to a message definition. 
 
The cause of this issue was the lack of appropriate design to facilitate the bind between the `send task` or `message throw event` and certain Message Definition in the BPMN model API.
 
The issue was addressed by introducing a new extension, `zeebe:publishMessage`, to the `bpmn:messageEventDefinition` (of Intermediate Throw Event, or End Event) and/or to the `bpmn:sendTask` to specify the details of the message to publish. The message to publish can be referenced directly from the `bpmn:messageEventDefinition` and from the `bpmn:sendTask` using the `messageRef` attribute according to BPMN spec.
 
Now, different elements can publish the same message with different message details. For example, you can specify your different correlation key, time to live, message id for the same message. The new design facilitates a more efficient and precise handling of message references in the BPMN model API.
</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12796"><strong>Provide Error Handler implementation for Job Streamer</strong></a><p>In the past, when pushing a job failed, the system didn't trigger the `JobYieldProcessor` to make the related job available for long polling. This sometimes resulted in stalled processes. 
The code didn't append the `JobIntent.YIELD` command when a job push failed. This issue was a result of a lack of an error handler registration to `RemoteJobStreamErrorHandlerService` inside `JobStreamServiceStep`.
We implemented error handling for the Job Streamer. With the feature request, an error handler was added to `RemoteJobStreamErrorHandlerService` in `JobStreamServiceStep`, this helped to append the `JobIntent.YIELD` command whenever a job push failed.
Now when a job push fails, the system auto triggers the `JobYieldProcessor` which keeps the related job available for long polling, maintaining the efficiency of processes.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12793"><strong>Allow configuring request timeout for InstallRequest</strong></a><p>In the past, users experienced timeout exceptions when sending InstallRequests over networks with higher latency between brokers. These InstallRequests are larger than other Raft requests and therefore take longer to send and receive a response. This caused snapshot replication to restart from the beginning.
The underlying issue was a lack of separate configuration for the request timeout for InstallRequest. A single configuration applied to all Raft requests, and increasing this affected other aspects of system performance, including the speed at which failures were detected and requests were retried.
We've introduced a dedicated configuration for setting the request timeout for InstallRequest. This configuration is separated from the existing Raft request timeout.
Now, users can independently set a higher request timeout for InstallRequests, improving flexibility and performance in high-latency network environments. Having separate settings prevents the need for a global increase in the Raft request timeout, ensuring faster detection of failures and retries of other requests.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12696"><strong>Forcefully terminate a process instance</strong></a><p>Users experienced critical system blocks due to runaway process instances caused by certain process model configurations. This included overloaded export systems and exhausted disk space, preventing command acceptance and causing work stoppage for all partitions.
The issue arose from problematic process models that created an overwhelming number of follow up records, outpacing the system's capacity for processing or exporting. The system was not designed to handle these instances efficiently, lacking an escape mechanism or an efficient cancellation process.
A multi-level solution was proposed to resolve the issue. First, an API endpoint was introduced to allow users to forcefully terminate problematic process instances without needing to process all events—this was achieved by modifying the state projection directly. Second, the cancellation of banned instances was implemented to ensure proper child cancellation within the processing.
Now, the system can handle runaway process instances more efficiently. The system is more resilient as users can forcefully terminate problematic instances via API, and permitted cancellations can be executed even for problematic nested instances. These changes improve the handling of incidents and enhance recovery options for the system.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12655"><strong>Automatically add `support` label to support related issues</strong></a><p>Users had to tag support related issues manually, making it possible to forget to add the `support` label. 
The system did not have an automatic mechanism in place to identify and label support related issues based on the text content of new issues or comments.
Developers introduced a new GitHub action. This action checks new issues as well as comments for the text `SUPPORT-XXXX`. If this exact text is found in any issue or comment, the action automatically adds `support` label to the issue.
Now, whenever an issue or comment containing `SUPPORT-XXXX` is posted, the system auto-tags the issue with the `support` label. This makes it much easier to track support related issues and reduces the risk of missing such issues due to lack of label.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12575"><strong>Improve the traversing of snapshot files</strong></a><p> Users experienced decreased performance in creating snapshots as the list of snapshot files grew. This was due to the inefficient collection of snapshot files in a list. 
 In the snapshot function in the source code, snapshot files were being collected in a list instead of using a more optimal method. 
 Rather than collecting snapshot files in a list, the method `Stream.forEachOrdered` was applied. This effectively optimized the traversal of snapshot files and helped to keep the order intact. 
 As a result of this change, snapshots are now created efficiently, even when there are a large number of snapshot files. Users should see an improvement in performance as a consequence.
</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12548"><strong>Provide Grafana Dashboards for the Actor metrics</strong></a><p>Users were unable to view Actor metrics directly in a dedicated Grafana Dashboard, forcing them to manually incorporate these metrics in their own dashboards. 
The system previously did not offer a dedicated Grafana dashboard for Actor metrics. They were labeled as experimental and not included out of the box.
A file named 'actor.json' was created and placed in the Grafana dashboards folder. 
Users can now find and view the Actor metrics directly in its own Grafana Dashboard, enhancing overall ease and efficiency.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12541"><strong>Jobs are pushed from relevant processors</strong></a><p> In the past, the `Processor` classes had the capability to push an activated job to a client. However, the method of delivery was scattered across different classes, leading to an inefficient structure.
 The mechanism to push jobs to clients was distributed across diverse `Processor` classes. It became a concern since `JobWorkerTaskProcessor`, `JobTimeOutProcessor`, `JobFailProcessor`, `JobRecurProcessor`, and `ResolveIncidentProcessor` classes all had their separate methods which complicated the job push process.
 To resolve this issue, we incorporated the `JobStreamer` API within the `BpmnJobActivationBehavior` class, centralizing the job push process. This class now handles the job stream, setting of `deadlines`, `variables`, and `worker` for `JobRecord` using `JobActivationProperties`, and activates the job using `JobBatchRecord`/`JobBatchIntent.ACTIVATE`. The job is then pushed on the `JobStream` through a `SideEffectProducer`.
 As a result of this change, `BpmnJobActivationBehavior` class now smoothly handles the job push process to clients. This centralized system improves efficiency and makes it easier for users to manage job activation and delivery.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12539"><strong>Create `ProcessInstanceBatch.TERMINATE` processor</strong></a><p> Users were unable to terminate multiple child processes simultaneously due to the lack of a processor for `ProcessInstanceBatch.TERMINATE` commands.
 The system lacked a method for processing `ProcessInstanceBatch.TERMINATE` commands, and a process for listing child instances was needed in the BpmnStateBehavior.
 A new processor was created for `ProcessInstanceBatch.TERMINATE` commands, along with necessary logic to write `ProcessInstance.TERMINATE` commands for each of the element instance keys. In addition, the BpmnStateBehavior was enhanced to list a specific amount of child instances, commencing at a particular key.
 Users can now simultaneously terminate multiple child processes using the `ProcessInstanceBatch.TERMINATE` command. Process termination is faster and more efficient as users can instantly terminate a batch of child processes.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12538"><strong>Use the `ProcessInstanceBatch Command` when terminating container elements</strong></a><p> Users were unable to effectively terminate container elements such as process, subprocess, etc. through `onTerminate`, as it wasn't utilizing the new `ProcessInstanceBatch` command.
 This was due to the `BpmnStateTransitionBehavior#terminateChildInstances` method only calling the child instances to terminate, rather than using the more efficient `ProcessInstanceBatch` command with the `TERMINATE` intent.
 The `BpmnStateTransitionBehavior#terminateChildInstances` method was modified to create a `ProcessInstanceBatch` record, in which the `batchElementInstanceKey` would be the key of the container element. This change facilitated the writing of a `ProcessInstanceBatch.TERMINATE` command using the newly created record. 
 Now, the system correctly implements the termination of container elements with the `ProcessInstanceBatch.TERMINATE` command when `onTerminate` is called. Consequently, container element termination is faster and more efficient.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12537"><strong>Create `ProcessInstanceBatch` Record and Intent</strong></a><p> In the past, users could not perform certain actions (such as termination or activation) in batches on process instances. This lack of batch processing functionality was a limitation.
 This issue arose because the 'ProcessInstanceBatch' record and intent, which would be used to carry out such batch actions, were absent in the system.
 To resolve this issue, a 'ProcessInstanceBatch' record and intent were created. The record was filled with data including a 'batchElementInstanceKey', and an 'index' for tracking the batch's location. The only intent created for now was 'TERMINATE'.
 Now, users can perform batch termination actions on process instances. This added functionality increases efficiency and user control over process instances.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12416"><strong>Remove the default un-overridable `-Xms128m` value</strong></a><p> Users previously found it difficult to tune the Java Virtual Machine (JVM) as they could not override the default `-Xms128m` setting.
 The `-Xms128m` was implemented as a default, un-overridable option in the `JAVA_OPTS` of the Docker environment variable, making user customization unfeasible.
 The `-Xms128m` command was removed from the default options of the `appassembler-maven-plugin` configuration; the system now sticks to JVM defaults.
 Users now have more flexibility in tuning the JVM as the `-Xms`, along with other JVM options, can now be easily overridden via the `JAVA_OPTS` environment variable.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12382"><strong>Docker: Run the zeebe process with an unprivileged user by default</strong></a><p> In the past, the Zeebe process was run by the root user in the Zeebe Docker image thereby exposing potential security risks, as it didn't follow OWASP's recommendation to run containers with unprivileged users.
 The Docker image was initially configured with the Zeebe process to run as a root user. Although there was an unprivileged user present in the image (zeebe user with UID 1000), this user by default was not utilized during operations.
 An update was applied to the Docker image to change the run user from root to the existing unprivileged user, 'zeebe'. This update did not need to use 'gosu' and accommodated for the possibility to exec into the container as root when necessary.
 Now, the Zeebe process in the Docker container runs with an unprivileged user by default, enhancing the security of the application according to the OWASP recommendations. Root access remains available as an option via the '--user root' flag during docker run and docker exec commands, offering the needed flexibility for different operational needs.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12283"><strong>Zeebe BPMN Model should provide reasonable defaults for definition attributes</strong></a><p>Users were required to manually set numerous attributes to open processes modeled with `zeebe-bpmn-model` in the Desktop Modeler as a C8 process.
The Zeebe BPMN Model did not offer sensible default values for definition attributes. Users had to input the process data manually, which caused inconvenience and disrupted efficiency. 
Developers worked on improving the 'zeebe-bpmn-model' by providing default values for these attributes. Users can modify these values as needed through the API.
Now, when users want to open processes modeled with `zeebe-bpmn-model` in the Desktop Modeler as a C8 process, they will find predefined attributes. This change significantly reduces the manual effort earlier involved, giving users a better and more efficient experience.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12085"><strong>Job Yield Processor is implemented to be used for Job Push fallback</strong></a><p>Previously, if a job that was pushed to the JobStreamer API failed to be handed over to the client due to a client failure, the job was not made activatable again. 
This issue was due to the lack of a JobYield processor that could handle these types of failures. 
We added a new JobIntent.YIELD intent and implemented a new JobYield processor. This processor functions similarly to the JobFailProcessor, making a job 'ACTIVATABLE' again in the event of a failure.
Now, when a job cannot be handed over to the client, the job becomes activatable again, enhancing the system's resilience to client failures.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12000"><strong>OAuth Auth Token authentication support in Zeebe Gateway</strong></a><p> Previously, Zeebe Gateway lacked native support for OAuth token authentication, meaning users had to handle authentication separately and potentially add more complexity to their infrastructures.
 The absence of OAuth authentication token validation in the Zeebe Gateway was a missing feature in the Camunda 8SM product. The gateway didn't utilize the identity-sdk for token verification. 
 OAuth authentication token validation has been added to Zeebe Gateway, making use of the identity-sdk for token verification. The authentication has been introduced without yet implementing authorisation, which is planned to be added in a later update. 
 Users of the Zeebe Gateway can now take advantage of native support for OAuth token authentication. This has streamlined the authentication process as it happens directly in the Zeebe Gateway, reducing the need for separate or more complex infrastructure setups and providing a more secure and efficient service. Authorisation functionality will be introduced in an upcoming release.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/11920"><strong>Support Broadcast signal for Signal End Events</strong></a><p>Previously, users experienced limitations when trying to activate a Signal End Event as the 'EndEventProcessor' wasn't configured to broadcast a signal. 
This restriction was due to the absence of a `SignalEndEventBehavior` in the process architecture, preventing the activation transition, execution of 'Signal:Broadcast' command, and enforcing proper input/output mappings.
Developers have introduced a new `SignalEndEventBehavior` into the system. This behaviour change ensures that, upon activation of the Signal End Event, the system applies input mappings, transitions to activated state, writes the `Signal:Broadcast` command, applies output mappings, and eventually transitions to complete the element.
Now, with the application of the fix, users can expect seamless Signal End Event activations with full support for broadcasting signals. This includes appropriate transitions from activation to completion along with accurate input/output mappings.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/11919"><strong>Support Broadcast signal for Signal Intermediate Throw Events</strong></a><p>Previously, when a Signal Intermediate Throw Event was activated, it did not broadcast a signal. The system additionally failed to apply input mappings, transition to an activated state, write the `Signal:Broadcast` command, apply output mappings, nor transition for element completion.
This was because the `IntermediateThrowEventProcessor` wasn't designed to carry out the mentioned operations, resulting in a lack of signal broadcast during a Signal Intermediate Throw Event.
We updated the `IntermediateThrowEventProcessor` to broadcast a signal once activated. This update includes the application of input mappings, transition to an activated state, writing the `Signal:Broadcast` command, application of output mappings, and incrementally transitioning to a completed stage.
Now, when a Signal Intermediate Throw Event gets activated, the system correctly broadcasts a signal and carries out necessary operations such as input and output mappings, state transitions, and writing of the broadcast command seamlessly.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/11708"><strong>Add gRPC job stream API</strong></a><p> Users were unable to register job streams to the gateway, leading to limitations in real-time job stream management.
 This was because the system lacked a gRPC API to facilitate the registration of job streams, specifically a unidirectional stream from the server to the client.
 We introduced a new long-living gRPC API in the system. This API, which takes the same activation properties as the job worker (excluding those related to long polling and similar items), returns a stream of a single `ActivatedJob`. For the time being, batching properties have been ignored for the alpha target.
 Users can now register job streams to the gateway, enhancing real-time job stream management due to the implementation of a long-living and properly configured gRPC API. The system now returns a continuous flow of `ActivatedJob`, promising an improved user experience.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/10031"><strong>Support BPMN Converging Inclusive Gateway</strong></a><p> Previously, users couldn't use the converging behavior of the inclusive gateway in BPMN due to a system restriction on the number of incoming sequence flows to an inclusive gateway. This limited the flexibility and complexity of BPMN workflows that were defined by the users.
 This issue was due to a validation restriction implemented in the BPMN software that restricted the number of incoming sequence flows to the inclusive gateway. The architecture of the software only supported diverging behavior for inclusive gateways.
 The validation that limited the number of incoming sequence flows to a maximum of one was removed. Further, the inclusive gateway was made active by fulfilling a set of conditions relating to the activity of child elements of flow scope instances and incoming sequence flows.
 With this fix, users can now employ both diverging and converging behaviors in the inclusive gateway of the system. The increased flexibility in sequence flows will allow users to define more complex BPMN workflows in the system.</p></div></p></div><div><h4>Bug Fixes</h4><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14496"><strong>Restore with fewer brokers fails to find all backups</strong></a><p> 
Previously, when users attempted to restore a backup taken with a `clusterSize=3` configuration from a broker with a `clusterSize=1` configuration, the operation would fail. This was due to the system not being able to locate all the necessary backups. 
 
The `PartitionRestoreService` was building known broker IDs based on the `clusterSize` and tried to find a partition backup by going through these IDs. This caused an issue when `clusterSize=1` because it only looked for backups taken by broker 1, and ignored backups taken by other brokers.
 
We altered the backup restoration process to find the backup associated with all brokers instead of just the ones identified by `clusterSize`. Now, during restoration, the system scans all available broker IDs, including those which might not currently be part of the configuration.
 
The restore function now finds backups from all brokers, not only those in the current configuration. This ensures that users can successfully complete restoration even if `clusterSize` during restoration is set to `1`.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14418"><strong>Inconsitency detected in PROCESS_SUBSCRIPTION_BY_KEY</strong></a><p> A key inconsistency was detected in PROCESS_SUBSCRIPTION_BY_KEY, marking a partition as dead and leading to an incident alert in our SaaS environment. This unexpected behavior was observed when a system role change occurred, and a corruption was identified in the new leader. 
 A leader election triggering an incident alert was initially thought to be the result of having two concurrent leaders handing out the same key, which would naturally lead to inconsistency. However, our research indicated that this issue was in fact due to a different scenario: a message subscription and process message subscription were both created as part of a call activity activation when a called process was not found. Upon resolving the incident and attempting the call activity activation again, the same subscriptions were recreated. The duplication contributed to the overall inconsistency failure.
 We developed a test case that successfully replicated the issue: https://github.com/camunda/zeebe/compare/8.3.0-alpha6...korthout-14418-test-inconsistency. Information from this test case helped us to re-engineer the call activity activation process to prevent the creation of duplicate subscriptions, in turn avoiding a key inconsistency situation.
 Now, when a role change happens, the new leader successfully undergoes the call activity activation process without creating duplicate subscriptions. Consequently, process interruptions due to key inconsistency are no longer observed, ensuring a smoother operational workflow.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14366"><strong>Cancel command fails because process is null (NPE)</strong></a><p>Users of version 8.3.0-alpha6 of Zeebe found that the cancellation of a process would fail, resulting in an unexpected 'NullPointerException'. 
The error occurred because the process model couldn't be found or wasn't available at the time of cancellation. This was due to the processState.getLatestProcessVersionByProcessId method failing to retrieve the necessary information about the process. 
The underlying issue was resolved by modifying the method that manages the process versions and handles the removal of process definitions. 
With this fix applied, users can now cancel a process instance without encountering the previous error, improving the reliability of the system.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14146"><strong>Remove DRG from cache upon deletion</strong></a><p> 
Users previously experienced an issue where upon deleting a Decision Requirements Graph (DRG), it was not removed from the cache. This resulted in the deleted DRG remaining available until the broker was restarted.
 
The underlying issue was a technical flaw in the system where the design didn't include the removal of the DRG from the cache upon deletion. Additionally, a bug was identified in the unit test designed to verify this functionality, where incorrect parenthetical placement resulted in the assertion not verifying anything.
 
The software was modified to ensure the successful removal of a DRG from the cache once it was deleted. The bug within the unit test was corrected by fixing the off parenthetical placement, thereby allowing it to correctly assert the functionality.
 
Now, when a user deletes a DRG, it is correctly removed from the cache, thereby preventing its access post-deletion. The corrected unit test is now able to accurately verify this functionality, ensuring this issue doesn't recur.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14047"><strong>Snapshot version of  benchmark application (starter/worker) doesn't work with SaaS</strong></a><p> 
Users found that the benchmark starter and worker did not function properly with SaaS clusters. This led to several complications when trying to incorporate them for benchmarking cluster plans. The issue necessitated the use of an older version of the benchmark applications to work around it.
 
After investigating, it was found that the client builder was fixed to use plaintext instead of connecting appropriately to SaaS. This flaw caused the issue as the client was not automatically enabling TLS upon setting the credentials, which was the previous behavior.
 
The problem was addressed by tracing the misconfiguration in the client builder and making necessary adjustments. The client builder's settings were adjusted to automatically enable TLS when credentials are set. This reestablishes the previous behavior and ensures an appropriate connection to SaaS.
 
Now when leveraging the benchmark starter and worker with SaaS clusters, users can expect a smooth operation without the need to fall back on older versions of the applications. The correct configuration eliminates the previously observed issues, bringing an improvement in user experience and efficacies in benchmarking cluster plans.
</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/14028"><strong>Should not activate link catch event in subprocess</strong></a><p> Users had been able to activate a link catch event inside an (event) subprocess, which contradicts with the BPMN specification and the Camunda documentation. The link catch event was being activated even though it wasn't in the same scope as the link throw event.
 The underlying mechanism in the Zeebe version `8.2.0` did not properly enforce the constraint from the BPMN specification to only activate link catch events when they exist in the same scope as the link throw event.
 A modification was made to the Zeebe event triggering functionality, ensuring that the link catch events can only be activated when they exist in the same scope as the link throw event.
 Now, the link catch event within an (event) subprocess does not get activated incorrectly. Additionally, the deployment of the process is rejected if there is no link catch event in the same scope as the link throw event, adhering to the specifications and rules.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13881"><strong>Upgrading leaves deadline entries without jobs</strong></a><p> 
After upgrading a Zeebe cluster to a version that carried fixes for two specific issues, entries in the job deadline column family were left free-floating. This resulted in continual error logs stating that a corresponding job could not be found. 
 
This issue was caused by an alteration in behavior, where duplicated or orphaned deadline entries, which were previously cleaned up, were now expected to be non-duplicated. The system did not clear these duplicate or isolated entries during the upgrade. 
 
A migration was instituted to clean up these orphaned entries during the upgrade, eliminating them from the state and removing the possibility of continuous error logs post upgrade. 
 
Upon upgrading the Zeebe clusters now, orphaned entries are accurately cleared from the system. This results in a clean state with no continuation of error logs related to jobless deadline entries, thus enhancing the functionality of the upgraded system.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13867"><strong>ConcurrentModificationException when clearing obsolete job activation requests</strong></a><p> 
A `ConcurrentModificationException` was observed in the system when clearing obsolete job activation (long polling) requests. The system was unstable as this error was thrown during the process of clearing the `activeRequests` LinkedList, which lacked thread safety. 
 
The underlying cause was due to the state modification in a callback mechanism that was executed on the gRPC executor instead of the actor itself. This lead to concurrent modification of a non-thread-safe structure, leading to the exception.
 
The issue was resolved by adjusting state modifications to execute on the actor rather than the gRPC executor. The proposed solution ensured that these modifications were made within a thread-safe environment, preventing concurrent modification of the LinkedList. This fix was provided in the following PR: https://github.com/camunda/zeebe/pull/13875.
 
The system now correctly clears job activation requests without throwing a `ConcurrentModificationException`. It is now stable and runs without the previous interruption during LinkedList modification. Moreover, due to improved thread safety, potential related errors, such as `IndexOutOfBoundsException` have also been prevented.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13814"><strong>Job streaming may trigger unnecessary polling</strong></a><p> 
Previously, users experienced unnecessary triggering of job polling when a job was handled or completed using job streaming.
 
The issue was due to the function `JobWorkerImpl#handleJobFinished` which wrongly decremented `remainingJobs` thus triggering unwanted job polling.
 
The code has been revised to correct the function and prevent it from decrementing the `remainingJobs` counter and triggering unintended job polling. 
 
Now, the job streaming function operates without triggering unnecessary polling, and the `remainingJobs` counter is correctly handled, providing a seamless user experience.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13796"><strong>IllegalStateArgument when removing job stream</strong></a><p> Users experienced an exception when attempting to remove a stream using `JobStreamRemover` in the gateway. The stream was not removed as expected, even after the client was disconnected.
 The exception was due to the future completing without an executor, which then resulted in a call to `Actor.call` within the actor context. 
 The issue was corrected by ensuring the use of an executor before the future completes. 
 Now, users can remove a stream via the command without experiencing an exception. Removal of the stream from both the gateway and the broker transpires as expected, even if the client disconnects.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13787"><strong>Release: `Repo dispatch Benchmark` fails on 8.0/8.1</strong></a><p>Before the fix, users encountered failure when building starter &amp; worker on versions 8.0/8.1 as the maven wrapper wasn't present. Also, building Zeebe on version 8.0 was failing because the Dockerfile didn't have a DIST=`build` setup. 
This was due to the fact that the maven wrapper was not backported to versions 8.0 and 8.1 causing workflow merge conflicts from main to stable. Moreover, the absence of `benchmark.yaml` workflow on each stable branch caused a discrepancy in maintaining a stable setup.
The issues were rectified by backporting the maven wrapper to versions 8.0 and 8.1 to prevent workflow merge conflicts. Addition of `benchmark.yaml` workflow to each stable branch was also done to uphold a stable setup. Workflows were then triggered via workflow dispatch from the zeebe-engineering-processes, referencing the release branch. Finally, the `dispatch-benchmark.yaml` workflow from the main was deleted for cleanup.
Now, building starter &amp; worker on versions 8.0/8.1, and building Zeebe on version 8.0 operate successfully without any failures. Users are able to maintain a stable setup with the help of `benchmark.yaml` workflow on each stable branch.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13715"><strong>Release Dry fails because of unrecognized argument</strong></a><p> 
Before the fix, if a user attempted to run the "Release Dry" function on their project, it was failing. This occurred due to an unrecognized named-value: 'env'. 
 
The failure was traced back to a mistake in the .github/workflows/release-main-dry-run.yml file. Specifically, in the called workflow camunda/zeebe/.github/workflows/release.yml, the 'env.RELEASE_BRANCH' was not recognized, leading to an invalid workflow error.
 
The issue was addressed by correcting the 'env' named-value reference in the mentioned yml files to ensure that the 'env.RELEASE_BRANCH' is recognized correctly, thus ensuring the validity of the workflow.
 
As a result of the fix, users can now successfully run the "Release Dry" function on their projects without encountering the previously mentioned error. The system appropriately recognizes the 'env' named-value, ensuring the validity of the workflow.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13650"><strong>`ZeebePartitionHealth` repeatedly reports change of health status</strong></a><p>
Previously, the `ZeebePartitionHealth` was erroneously signalling a change in health status, even when the health status was unaltered. This resulted in unnecessary calls to listeners and logs, thereby causing noise and confusion.
The underlying cause of the issue was a flawed comparison in the code. Rather than comparing the actual health reports, the code was checking for identity, leading to frequent unnecessary updates. 
The problematic code in the `ZeebePartitionHealth` function was modified to correctly compare health reports instead of checking for identity.
With the fix applied, `ZeebePartitionHealth` only signals a change in health status when an actual change occurs. This eliminates redundant calls to listeners and log entries, thereby reducing noise and confusion.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13521"><strong>Process instances are banned when trying to activate a call activity with large variables</strong></a><p>
In the past, when users attempted to activate a call activity with large variables, if processing exceeded the batch size, the entire process instance was banned. The instance was left in a limbo state and could not be recovered, causing data loss. The only workaround was avoiding the issue by recreating the instance with smaller payloads.
The underlying issue was in the `BpmnStreamProcessor`. If there was an error handling the large variables, it could not raise an incident for every process instance element type. This led to problems with the `CallActivityProcessor`, causing instances to be banned if activation failed due to large variables.
The error handling mechanism was improved in the `BpmnStreamProcessor`. The ability to pass error handling to the respective `BpmnElementProcessor` was added, thereby ensuring that larger variables didn't cause instances to be banned.
In the current situation, users can activate a call activity with large variables without fear of the entire process instance getting banned and becoming inaccessible. If there are errors in handling the large variables, the system raises an incident, allowing for modification and preventing data loss.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13471"><strong>PartitionListeners are notified even if the transition is cancelled causing NPE</strong></a><p>
Users were experiencing a `NullPointerException` when a transition to leader was cancelled. This caused the partition to become inactive, but paradoxically, the partition was still marked as healthy.
The issue arose due to a bug in the `PartitionTransitionProcess`. The underlying design caused the `PartitionStartupAndTransitionContextImpl.lambda$notifyListenersOfBecomingLeader` function to be invoked even when services were not installed and the transition to leader was already cancelled. This led to an attempt to access a `null` log stream, which in turn led to a `NullPointerException`.
We modified the `PartitionTransitionProcess` to properly handle cancelled transitions. We ensured that partition listeners are only invoked when the transition is successfully completed and services are correctly installed - preventing an attempt to access a `null` log stream and thereby avoiding a `NullPointerException`.
Transitions that are cancelled no longer trigger the invocation of partition listeners. The system no longer throws a `NullPointerException` under these circumstances. Now, if a transition to leader is cancelled, the associated partition is correctly identified as inactive, improving the reliability and consistency of partition status reporting.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13431"><strong>Gateway readiness fails when tls enabled</strong></a><p>Previously, when users enabled tls on the gateway, the readiness check would fail. This became evident, with an error indicating an SSL issue, when querying the health of the gateway.
This failure was a result of a problem in the ResponsiveHealthIndicator. It was supposed to demonstrate the responsiveness of the service by querying the topology. However, it was unable to support special configurations such as the enabling of TLS and authentication, thus failing the readiness check.
The problematic ResponsiveHealthIndicator was removed from the system. This simplified the liveness check and avoided the complexities that were causing the failure. Additionally, we improved how the grpc readiness/connectivity is checked via a grpc_health_probe.
Now, users can enable tls on the gateway without the readiness check failing. Queries to the health of the gateway returns successful even with special configurations like the enabling of TLS and authentication.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13254"><strong>When the deployment of a process definition failed, clients can still start new process instances of the actually non-existing process definition</strong></a><p> 
When the deployment of client process definitions failed due to issues like `EXCEEDED_BATCH_RECORD_SIZE` error, it was observed that the clients were still able to initiate new process instances using these failed definitions as they remained cached in the system memory. In turn, this led to errors when attempting to replay these instances after a leader change or system restart. 
During the processing of deployment commands, the system was caching the transformed resources. However, when the deployment failed, the RocksDB transaction was rolled back which meant that the process definitions weren't stored persistently. Nevertheless, these definitions remained in the caching system, leading to a discrepancy between the actual state of the deployments and the cached state.
To rectify this, a mechanism to clear the cache on instance of an exception thrown during the deployment process was implemented. This measure will automatically remove the failed deployments from the cache, avoiding issues arising from a discrepancy between cached and actual data.
Now, failed deployments will not remain in the cache, ensuring that clients cannot start instances off of non-existent processes and avoiding replay errors in the system. As a result of this fix, the system maintains consistency between actual deployment status and cached data.
</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13164"><strong>Segfault on enabling async scheduled task</strong></a><p>
Users experienced a segmentation fault (segfault) when either of the experimental feature flags were enabled while the other was disabled: `ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLEMESSAGETTLCHECKERASYNC` and `ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLETIMERDUEDATECHECKERASYNC`.   
The issue arose because the stream processor had its own transaction context while the scheduled tasks shared a transaction context. This caused a conflict when scheduled tasks ran on different actors, using the shared transaction context.
We have addressed this problem by ensuring that each scheduled task runs within its own transaction context. The stream processor and scheduled tasks no longer share a common context, preventing conflicts between the asynchronous and scheduled tasks.
Users can now safely enable either or both of the experimental feature flags `ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLEMESSAGETTLCHECKERASYNC` and `ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLETIMERDUEDATECHECKERASYNC` without experiencing a segfault.
</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13123"><strong>NPE when processing Job.TIME_OUT command</strong></a><p> The system experienced a Null Pointer Exception (NPE) when the `JobTimeOutProcessor` tried to time out a job that wasn't available in the state, which in turn halted related process instances, rendering them unrecoverable. A sequence scenario on the log: Job.COMPLETE, Job.TIME_OUT, led to the removal of the job from the state, causing the time out command to fail when trying to locate the job.
 The `JobTimeOutProcessor` had an assumption that the job set to be timed out would be present in the state. This underlying logic was incorrect because the first command of Job.COMPLETE removed the job from the state, making it unattainable for the Job.TIME_OUT command.
 A rejection mechanism was included in the logic of the command processing. When the Job.TIME_OUT command does not find the job to time out, the command is now rejected. This has been tested via a new unit test in `JobTimeOutTest`.
 When the job to be timed out doesn't exist, the system now robustly handles this scenario by rejecting the Job.TIME_OUT command, ensuring the process instance related to the job isn't rendered unrecoverable. Processing can be paused and resumed without causing an unrecoverable NPE, significantly improving product reliability.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13061"><strong>Cancel on-going remote stream registration on stream removal</strong></a><p>
Users experienced additional latency during a push, or possible unnecessary job activation when the client-side stream had disappeared due to an existing race condition. This race condition resulted in a remote stream existing server side, even though the client-side stream was removed.
The interleaving of asynchronous removal and registration requests allowed for the existence of a remote stream at the server side even after its removal at the client side. This was due to the asynchronous registration of remote streams and immediate removal at the client side upon removal request submission.
The race condition was mitigated by associating a `ClientStreamRegistration` state machine to each stream that managed the current state of the remote registration. This state machine sequenced the registration and removal of remote streams. All requests to a broker per stream had to wait until previous requests were finished before sending the next one, thus preventing out-of-order processing. In addition, the state machine allowed for the cancellation of in-flight registration requests on the transitioning to the `REMOVING` state.
As a result, users no longer experience an unnecessary additional latency during a push, or unnecessary job activation. The remote streams are successfully and completely removed, thus ensuring a reduced load on the server and an improved performance of the system.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13046"><strong>Error message truncation may be too strict</strong></a><p> Users were unable to view error messages beyond 500 characters in the Operate application, causing inconvenience as the full error details weren't visible. This necessitated an additional step of checking worker logs for full error messages.
 The implementation of a 500 character limit to error messages in Issue #11460 was overly strict. This truncation was initiated to prevent error messages from exceeding the maximum message size.
 We revised the character limit for error messages, increasing the limit significantly to allow for more detailed error messages. Additionally, we considered making the character limit adjustable for greater flexibility, but opted for a simple solution for the moment.
 Users can now view more detailed error messages directly in the Operate application. This allows for a more comprehensive understanding of any errors that occur, enhancing user experience and reducing reliance on worker logs for full error details.
</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13041"><strong>Don't mutate state through `JobBackoffChecker`</strong></a><p> 
Before the fix, users experienced difficulties when trying to cleanup backoffs while operating with `JobBackoffChecker`.
 
The issue was a result of the `JobBackoffChecker` mutating state directly, an undesirable behavior that was inherited from the underlying architecture of the system's functionality.
 
This problem was addressed by restricting the `JobBackoffChecker` from directly changing the system state. Thus, ensuring a safe and reliable method of cleaning up backoffs.
 
Currently, the `JobBackoffChecker` securely manages cleanup of backoffs without mutating the state, enhancing overall performance and reliability of the system.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13038"><strong>Handle stream consumers changes during push</strong></a><p>
In the past, when executing a retry push from the broker side, the system selected a random stream consumer to push to. However, if during a retry all consumers were removed, the system threw an error as it attempted to generate a random index from a zero range. 
The problem was a result of the `AggregatedRemoteStream` not being thread-safe or immutable. This led to errors when trying to mutate the consumer list during the execution of the retry push operation.
We opted for an immutability approach for the `AggregatedRemoteStream` class. The solution involved copying the `AggregatedRemoteStream` record when picking target and passing this around downstream. This approach was simpler to handle and more effective in the common use case.
Now, the system can effectively handle streams with no consumers even during retries. It does not throw errors in such scenarios, but rather properly bails out early, thus enhancing the reliability of the broker's push operation.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13036"><strong>Endless client job stream registration</strong></a><p>Prior to the fix, the registration of new client job streams was continuous and unending, leading to unnecessary system commotion and creating confusion for users over the success of the registration process.
The problem originated from the improper usage of the `ClusterCommunicationService#subscribe(String, Function&lt;byte[], M&gt;, BiConsumer&lt;MemberId, M&gt;, Executor executor)`. Any subscriber that was a consumer didn't send a response back, causing the client to expect a response and constantly retry the registration process.
A solution has been implemented to correctly utilize the `ClusterCommunicationService#subscribe` function, ensuring the client's expectations are met. 
Now, client streams are successfully and properly registered on both sides, ensuring efficient communication and enhancing system performance. The issue of endless registration attempts has been addressed, reducing system noise and providing users with clear feedback.
</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12957"><strong>Straight-through processing loop may cause problems</strong></a><p>Apologies for confusion, but the provided prompt doesn't state a specific GitHub issue to analyze and write a release note. Please provide the GitHub issue details or the link to the issue.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12933"><strong>Failing jobs with job push enabled breaks the job lifecycle</strong></a><p> In the past, when a job with enabled job push failed with remaining retries and no backoff period, the job was first activated and then immediately marked as failed again. This led to prematurely pushing out jobs that were actually in a failed state and presumably caused data inconsistencies due to the job lifecycle not being followed correctly. 
 The issue stemmed from the intricate relationship between JobFailProcessor, CommandControl, and BpmnJobActivationBehavior. The system did not correctly sequence events, so the job was marked as activated before it was properly marked as failed, leading to an inappropriate transition.
 We refactored the relevant processors to use the TypedRecordProcessor interface, which ensured the correct writing sequence of job events on the log: ACTIVATED -&gt; FAIL -&gt; FAILED -&gt; ACTIVATED. 
 Now, when a job fails with remaining retries and no backoff, it correctly transitions to a failed state before being activated again. This improved job lifecycle management ensures jobs are not prematurely pushed out in a failed state, enhancing data consistency.
</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12915"><strong>Allow extending journal record format with out breaking compatibility</strong></a><p> Previously, users were unable to modify the journal record schema without breaking backward compatibility and this halted systems from updating to new versions due to inconsistencies or unavailability during rolling updates.
 The issue was caused by the change in the journal record schema to SBE. The change was supposed to facilitate extensions without breaking changes, however, it failed to meet its goal. Incompatibility issues arose when a broker on a newer version couldn't receive events via raft replication written by a leader in an older version and vice versa.
 Modifications were made to ensure backward compatibility by allowing followers on newer versions to receive events from leaders on older versions. A raft-protocol-version was incorporated, with its absence being interpreted as an old version. To address checksum issues affected by the schema version, when a follower on a new version receives a request from an old leader, it writes the record using the old SBE version and the checksum calculated will match the original one.
 With these changes, users are now able to extend the journal and raft record schema without breaking compatibility. Brokers on newer versions can receive records written with old versions, ensuring the system's efficiency during rolling updates.
</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12886"><strong>Zeebe should only log as JSON, if configured to do so</strong></a><p>Previously, the Zeebe logs were not uniformly formatted as JSON, which made it difficult to classify and parse these logs effectively. The huge ASCII banner showing "Zeebe" at startup also displayed as garbage in the logs.
Various parts of the system (e.g., startup banner, Tomcat logs) were not using the same logging format. The JAVA_TOOL_OPTIONS output, originating from the JVM, couldn't be altered or suppressed. 
The Zeebe code was adjusted to switch from Tomcat to WebFlux/Undertow for system-wide logging consistency. Additionally, instead of using JAVA_TOOL_OPTIONS to pass options, JAVA_OPTS were used. This reduced the amount of non-JSON logs, though some changes were still needed on Helm chart and controller. 
Now, more of the log information is properly formatted as JSON, improving readability and reducing "garbage" output in logs. Log categorization and parsing are more efficient because of these changes.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12875"><strong>`ModifyProcessInstanceRejectionTest#shouldRejectActivationWhenAncestorScopeIsNotFlowScope` test fails on Windows</strong></a><p> 
The `ModifyProcessInstanceRejectionTest#shouldRejectActivationWhenAncestorScopeIsNotFlowScope` test failed consistently on Windows 10 systems resulting in incorrect validation of the command rejection.
 
The issue was triggered by different interpretations of line breaks within the error messages between different operating systems. In Windows, a line break is represented by "\r\n", while other operating systems may only use "\n".
 
The solution involved updating the mechanisms of comparison in the test to account for different possible representations of line breaks across different operating systems.
 
With the applied fix, the `ModifyProcessInstanceRejectionTest` test operates and passes successfully on Windows 10 machines without being affected by the system's line break representation.
</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12837"><strong>Catch all error events by using empty 'errorCode' does not work</strong></a><p>Previously, using an error catch event with an empty `errorCode` resulted in an incident labeled as `errorType=UNHANDLED_ERROR_EVENT` and the system only reported boundary error events with a non-empty `errorCode`. 
This was due to the system not acknowledging an empty `errorCode` as a valid error code in the Zeebe Version `8.2.0-alpha3` and onwards. 
The issue was resolved by modifying the system to accept and process an empty `errorCode` correctly in the error catch event.
Now, the error catch event accepts an empty `errorCode` and it will no longer lead to an `UNHANDLED_ERROR_EVENT`, ensuring a smoother handling and reporting of error events within the system.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12833"><strong>Cannot resolve output mapping incident on a none end event</strong></a><p>Users were previously unable to resolve incidents on a none end event due to an issue with mapping a non-existent variable. This caused processes to hang and prevented users from completing tasks successfully.
The underlying problem was a missing implementation of the `onComplete` method on `NoneEndEventBehavior` in the `EndEventProcessor`. This led to the system not handling missing variables in the output mapping of none end events appropriately.
Developers rectified the technical issue by adding the necessary method in the `EndEventProcessor`. This implementation allowed for better handling of end events, including those with missing variables.
Users can now resolve incidents on a none end event seamlessly. If a similar situation with a non-existent variable occurs, the system doesn't hang, allowing tasks to be completed without unnecessary interruptions.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12754"><strong>Journal reset resulting in an intermediate state which is detected as corruption</strong></a><p> 
In the past, the startup process faced trouble due to potentially false positive error messages. This unexpected behaviour was a consequence of system shutdowns occurring during the reset segment deletion process. The erroneous system state was detected as corruption.
 
This problem arose because the reset/deleting segments operation was not atomic. The software failed to handle this correctly, leading to a situation where an old snapshot held partially deleted segments, resulting in a system restart with an invalid intermediate state.
 
To address this issue, we implemented an interim solution by deleting the segments in a reverse order. This ensured that there were no gaps in the log/snapshot, preventing corruption. Also, it is now understood that data loss is acceptable here as it was the desired outcome of the reset operation.
 
As a result of this solution, the specific case causing system corruption has been resolved. However, the core issue of the follower's install operation not being atomic remains. This is a known challenge, and a comprehensive fix is planned for an upcoming release.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12699"><strong>Do not write the entire command for rejection</strong></a><p>Previously, the system would go into a loop in the StreamProcessor, repeatedly failing to write the rejection record whenever it rejects a deployment request. This issue was caused by the system attempting to write the entire deployment, which often exceeded the maxMessageSize, fully blocking the partition and preventing any progress.
The underlying cause of this issue was the system trying to write a rejection record that was too large. The rejection record was large because it included the full details of the original command - the deployment that was rejected due to exceeding the maxMessageSize.
The solution was to modify the way rejections are handled. Instead of including the full details of the rejected command, rejection records will now only contain a reference to the original command. For cases where ExceededBatchSize exceptions still occur, an added safety measure was implemented to trim the rejection records.
As a result of the fix, rejected deployments are now handled more efficiently. Failure loops in the StreamProcessor no longer occur because rejection records are smaller and can be written reliably. Users will no longer experience full blocks in the partition, allowing for smooth progress.
</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12623"><strong>After restoring from backup, a partition re-takes the backup</strong></a><p>In the past, after restoring Zeebe from a backup, Zeebe sometimes created duplicate backups of the partition, wasting resources.
This situation occurred because the system was designed to allow a partition to retake a backup if the leader that initiated the original backup is not the current leader at the time of restore. 
We changed the technical architecture to prevent a partition from retaking a backup after a restore, regardless of the leadership status.
Now, Zeebe no longer creates duplicate backups after a restore, optimizing the resource utilization.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12622"><strong>List backup fails when a partition has same backup taken by multiple nodes</strong></a><p>
Previously, trying to list backups sometimes resulted in failure if the same partition had been backed up multiple times by multiple nodes. Users experienced an error message about duplicate keys, thus being unable to successfully complete a list backup operation.
Cause:
The root cause of this issue was a complication that arose if there was a leader change while backing up. This caused the system to attempt another backup for the same id that already had a backup, creating duplicate backup ids for a partition.
Fix:
The system has been updated to handle the occurrence of duplicate backup ids for a partition. Appropriate error handling mechanisms were put in place to prevent such cases from interrupting the backup listing process.
Result:
Now, users can list backups without encountering any errors, even if a single partition has backups taken by different nodes. This enhancement brings about a smoother experience in managing backups, improving the overall reliability of the system.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12597"><strong>Listing backups fails if more than 255 backups are available</strong></a><p>Attempts to list all available backups consistently failed when there were more than 255 backups, producing an unhelpful error message instead of the expected list of backups. The system was not able to handle a `BackupListResponse` with more than 255 entries, impacting user functionality.
In the existing setup, the `groupSizeEncoding` used a `uint8` to denote the number of entries - this inhibited the system from dealing with more than 255 entries in `BackupListResponse`.
The `groupSizeEncoding` was modified to use a `uint16`, meaning it can now handle up to 65535 backups. Furthermore, listing of backups was redesigned to limit the number of listed backups to a reasonable number and prevent potential timeouts.
Users can now effortlessly list all existing backups, even when there are more than 255 backups. The software now successfully handles a larger number of backups and delivers a better user experience.
</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12591"><strong>8.2.3 Degradation: Creating an oversized BPMN causes unrecoverable failure </strong></a><p> In version 8.2.3, attempting to upload an oversized BPMN caused unrecoverable failure of Zeebe, affecting the system's operability and health of the partitions. Further BPMN upload became impossible and partitions became unhealthy and unrecoverable.
 The issue was due to an error in the CommandAPI, which failed to reject requests exceeding the maxMessageSize. This caused an oversized command to be written to the log stream. Furthermore, when the engine couldn't write the follow-up event because it was over the batch size limit, it tried to write a rejection record containing the whole command, leading to an endless error handling loop in the processing machine.
 The issue was resolved by modifying the CommandAPI to reject the request if it exceeded maxMessageSize, ensuring that oversized commands would not be written to the log stream. Furthermore, the necessity of writing the entire command in the rejection record was revisited.
 Now, any attempts to upload an oversized BPMN will be properly rejected by the system, preventing unperturbed functioning and ensuring the health of partitions. This fix is incorporated from version 8.2.6 onwards.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12509"><strong>MessageTTL checking fails with deserialization errors</strong></a><p> Users experienced failed MessageTTL (Message Time To Live) checks due to deserialization errors. This terminated the processing actor, preventing processing on the impacted partition. 
 The issue was caused by unsafe concurrent access to the writer of a shared record value, which occurred when messages from two different partitions expired simultaneously. This bug emerged from a regression introduced in the Zeebe versions &gt;= 8.1.9 and &gt;= 8.2.0.
 A patch was implemented to ensure safe concurrent access by properly synchronizing the writer access preventing simultaneous expire messages from two different partitions.
 The system now handles MessageTTL checking without the deserialization errors and processing continues on all partitions, allowing user operations to proceed as expected.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12433"><strong>Broker cannot start with S3 accessKey and secretKey not supplied </strong></a><p>Previous to the fix, brokers failed to start when S3 access key and secret key were not supplied, causing a disruption in service operation.
The core issue was a flawed logic within the `io.camunda.zeebe.backup.s3.S3BackupStore.buildClient` method. It still attempted to call `AwsBasicCredentials.create(credentials.accessKey(), credentials.secretKey())` even when no credentials were provided, basing this on a change made in commit dfd3b9e1034365b3fc1859d5e35353826e1222b3.
The problem was resolved by rectifying the logic in `io.camunda.zeebe.backup.s3.S3BackupStore.buildClient`, allowing it to handle cases where no credentials are provided. Test cases were incorporated to account for situations where ACCESS_KEY, SECRET_KEY are not passed.
Brokers can now start as expected even if S3 access key and secret key are not supplied. This leads to a smoother, more versatile operation.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12374"><strong>CorruptedJournalException: Fail to read version byte from segment</strong></a><p>An error occurred while generating a response.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12328"><strong>Cannot disable Raft flush without specifying a delay</strong></a><p> 
In the previous system, users were unable to disable the Raft flush without specifying a delay. This was because the system required both properties in the configuration to initiate, even when the second one was null. 
 
The issue originated from how the Spring deserialization for configuration was handled. Since we were using an internal `record` for configuration, both properties had to be passed, which was problematic when the second property (specifying a delay) was left blank or null.
 
To address this, the system’s deserialization method for configurations was adjusted. Instead of expecting both properties of the `record` to have a value, the system now correctly handles situations where only one property is specified and the second one is null.
 
Now, users can disable the Raft flush feature without being required to specify a delay time. The system starts as expected with the environment variable `ZEEBE_BROKER_CLUSTER_RAFT_FLUSH_ENABLED=false` without needing to add a value for `ZEEBE_BROKER_CLUSTER_RAFT_FLUSH_DELAYTIME`.
</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12326"><strong>The `newThrowErrorCommand` incorrectly handled in `8.2.0`</strong></a><p> Users experienced an error while executing the `newThrowErrorCommand` via BPMN in version `8.2.0` of Zeebe. This action led to an uncaught error event that yielded the message "Expected to throw an error event with the code '500' with message 'Got a 500', but it was not caught. No error events are available in the scope."
 This issue arose after the introduction of support for FEEL expressions in error codes. The error was attributed to the misinterpretation of static expressions, which were assumed to only be of the `String` type, when they could also be of the `Number` type.
 The problem was addressed by modifying the transformer code to support `Number` type in addition to the `String` type. An alternative solution that was pondered but not implemented included adding a `getAsString` method to the `StaticExpression` to parse the `Number` to a `String`.
 With this fix, users can now execute the `newThrowErrorCommand` via BPMN without encountering the previous error. Processes are now handled as expected, matching the behavior in previous versions like `8.1.9` or `8.1.10`.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12173"><strong>Zeebe node sends messages to wrong node</strong></a><p> Users observed that zeebe-2 node of a test cluster was unable to get ready due to lack of heartbeats from the leader node, zeebe-0, despite logs showing zeebe-0 sending messages to zeebe-2 and receiving acknowledgments. This issue was further complicated by zeebe-1 erroneously receiving messages for zeebe-2.
 The issue was rooted in the channel state in NettyMessagingService being inconsistent and causing AppendRequest messages meant for zeebe-2 to be sent to zeebe-1 instead. A thorough investigation pinpointed that IPAddress was being used to find the channel pool, resulting in a wrong channel being used when IP was reassigned.
 To resolve this issue, developers improved the method of finding channel pool by using both address and InetAddress. This effectively prevents the system from choosing the wrong channel when an IP is reassigned. Further, the team introduced a new protocol version which includes the recipient's address with all messages to ensure they are accurately delivered.
 With these fixes, all nodes of the test cluster, including zeebe-2, receive the correct messages from the leader node, zeebe-0. The issue of erroneous message receipt by zeebe-1 is also resolved, ensuring smooth operation of the nodes and preventing cluster failure.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12007"><strong>ExceededBatchRecordSizeException: Can't append entry</strong></a><p>Users experienced an 'ExceededBatchRecordSizeException: Can't append entry' error, which resulted in partition health issues. Despite high backpressure, the system was unable to activate jobs as expected.
The issue was due to a calculation error where the expected event length was inaccurate. Two potential causes were identified; the check determining whether a record of a certain size could be appended sometimes inaccurately returned 'true', and the predicted event length was slightly off, potentially allowing an event to be appended that would exceed the batch size.
An additional buffer to the expected event length was added. This means that even if a larger than expected event was processed, it would fit in the batch. The buffer size is set to be relatively small.
With the fix applied, users should no longer encounter the 'ExceededBatchRecordSizeException: Can't append entry' error, and the system can now accurately predict whether an incoming event would exceed the batch size, preventing job activation failures due to this reason.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/11594"><strong>Triggering due timer events causes periodic latency spikes</strong></a><p> Users were experiencing periodic latency spikes as the result of due timer events. The Stream Processor could be blocked while the checker for due timer events was running, which concurrently tried to submit a batch of commands to trigger timer events. 
 This behavior was a result of the Stream Processor and the timer checker sharing an actor. Also, the checker's attempt to submit all due timer events in a single batch was causing the Stream Processor to deal with this high load at once.
 The underlying issue was addressed by ensuring that the Stream Processor and the timer checker do not share an actor anymore. So the Stream Processor can still process while the checker is gathering timers to trigger. Secondly, the checker now submits a batch with a limited number of commands to offset triggering the timer events with incoming commands from users. 
 Due to these fixes, users will notice a significant reduction in occasional latency spikes. The Stream Processor will keep processing while the timer checker is operating and the balance between incoming user commands and triggering timer events is now maintained.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/11578"><strong>Multi-Instance with messageevent-based subprocess that uses inputElement as correlationKey fails</strong></a><p>Users were experiencing an issue where the Multi-Instance didn't start if it contained a Message Event-based Subprocess using the inputElement as the correlationKey. Instead, an Incident was created, disrupting workflow.
The underlying cause for this issue was a fault within the technical engineering of the software. There was a failure in evaluating the expression 'myObject.myId'. This resulted in no variable being found for the name 'myObject'.
Post a thorough review and discussion within the team, a fix was implemented and thoroughly tested on Zeebe's engine. 
With the fix, the Multi-Instance should now start properly and processes as expected when it contains a Message Event-based Subprocess that uses the inputElement as the correlationKey. This eliminates the unwanted creation of an incident, ensuring a smoother user experience. It is noticeable in the  `8.0.15`, `8.1.13`, `8.2.6`, and `8.3.0` patches.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/11414"><strong>Unhandled `NoSuchElementException` when looking for executable process while deploying BPMN resource</strong></a><p> Users experienced an unhandled `NoSuchElementException` when deploying a BPMN resource, which resulted in a crashing application.
 The issue was due to the system code being unable to handle an executable process not found in the BPMN file during deployment. As a result, an unhandled `NoSuchElementException` was thrown, leading to application failure. 
 To resolve the issue, the code was updated with a detailed error message that gets triggered when an executable process is not present in a persisted process. This change allows for better tracing of the issue in the future and gives context for further debugging. 
 Now, instead of an unhandled exception, the system issues a detailed error message when it encounters the scenario of an executable process not being found. This process leads to improved traceability and no crashing of the application.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/11355"><strong>Not possible to cancel process instance with many active element instances</strong></a><p> 
Users experienced a system crash when attempting to cancel a process instance that contained numerous active element instances. This issue was causing Zeebe brokers to crash loop on production, which practically resulted in an unusable cluster.
 
The underlying cause of this issue was in the Zeebe's handling of process instances with many active elements. The system failed when it tried to write follow-up records for terminating many activities at once, exceeding the maximum allowed message size and thus triggering a crash.
 
The team implemented a two-part fix: an immediate solution to avoid further incidents, and a more comprehensive solution that correctly supports the cancellation of instances with many tokens. These modifications were designed not to exceed the maximum message size limit during the cancellation of processes with many activities, thereby preventing system crashes.
 
With this fix, users can now cancel process instances, even those with many active element instances, without experiencing system crashes or making the cluster unusable. The system can now correctly handle the termination of many activities in the process instance, resulting in a more stable and reliable platform.</p></div></p></div><div><h4>Documentation</h4><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/13058"><strong>Document new Deployment Distribution logic</strong></a><p>Previously, users lacked proper documentation that clearly explained how the Deployment Distribution logic functioned in conjunction with the Generalized Record Distribution (Command Distribution). This included specificities concerning resource deployments.
The cause of this issue was the transition to a new Deployment Distribution system, which required complex explanation and examples. The original documentation was not exhaustive and lacked specific information about certain cases, such as resource deployments.
Updates were made to the documentation to reflect the new Deployment Distribution over Generalized Record Distribution. A comprehensive example, shedding light on resource distribution, was also included to provide clarity.
Users can now reference the new detailed documentation, which provides in-depth explanation of the Deployment Distribution mechanism. It acts as a useful guide for understanding the logic of distribution. The provided example should make scenarios concerning resource deployments clearer.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/issues/12584"><strong>Document guidelines on how to handle flaky tests</strong></a><p> Prior to this update, contributors faced challenges in dealing with flaky tests in their contributions. There were no documented guidelines in place to instruct them on how to handle such situations. 
 The lack of a clear guide for handling flaky tests was derived as an action point during a ZPA team's recent retrospective meeting. The absence of such a guide made it difficult for contributors to progress when faced with flaky tests.
 A comprehensive guide was created for handling flaky tests. The guide details the necessary steps that the contributors need to follow when they encounter flaky tests in their contributions. 
 Now, contributors have a readily available guide to turn to when they face challenges regarding flaky tests in their contributions. This updated feature enables them to tackle such testing issues more efficiently thus, resulting in smoother progress of contribution work.</p></div></p></div><div><h4>Merged Pull Requests</h4><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/pull/14528"><strong>fix(benchmark): use JDK_JAVA_OPTIONS for opts to be picked up by jib images</strong></a><p>Previously, users experienced issues with the jib images not picking up JAVA_OPTIONS in the Zeebe project.
This was due to using JAVA_OPTIONS environment variable which, in the context of jib images, wasn't being recognized.
The code was adjusted to adopt the JDK_JAVA_OPTIONS used in the benchmark charts instead of standard JAVA_OPTIONS.
Now, jib images can effectively pick up the necessary options, improving the overall functionality of the platform.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/pull/14518"><strong>fix(raft): reset term from last log if metastore is empty</strong></a><p>
After restoration from a backup, users faced unexpected election results with the raft node due to it restarting the term at 0. This was an issue as the acceptance or rejection of a poll request is based on the last log entry's term.
The problem originated from the fact that the backup only contains journal segments, leaving the metastore empty after restoration. As a result, when the term restarted at 0, the last log entry's term could be higher than the local term, leading to unexpected election activities.
Fix:
The solution was to start the term at the same level as the last log entry instead of restarting at 0 after the restore. This mitigates any risk of the term mismatch affecting the election results.
Now when restoring from backup, the raft node accurately begins the term at the level of the last log entry, avoiding any unexpected discrepancies in the election process. The system functions as expected even if the metastore is empty post-restoration.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/pull/14516"><strong>feat(engine): add signal catch event for event-based gateway</strong></a><p> In the past, the system didn't support the signal catch event for event-based gateway.
 There was no implemented functionality to handle the signal catch event in the event-based gateway in the previous versions of the software.
 We have added the signal catch event functionality to the event-based gateway. This fix includes various unit and integration tests that confirm the functionality aligns with the acceptance criteria of the issue.
 Users can now use the signal catch event for event-based gateways, ensuring greater flexibility and functionality when using the system.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/pull/14514"><strong>Job Worker Name property null check</strong></a><p>Users previously encountered errors when the Job Worker Name property was null.
The underlying cause of this issue was a missing null check in the code handling the job worker name property.
A null check was added to the worker name property code. 
The system now handles null values in the job worker name property without errors.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/pull/14512"><strong>fix(restore): use backup from any broker</strong></a><p>Previously, users were unable to restore using backups from any broker, limiting the scope and functionality of the backup system. 
The issue was due to the system erroneously enumerating possible broker ids based on the current configuration, thus limiting the range of usable backups.
The system was modified to search for matching backups from any broker rather than restricting itself to backups based on specific broker ids.
Users are now able to restore using backups from any broker, enhancing the flexibility and efficiency of the backup system.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/pull/14493"><strong>Fail writing of checksum on IOException</strong></a><p>Users previously experienced silent failures when a write error occurred with the PrintWriter because it doesn't fail on write error.
The underlying Java PrintWriter class does not throw I/O exceptions on method calls, although some of its constructors may throw exceptions. The product had to manually check for these errors.
Implemented a solution to manually invoke the checkError() method to verify if any errors occurred during the PrintWriter operation.
Any write errors that occur with the PrintWriter are now detected and appropriately handled, providing a more reliable printing experience.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/pull/14239"><strong>deps(maven): bump org.rocksdb:rocksdbjni from 8.3.2 to 8.5.3</strong></a><p>Given the limited information provided in the provided text, a hypothetical release note could look like this:
The use of version org.rocksdb:rocksdbjni 8.3.2 was causing a persistent database corruption in an unlikely edge case.
The issue was a result of a bug in `GenericRateLimiter` and an error when reading from offset 0 of a file in certain conditions. 
Bumped org.rocksdb:rocksdbjni to 8.5.3 which includes the necessary fixes for the identified issues.
With the version upgrade of org.rocksdb:rocksdbjni to 8.5.3, the database corruption in the edge case scenario is no longer present. This version has also eliminated any errors associated with the `GenericRateLimiter` and reading from offset 0 of a file, ensuring smooth data operations.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/pull/14184"><strong>Reuse already created checksum collections on persist</strong></a><p>Prior to the fix, the system inefficiency was evident due to unnecessary re-reading of files to create checksum collections every time a snapshot was persisted.
This was caused by the programmatic implementation which did not allow for the use of pre-existing checksum collections from bytes already read from files. Instead, the system was compelled to recreate checksum collections, resulting in repetitions.
Modifications were made to improve the system's efficiency. The system now supports updating the checksum collection from bytes which could be received or read from a file. The ability to persist snapshots using the given checksum collection has also been incorporated.
With this fix, the system will not recreate checksum collections frequently. Existing ones will be repurposed, leading to enhanced system efficiency. The users should now be able to notice relative speed and smoothness in the overall operation.
</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/pull/14074"><strong>deps(ubuntu): use release tag without date </strong></a><p>Previously, the use of a dated Ubuntu tag introduced extra complexity for Renovate, leading to an inability to concurrently update both digests when the tag was changed. 
The issue was rooted in the design choice of using dated Ubuntu tags, which complicated Renovate's handling of the updates and revealed an inherent limitation in Renovate's updating capabilities.
This issue was addressed by switching to an undated Ubuntu release tag for Renovate’s operations.
The system now uses an undated Ubuntu release tag which simplifies Renovate's operations, allowing for concurrent updates of both digests whenever necessary.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/pull/14071"><strong>deps(deps): update ubuntu docker tag to jammy-20230804 (main)</strong></a><p>The system was running on an outdated version of the Ubuntu Docker image, which could have resulted in unexpected behavior or compatibility issues.
The system was configured to use the `jammy-20230624` Ubuntu Docker image, rather than the more up-to-date `jammy-20230804` image.
The system configuration was updated, replacing the `jammy-20230624` Ubuntu Docker image with the `jammy-20230804` image.
The system now utilizes the `jammy-20230804` Ubuntu Docker image, ensuring it is running on the most current version, which ultimately enhances overall stability and performance.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/pull/13816"><strong>chore(monitor): Operate datasource variable reference</strong></a><p>Previously, operate dashboards were unable to utilize the selected datasource variable which prevented the dashboards from displaying accurate and desired information.
The issue arose from a lapse in the programming of the system which didn't account for the interchangeability of the datasource variable in operate dashboards.
The code was modified to ensure that operate dashboards can now use the selected datasource variable as intended, allowing for more flexibility.
Now, operate dashboards are able to utilize the selected datasource variable enabling them to display more accurate and specific data.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/pull/13376"><strong>Remove process instance keys from `PROCESS_INSTANCE_KEY_BY_DEFINITION_KEY` ColumnFamily</strong></a><p> Users were not able to remove data from the `PROCESS_INSTANCE_KEY_BY_DEFINITION_KEY` ColumnFamily, obstructing the termination of specific process instances when deleting a process definition.
 The system lacked the ability to track process instances running for a specific process definition key and consequently could not identify which process instances needed termination when deleting a process definition. 
 Code modifications were made to enable data removal from the `PROCESS_INSTANCE_KEY_BY_DEFINITION_KEY` ColumnFamily.
 The system can now keep track of the process instances running for a specific process definition key, allowing it to terminate needed process instances upon deletion of a process definition.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/pull/13168"><strong>deps(maven): bump rest-assured from 5.3.0 to 5.3.1</strong></a><p>
In the previous release, users experienced an array of issues related to functionality such as not being able to POST xml files using InputStream and unable to accept cookies that have an expiration date. There was an issue in MockMvcRequestSenderImpl's convertCookies function as it was not copying the httpOnly, sameSite and expires from the servletCookies into the cookieBuilder.
The root cause of these issues was tied to several bugs in the system code. The function, 'PathSupport.getPath', contained unexpected logic that caused malfunction (issue 1682), the POST xml file problem was due to an inadequacy in file handling (issue 1160), and the inability to accept expiry cookies was due to related guidelines not being followed correctly (issue 1430). On top of this, certain dependencies like Guava library, Kotlin extension module, Groovy, faster jackson, and Hamcrest were using outdated versions.
A series of patches and updates were applied. Errors in the logic for 'PathSupport.getPath' were pinpointed and corrected, and the POST xml file issue was resolved by enhancing InputStream's file handling capabilities. Issue 1462 was fixed by modifying the convertCookies function to correctly copy httpOnly, sameSite and expires from the servletCookies into the cookieBuilder. The Guava library was updated to version 32.0.1-jre, the Kotlin extension module now uses Kotlin 1.8.22, Groovy was upgraded to 4.0.11, faster jackson was updated to 2.14.3, and Hamcrest was upgraded to version 2.2.
Users are now able to POST xml files using InputStream and can accept cookies that have an expiration date. The MockMvcRequestSenderImpl's convertCookies function properly copies the httpOnly, sameSite, and expires from the servletCookies into the cookieBuilder. Also, users can experience better performance with the upgraded versions of dependencies.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/pull/13069"><strong>Monitor Snyk projects on release</strong></a><p> Before the fix, during the release workflow, Snyk projects were not monitored effectively, implying vulnerabilities posed by dependencies might not have been caught or tracked efficiently.
 The release job was not adequately configured to monitor Snyk projects for a given release, and there was no cleanup mechanism for out-of-date minor versions.
 To resolve this, a job was added to the release workflow that monitors Snyk projects, using the released Docker image and local Maven POMs from the release branch. The version and Docker image name were overridden. During a dry run, a test of the Snyk artifacts was performed. Additionally, projects were created under the grouping using only the major and minor release and names included only minor releases, enabling project replacement on patch release.
 Now, during the release workflow, Snyk projects are better monitored. Providing an improved view of any vulnerability posed by dependencies. It's important to note that projects now replace on patch releases, meaning we only keep track of the latest release on a minor branch.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/pull/12633"><strong>deps(go): bump go from 1.17 to 1.19</strong></a><p>The system was operating on an outdated version of Go 1.17, potentially leading to missing recent enhancements or security patches.
The outdated version of Go was specified in the technical environment of the product, resulting from outdated dependencies in the system configurations.
The Go language dependency was updated from version 1.17 to 1.19 in the system's settings and configurations.
The system operates on the updated version of Go 1.19, ensuring it benefits from the recent enhancements and security patches.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/pull/12534"><strong>Bump TCC concurrency to stabilize IT stage</strong></a><p> The splitting of tests in the IT stage led to increased contention around VM usage. This issue caused tests to time out due to slow startups resulting from an excess of containers contending for resources. 
 The root cause of this problem was a technical design issue involving the concurrency configuration for TCC within the IT stage. The concurrency wasn't high enough to handle the increased use of resources following the split of tests.
 The concurrency for TCC within the IT stage was increased. This adjustment aimed to mitigate the issue of slow startups caused by high resource contention.
 Now, the system effectively handles the contention around VM usage, resulting in reduced test timing out incidences and faster test startups.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/pull/12402"><strong>Aggregate equivalent client streams</strong></a><p> Previously, client streams with the same `streamType` and `metadata` were not aggregated into a single stream which caused unnecessary data traffic on the server side.
 The application lacked proper logic to aggregate equivalent client streams into one to reduce server-side traffic. 
 We developed an aggregation method which coalesces client streams with the same `streamType` and `metadata` into a single stream. The streamId for the aggregated stream is now generated randomly, which helps prevent inconsistencies in state that could occur due to concurrent add and removal requests. 
 Client streams with similar properties are now aggregated to a single stream. This leads to smoother data flow, as payloads pushed to this server stream will be distributed to one of the registered client stream. Consequently, the maintenance of server traffic and efficiency has improved.
</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/pull/12263"><strong>deps(maven): bump reactive-streams from 1.0.3 to 1.0.4</strong></a><p>Prior to the update, our system was making use of the older version 1.0.3 of the reactive-streams library, which lacked the latest improvements in rule clarifications, technology compatibility kit (TCK), and licensing changes.
The library reactive-streams being used in the system was not updated to the latest version, causing the system to miss out on recent updates and improvements. This is a routine software dependency maintenance issue.
The system's dependency on reactive-streams was updated from version 1.0.3 to 1.0.4. This was achieved by adjusting the Maven build file to incorporate the latest version.
The system now uses reactive-streams version 1.0.4. This brings in the latest rule clarifications and improvements on the subscriber rule §2.3, improved JavaDoc, and a change in the project licensing to MIT No Attribution (SPDX: MIT-0). As there were no breaking or semantical changes to the interface, this update should be seamless to the end-user.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/pull/12174"><strong>Updated Slack and forum link</strong></a><p>Previously, users were directed to outdated Slack and forum links. 
This was due to the fact that the links had not been updated in the system. 
The outdated Slack and forum links have been replaced with current, active ones.
Currently, users following the given Slack and forum links will be directed to the correct and active channels.</p></div></p><p><div class="release-note" style="white-space: pre-wrap;"><a href="https://github.com/camunda/zeebe/pull/12170"><strong>docs(CONTRIBUTING): update "Starting on an issue" steps</strong></a><p>Users were experiencing confusion when starting on an issue due to outdated steps documented on contributing guidelines. 
The issue was a result of outdated documentation within the contributing guidelines, specifically the "Starting on an issue" section, which didn't match the updated feature or process. 
The "Starting on an issue" steps in the contributing guidelines have been updated in the documentation to align with the current functionality of the system.
Users can now access updated steps in the "Starting on an issue" section of the contributing guidelines, ensuring a clearer understanding and ease of initiating work on issues.</p></div></p></div></div></div></div></div></div>
  <script type="text/javascript">__meteor_runtime_config__ = JSON.parse(decodeURIComponent("%7B%22meteorRelease%22%3A%22METEOR%402.13.3%22%2C%22gitCommitHash%22%3A%2202796d4dab3de33a2f67d5a4990ce83aea3a0c48%22%2C%22meteorEnv%22%3A%7B%22NODE_ENV%22%3A%22development%22%2C%22TEST_METADATA%22%3A%22%7B%7D%22%7D%2C%22PUBLIC_SETTINGS%22%3A%7B%7D%2C%22ROOT_URL%22%3A%22http%3A%2F%2Flocalhost%3A3000%2F%22%2C%22ROOT_URL_PATH_PREFIX%22%3A%22%22%2C%22_hmrSecret%22%3A%224b5c5bf116ced0f56a2ba9ae0894fd9f615cad607c86cd8b3c1337d10ab740ddd5fea78739b926334aa5825c2eee9751bb5128b65cea696ae5837f19e30648cc%22%2C%22reactFastRefreshEnabled%22%3Atrue%2C%22autoupdate%22%3A%7B%22versions%22%3A%7B%22web.browser%22%3A%7B%22version%22%3A%22cf5c3edbc595f6c6f900b5628d978b63725e063c%22%2C%22versionRefreshable%22%3A%22a580e09175421ec6994fc6da61a0413f3a15d2b1%22%2C%22versionNonRefreshable%22%3A%22134d78343787806e7c53c8a0c30039b522e64e3d%22%2C%22versionReplaceable%22%3A%225e419a97fc48cdc45b8d4eef099c552750f85fbc%22%2C%22versionHmr%22%3A1699434984109%7D%2C%22web.browser.legacy%22%3A%7B%22version%22%3A%22ad297e2b708a069811ebe8e7b1def93ec39597ee%22%2C%22versionRefreshable%22%3A%22a580e09175421ec6994fc6da61a0413f3a15d2b1%22%2C%22versionNonRefreshable%22%3A%22f5c98f157b813cf174ae3e2a3529ed7a2de00a3c%22%2C%22versionReplaceable%22%3A%22f9b28f455090193d6d31a34608a015dde061f61e%22%2C%22versionHmr%22%3A1699434984817%7D%7D%2C%22autoupdateVersion%22%3Anull%2C%22autoupdateVersionRefreshable%22%3Anull%2C%22autoupdateVersionCordova%22%3Anull%2C%22appId%22%3A%22n2g3fe6b4xa8.nvm3o35b68qo%22%7D%2C%22appId%22%3A%22n2g3fe6b4xa8.nvm3o35b68qo%22%2C%22isModern%22%3Atrue%7D"))</script>

  <script type="text/javascript" src="./release-note_files/meteor.js"></script>
  <script type="text/javascript" src="./release-note_files/meteor-base.js"></script>
  <script type="text/javascript" src="./release-note_files/mobile-experience.js"></script>
  <script type="text/javascript" src="./release-note_files/modules-runtime.js"></script>
  <script type="text/javascript" src="./release-note_files/modules-runtime-hot.js"></script>
  <script type="text/javascript" src="./release-note_files/modules.js"></script>
  <script type="text/javascript" src="./release-note_files/modern-browsers.js"></script>
  <script type="text/javascript" src="./release-note_files/babel-compiler.js"></script>
  <script type="text/javascript" src="./release-note_files/es5-shim.js"></script>
  <script type="text/javascript" src="./release-note_files/promise.js"></script>
  <script type="text/javascript" src="./release-note_files/ecmascript-runtime-client.js"></script>
  <script type="text/javascript" src="./release-note_files/hot-module-replacement.js"></script>
  <script type="text/javascript" src="./release-note_files/react-fast-refresh.js"></script>
  <script type="text/javascript" src="./release-note_files/ecmascript.js"></script>
  <script type="text/javascript" src="./release-note_files/ecmascript-runtime.js"></script>
  <script type="text/javascript" src="./release-note_files/babel-runtime.js"></script>
  <script type="text/javascript" src="./release-note_files/fetch.js"></script>
  <script type="text/javascript" src="./release-note_files/dynamic-import.js"></script>
  <script type="text/javascript" src="./release-note_files/base64.js"></script>
  <script type="text/javascript" src="./release-note_files/ejson.js"></script>
  <script type="text/javascript" src="./release-note_files/diff-sequence.js"></script>
  <script type="text/javascript" src="./release-note_files/geojson-utils.js"></script>
  <script type="text/javascript" src="./release-note_files/id-map.js"></script>
  <script type="text/javascript" src="./release-note_files/random.js"></script>
  <script type="text/javascript" src="./release-note_files/mongo-id.js"></script>
  <script type="text/javascript" src="./release-note_files/ordered-dict.js"></script>
  <script type="text/javascript" src="./release-note_files/tracker.js"></script>
  <script type="text/javascript" src="./release-note_files/minimongo.js"></script>
  <script type="text/javascript" src="./release-note_files/check.js"></script>
  <script type="text/javascript" src="./release-note_files/retry.js"></script>
  <script type="text/javascript" src="./release-note_files/callback-hook.js"></script>
  <script type="text/javascript" src="./release-note_files/ddp-common.js"></script>
  <script type="text/javascript" src="./release-note_files/reload.js"></script>
  <script type="text/javascript" src="./release-note_files/socket-stream-client.js"></script>
  <script type="text/javascript" src="./release-note_files/ddp-client.js"></script>
  <script type="text/javascript" src="./release-note_files/ddp.js"></script>
  <script type="text/javascript" src="./release-note_files/ddp-server.js"></script>
  <script type="text/javascript" src="./release-note_files/allow-deny.js"></script>
  <script type="text/javascript" src="./release-note_files/mongo-dev-server.js"></script>
  <script type="text/javascript" src="./release-note_files/logging.js"></script>
  <script type="text/javascript" src="./release-note_files/mongo.js"></script>
  <script type="text/javascript" src="./release-note_files/reactive-var.js"></script>
  <script type="text/javascript" src="./release-note_files/minifier-css.js"></script>
  <script type="text/javascript" src="./release-note_files/standard-minifier-css.js"></script>
  <script type="text/javascript" src="./release-note_files/standard-minifier-js.js"></script>
  <script type="text/javascript" src="./release-note_files/typescript.js"></script>
  <script type="text/javascript" src="./release-note_files/shell-server.js"></script>
  <script type="text/javascript" src="./release-note_files/static-html.js"></script>
  <script type="text/javascript" src="./release-note_files/react-meteor-data.js"></script>
  <script type="text/javascript" src="./release-note_files/zodern_types.js"></script>
  <script type="text/javascript" src="./release-note_files/sakulstra_aggregate.js"></script>
  <script type="text/javascript" src="./release-note_files/webapp.js"></script>
  <script type="text/javascript" src="./release-note_files/hot-code-push.js"></script>
  <script type="text/javascript" src="./release-note_files/launch-screen.js"></script>
  <script type="text/javascript" src="./release-note_files/autoupdate.js"></script>
  <script type="text/javascript" src="./release-note_files/global-imports.js"></script>
  <script type="text/javascript" src="./release-note_files/app.js"></script>



<div class="keysmith__toast-container"><img class="keysmith__toast-container__icon" src="chrome-extension://igfbafcgnaemgipnngnppdhilhpmmnoh/images/icon.png"><div class="keysmith__toast-container__content"><div class="keysmith__toast-container__header"></div><div class="keysmith__toast-container__detail"></div></div></div></body></html>