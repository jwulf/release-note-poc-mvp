[
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14019",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\n\r\nFor multi-tenancy we need to store a tenant id in the state. We've decided to do this using a new object: `DbTenantAwareKey`.\r\n\r\nThis key will implement the `DbKey` interface. It will always contain a `DbString` which is used to store the tenantId. Besides this it can wrap any other key.\r\n\r\nEg:\r\n```java\r\npublic record DbTenantAwareKey<WrappedKey extends DbKey>(DbString tenantKey, WrappedKey wrappedKey)\r\n    implements DbKey\r\n```\n",
    "title": "Create `DbTenantAwareKey`",
    "releaseNoteText": "**Consequence:** Multi-tenancy was not properly supported, resulting in data inconsistencies for different tenants. \n\n**Cause:** The lack of a tenant id in the state caused confusion and made it difficult to differentiate between different tenants' data.\n\n**Fix:** Introduce a new object called `DbTenantAwareKey` to store the tenant id in the state. This key will implement the `DbKey` interface and always contain a `DbString` to store the tenantId. It can also wrap any other key as needed.\n\n**Result:** With the implementation of `DbTenantAwareKey`, multi-tenancy is now properly supported. Each tenant's data can be appropriately identified and managed, ensuring data consistency and preventing mix-ups between different tenants."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13989",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nZeebe auth data (ex. the user’s tenant access list) should be sent from the gateway to the broker in a way that is easily extendable in the future. Extendability is important since Zeebe might need to support user permissions (https://github.com/camunda/product-hub/issues/495) in the future, and it would be better if we already have the code that can contain these future extensions without any significant changes.\r\n\r\nSince Zeebe auth data may be used in all types of Zeebe Records, it would be better to place it in the `RecordMetadata` since:\r\n* It is a single change that makes auth data available to all records.\r\n* Conceptually, auth data is more closely related to `Intent`s which are already placed in the `RecordMetadata`.\r\n* It doesn't pollute the record value since auth data is irrelevant to the outcome of commands.\r\n\r\nWhen placed in the `RecordMetadata` the auth data should be encoded so that:\r\n* It can be extracted only when needed\r\n* It can be extended without any further changes to the `RecordMetadata` strucutre\r\n\r\n### AC\r\n- [x] Auth data is contained in `RecordMetadata`\r\n- [x] Auth data is encoded within the `RecordMetadata`\r\n- [x] Auth data contains a flag to indicate the mechanism used to encode/decode it\r\n- [x] Auth data is contained in `ExecuteCommandRequest` (to enable Gateway-to-Broker requests)\r\n\n",
    "title": "Add auth data to Zeebe records",
    "releaseNoteText": "**Consequence:** Previously, the Zeebe system did not include authentication data in the Zeebe records. This meant that user permissions and access lists were not accounted for, limiting the ability to enforce security measures and conduct proper access control.\n\n**Cause:** The lack of authentication data in Zeebe records was due to the absence of a mechanism to include this information. The current record structure did not provide a space to store such data, resulting in a lack of extendability for future updates.\n\n**Fix:** In order to address this issue, authentication data has been added to the `RecordMetadata`. This ensures that the data is easily accessible and allows for future extensions without the need for significant changes to the codebase. The auth data is encoded within the `RecordMetadata` structure and includes a flag to indicate the mechanism used for encoding and decoding.\n\n**Result:** With the implementation of this fix, Zeebe now includes auth data in all types of Zeebe records. The auth data is stored in the `RecordMetadata`, allowing for easy access and extending support for user permissions in the future. This update enhances the security and access control capabilities of Zeebe, ensuring a more robust and flexible system for users."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13988",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nZeebe auth data (ex. the user’s tenant access list) should be sent from the gateway to the broker in a way that is easily extendable in the future. Extendability is important since Zeebe might need to support user permissions (https://github.com/camunda/product-hub/issues/495) in the future, and it would be better if we already have the code that can contain these future extensions without any significant changes.\r\n\r\nIdeally, the data interchange protocol/mechanism should be pluggable, so that Zeebe has the ability to move to something different in the future.\r\n\r\nFor this iteration, unsigned JWT tokens will be used, for the following reasons:\r\n* Using JWT is future-proof.\r\n   * If we decide to export auth data in the future, we can use signed JWTs (JWS), so users have a high confidence that the auth data is valid.\r\n* JWTs are already used by other C8 components, so we ensure wider compatibility.\r\n* We already have TLS secure communication so we can trust unsigned JWT tokens (for this iteration).\r\n* Lower implementation effor than other options (ex. MsgPack), as JWT libraries already provide APIs to work with different Java types.\r\n\r\n## AC\r\n- [x] A general API for auth data encoding is available.\r\n- [x] A general API for auth data decoding is available.\r\n- [x] Auth data can be encoded in a JWT token string\r\n- [x] Auth data can be decoded from a JWT token string\n",
    "title": "Provide an API for data interchange of Zeebe auth data",
    "releaseNoteText": "**Consequence:**\nZeebe authentication data, such as the user's tenant access list, was not easily sent from the gateway to the broker in a way that allowed for future extensions. This limitation had the potential to hinder the support of user permissions in the future.\n\n**Cause:**\nThe data interchange protocol/mechanism for Zeebe authentication data was not pluggable, making it difficult to adapt to different protocols in the future. Additionally, the use of unsigned JWT tokens was chosen as a temporary solution, which posed limitations on the validation and confidence in the authentication data.\n\n**Fix:**\nA general API for encoding and decoding auth data was implemented, allowing for easier and extensible interchange of Zeebe authentication data. The API supports encoding auth data into JWT token strings.\n\n**Result:**\nWith this fix, Zeebe now has a more flexible and future-proof solution for exchanging authentication data. The general API provides the ability to encode auth data into JWT token strings, allowing for wider compatibility and easier integration with other components. This improvement allows for the possibility of extending Zeebe's support for user permissions in the future without significant changes to the codebase."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13987",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nA `TenantAccessChecker` class should be implemented in the `engine` module. It should be used by `*Processor` classes, or any other classes that process `Command` records created by Client requests. The purpose of the class is to determine if the User making the request has access to the requested Tenant data.\r\n\r\nThe class should provide the following methods:\r\n* `hasAccess(String tenantId, , List<String> authorizedTenants)` - to determine if a user has access to data from a given tenant. The output can be `Either<Exception, String>` providing the tenantId.\r\n   * If the `tenantId` is listed in the `RecordValueWithTenantPermissions`, the `tenantId` is returned. Otherwise, an error is raised.\r\n* `hasFullAccess(List<String> tenantIds, List<String> authorizedTenants)` - to determine if a user has access to all of the `tenantIds` provided in a list. This will be used later for job polling.\r\n\r\n:information_source: We might expand or refactor this class according to different needs as development on the multi-tenancy topic progresses.\r\n\r\n### AC\r\n- [x] A `TenantAccessChecker` class\r\n- [x] Test coverage for the `TenantAccessChecker` class\n",
    "title": "Provide a `TenantAccessChecker` API for multi-tenancy",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/8263",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nCame up in a recent [incident review ](https://docs.google.com/document/d/1E03kB3UCfM3l8X0xU7RjzqvOnX8dk4GtKAmH5-_vY60/edit) that it would be nice to see whether instances are blacklisted or not, maybe also the rate of blacklisting.\r\n\r\nI realized that we added a while ago the metrics for that, but never added this to the dashboard, see https://github.com/camunda-cloud/zeebe/pull/6715. \r\n\r\n**Describe the solution you'd like**\r\n\r\nEither show a graph of blacklisted instance count or just some indication whether instances are blacklisted.\r\n\r\nI had yesterday a **hard time**  to find a good visualization, since the count is only exported if there was a new blacklisted instance. If the pod is restarted the metric is not exported. _It might make sense to refill the metric on restart_ OR we need to work with it, but this would mean we can only show limited data, see below.\r\n\r\n**Example A - Show an indication that instances are blacklisted**\r\n\r\n![gauge-blacklisted](https://user-images.githubusercontent.com/2758593/143187841-3f9d4a4c-50b9-49de-95b0-43650d479103.png)\r\n![gauge-blacklisted-general](https://user-images.githubusercontent.com/2758593/143187843-2c6c629b-0687-463a-82de-970f39a4e372.png)\r\n\r\nIn general I like this, since it shows directly whether there is something wrong.\r\nThe problem here is if the time frame is smaller (were no instances are blacklisted) than this indication is green. :-1: \r\n\r\n**Example B - Graph**\r\n\r\nShowing a graph is not that fruitful, since as described above the metric is not always exported. \r\n![graph-blacklisted](https://user-images.githubusercontent.com/2758593/143187948-f6d36e3e-1b15-48a6-82ba-872d58592ca5.png)\r\n\r\nShowing zero for null values\r\n![graph-zero-blacklisted](https://user-images.githubusercontent.com/2758593/143188265-1b85b331-2306-40d2-94df-0132255c767c.png)\r\n\r\n**Example C - Rate**\r\n\r\nThe rate of such metric is also not really useful, since the change is too rare.\r\n\r\n![graph-rate-blacklisted](https://user-images.githubusercontent.com/2758593/143188214-3b44348e-3b0c-4464-85ed-5ccbb62ed60f.png)\r\n\r\n\r\n**Example D - Table**\r\n![table-blacklisted](https://user-images.githubusercontent.com/2758593/143188772-b439d832-b192-47ed-8b6e-dbe31842a7b9.png)\r\n\r\nOther alternative would be to show in a table the recent count, but this is also very limited (to the time frame) and other\r\n\r\n**Describe alternatives you've considered**.\r\n\r\nIdeally we would export the metric always, this would simplify things enormous. Then we can choose better one or more of the possible visualization from above.\r\n\r\n**Additional context**\r\n\r\nBlacklisting always shows that something problematic happened in the process execution (processing). An exception was thrown during the execution, which mostly an indication of a bug. \r\n\r\nI would like to find a good way how we can visualize it and hope someone has some comments, opinion or ideas.\r\n\r\n\\cc @pihme\r\n\n\n pihme: What about exposing the metric as a rate and visualizing it with a heat map? \n npepinpe: Prioritized as planned for now under the assumption we won't work on a better long term alternative to blacklisting in the next quarter, and this will already be an improvement in terms of visibility. Before opening a PR for this, please discuss and decide on the visualization/metric that we want.\n Zelldon: The question for me is really what we want to achieve with the metric.\r\n\r\nDo we want to know really how many (exact) are blacklisted? Then Prometheus might be not the best fit, but if we just want to have an indicator that SOMETHING is blacklisted then this could work:\r\n \r\n![blacklist](https://user-images.githubusercontent.com/2758593/148549724-9548b692-0f78-44a6-9783-0692e0bd9919.png)\r\n\r\nHere we could change the colors and shown values to something like Blacklisted (if x > 0) and nothing (if x <= 0).I think this is similar what I have shown above with example A. This would potentially already help.\r\n\n pihme: What about exposing the metric as a rate and visualizing it with a heat map?\n Zelldon: This wouldn't help since the metric is not exported all the time. Plus what should tell me the heatmap would be the question?\n pihme: I would expect to see a change in the blacklisting rate.\r\n\r\nKinda like here:\r\n![image](https://user-images.githubusercontent.com/14032870/148551718-06e323ce-d576-4e46-bd78-130a9e97a4d5.png)\r\n\r\nIf it is not exported for some time, I will just see a black column, but I can always make the time scale wider and then I should see data pretty much for all the time, because each export would be aggregated in the rate (I hope).\r\n\r\nThen I could look at a big enough time scale and if my blacklisting suddenly jumps from 2 per day to 200 per day, that would be my signal. And if the jump is correlated to e.g. the point there was an update or a pod restart, that would also be interesting.\r\n\r\nThis is all speculation though, one would have to see what it actually looks like.\n Zelldon: Thanks @pihme \r\n\r\nyeah so it would look like this\r\n\r\n15 mins\r\n![heatmap](https://user-images.githubusercontent.com/2758593/148553374-792367bc-f419-43cf-a476-70b9f9244280.png)\r\n\r\n90 days\r\n![heatmap2](https://user-images.githubusercontent.com/2758593/148553377-ae5ce979-9e40-4134-a8c1-76a636c77043.png)\r\n\r\nProblem is that we not store data long enough on our prometheus server. So there can be data deleted which contained some blacklisting, which is why I ask what we want to see. The real count or just an indication all in all I think our current metric doesn't work well. Ideally we would report always or on bootstrap with the current value of blacklisted instances.\r\n\n pihme: Thanks @Zelldon. Yeah I regularly forget we forget data. I think 90 days would be good enough, and yes, the visualization looks like what I had in mind. But if the history is 30 days or less, it becomes less useful. So I see your point.\n Zelldon: I think the best would be as described as alternative above:\r\n\r\n> **Describe alternatives you've considered.**\r\n> Ideally we would export the metric always, this would simplify things enormous. Then we can choose better one or more of the possible visualization from above.\r\n\r\nThis means at least on restart we export the metric at least once. \n Zelldon: @remcowesterhoud since you mentioned to me that you had to look at a cluster with zdb to find out whether something was blacklisted might be useful for you to just have this in the metrics :) \n menski: Right now we don't see value to implement this, we have other ways to identify blacklisted instances, which also allow us to see the corresponding process instance keys, i.e. logs or zdb.\n\nHopefully at one point we can get rid of blacklisting\n korthout: ZPA triage:\r\n- we want to gain more insights into the frequency of blacklisting before choosing to replace the concept\n remcowesterhoud: Attaching this to the \"Blacklisting replacement\" as having insights in these metrics may be useful for this epic.\n Zelldon: Reopening since we still need panels on the dashboard. \n koevskinikola: ZPA triage:\r\n- @Zelldon you mentioned [HERE](https://github.com/camunda/zeebe/pull/12606#issuecomment-1528862192) that you'll add the Grafana dashboards. Is this still your plan, or ZPA should do something from our side?\r\n  - ZPA also has this issue marked as `upcoming` so we would do it soon either way.",
    "title": "Show blacklisting in the Grafana Dashboard",
    "releaseNoteText": "**Consequence:** The Grafana Dashboard does not currently show any indication of blacklisted instances or the rate of blacklisting. This makes it difficult for users to quickly identify if instances are blacklisted or not, resulting in a loss of visibility and potential delays in detecting and resolving issues with blacklisting.\n\n**Cause:** The metrics for blacklisting were added previously, but they were never incorporated into the Grafana Dashboard.\n\n**Fix:** The metric for blacklisted instances will now be displayed on the Grafana Dashboard, providing users with a clear indication of whether instances are blacklisted or not. Additionally, the metric will be refilled on restarts to ensure continuous visibility.\n\n**Result:** With this fix, users will have improved visibility into blacklisted instances through the Grafana Dashboard. They will be able to easily identify if instances are blacklisted or not, enabling them to take appropriate action and resolve any issues in a timely manner."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14497",
      "component": "Zeebe",
      "subcomponent": "Gateway",
      "context": "Enhancements"
    },
    "gitHubIssueText": "All clients should be able to use the default tenant. Even the client is not associated to any tenants in Identity.\r\n\r\n```java\r\n@Test\r\n@Disabled(\"Not yet supported: https://github.com/camunda/zeebe/issues/14497\")\r\nvoid shouldDenyDeployProcessWhenNoTenantAssociated() {\r\n  // given\r\n  try (final var client = createZeebeClient(ZEEBE_CLIENT_ID_WITHOUT_TENANT)) {\r\n    // when\r\n    final Future<DeploymentEvent> result =\r\n        client.newDeployResourceCommand().addProcessModel(process, \"process.bpmn\").send();\r\n\r\n    // then\r\n    assertThat(result)\r\n        .describedAs(\"Expect that process can be deployed for the default tenant\")\r\n        .succeedsWithin(Duration.ofSeconds(10));\r\n  }\r\n}\r\n```\n\n remcowesterhoud: We shouldn't implement this, details https://camunda.slack.com/archives/C05Q0EXE9V4/p1696235964583069\n korthout: For those reading this and wonder what's in the internal slack thread:\r\n\r\nWe're not implementing to support a specific use case.\r\n\r\nThe use-case is as follows:\r\n- Team A utilized a 8.2 cluster\r\n- They want to onboard Team B on the same cluster\r\n- They update to 8.3, activate MT, all their data is owned by the default (`<default>`) tenant\r\n- Team B should not be able to see data from Team A that already used the cluster, so they should not be able to access the default tenant\r\n- This can be configured explicitly in Identity",
    "title": "Allow using the default tenant even when no tenant associated to requester",
    "releaseNoteText": "**Consequence:** Previously, users were unable to use the default tenant when they were not associated with any tenants in Identity. This limited the ability for all clients to utilize the default tenant, even if they were not explicitly associated with any tenants.\n\n**Cause:** The underlying cause of this issue was the lack of support for using the default tenant when no tenant was associated with the requester.\n\n**Fix:** We have implemented a fix to allow the usage of the default tenant in cases where the requester is not associated with any tenants in Identity.\n\n**Result:** With this fix in place, all clients, regardless of their association with tenants in Identity, can now utilize the default tenant. This enhances the flexibility and accessibility of the system, allowing for smoother onboarding experiences and ensuring that data from different teams remains segregated appropriately."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14278",
      "component": "Zeebe",
      "subcomponent": "Gateway",
      "context": "Enhancements"
    },
    "gitHubIssueText": "null\n",
    "title": "Gateway supports multi-tenancy in EvaluateDecision RPC",
    "releaseNoteText": "**Consequence:** Previously, the Gateway did not support multi-tenancy in the EvaluateDecision RPC, causing limitations in the system's ability to handle multiple tenants simultaneously.\n\n**Cause:** This issue was caused by a missing implementation in the Gateway, which prevented seamless integration of multi-tenancy in the EvaluateDecision RPC.\n\n**Fix:** The Gateway has been updated to include support for multi-tenancy in the EvaluateDecision RPC, enabling the system to efficiently handle requests from multiple tenants concurrently.\n\n**Result:** With this fix, the Gateway now fully supports multi-tenancy in the EvaluateDecision RPC. Users will experience improved performance and scalability when making decision evaluations for multiple tenants simultaneously."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14276",
      "component": "Zeebe",
      "subcomponent": "Gateway",
      "context": "Enhancements"
    },
    "gitHubIssueText": "null\n",
    "title": "Gateway supports multi-tenancy in PublishMessage RPCs",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14254",
      "component": "Zeebe",
      "subcomponent": "Gateway",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nThe Gateway supports receiving and forwarding (to the Broker) `ActivateJobs` and `StreamActivatedJobs` RPC calls with a List of `tenantId`s.\r\n\r\nThe IDs specify the tenants whose jobs the Client/user wants to activate/process.\r\n\r\nNote: if the tenant ids list is empty, all of the user's authorized tenants should be used instead.\r\n\r\nThe following error codes may be returned:\r\n* PERMISSION_DENIED (code: 7) \r\n   * when a user attempts to poll/stream jobs for a tenant they are not authorized for, when multi-tenancy is enabled.\r\n* INVALID_ARGUMENT (code: 3)\r\n   * For a provided (non-default) tenant id, when multi-tenancy is disabled\r\n   * For an invalid tenant id (i.e. doesn't match the pre-defined format), when multi-tenancy is enabled.\n",
    "title": "Job polling/pushing in the Gateway supports multi-tenancy",
    "releaseNoteText": "**Consequence:** When multi-tenancy is enabled, the Gateway may return a PERMISSION_DENIED error code (7) if a user attempts to poll or stream jobs for a tenant they are not authorized for. Similarly, the Gateway may return an INVALID_ARGUMENT error code (3) when a non-default tenant ID is provided and multi-tenancy is disabled, or when an invalid tenant ID is provided and multi-tenancy is enabled.\n\n**Cause:** The issue is caused by a lack of proper authorization checks in the Gateway when processing ActivateJobs and StreamActivatedJobs RPC calls with a list of tenant IDs. It also stems from the Gateway not verifying the validity of the tenant ID when multi-tenancy is enabled.\n\n**Fix:** The Gateway has been fixed to perform comprehensive authorization checks when processing ActivateJobs and StreamActivatedJobs RPC calls with a list of tenant IDs. It now properly validates the tenant ID format when multi-tenancy is enabled.\n\n**Result:** After applying the fix, users will no longer receive a PERMISSION_DENIED error code when attempting to poll or stream jobs for unauthorized tenants. The Gateway will also return an INVALID_ARGUMENT error code only when appropriate, based on the multi-tenancy configuration and the validity of the provided tenant ID."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14211",
      "component": "Zeebe",
      "subcomponent": "Gateway",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nThe Gateway supports receiving and forwarding (to the Broker) `CreateProcessInstance*` RPC calls with a `tenantId`.\r\n\r\n### AC\r\n- [x] The following gRPC messages contain a new `tenantId` property:\r\n       - [ ] `CreateProcessInstanceRequest`\r\n       - [ ] `CreateProcessInstanceWithResultRequest`\r\n       - [ ] `CreateProcessInstanceResponse`\r\n- [x] The `createProcessInstance(...)` Gateway endpoint passes the gRPC `CreateProcessInstanceRequest#tenantId` property to the `BrokerCreateProcessInstanceRequest`. The following scenarios are possible as well:\r\n      - If multi-tenancy is disabled (see #13237), the `BrokerCreateProcessInstanceRequest#tenantId` is set to `<default>`.\r\n      - If multi-tenancy is enabled, and `BrokerCreateProcessInstanceRequest#tenantId` is `null`, the request is rejected.\r\n- [x] The `createProcessInstanceWithResult(...)` Gateway endpoint passes the gRPC `CreateProcessInstanceWithResultRequest#tenantId` property to the `BrokerCreateProcessInstanceWithResultRequest`. The following scenarios are possible as well:\r\n      - If multi-tenancy is disabled (see #13237), the `BrokerCreateProcessInstanceWithResultRequest#tenantId` is set to `<default>`.\r\n      - If multi-tenancy is enabled, and `BrokerCreateProcessInstanceWithResultRequest#tenantId` is `null`, the request is rejected.\n",
    "title": "Gateway supports multi-tenancy in CreateProcessInstance* RPCs",
    "releaseNoteText": "**Consequence:** Users can now create process instances with a specific tenant ID in the Gateway.\n**Cause:** The Gateway was not previously able to receive and forward `CreateProcessInstance*` RPC calls with a `tenantId` property.\n**Fix:** The Gateway now supports receiving and forwarding `CreateProcessInstance*` RPC calls with a `tenantId` property. The `createProcessInstance(...)` and `createProcessInstanceWithResult(...)` Gateway endpoints now pass the `tenantId` property to the corresponding `BrokerCreateProcessInstanceRequest` and `BrokerCreateProcessInstanceWithResultRequest`. If multi-tenancy is disabled, the `tenantId` is set to `<default>`. If multi-tenancy is enabled and the `tenantId` is `null`, the request is rejected.\n**Result:** Users can now successfully create process instances with a specific tenant ID using the Gateway. The `CreateProcessInstanceRequest`, `CreateProcessInstanceWithResultRequest`, and `CreateProcessInstanceResponse` gRPC messages now contain a new `tenantId` property, and the `tenantId` is correctly passed to the `BrokerCreateProcessInstanceRequest` and `BrokerCreateProcessInstanceWithResultRequest`."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14041",
      "component": "Zeebe",
      "subcomponent": "Gateway",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nProvide a Gateway configuration flag: `zeebe.gateway.multiTenancy.enabled` - default **FALSE** (disabled)\r\n  * The flag should toggle multi-tenancy for the Zeebe cluster.\r\n  * An `enabled` flag is preferred, as additional MT-related configuration properties might be added in the future (stretch goal).\n",
    "title": "I can use a Gateway configuration flag for multi-tenancy",
    "releaseNoteText": "**Consequence**: The lack of a gateway configuration flag for multi-tenancy impacted the ability to enable multi-tenancy for the Zeebe cluster. Users were unable to toggle multi-tenancy and had to work with the default setting, which was disabled.\n\n**Cause**: The absence of a gateway configuration flag prevented users from enabling multi-tenancy for the Zeebe cluster. The system did not have the necessary engineering to support this feature.\n\n**Fix**: The development team implemented a new gateway configuration flag, `zeebe.gateway.multiTenancy.enabled`, with a default value of `FALSE` to toggle multi-tenancy for the Zeebe cluster. This fix ensures that users have the option to enable or disable multi-tenancy through the configuration flag.\n\n**Result**: With the new gateway configuration flag for multi-tenancy, users can now enable multi-tenancy for the Zeebe cluster by setting the `zeebe.gateway.multiTenancy.enabled` flag to `TRUE`. This provides users with greater flexibility and control over the multi-tenancy feature, improving their overall experience with the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13237",
      "component": "Zeebe",
      "subcomponent": "Gateway",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\n\nFor multi-tenancy, the goal of the Zeebe Gateway is to:\n1. Provide data on the user's authorized tenants.\n2. Forward this data (through the `RecordMetadata`) to the Zeebe Broker through a `Broker*Request` instance.\n\nFull support for multi-tenancy in the Zeebe gateway will be provided once https://github.com/camunda-cloud/identity/issues/1819 is completed.\n\nFor this issue, the Zeebe Gateway should (only) support the following:\n\n* Provide a Gateway configuration flag: `zeebe.gateway.multiTenancy.enabled` - default **FALSE** (disabled)\n  * The flag should toggle multi-tenancy for the Zeebe cluster.\n  * An `enabled` flag is preferred, as we will probably add additional multi-tenancy configuration properties in the future.\n* Expand the [`IdentityInterceptor`](https://github.com/camunda/zeebe/blob/1434d4682eba842ef022ec500cd80ebe7cc29a2a/gateway/src/main/java/io/camunda/zeebe/gateway/interceptors/impl/IdentityInterceptor.java#L65) class. After the token is successfully verified the following should be performed:\n   1. Provide a list of tenant ids the user has access to:\n      * If multi-tenancy is disabled, the list should only contain the `<default>` tenant id (see #13235)\n      * If multi-tenancy is enabled, the Identity SDK should be used to fetch the list of tenant ids (out-of-scope for this issue)\n   2. [Proposal] Place the list of tenant IDs in a [gRPC context](https://grpc.github.io/grpc-java/javadoc/io/grpc/Context.html) to be forwarded to the `EndpointManager`/`RequestMapper` (consider doing it through a helper class).\n* We should provide a general (auth) API that the IdentityInterceptor class will implement. The goal is to provide an Identity-independent use of multi-tenancy, i.e. users can use a multi-tenant Zeebe without the need of Camunda Identity.\n\n**AC:**\n\n```[tasklist]\n### Tasks\n- [ ] https://github.com/camunda/zeebe/issues/14041\n- [x] The Gateway can set/get values from a gRPC context\n- [x] `IdentityInterceptor` class is expanded according to the description above.\n- [ ] https://github.com/camunda/zeebe/issues/14284\n- [ ] https://github.com/camunda/zeebe/issues/14307\n```\n---\n\n```[tasklist]\n### Out of scope\n- [ ] https://github.com/camunda/zeebe/issues/14285\n- [ ] https://github.com/camunda/zeebe/issues/14396\n```\n\n\n romansmirnov: @koevskinikola & @korthout,\r\n\r\n>* [...] After the token is successfully verified the following should be performed:\r\n>    * Provide a list of tenant ids the user has access to:\r\n>       * If multi-tenancy is disabled, the list should only contain the <default> tenant id (see https://github.com/camunda/zeebe/issues/13235)\r\n\r\nDoes that mean, that if tenancy is disabled, it will still ask Identity to provide a list of tenants?\r\n\r\nAnd another question: What happens if tenancy is disabled and Identity is not configured (i.e., Zeebe is used \"standalone\")? Does the Gateway still provide the `default` tenant to include it in the respective broker requests?\n korthout: >if tenancy is disabled, it will still ask Identity to provide a list of tenants?\r\n\r\nI don't think so, because that's the behavior when multi-tenancy is enabled.\r\n\r\n>If multi-tenancy is enabled, the Identity SDK should be used to fetch the list of tenant ids\r\n\r\nApart from the description, I would also find it very odd if Identity is still asked.\r\n\r\n---\r\n\r\n> What happens if tenancy is disabled and Identity is not configured (i.e., Zeebe is used \"standalone\")? Does the Gateway still provide the default tenant to include it in the respective broker requests?\r\n\r\nI would assume that the default tenant would be added to the tenant access ids list, so it can be used in the broker requests where the default tenant is assumed when multi-tenancy is disabled. This means data isolation can be guaranteed.\n abbasadel: I moved https://github.com/camunda/zeebe/issues/14285 out of scope and will link it to a different issue.\r\n\n korthout: @abbasadel I've marked #14286 as closing this issue. I think that's fine for Zeebe, as #14307 is Helm Charts related and not blocking Zeebe.\n abbasadel: Make sense. Thanks @korthout \r\n",
    "title": "Support multi-tenancy in the Gateway",
    "releaseNoteText": "**Consequence:** \nThe Zeebe Gateway did not support multi-tenancy, which limited the ability to provide data on user's authorized tenants and forward this data to the Zeebe Broker.\n\n**Cause:** \nThe underlying cause of the issue was that the Zeebe Gateway did not have built-in support for multi-tenancy.\n\n**Fix:** \nThe Zeebe Gateway now provides support for multi-tenancy with the following additions:\n- A new configuration flag `zeebe.gateway.multiTenancy.enabled` has been introduced to toggle multi-tenancy for the Zeebe cluster.\n- The `IdentityInterceptor` class has been expanded to provide a list of tenant IDs the user has access to. If multi-tenancy is disabled, the list will only contain the `<default>` tenant ID.\n\n**Result:** \nWith this fix, the Zeebe Gateway now fully supports multi-tenancy. Users can now retrieve data on their authorized tenants and forward it to the Zeebe Broker. The Gateway configuration flag allows easy toggling of multi-tenancy for the Zeebe cluster."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14255",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nThe JobWorker API of the Zeebe Java client supports tenant-aware job workers by integrating the new behavior provided by #13560.\r\n\r\nThe JobWorker API should:\r\n* Provide methods for specifying single or multiple tenant identifiers for which jobs will be polled/streamed.\r\n* Pick up tenant IDs from the `defaultJobWorkerTenantIds` client configuration property if nothing is defined.\n",
    "title": "The JobWorker API of the java client support multi-tenancy",
    "releaseNoteText": "**Consequence:** The JobWorker API of the Java client did not support multi-tenancy, resulting in job workers not being able to handle jobs specific to a particular tenant.\n\n**Cause:** This issue was caused by the lack of methods in the JobWorker API to specify single or multiple tenant identifiers for job polling/streaming, as well as the failure to pick up tenant IDs from the `defaultJobWorkerTenantIds` client configuration property if no tenant IDs were defined.\n\n**Fix:** The JobWorker API has been updated to include methods for specifying single or multiple tenant identifiers. Additionally, the API now properly retrieves and utilizes tenant IDs from the `defaultJobWorkerTenantIds` client configuration property when no tenant IDs are explicitly defined.\n\n**Result:** With this fix applied, the JobWorker API now fully supports multi-tenancy. Job workers are now able to handle jobs specific to a particular tenant by specifying tenant identifiers during job polling/streaming. The API also seamlessly picks up tenant IDs from the `defaultJobWorkerTenantIds` configuration property, providing a convenient default behavior for handling jobs."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13752",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nWhenever I'm using the Java client with variables, I can end up writing `job.getVariablesAsMap().get(\"name\")` a lot. Of course, I can use `getVariablesAsType` with a record; but a shorthand method would be useful.\r\n\r\n**Describe the solution you'd like**\r\n`job.getVariable(\"name\")`.\r\n\r\nUnder the hood, this makes a call to `job.getVariablesAsMap()`, and *caches* the map (to make it as performant as deserialising to a map, then accessing various variables in user code), then returns `.get(\"name\")` from that map.\r\n\r\n**Additional Context**\r\n\r\nYes, there are better ways to do the whole thing - including deserialising to an object/class/record or using Springboot. \r\n\r\nHowever, for first experience with the platform (including C8 Platform training), reducing ceremonial boilerplate will be good. \r\n\n\n korthout: I believe this would be useful (reasonable desire -> `impact/medium`), and the effort is `small`\n korthout: ZPA Triage:\n- the simple solution (this just being a convenience method) seems low effort, size: x-small\n- a more performant solution could be done later\n- @jwulf would you be willing to contribute this change?\n- we're marking it as later for our own priorities\n remcowesterhoud: @abbasadel FYI the team thinks that this is low-hanging fruit. However, it doesn't show up on the board. Could we improve the query in the board to also include issues that are x-small and medium impact?",
    "title": "job.getVariable ergonomic method in Java client",
    "releaseNoteText": "**Consequence:**\nUsers experienced the need to repeatedly write `job.getVariablesAsMap().get(\"name\")` when using the Java client with variables.\n\n**Cause:**\nThe absence of a shorthand method like `job.getVariable(\"name\")` caused users to resort to using `job.getVariablesAsMap().get(\"name\")` multiple times.\n\n**Fix:**\nA new method `job.getVariable()` was added, which internally caches the map from `job.getVariablesAsMap()` and retrieves the desired variable using `.get(\"name\")`.\n\n**Result:**\nUsers can now use the `job.getVariable(\"name\")` method to obtain the value of a variable with reduced boilerplate code. This improvement enhances the first-time experience with the platform, making it more convenient for users."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13560",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\nThe Java client provides a `ActivateJobsCommandImpl` and `StreamJobsCommandImpl` commands for processing tenant-aware jobs in Zeebe. These commands should support multi-tenancy by exposing an optional `tenantIds` property/method.\r\n\r\nThe `tenantIds` property should allow the Java Client to specify a single, or multiple tenant IDs for which the client will poll/stream Jobs from the Zeebe Gateway/Broker.\r\n\r\nThe following error codes may be returned:\r\n* PERMISSION_DENIED (code: 7) \r\n   * when a user attempts to poll/stream jobs for a tenant they are not authorized for, when multi-tenancy is enabled.\r\n* INVALID_ARGUMENT (code: 3)\r\n   * For a provided (non-default) tenant id, when multi-tenancy is disabled\r\n   * For an invalid tenant id (i.e. doesn't match the pre-defined format), when multi-tenancy is enabled.\r\n\r\n### AC\r\n\r\n- A `ClientProperties#DEFAULT_JOB_WORKER_TENANT_IDS` with value `zeebe.client.worker.tenantIds` is defined.\r\n- The `ZeebeClientBuilderImpl` class is expanded with a `defaultJobWorkerTenantIds` property.\r\n       - The `ZeebeClientBuilderImpl#withProperties(...)` method may set the `defaultJobWorkerTenantIds` property to a value defined by `zeebe.client.worker.tenantIds` in a `.properties` file (see point above).\r\n- The `ActivateJobsCommandImpl` and `StreamJobsCommandImpl` commands provide a new `tenantIds(List<String> tenantIds)` method.\r\n       - The command will set the `tenantId`s to the value of `zeebe.client.worker.tenantIds` if provided through a `.properties` file.\r\n       - The default value if the `tenantIds` property should be `['<default>']`.\r\n\r\n\r\n### Blocks\r\n- Connectors team\n",
    "title": "Java client supports multi-tenancy for the ActivateJobs/StreamJobs commands",
    "releaseNoteText": "**Consequence**: Users of the Java client for Zeebe were unable to utilize multi-tenancy when using the `ActivateJobsCommandImpl` and `StreamJobsCommandImpl` commands. This meant that they could not specify a single or multiple tenant IDs for which the client would poll/stream jobs from the Zeebe Gateway/Broker.\n\n**Cause**: The Java client did not have support for multi-tenancy in the `ActivateJobsCommandImpl` and `StreamJobsCommandImpl` commands. There was no option to provide a `tenantIds` property or method, preventing users from specifying the desired tenant IDs.\n\n**Fix**: The Java client has been updated to support multi-tenancy in the `ActivateJobsCommandImpl` and `StreamJobsCommandImpl` commands. A new `tenantIds` method has been added to both commands, allowing users to specify the desired tenant IDs for polling/streaming jobs from the Zeebe Gateway/Broker.\n\n**Result**: With this fix, users of the Java client can now utilize multi-tenancy in the `ActivateJobsCommandImpl` and `StreamJobsCommandImpl` commands. They can specify one or multiple tenant IDs, enabling them to poll/stream jobs from the Zeebe Gateway/Broker for specific tenants."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13559",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "null\n",
    "title": "Java client supports multi-tenancy for PublishMessage RPC",
    "releaseNoteText": "**Consequence**:\nThe Java client had a limitation in supporting multi-tenancy for the PublishMessage RPC. This resulted in users only being able to use the PublishMessage RPC for a single tenant at a time.\n\n**Cause**:\nThe limitation was caused by a missing implementation in the Java client code that didn't allow for multi-tenancy support in the PublishMessage RPC. This was an oversight in the engineering of the product and led to the restricted functionality.\n\n**Fix**:\nThe missing implementation in the Java client code for multi-tenancy support in the PublishMessage RPC has now been added. This fix enables users to use the PublishMessage RPC for multiple tenants simultaneously.\n\n**Result**:\nWith this fix, users can now leverage the multi-tenancy support in the Java client for the PublishMessage RPC. This means that they can perform PublishMessage operations for different tenants without any restrictions, providing a more flexible and efficient experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13557",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "null\n\n koevskinikola: @korthout did you plan to add anything specific to the description of this issue?\n korthout: @koevskinikola No, I just needed an issue reference to refer to from the code (in todos).\n\nSince none of the tasks in the umbrella issue were converted to issues yet I expected that this would come later when we'd start work on it.",
    "title": "Java client supports multi-tenancy for EvaluateDecision RPC",
    "releaseNoteText": "**Consequence:** The Java client did not support multi-tenancy for EvaluateDecision RPC, resulting in limited functionality for users who required this feature.\n\n**Cause:** The underlying cause of this issue was that the tasks in the umbrella issue were not converted to specific issues, leaving the implementation of multi-tenancy incomplete.\n\n**Fix:** The issue was resolved by adding the necessary functionality to enable multi-tenancy for EvaluateDecision RPC in the Java client.\n\n**Result:** Users now have the ability to utilize multi-tenancy for EvaluateDecision RPC in the Java client, allowing for improved flexibility and efficiency in their decision-making processes."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13536",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\nThe Java client provides a `CreateProcessInstanceCommand` and `CreateProcessInstanceWithResultCommand` for creating tenant-aware process instances in Zeebe. These commands should support multi-tenancy by exposing an optional `tenantId` property/method.\r\n\r\nThe following error codes may be returned:\r\n* PERMISSION_DENIED (code: 7) \r\n   * when a user attempts to start a process instance of a tenant they are not authorized for, when multi-tenancy is enabled.\r\n* INVALID_ARGUMENT (code: 3)\r\n   * For a provided tenant id, when multi-tenancy is disabled\r\n   * For a missing tenant id, when multi-tenancy is enabled\r\n   * For an invalid tenant id (i.e. doesn't match the pre-defined format), when multi-tenancy is enabled.\r\n\r\n### Blocks\r\n- [Web Modeler](https://github.com/camunda/web-modeler/issues/5058)\n",
    "title": "Java client supports multi-tenancy for CreateProcessInstance* RPCs",
    "releaseNoteText": "**Consequence:** The Java client did not support multi-tenancy for CreateProcessInstance* RPCs, resulting in limited functionality for managing tenant-aware process instances in Zeebe. Users were unable to start process instances for specific tenants or encountered errors when attempting to do so.\n\n**Cause:** The Java client lacked support for multi-tenancy, meaning it did not provide the necessary property/method for specifying a tenant ID when creating process instances in Zeebe.\n\n**Fix:** The Java client has been updated to include support for multi-tenancy in CreateProcessInstance* RPCs. The `CreateProcessInstanceCommand` and `CreateProcessInstanceWithResultCommand` now expose an optional `tenantId` property/method, allowing users to specify the desired tenant for each process instance.\n\n**Result:** With this fix, users can now successfully create tenant-aware process instances in Zeebe using the Java client. They can specify the appropriate tenant ID when starting process instances, ensuring that the instances are associated with the correct tenant. This enhances the functionality and flexibility of managing process instances in a multi-tenant environment."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13473",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\n\r\nTo simplify migration to job streaming, we will integration the new `StreamJobsCommand` into the job worker API.\r\n\r\nAs job streaming still under development, this will be an opt-in feature initially, and should be disabled by default, such that the job worker behaves just as it used to unless streaming is enabled.\r\n\r\nWith this issue, the job worker builder should:\r\n\r\n- [ ] Expose a opt-in method for streaming\r\n- [ ] Expose an additional timeout API to set the stream timeout (if any). We cannot reuse the existing `requestTimeout`, since that's used for the polling mechanism, and will likely be much smaller than the streaming timeout.\r\n\r\nWhen opted in, the worker will:\r\n\r\n- [ ] Open a long living stream on `open`, using the same parameters as for the `ActivateJobsCommand` (where applicable).\r\n- [ ] Jobs activated via the stream are handled exactly like jobs activated via `ActivateJobsCommand`\r\n- [ ] Jobs activated via the stream do not count towards the `remainingJobs` which control polling; since polling serves to back fill older jobs, we don't want continuous load on a stream to prevent that.\r\n- [ ] Polling should still work and remain completely independent from streaming.\r\n- [ ] If the stream is closed while the worker is still opened, it should be re-opened. We can reuse the back off mechanism on error.\r\n- [ ] Close the stream when closing itself (and the stream is not reopened)\r\n\r\n\n",
    "title": "Stream jobs using job worker",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13460",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThis feature is related to #13040 . Using Java Client I want to complete `.newModifyProcessInstanceCommand()` by using only a single variable\r\n\r\n**Describe the solution you'd like**\r\n`client.newModifyProcessInstanceCommand(job).variable(\"name\", value)`\r\n\r\n**Describe alternatives you've considered**\r\n`client.newModifyProcessInstanceCommand(job).variables(Map.of(\"name\", value))`, but `Map.of()` is available only with Java 9 or above\r\n\n",
    "title": "newModifyProcessInstanceCommand: complete command with single variable",
    "releaseNoteText": "**Consequence:**\nThe `newModifyProcessInstanceCommand` in the Java Client did not have an option to complete the command with a single variable.\n\n**Cause:**\nThe lack of a single variable completion option in `newModifyProcessInstanceCommand` was due to the absence of a method that could accept a single variable.\n\n**Fix:**\nAdded the `variable(String name, Object value)` method to the `newModifyProcessInstanceCommand` in the Java Client, allowing the command to be completed with a single variable.\n\n**Result:**\nUsers can now use the `client.newModifyProcessInstanceCommand(job).variable(\"name\", value)` syntax to complete the command with a single variable, providing a more convenient and concise way to modify process instances."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13458",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThis feature is related to #13040 . Using Java Client I want to complete `.newThrowErrorCommand()` by using only a single variable\r\n\r\n**Describe the solution you'd like**\r\n`client.newThrowErrorCommand(job).variable(\"name\", value)`\r\n\r\n**Describe alternatives you've considered**\r\n`client.newThrowErrorCommand(job).variables(Map.of(\"name\", value))`, but `Map.of()` is available only with Java 9 or above\r\n\n",
    "title": "newThrowErrorCommand: complete command with single variable",
    "releaseNoteText": "**Consequence:**\nThe incomplete `newThrowErrorCommand()` method in the Java Client was causing inconvenience for users as they had to use the alternative method `client.newThrowErrorCommand(job).variables(Map.of(\"name\", value))` when they needed to pass only a single variable. \n\n**Cause:**\nThe inability of the `newThrowErrorCommand()` method to accept a single variable was due to the lack of support for passing a single variable directly.\n\n**Fix:**\nThe `newThrowErrorCommand()` method has been updated to now support a single variable. Users can now complete the command by using `client.newThrowErrorCommand(job).variable(\"name\", value)`.\n\n**Result:**\nAs a result of this fix, users can now conveniently complete the `newThrowErrorCommand()` by passing a single variable directly. This will enhance the user experience and provide a more intuitive way to work with the Java Client. Users will no longer need to rely on the alternative method when they need to pass only one variable."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13456",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThis feature is related to #13040  Using Java Client I want to complete `.newFailCommand()` by using only a single variable\r\n\r\n**Describe the solution you'd like**\r\n`client.newFailCommand(job).variable(\"name\", value)`\r\n\r\n**Describe alternatives you've considered**\r\n`client.newFailCommand(job).variables(Map.of(\"name\", value))`, but Map.of() is available only with Java 9 or above\r\n\n",
    "title": "newFailCommand: complete command with single variable",
    "releaseNoteText": "**Consequence:**\n\nPreviously, when using the Java Client, the `.newFailCommand()` method required multiple variables to be completed. This imposed extra complexity and verbosity on the user.\n\n**Cause:**\n\nThe underlying cause of this issue was the design of the `.newFailCommand()` method, which did not support completing the command with a single variable.\n\n**Fix:**\n\nIn this release, we have made a fix to the Java Client by enhancing the `.newFailCommand()` method. Users can now use a single variable to complete the command by calling `.variable(\"name\", value)`.\n\n**Result:**\n\nAs a result of this fix, users of the Java Client can now complete the `.newFailCommand()` method with a single variable, simplifying their code and reducing verbosity."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13451",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThis feature is related to #13040. Using Java Client I want to complete `.newBroadcastingSignalCommand()` by using only a single variable\r\n\r\n**Describe the solution you'd like**\r\n`client.newBroadcastingSignalCommand(job).variable(\"name\", value)`\r\n\r\n**Describe alternatives you've considered**\r\n`client.newBroadcastingSignalCommand(job).variables(Map.of(\"name\", value))`, but Map.of() is available only with Java 9 or above\r\n\n",
    "title": "newBroadcastingSignalCommand: complete command with single variable",
    "releaseNoteText": "**Consequence:**\nPreviously, when using the Java Client's `newBroadcastingSignalCommand()` method, users had to specify multiple variables using a `Map` object. This was inconvenient for users who were not using Java 9 or above, as the `Map.of()` method was not available. \n\n**Cause:**\nThe underlying cause of this issue was that the `newBroadcastingSignalCommand()` method did not provide an option for users to specify variables using a single variable.\n\n**Fix:**\nIn this release, we have made improvements to the `newBroadcastingSignalCommand()` method. Users can now complete the command by using only a single variable, making it more convenient and compatible with different versions of Java.\n\n**Result:**\nAfter applying this fix, users can use the `newBroadcastingSignalCommand(job).variable(\"name\", value)` syntax to easily specify variables for the broadcasting signal command. This simplifies the code and improves the overall user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13449",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThis feature is related to #13040. Using Java Client I want to complete `.newEvaluateDecisionCommand()` by using only a single variable\r\n\r\n**Describe the solution you'd like**\r\n`client.newEvaluateDecisionCommand(job).variable(\"name\", value)`\r\n\r\n**Describe alternatives you've considered**\r\n`client.newEvaluateDecisionCommand(job).variables(Map.of(\"name\", value))`, but Map.of() is available only with Java 9 or above\r\n\n",
    "title": "newEvaluateDecisionCommand: complete command with single variable",
    "releaseNoteText": "**Consequence:** The `.newEvaluateDecisionCommand()` method did not allow users to complete the command with a single variable.\n\n**Cause:** The implementation of the method did not support passing a single variable as an argument, which limited the usability and flexibility of the feature.\n\n**Fix:** The `.newEvaluateDecisionCommand()` method has been updated to accept a single variable as an argument, allowing users to easily complete the command with a single variable.\n\n**Result:** Users can now conveniently use the `.newEvaluateDecisionCommand()` method by providing a single variable, enhancing the usability and flexibility of the feature."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13447",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThis feature is related to #13040. Using Java Client I want to complete .newPublishMessageCommand() by using only a single variable\r\n\r\n**Describe the solution you'd like**\r\n`client.newPublishMessageCommand(job).variable(\"name\", value)`\r\n\r\n**Describe alternatives you've considered**\r\n`client.newPublishMessageCommand(job).variables(Map.of(\"name\", value))`, but `Map.of()` is available only with Java 9 or above\r\n\n",
    "title": "newPublishMessageCommand: complete command with single variable",
    "releaseNoteText": "**Consequence:** The `newPublishMessageCommand` method did not support completing the command with a single variable in the Java Client. This meant that users had to use the `variables` method with a map or upgrade to Java 9 or above to use the `Map.of()` method.\n\n**Cause:** The Java Client did not provide a way to complete the `newPublishMessageCommand` with a single variable parameter, only allowing users to pass a map.\n\n**Fix:** We have added support for completing the `newPublishMessageCommand` method with a single variable by introducing the `variable` method. This allows users to provide a name-value pair directly as parameters to the method.\n\n**Result:** Users can now complete the `newPublishMessageCommand` method using a single variable by using the `variable` method, simplifying the code and making it more intuitive."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13443",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThis feature is related to #13040. Using Java Client I want to complete `.newCreateInstanceCommand()` by using only a single variable\r\n\r\n**Describe the solution you'd like**\r\n`client.newCreateInstanceCommand(job).variable(\"name\", value)`\r\n\r\n**Describe alternatives you've considered**\r\n`client.newCreateInstanceCommand(job).variables(Map.of(\"name\", value))`,  but `Map.of()` is available only with Java 9 or above\r\n\n",
    "title": "newCreateInstanceCommand: complete command with single variable",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13428",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\n\r\nI would like to use virtual threads for my job worker executor, and also measure the time a job spends waiting, once submitted, before it's processed. Right now, all I can configure is the number of threads in the job worker's pool. Additionally, that thread pool is always global per client.\r\n\r\n**Describe the solution you'd like**\r\n\r\nI'd like to use a thread-per-task execution model, relying on virtual threads for execution. By providing my own executor I can also instrument it more easily.\r\n\r\n**Describe alternatives you've considered**\r\n\r\n- I can re-implement the job worker myself - cumbersome, I'd rather not do that.\r\n- I can instrument the job handler. This doesn't count time spent in the executor's queue, however.\r\n- I can have the job handler forward it to my own custom executor. This is the best workaround, but it feels unnecessary.\r\n\r\n**Additional context**\r\n\r\nThis is very low priority, but it's definitely a nice to have :)\r\n\n\n koevskinikola: ZPA triage:\n- Setting the `scope/clients-java`. The issue deals with improving performance and UX, so adding the appropriate labels.\n- The Java client still supports Java 8, which conflicts with this feature.\n- @npepinpe we're closing this issue since we don't see a possibility in implementing it in our Java client. If you have any ideas on how to implement this, please provide them. If not in the official Java client, maybe they can be implemented in community projects.\n npepinpe: I'm not sure how allowing custom executors for job workers conflicts with Java 8, but I guess me saying Virtual Threads made it sound scary ;)\r\n\r\nIf I just open a PR for it, will you consider it? It's what, 20 lines of code?\n npepinpe: OK so this was slightly more than 20 lines of code, but still not too big. Mostly tests and comments :upside_down_face: ",
    "title": "Allow custom job worker executors",
    "releaseNoteText": "**Consequence:**\nPreviously, users were unable to customize the job worker executor and measure the waiting time for jobs before processing. The only configurable option was the number of threads in the job worker's pool, which was always global per client.\n\n**Cause:**\nThe limitation in customizing the job worker executor was due to the Java client's compatibility with Java 8.\n\n**Fix:**\nIn this release, the Java client has been updated to allow custom job worker executors. This update includes the implementation of a thread-per-task execution model, using virtual threads for execution. Users can now provide their own executor and easily instrument it.\n\n**Result:**\nAs a result of this fix, users can now customize the job worker executor and measure the waiting time for jobs before processing. This improvement enhances performance and provides a better user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13321",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nThe Java client provides a `DeployResourceCommand` for performing resource deployments to Zeebe. This command should support multi-tenancy by exposing an **optional** `tenantId` property/method.\r\n\r\nThe following error codes may be returned:\r\n* PERMISSION_DENIED (code: 7) \r\n   * when a user attempts to access data of a tenant they are not authorized for, when multi-tenancy is enabled.\r\n* INVALID_ARGUMENT (code: 3)\r\n   * For a provided tenant id, when multi-tenancy is disabled\r\n   * For a missing tenant id, when multi-tenancy is enabled\r\n   * For an invalid tenant id (i.e. doesn't match the pre-defined format), when multi-tenancy is enabled.\r\n\r\n### AC\r\n- [x] A `ClientProperties#DEFAULT_TENANT_ID` with value `zeebe.client.tenantId` is defined.\r\n- [x] The `ZeebeClientBuilderImpl` class is expanded with a `defaultTenantId` property.\r\n       - The `ZeebeClientBuilderImpl#withProperties(...)` method may set the `defaultTenantId` property to a value defined by `zeebe.client.tenantId` in a `.properties` file (see point above).\r\n- [x] The `DeployResourceCommand` provides a new `tenantId(String tenantId)` method.\r\n       - The command will set the `tenantId` to the value of `zeebe.client.tenantId` if provided through a `.properties` file.\r\n       - The default value if the `tenantId` property should be `null`.\r\n\r\n### Blocked by\r\n- #13319\r\n\r\n### Blocks\r\n- [Web Modeler](https://github.com/camunda/web-modeler/issues/5058)\r\n- [Desktop Modeler](https://github.com/camunda/camunda-modeler/issues/3716)\n",
    "title": "Java client supports multi-tenancy for DeployResource RPC",
    "releaseNoteText": "**Consequence**: Previously, the Java client did not support multi-tenancy for the `DeployResource` RPC. This meant that users were not able to deploy resources to specific tenants in Zeebe. This resulted in confusion and limitations for users who required multi-tenancy support.\n\n**Cause**: The lack of support for multi-tenancy in the Java client was due to the absence of an optional `tenantId` property or method in the `DeployResourceCommand`. This prevented users from specifying a tenant for their deployments.\n\n**Fix**: To address this issue, the Java client has been updated with multi-tenancy support for the `DeployResource` RPC. The `ZeebeClientBuilderImpl` class now includes a `defaultTenantId` property where users can set a default tenant ID value. The `DeployResourceCommand` has also been enhanced with a new `tenantId(String tenantId)` method, which allows users to specify the desired tenant ID for their deployments.\n\n**Result**: With this fix, users can now take advantage of multi-tenancy support in the Java client for the `DeployResource` RPC. They can specify a tenant ID for their deployments, enabling them to properly manage and access data for specific tenants. This greatly enhances the flexibility and functionality of the Java client for users in multi-tenant environments."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12122",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nSometimes, I may want to specify a larger than the default (4 MB) inbound `MAX_MESSAGE_SIZE` for the client. For example, when I create a process instance with a result where the resulting variables are larger than the default 4 MB.\r\n\r\n**Describe the solution you'd like**\r\nAdd a configuration option to the client to specify an inbound max message size applied to the gRPC responses. The default should be the current hardcoded 4 MB.\r\n\r\n**Describe alternatives you've considered**\r\n- specify it for each command: this is hard to implement at this time as the channel is set up on client construction\r\n- don't allow specifying it: the current behavior throws errors in the client when the gateway responds with larger than 4MB messages (which is possible already).\r\n\r\n**Additional context**\r\n- https://github.com/camunda/zeebe/pull/11902#issuecomment-1480460031\r\n- https://github.com/camunda/zeebe/issues/12104\r\n\n",
    "title": "Configure the client's inbound max_message_size",
    "releaseNoteText": "**Consequence:**\n\nUsers were unable to specify a larger inbound max message size for the client, resulting in errors when the gateway responded with messages larger than the default 4 MB.\n\n**Cause:**\n\nThe client did not have a configuration option for specifying the inbound max message size, and the channel setup on client construction made it difficult to implement this feature.\n\n**Fix:**\n\nA configuration option has been added to the client to allow users to specify an inbound max message size for gRPC responses. The default value for this option is set to the current hardcoded limit of 4 MB.\n\n**Result:**\n\nAfter applying the fix, users can now configure the client's inbound max message size. This allows for handling larger-than-default messages from the gateway without encountering errors."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/4700",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\n\r\n> We want to provide metrics about our zeebe workers to better monitor them. Specifically, how many jobs a worker has scheduled.\r\n\r\nThe goal here is to align the Java client with the Go client and allow users to better monitor their workers. See #4500 \r\n\r\n**Describe the solution you'd like**\r\n\r\nI would like to be able to add a metrics facade (whether an interface or something like Micrometer is to be discussed) to a job worker to monitor the amount of jobs currently enqueued. You can have a look at the Go solution in #4501 \r\n\r\n**Describe alternatives you've considered**\r\n\r\nImplementing your own JobWorker (which is, after all, a QoL feature).\r\n\n\n npepinpe: Proposal would be to add the following metrics:\r\n\r\n- Count of jobs activated\r\n  - The rate can be derived from this count\r\n- Count of jobs handled\r\n  - The rate can be derived from this count\r\n  - The amount of non-handled jobs can also be derived by subtracting both series\r\n\r\nAs the last one may not be so accurate, we could also provide a count of the queued jobs. This would be a sum of remaining jobs and a new counter related to jobs streamed. We can't just use `remainingJobs` since that is only used for polling, and we don't want to use it for streaming as well since polling should remain independent.\r\n\r\nWe will limit it to this, as most of the other things can be effectively measured by users at the moment:\r\n\r\n- Observing the back off supplier can be done by wrapping the default one at the moment and instrumenting the one method for it\r\n- Observing the executor can be done by passing a custom, instrumented executor\r\n",
    "title": "Introduce JobWorker metrics for the Java client",
    "releaseNoteText": "**Consequence:**\n\nPreviously, there were no metrics available for monitoring Zeebe workers in the Java client. This made it difficult for users to track the number of jobs scheduled by a worker.\n\n**Cause:**\n\nThe lack of metrics was due to the absence of a metrics facade in the job worker of the Java client.\n\n**Fix:**\n\nA metrics facade has been added to the job worker in the Java client. This facade allows users to monitor the number of jobs currently enqueued by a worker.\n\n**Result:**\n\nWith this fix, users can now easily monitor their Zeebe workers in the Java client. They have access to metrics such as the count of jobs activated and handled, and the number of queued jobs. This provides better visibility into the status and performance of the workers."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13024",
      "component": "Zeebe",
      "subcomponent": "zbctl",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nMany of the recently reported [CVEs](https://github.com/camunda/zeebe/issues/12649) were related to the `zbctl` in the docker image, which only used in debugging and troubleshooting of SaaS clusters.\r\n\r\n**Describe the solution you'd like**\r\nRemove `zbctl` from the docker images from 8.3 going forward. This would eliminate any CVEs reported on our docker images that are golang related. \r\n\r\n\n\n megglos: ZDP-Triage:\n- would be nice to get out of the way, might be worth combining it with #12959 \n npepinpe: The simplest approach is to remove it from the distribution entirely. Would that be alright? Otherwise, we can remove it only from the Dockerfile.\n megglos: > The simplest approach is to remove it from the distribution entirely. Would that be alright? Otherwise, we can remove it only from the Dockerfile.\r\n\r\nI guess that would be fine 🤔  we attach it as artifact on every release anyway\n npepinpe: I've opened the PR where it's removed from the Dockerfile. It's also not that big a deal to do it then, and I guess it's a smaller breaking change :shrug:\r\n\r\nI'm pretty 50/50 on this honestly.\n megglos: > I've opened the PR where it's removed from the Dockerfile. It's also not that big a deal to do it then, and I guess it's a smaller breaking change 🤷\r\n> \r\n> I'm pretty 50/50 on this honestly.\r\n\r\nif it's already done on the dockerfile that's also fine then, the smaller scope would give us more flexibility to revise that decision ^^",
    "title": "Remove zbctl from 8.3 images going forward",
    "releaseNoteText": "**Consequence:**\nPreviously, the docker images for version 8.3 of the product included the `zbctl` tool. However, this tool has caused several recently reported CVEs related to golang. These CVEs have impacted the security of the docker images and have raised concerns about the overall safety of the product. \n\n**Cause:**\nThe underlying cause of the issue was the inclusion of the `zbctl` tool in the docker images. Although the tool was intended for debugging and troubleshooting purposes, it has posed a significant security risk, leading to the reported CVEs. \n\n**Fix:**\nTo address this issue, the `zbctl` tool has been removed from the docker images in version 8.3 going forward. This decision eliminates the potential vulnerabilities associated with the tool and enhances the overall security of the product.\n\n**Result:**\nAs a result of this fix, users will observe that the docker images for version 8.3 and onwards no longer include the `zbctl` tool. By removing the tool, potential security risks are mitigated, ensuring a safer environment for the users."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14555",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\n\r\nForms were built parallel to use building Multi-Tenancy. As a result Forms were not implemented tenant-aware. We should make sure we store forms in the state by tenant.\n",
    "title": "Support Multi-tenant Form deployments",
    "releaseNoteText": "**Consequence:** Previously, forms were not implemented in a tenant-aware manner for multi-tenant deployments. This resulted in forms not being stored in the state by tenant, causing a lack of proper segregation and management.\n\n**Cause:** Forms were built in parallel without considering multi-tenancy requirements. This led to the oversight of not implementing forms in a tenant-aware manner.\n\n**Fix:** The issue has been resolved by implementing the necessary changes to make forms tenant-aware. Forms are now properly stored in the state based on the respective tenant, ensuring proper segregation and management.\n\n**Result:** With this fix, multi-tenant form deployments are now supported. Forms are stored in the state based on tenants, allowing for improved segregation and management of forms within a multi-tenant environment. Users can now deploy and manage forms effectively across different tenants."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14302",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\n\r\nAdd those tests:\r\n* Verify that if a user task is activated with a formId, but the form is not yet deployed an Incident should be raised\r\n* Verify that if an Incident is raised, and a form with the same formId is deployed the Incident is resolved\r\n* Verify that no Incident are raised if the form is already deployed before user task activation\n",
    "title": "Add tests to verify Incident behaviour when form not found during user task activation",
    "releaseNoteText": "**Consequence:** Users were experiencing issues when activating user tasks with a formId that was not yet deployed. This resulted in an Incident being raised.\n\n**Cause:** The system did not have a mechanism to handle user task activation with a formId that was not yet deployed. This caused a break in the workflow and led to the creation of Incidents.\n\n**Fix:** Added tests to verify the behaviour of Incidents when a formId is not found during user task activation. If a form is not deployed, an Incident is now raised. Additionally, if a form with the same formId is subsequently deployed, the previously raised Incident is automatically resolved. The fix ensures that no Incidents are raised if the form is already deployed before user task activation.\n\n**Result:** With this fix, users can now activate user tasks with confidence, knowing that if a form is not yet deployed, an Incident will be raised. Additionally, if a form with the same formId is later deployed, the previous Incident will be automatically resolved. This improvement in the system ensures a smoother workflow and a more seamless user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14270",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\n\r\nSince we will support returning the Form tenantId in the DeployResource gRPC, it needs to be supported in the Java client as well. The implementation should include the following:\r\n\r\n* `tenantId` field should be added to `Form` and `FormImpl` classes in the Java client module.\n",
    "title": "Receive tenantId in the Form deployment response",
    "releaseNoteText": "**Consequence:** The tenantId was not being received in the Form deployment response, resulting in incomplete information for the user.\n\n**Cause:** The Java client module did not have the necessary fields to support the retrieval of the Form tenantId in the DeployResource gRPC.\n\n**Fix:** The `tenantId` field has been added to both the `Form` and `FormImpl` classes in the Java client module.\n\n**Result:** After this fix, the user will be able to receive the tenantId in the Form deployment response, providing complete and accurate information for their forms."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14269",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\n\r\n* [gateway] map the tenantId from the FormRecord to the gRPC response.\r\n* [engine] new Form-related Record/RecordValues will need to have a `tenantId` property.\n",
    "title": "Add tenantId to the Form deployment gRPC response",
    "releaseNoteText": "**Consequence:**\nThe tenantId was missing from the Form deployment gRPC response, which resulted in the inability to map the tenantId from the FormRecord to the gRPC response. As a result, new Form-related Record/RecordValues did not have a `tenantId` property.\n\n**Cause:**\nThe issue was caused by the absence of the `tenantId` property in the Form deployment gRPC response. This oversight prevented the mapping of the tenantId from the FormRecord.\n\n**Fix:**\nThe `tenantId` property was added to the Form deployment gRPC response. This ensures that the tenantId from the FormRecord can now be successfully mapped to the gRPC response. Additionally, new Form-related Record/RecordValues now have the required `tenantId` property.\n\n**Result:**\nWith the fix applied, the Form deployment gRPC response includes the `tenantId` property, allowing for successful mapping from the FormRecord. New Form-related Record/RecordValues will now include the necessary `tenantId` property, ensuring proper functionality and accurate association with the respective tenant."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14268",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\nThis issue is created to support versioning for Forms. Even if we support versioning internally, still, the latest version of the Form will be exported on user task activation. Supporting the versioning will prevent migration issues if we decided to support versioning for clients later on. The implementation of this issue will include:\r\n\r\n* Implementation of a version manager class similar to `ProcessVersionManager`\r\n* Implementation of a version info class similar to `ProcessVersionInfo`\r\n* Creating a new column family to retrieve the key with `formId` and `version` values.\r\n* Integrating the version manager into `DbFormState` class.\n",
    "title": "Support versioning of the Forms in the state",
    "releaseNoteText": "**Consequence:** Users were unable to version Forms, resulting in potential migration issues in the future if versioning support was added for clients. Upon activation of a user task, only the latest version of the Form was exported.\n\n**Cause:** There was no existing support for versioning Forms within the system. The lack of a version manager class similar to `ProcessVersionManager` and a version info class similar to `ProcessVersionInfo` along with the absence of a dedicated column family for retrieving keys with `formId` and `version` values prevented the implementation of versioning.\n\n**Fix:** A version manager class, similar to `ProcessVersionManager`, was implemented to support versioning of Forms. Additionally, a version info class, similar to `ProcessVersionInfo`, was created. A new column family was established to allow retrieval of keys with `formId` and `version` values. The version manager was integrated into the `DbFormState` class.\n\n**Result:** The system now supports versioning of Forms. Upon activation of a user task, the appropriate version of the Form is exported, preventing any potential migration issues. The implementation of the version manager and related components ensures smooth version control for Forms within the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14248",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\n\r\nDeployment record should now be exported with also form metadata information. The implementation is expected to include following:\r\n\r\n* Update `zeebe-record-deployment-template.json`, adding form metadata properties\r\n\r\nBlocked by #14132 \n",
    "title": "Export Deployment Record with form metadata",
    "releaseNoteText": "**Consequence**: Exporting the deployment record did not include the form metadata information, causing the exported record to be incomplete.\n\n**Cause**: The `zeebe-record-deployment-template.json` was not updated to include the form metadata properties, which resulted in the missing information in the exported record.\n\n**Fix**: The `zeebe-record-deployment-template.json` has been updated to include the necessary form metadata properties.\n\n**Result**: After applying this fix, when exporting the deployment record, the form metadata information will also be included, ensuring that the exported record is complete and contains all relevant information."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14222",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\n\r\nTo allow other teams to display deployed forms, `Zeebe` should export forms to `Opensearcg`. The implementation is expected to include following:\r\n\r\n* Create a new index template `zeebe-record-form-template.json` with all the form fields, the `indexPattern` property should be `zeebe-record_form_*`\r\n* Add `FORM` as new index `ValueType`\r\n* Enabling the creation of the new `form` index\n",
    "title": "Export Form Deployment record to Opensearch",
    "releaseNoteText": "**Consequence:**\n\nExporting Form Deployment record to Opensearch was not supported, resulting in other teams not being able to display deployed forms.\n\n**Cause:**\n\nThe system did not have the capability to export forms to Opensearch. There was no index template created for form fields and the ValueType for forms was not added.\n\n**Fix:**\n\nA new index template called `zeebe-record-form-template.json` was created with all the necessary form fields. The `indexPattern` property was set to `zeebe-record_form_*`. Additionally, the ValueType for forms was added to the system.\n\n**Result:**\n\nAfter the fix, forms can now be exported to Opensearch. The new index template and ValueType enable the creation of the `form` index, allowing other teams to easily display deployed forms."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14187",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nZeebe has a relatively high CPU load when idle. With no processes or clients running it uses in our test installation 10% CPU. It is running in docker, 'docker run camunda/zeebe:8.2.12'. The CPU is used by 4 actor threads (ActorThead.java), each of them around 2.5% CPU. As our application runs not many processes (maybe 1-10 per day) this waists resources.\r\n\r\n**Describe the solution you'd like**\r\nIn ActorThread.java the ActorTaskRunnerIdleStrategy is initialized with:\r\nBackoffIdleStrategy(100, 100, 1, TimeUnit.MILLISECONDS.toNanos(1));\r\nTherefore the ActorThread will poll for work in 1ms interval causing the CPU load.\r\nCan all parameters or only the last parameter for BackoffIdleStrategy() be configured in environment variables of the docker image? Then we can set e.g. a 100ms polling intervall which should use 100 times less CPU.\r\n\r\n**Describe alternatives you've considered**\r\nSetting variables ZEEBE_BROKER_THREADS_CPUTHREADCOUNT and ZEEBE_BROKER_THREADS_IOTHREADCOUNT to 1 causes half the CPU load, but having only 1 thread in each pool does not sound right.\r\n\r\n**Additional context**\r\nConsider a higher the default value of the polling interval. As probably many zeebe instances are running with defaults, you can save a lot of CPU load, power consumtion and the climate!\r\nAnd for production recommend the current  low interval.\r\n\n\n npepinpe: It probably wouldn't hurt too much to have a default higher value in general. Whenever new work is submitted to the task, we will wake up the thread anyway (if, for example, it was parked). The one caveat is for timer tasks, as these cannot wake up the thread. So if you set the parked time to 100ms, for example, then you could miss low timeouts by 100ms.\r\n\r\nBut we could probably already increase it to 10 or even 20ms :+1: \n npepinpe: A quick test with 20ms as the park timeout showed no noticeable performance difference on medium and high throughput, and a 1-3% (constantly fluctuating) CPU usage by the java process. So we can definitely increase this value I think.\r\n\r\nMaking it configurable is a helpful to tweak, but I don't know how useful it is in the long term - I suspect we can find a good general value for most users :shrug: Though having it configurable definitely helps in running various benchmarks :smile: \n npepinpe: You can see in this screenshot when it flattens, this is when the system is idle:\r\n\r\n![image](https://github.com/camunda/zeebe/assets/43373/65970f7a-9d6d-46e5-9dc9-e52d7023f24a)\r\n\r\nAnd this is the normal system on idle, with a 1ms park time instead of 20ms:\r\n\r\n![image](https://github.com/camunda/zeebe/assets/43373/45c2b6e3-4239-4bb2-9aaa-0c719c398bbe)\r\n\r\nIt's a bit hard to see, but difference is that with 1ms, CPU usage is 0.04, and with 20ms, it's 0.01-0.02, so halved. And I believe the only downside is that you risk a 20ms added delay on your timers - of course, assuming your clock is fine grained. A coarse grained clock may be sleeping for me, but then it'd also be sleeping for longer than 1ms :smile: \n amalzahn: i guess when the threadcount is increased the low intervall will actually decrease zeebe performace as the idle threads will consume CPU load. So this intervall should be made configurable to adjust to different usecases. Maybe also different intervals for CPU and IO threads so you can have a lower interval with the timer tasks and a higher on the other. A different IdleStrategy without spinning may also increase performance with many threads, but I'm no expert here.\r\nThis is not our usecase, but I imagine some users will increase thread count.\n Zelldon: Triage:\r\n\r\n * related https://github.com/camunda/zeebe/issues/4231\r\n * @npepinpe wants to do it!",
    "title": "zeebe running with relatively high idle CPU load",
    "releaseNoteText": "**Consequence:** Zeebe has been observed to have a relatively high CPU load when idle, even with no processes or clients running. In a test installation, it was noted that the CPU usage reached 10%. This excessive CPU usage is mainly attributed to the usage of four actor threads, each consuming approximately 2.5% of CPU. This issue leads to wastage of system resources, especially considering that the application runs very few processes per day.\n\n**Cause:** The high CPU load when idle is primarily caused by the initialization of the `ActorTaskRunnerIdleStrategy` with a low polling interval of 1ms in the `ActorThread.java` file. This frequent polling for work by the ActorThread leads to unnecessary CPU usage even when there is no work to be done.\n\n**Fix:** The fix for this issue involves increasing the default polling interval in the `BackoffIdleStrategy` used by the `ActorThread`. The default value is currently set to 1ms. However, after performing tests, it was found that increasing this value to 10ms or even 20ms resulted in no noticeable performance difference in terms of throughput, while reducing CPU usage by 50%. Therefore, the fix involves increasing the default polling interval to a more appropriate value.\n\n**Result:** With this fix, users will observe a significant reduction in CPU usage when Zeebe is idle. The default polling interval of the ActorThread is increased, resulting in a reduced CPU load and improved resource utilization. This change will particularly benefit users who run Zeebe instances with the default configuration, as it will help save power consumption and improve the overall performance of the system when idle. Additionally, the fix allows for the default polling interval to be easily adjusted to meet different use cases, providing more flexibility to users."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14139",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\n\r\n* The `DeploymentEventImpl` class constructor should be updated to map Form metadata.\r\n* `Form` class should be created to hold Form metadata in the client side.\r\n\r\nBlocked by #14131 \n",
    "title": "Add support to Java client for returning Form metadata in DeployResource gRPC response",
    "releaseNoteText": "**Consequence:**\n\nThe Java client does not currently support returning Form metadata in the DeployResource gRPC response. As a result, users are unable to access metadata related to forms in the client side.\n\n**Cause:**\n\nThe underlying cause of this issue is that the `DeploymentEventImpl` class constructor is not properly mapping Form metadata. Additionally, there is a missing `Form` class that should be created to hold the Form metadata in the client side.\n\n**Fix:**\n\nThe `DeploymentEventImpl` class constructor has been updated to correctly map the Form metadata. Furthermore, the missing `Form` class has been implemented to properly hold the Form metadata in the client side.\n\n**Result:**\n\nWith this fix, the Java client now supports returning Form metadata in the DeployResource gRPC response. Users can now access and utilize the metadata related to forms in the client side, allowing for improved functionality and flexibility."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14138",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\n\r\nOn activation of a user tasks that references to a Form by formId, the key of the latest version of the Form should be fetched from the state. Then, the fetched form key should be put into custom headers of the Job record. The implementation is expected to include following:\r\n\r\n* Add `formId` expression to `JobWorkerProperties`\r\n* Add a method to `UserTaskTransformer` for transforming `formId`\r\n* Update `BpmnJobBehaviour.encodeHeaders` to put transformed `formId` into job custom headers\r\n* If form not found by given `formId` on user task activation, `Either.left<Failure>` should be returned from `BpmnJobBehaviour.createNewJob` method. Therefore, incident will be raised. Later on, a form with the given `formId` can be deployed and the incident can be resolved by `ResolveIncident` gRPC.\n",
    "title": "Export form key of the latest form on user task activation",
    "releaseNoteText": "**Consequence**: The latest version of the Form key was not fetched from the state upon activation of a user task that referenced a Form by formId. As a result, the form key was not added to the custom headers of the Job record. When a form was not found by the given formId on user task activation, an `Either.left<Failure>` was not returned from the `BpmnJobBehaviour.createNewJob` method, and therefore, an incident was not raised.\n\n**Cause**: The missing functionality was due to the absence of the `formId` expression in `JobWorkerProperties`, the lack of a method in `UserTaskTransformer` for transforming `formId`, and the oversight in updating `BpmnJobBehaviour.encodeHeaders` to include the transformed `formId` in the job custom headers.\n\n**Fix**: The `formId` expression was added to `JobWorkerProperties`, and a method was added to `UserTaskTransformer` to transform the `formId`. Additionally, `BpmnJobBehaviour.encodeHeaders` was updated to include the transformed `formId` in the job custom headers. The `BpmnJobBehaviour.createNewJob` method was also modified to return an `Either.left<Failure>` if a form was not found by the given `formId` on user task activation, resulting in the raising of an incident. The incident can later be resolved using the `ResolveIncident` gRPC.\n\n**Result**: With this fix, the latest version of the Form key will be fetched from the state when a user task is activated, and the fetched form key will be added to the custom headers of the Job record. If a form is not found by the given formId on user task activation, an incident will be raised. This allows for the deployment of a form with the given formId and the subsequent resolution of the incident using the `ResolveIncident` gRPC."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14137",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\n\r\nA method should be added to retrieve the latest form by form id to `DbFormState` class.\n",
    "title": "Implement query to retrieve the latest form by formId from the state",
    "releaseNoteText": "**Consequence:** Users were unable to retrieve the latest form by formId from the state.\n\n**Cause:** The `DbFormState` class did not have a method to retrieve the latest form by form id.\n\n**Fix:** Added a new method to the `DbFormState` class that allows retrieval of the latest form by form id.\n\n**Result:** Users can now retrieve the latest form by formId from the state using the new method in the `DbFormState` class."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14136",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\n\r\nTo allow other teams to display deployed forms, `Zeebe` should export forms to `Elasticsearch`. The implementation is expected to include following:\r\n\r\n* Create a new index template `zeebe-record-form-template.json` with all the form fields, the `indexPattern` property should be `zeebe-record_form_*`\r\n* Add `FORM` as new index `ValueType`\r\n* Enabling the creation of the new `form` index\n",
    "title": "Export Form Deployment record to Elasticsearch",
    "releaseNoteText": "**Consequence:** The deployed forms cannot be displayed by other teams as the export to Elasticsearch functionality is missing.\n\n**Cause:** The export to Elasticsearch functionality for deployed forms was not implemented in the Zeebe system.\n\n**Fix:** A new index template called `zeebe-record-form-template.json` was created with all the necessary form fields. The `indexPattern` property was set to `zeebe-record_form_*`, and `FORM` was added as a new index `ValueType`. Additionally, the creation of the new `form` index was enabled.\n\n**Result:** After applying the fix, the Zeebe system now exports deployed forms to Elasticsearch. This allows other teams to easily display and access the deployed forms."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14135",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\n\r\nA `Start Event` can be linked to a deployed form by the new `formId` field. The implementation is expected to include following:\r\n\r\n* Add the new `formId` attribute into the `ZeebeFormDefinition`\r\n* Update `ZeebeElementValidator.validate()` method in order to enable `zeebe` to validate also for group of fields. In the case, a start event with linked form is valid if only one field between `formKey` and `formId` is present\n",
    "title": "Allow binding forms to start events by formId",
    "releaseNoteText": "**Consequence:** Previously, it was not possible to link a `Start Event` to a deployed form using the `formId` field. This caused limitations in the configuration of forms for start events.\n\n**Cause:** The absence of the `formId` attribute in the `ZeebeFormDefinition` prevented the linkage of forms to start events. Additionally, the `ZeebeElementValidator.validate()` method did not have the capability to validate groups of fields, resulting in the inability to determine the validity of start events with linked forms.\n\n**Fix:** The `formId` attribute has been added to the `ZeebeFormDefinition` to allow the linkage of forms to start events. The `ZeebeElementValidator.validate()` method has also been updated to validate groups of fields, making it possible to determine the validity of start events with either the `formKey` or `formId` present.\n\n**Result:** With this fix, users can now link a `Start Event` to a deployed form using the new `formId` field. This provides flexibility in the configuration of forms for start events and allows for validation of start events with linked forms based on the presence of either the `formKey` or `formId`."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14134",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\n\r\nA `User Task` can be linked to a deployed form by the new `formId` field. The implementation is expected to include following:\r\n\r\n* Add the new `formId` attribute into the `ZeebeFormDefinition`\r\n* Update `ZeebeElementValidator.validate()` method in order to enable `zeebe` to validate also for group of fields. In the case, a user task with linked form is valid if only one field between `formKey` and `formId` is present\n",
    "title": "Allow binding forms to user tasks by formId",
    "releaseNoteText": "**Consequence:** The user task binding to a deployed form was not possible by using the formId field. \n\n**Cause:** The formId attribute was not present in the ZeebeFormDefinition, which prevented the user from linking a user task to a deployed form using the formId. Additionally, the ZeebeElementValidator.validate() method did not have the capability to validate a group of fields, causing issues when validating a user task with a linked form.\n\n**Fix:** The formId attribute has been added to the ZeebeFormDefinition, allowing users to bind a user task to a deployed form using the formId field. The ZeebeElementValidator.validate() method has been updated to validate either the formKey or the formId field, ensuring that a user task with a linked form is valid if at least one of these fields is present.\n\n**Result:** Users can now link a user task to a deployed form by using the formId field. The ZeebeElementValidator.validate() method properly validates user tasks with linked forms, allowing for smooth and error-free form bindings."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14133",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\n\r\nThis issue is created to implement state behaviour of the Form. It is expected to include following:\r\n\r\n* Create Form DB classes\r\n* Create Form column family definitions\r\n* Create a method to store the Form\r\n* Create a method to retrieve the latest form by form id\r\n* Create a method to retrieve a Form by key\n",
    "title": "Save Form to the state",
    "releaseNoteText": "**Consequence:** Saving the Form to the state was not possible, causing user data to not be persisted.\n\n**Cause:** The Form DB classes, Form column family definitions, and the necessary methods to store and retrieve the Form were not implemented.\n\n**Fix:** Implemented the Form DB classes, created Form column family definitions, and added methods to store and retrieve the Form.\n\n**Result:** Users can now successfully save the Form to the state, ensuring that their data is persisted and can be retrieved later when needed."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14132",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\n\r\nThis issue is created to implement the processing of the Form deployment in the engine. It is expected to include following:\r\n\r\n* Create Form intent\r\n* Create Form transformer\r\n* Create Form applier\r\n* Include Form metadata to deployment record\r\n* Transform deployed Form in `DeploymentCreateProcessor` class\n",
    "title": "Handle Form Deployment in the Engine",
    "releaseNoteText": "**Consequence:**  Users were unable to deploy forms in the engine, resulting in a lack of functionality for creating and using forms within the system. \n\n**Cause:** The processing of form deployment was not implemented in the engine, causing the inability to create forms and include form metadata in the deployment records. \n\n**Fix:** Several components were added to handle form deployment, including a form intent, a form transformer, and a form applier. The deployed form was also transformed in the `DeploymentCreateProcessor` class. \n\n**Result:** After this fix, users can now deploy forms in the engine. Form creation and usage are now fully functional, allowing for a more comprehensive and efficient system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14131",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\n\r\nThis issue is created to update the `DeployResource` gRPC endpoint for supporting Form deployments.. It is expected to include following:\r\n\r\n* Add Form metadata response definition to `DeployResource` gRPC in `gateway.proto` file\r\n* Fill Form metadata in gateway `ResponseMapper` class\r\n\r\nBlocked by #14132 \n",
    "title": "Implement Form Deployment gRPC endpoint in the Gateway",
    "releaseNoteText": "**Consequence**\nThe `DeployResource` gRPC endpoint did not support Form deployments, causing a limitation in the system's ability to deploy forms.\n\n**Cause**\nThe `DeployResource` gRPC endpoint did not include the necessary Form metadata response definition in the `gateway.proto` file, and the Form metadata was not filled in the gateway `ResponseMapper` class.\n\n**Fix**\nThe `DeployResource` gRPC endpoint was updated to include the Form metadata response definition in the `gateway.proto` file. Additionally, the Form metadata was filled in the gateway `ResponseMapper` class.\n\n**Result**\nWith this fix, the system can now support Form deployments through the `DeployResource` gRPC endpoint. The necessary Form metadata will be included in the response, providing accurate information about the deployed forms."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13516",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\nIT tests for the Job Push is missing. To cover the happy path scenario end to end, we need to implement IT tests. ~~These cases should also cover closing a stream which will actually test the `onClose` hook.~~ Please refer to these two discussions for more details: \r\nhttps://github.com/camunda/zeebe/pull/13351#discussion_r1263414875\r\nhttps://github.com/camunda/zeebe/pull/13351#discussion_r1263834385\r\n\n\n berkaycanbc: @koevskinikola Based on the investigation we did, the only way to trigger `onClose` handler is to call `onComplete` or `onError` from the server side which is not the case for our implementation. Therefore, we will opt-out tests for it. At the same time, we want to keep it in case we decide to call `onComplete` later on. (e.g. during server shutdown)\r\n\r\nCc: @npepinpe ",
    "title": "Add integration tests for Job Push",
    "releaseNoteText": "**Consequence:**\nIntegration tests for Job Push were missing, leaving a gap in the coverage of the happy path scenario. This made it difficult to ensure that the system was working correctly end-to-end.\n\n**Cause:**\nThe absence of integration tests for Job Push was due to an oversight during the development process. These tests were not implemented, resulting in a gap in the test coverage.\n\n**Fix:**\nIntegration tests were added for Job Push to cover the happy path scenario. This involved writing test cases that simulated the entire process, ensuring that all steps were functioning correctly.\n\n**Result:**\nWith the addition of integration tests for Job Push, the system now has improved test coverage for the happy path scenario. This helps to ensure that the functionality is working correctly and allows for easier detection and resolution of any issues that may arise."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13465",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\n\nWe need to validate user input before registering workers to the job stream. That will provide two main benefits:\n- We will be able to return better validation error messages to the client\n- Without validation, worker can register to the stream with a wrong job type and later no jobs will be pushed. We will prevent that with validation.\n\n**Expected validation checks:**\n- type is blank (empty string, null)\n- ~worker is blank (empty string, null)~ this is going to be optional as we do in polling mechanism\n- timeout less than 1\n",
    "title": "Validate user input before registering a worker to the job stream",
    "releaseNoteText": "**Consequence:**\nUser input was not being validated before registering a worker to the job stream. This resulted in several issues. Firstly, the system was unable to provide accurate validation error messages to the client. Additionally, workers were able to register with incorrect job types, leading to a lack of job pushes in the future.\n\n**Cause:**\nThe underlying cause of this issue was the lack of validation for user input when registering a worker to the job stream. This validation was necessary in order to ensure the accuracy and integrity of the system.\n\n**Fix:**\nTo address this issue, the team implemented validation checks for user input before registering a worker to the job stream. Specifically, they added checks for blank or null values in the type field and a minimum timeout value of 1.\n\n**Result:**\nAs a result of this fix, user input is now properly validated before registering a worker to the job stream. This ensures that validation error messages can be accurately returned to the client, and workers with incorrect job types are prevented from registering. The system now operates with improved accuracy and integrity in handling worker registrations."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13429",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\n\r\nWith the new `StreamJobs` RPC introduced via #13351, we can now easily stream jobs from a Zeebe cluster. As a first step, we'll introduce a new command in `ZeebeClient` which will wrap the underlying gRPC call and integrate it into our client.\r\n\r\nWe'll introduce a new command, `StreamJobsCommand`, and a new API, `ZeebeClient#newStreamJobsCommand`. This will follow closely the `ActivateJobsCommand`, and have three steps:\r\n\r\n1. Set the job type (REQUIRED)\r\n2. Set a job consumer (REQUIRED)\r\n3. Set metadata (e.g. worker, job timeout, etc.) (OPTIONAL)\r\n\r\n### Long living stream\r\n\r\nSince this call is meant to be a long living stream, it diverges from our normal calls, which are all meant to eventually end.\r\n\r\n#### Request timeout\r\n\r\nFor example, the client has a default request timeout which is applied everywhere. This is counterproductive for this feature. Instead, **we will not apply the default request timeout here**. Users can still provide one which will be respected, but by default there will be no request timeout.\r\n\r\n#### Consumer\r\n\r\nSince the stream is long living, we don't want to wait for the command to complete before passing along the results. As opposed to `ActivateJobsCommand`, the `StreamJobsCommand` will take in a consumer as a mandatory build step, and results will be piped there. Future integration in the job worker can then do whatever with it, including passing it to the `JobHandler`.\r\n\r\n#### Cancellation/termination\r\n\r\nUsing the standard gRPC tools, you would normally cancel the stream either by throwing an exception (which is sent over the client's `StreamObserver#onError`), or by using `CancellableContext`. Closing the underlying transport channel would also work.\r\n\r\nThis doesn't fit the current `ZeebeClient` abstraction, and refactoring that goes beyond the scope of this issue. So instead, we'll piggyback on top of `Future#cancel(boolean)` - by calling this method, the user will be able to terminate the job stream gracefully, completing the future and notifying the server that the call is closed.\n\n npepinpe: Blocked by https://github.com/camunda/zeebe/issues/13430",
    "title": "Implement ZeebeClient wrapper for the StreamJobs RPC",
    "releaseNoteText": "**Consequence**: The lack of a ZeebeClient wrapper for the StreamJobs RPC prevented users from easily streaming jobs from a Zeebe cluster. This meant that users had to manually handle the gRPC call and integrate it into their client.\n\n**Cause**: The lack of a ZeebeClient command and API for the StreamJobs RPC was the underlying cause of this issue. The existing ActivateJobsCommand was not suitable for this long-lasting streaming functionality.\n\n**Fix**: A new command, StreamJobsCommand, and a new API, ZeebeClient#newStreamJobsCommand, were introduced to wrap the underlying gRPC call and provide integration into the client. The StreamJobsCommand has three steps: setting the job type, setting a job consumer, and setting metadata (optional).\n\n**Result**: With this fix, users can now easily stream jobs from a Zeebe cluster using the ZeebeClient wrapper for the StreamJobs RPC. The StreamJobsCommand allows users to set the job type, a job consumer, and metadata. Additionally, the integration ensures that the stream is long-lived and does not apply the default request timeout. Users can still provide a request timeout if desired."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13426",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nSupport tenant validation when an incident is resolved.\r\n\r\n:information_source: No changes on the Client side, as the tenant ownership is clear from the `IncidentRecord`, and the user's tenant id list is obtained from the gateway.\r\n\r\n```[tasklist]\r\n### Task breakdown\r\n- [x] The `ResolveIncidentProcessor` resolves an incident only if the tenant that owns the incident is accessible by the user making the request.\r\n- [x] The `ResolveIncidentProcessor` rejects a command if the tenant that owns the incident is **not** accessible by the user making the request.\r\n- [x] ~Gateway provides a user's tenant id list with the broker request when a ResolveIncident RPC call is made~ (Done with #13989 and #14283)\r\n```\r\n\r\n### Blocked by\r\n- #13345\r\n- #13346 \n\n koevskinikola: Closing as https://github.com/camunda/zeebe/pull/14467 resolves this issue.",
    "title": "A ResolveIncident request/command supports tenant validation",
    "releaseNoteText": "**Consequence:** Users were able to resolve incidents regardless of whether they had access to the tenant that owns the incident. \n\n**Cause:** The ResolveIncidentProcessor did not perform any validation to check if the user making the request had access to the tenant that owns the incident. \n\n**Fix:** The ResolveIncidentProcessor was updated to include a validation step that checks if the user making the request has access to the tenant that owns the incident. \n\n**Result:** Users will now be able to only resolve incidents if they have access to the tenant that owns the incident. This ensures better security and prevents unauthorized access to incidents."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13425",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nSupport tenant validation when an error is thrown for a job.\r\n\r\n:information_source: No changes on the Client side, as the tenant ownership is clear from the `JobRecord`, and the user's tenant id list is obtained from the gateway.\r\n\r\n```[tasklist]\r\n### Task breakdown\r\n- [x] The `JobThrowErrorProcessor` throws an error on a job only if the tenant that owns the job is accessible by the user making the request.\r\n- [x] The `JobThrowErrorProcessor` rejects a command if the tenant that owns the job is **not** accessible by the user making the request.\r\n- [x] ~Gateway provides a user's tenant id list with the broker request when a ThrowError RPC call is made~ (Done with #13989 and #14283)\r\n```\r\n\r\n### Blocked by\r\n- #13345\n\n koevskinikola: PR https://github.com/camunda/zeebe/pull/14466 resolved this issue.",
    "title": "A ThrowError request/command supports tenant validation",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13388",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nSupport multi-tenancy when setting variables on a particular scope (e.g. process instance, flow element instance).\r\n\r\n:information_source: No changes on the Client side, as the tenant ownership is clear from the `ElementInstance`, and the user's tenant id list is obtained from the gateway.\r\n\r\n```[tasklist]\r\n### Task breakdown\r\n- [ ] The `VariableDocumentRecord` and `VariableRecord` contain a new `tenantId` property (see #13235)\r\n- [ ] The `VariableState` contains a `tenantId` key, and can store and fetch variables by `tenantId`\r\n- [x] Elasticsearch/Opensearch `variable-template.json` and `variable-document-template.json` contain  a`tenantId` property.\r\n- [x] The `UpdateVariableDocumentProcessor` updates variables of a given scope only if the tenant that owns the scope is accessible by the user making the request.\r\n- [x] The `UpdateVariableDocumentProcessor` rejects a command if the tenant that owns the \"scope\" is **not** accessible by the user making the request.\r\n- [x] The `VariableBehavior` class appends the `tenantId` from the provided \"scope\" to all `VariableRecord` instances.\r\n- [x] ~Gateway provides a user's tenant id list with the broker request when a `SetVariables` RPC call is made~ (authorized tenants are passed in the `RecordMetadata`)\r\n- [x] Follow-up (`VariableDocumentRecord` and `VariableRecord`) events contain a `tenantId` inherited from the scope that was updated.\r\n```\r\n\r\n### Additional context\r\n- The `VariableDocumentRecord` class needs to contain a `tenantId` property only when used as a response to the client request, and as an event to be exported.\r\n\r\n### Blocked by\r\n- #13279\r\n\n\n koevskinikola: Update: All (impacted) Elasticsearch/Opensearch record templates have been updated to include a `tenantId` property with #13520.",
    "title": "Support multi-tenancy for setting variables on a process instance scope",
    "releaseNoteText": "**Consequence:** Users were unable to set variables on a process instance scope in a multi-tenant environment.\n\n**Cause:** The system did not have support for multi-tenancy when setting variables on a specific scope, such as a process instance or flow element instance.\n\n**Fix:** The `VariableDocumentRecord` and `VariableRecord` classes were updated to include a new `tenantId` property. The `VariableState` now supports storing and fetching variables by `tenantId`. The Elasticsearch/Opensearch `variable-template.json` and `variable-document-template.json` were also updated to include the `tenantId` property. The `UpdateVariableDocumentProcessor` was modified to update variables only if the tenant that owns the scope is accessible by the user making the request. The processor now rejects a command if the tenant that owns the scope is not accessible. The `VariableBehavior` class was updated to append the `tenantId` from the scope to all `VariableRecord` instances. Follow-up events from `VariableDocumentRecord` and `VariableRecord` now inherit the `tenantId` from the updated scope. The user's tenant id list is now passed in the `RecordMetadata`.\n\n**Result:** Users can now set variables on a process instance scope in a multi-tenant environment. The system properly handles multi-tenancy by ensuring that users can only update variables on scopes accessible to them."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13354",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "# Description\r\nIf a process contains a signal start event we must make sure to unsubscribe this signal.\r\n\r\nIf the deleted process was the latest, the previous version will become the new latest version. If this previous version contains a signal start event we should make sure that the signal subscription is created and process instances are started as expected.\r\n\r\nBlocked by: #9769 \n",
    "title": "Delete and recreate signal subscription of previous version",
    "releaseNoteText": "**Consequence**: The previous version of a process was not being properly unsubscribed, resulting in unexpected behavior with signal subscriptions. This could lead to incorrect process instance creation and signals not being received in certain scenarios. \n\n**Cause**: The issue was caused by not properly handling the deletion and recreation of signal subscriptions when a previous version of a process becomes the new latest version. This led to the signal subscription not being properly created and linked to the process instances.\n\n**Fix**: The code was updated to properly handle the deletion and recreation of signal subscriptions when a previous version becomes the new latest version. The process now correctly unsubscribes the signal from the previous version and creates the signal subscription for the new latest version.\n\n**Result**: With this fix, when a previous version of a process becomes the new latest version, the signal subscription is correctly created, ensuring that the process instances are started and that signals are received as expected."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13349",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\n\r\nThe process deleting event will mark the process definition as pending deletion.\r\n\r\n- Add the `DELETING` intent to the `ProcessIntent`\r\n- Add an `EventApplier` to handle the `DELETING` intent\r\n    - The applier must change the state the `PersistedProcess` to `PENDING_DELETION`\r\n        - Don't forget to change the state in the `CoulmnFamily`, as well as in the cache.\r\n    - This means a new method must be created on the `DbProcessState`\r\n        - Include tests!\r\n\r\n**Out of scope**\r\n- Writing the event will happen in a different issue\r\n\r\n**Blocked by**\r\n#13348 \n",
    "title": "Add and handle Process Deleting event",
    "releaseNoteText": "**Consequence:** The process deleting event was not being handled, resulting in the process definition not being marked as pending deletion.\n\n**Cause:** The cause of this issue was the absence of the `DELETING` intent in the `ProcessIntent` and a missing `EventApplier` to handle the `DELETING` intent. Additionally, there was no method on the `DbProcessState` to change the state to `PENDING_DELETION`, and the state was not updated in the `ColumnFamily` or the cache.\n\n**Fix:** The fix involved adding the `DELETING` intent to the `ProcessIntent` and creating an `EventApplier` to handle this intent. The `EventApplier` was responsible for changing the state of the `PersistedProcess` to `PENDING_DELETION` and updating the state in the `ColumnFamily` and the cache. A new method was also created on the `DbProcessState` to support the state change, and tests were included to ensure its correctness.\n\n**Result:** With this fix, the process deleting event is now properly handled. When this fix is applied, the process definition will be correctly marked as pending deletion, ensuring that it can be effectively deleted in the future."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13348",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\n\r\nWe need to keep track of process definitions that are pending deletion. For this we will introduce a `state` on the `PersitedProcess`.\r\n\r\n- Add a `state` property on the `PersistedProcess`\r\n- State should be a new enum (`PeristedProcessState`)\r\n- As of now we will know 2 states:\r\n    - `ACTIVE` - This is the default.\r\n    - `PENDING_DELETION` - Used to mark a process for deletion\r\n\r\nIn the future the states could be extended with other states, e.g. `SUSPENDED`.\r\n\n",
    "title": "Add a `state` to `PersistedProcess`",
    "releaseNoteText": "**Consequence**: The system did not have a way to track process definitions that were pending deletion.\n\n**Cause**: The `PersistedProcess` did not have a `state` property to indicate the status of the process.\n\n**Fix**: A `state` property was added to the `PersistedProcess` class as a new enum called `PersistedProcessState`. The `state` property can have two values: `ACTIVE` and `PENDING_DELETION`.\n\n**Result**: Users can now easily mark a process for deletion by setting its `state` to `PENDING_DELETION`. This allows for better organization and management of process definitions in the system. Additionally, in the future, more states can be added to the `PersistedProcessState` enum to accommodate other statuses, such as `SUSPENDED`."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13347",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nSupport tenant validation when a job's retries are updated.\r\n\r\n:information_source: No changes on the Client side, as the tenant ownership is clear from the `JobRecord`, and the user's tenant id list is obtained from the gateway.\r\n\r\n```[tasklist]\r\n### Task breakdown\r\n- [x] The `JobUpdateRetriesProcessor` updates a job only if the tenant that owns the job is accessible by the user making the request.\r\n- [x] The `JobUpdateRetriesProcessor` rejects a command if the tenant that owns the job is **not** accessible by the user making the request.\r\n- [x] ~Gateway provides a user's tenant id list with the broker request when a UpdateJobRetries RPC call is made~ (Done with #13989 and #14283)\r\n```\r\n\r\n### Blocked by\r\n- #13345\n\n koevskinikola: PR https://github.com/camunda/zeebe/pull/14466 resolved this issue.",
    "title": "An UpdateJobRetries request/command supports tenant validation",
    "releaseNoteText": "**Consequence:** Users were able to update job retries for jobs that they did not have access to.\n\n**Cause:** The `JobUpdateRetriesProcessor` did not include tenant validation when updating job retries.\n\n**Fix:** The `JobUpdateRetriesProcessor` now checks if the user making the request has access to the tenant that owns the job before updating the job retries.\n\n**Result:** Users can only update job retries for jobs that they have access to."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13346",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nSupport tenant validation when a job is failed.\r\n\r\n:information_source: No changes on the Client side, as the tenant ownership is clear from the `JobRecord`, and the user's tenant id list is obtained from the gateway.\r\n\r\n```[tasklist]\r\n### Task breakdown\r\n- [x] The `JobFailProcessor` completes a job only if the tenant that owns the job is accessible by the user making the request.\r\n- [x] The `JobFailProcessor` rejects a command if the tenant that owns the job is **not** accessible by the user making the request.\r\n- [x] The `IncidentRecord` has a `tenantId` property.\r\n- [x] The `IncidentState` can persist the new `tenantId` key.\r\n- [x] The `JobFailProcessor` creates an `IncidentRecord` with the `tenantId` of the `JobRecord`.\r\n- [x] The Elasticsearch/Opensearch `incident-template.json` has a `tenantId` property.\r\n- [x] ~Gateway provides a user's tenant id list with the broker request when a FailJob RPC call is made~ (Done with #13989 and #14283)\r\n```\r\n\r\n### Blocked by\r\n- #13345\n\n koevskinikola: Closing as https://github.com/camunda/zeebe/pull/14467 fully resolves this issue.",
    "title": "A FailJob request/command supports tenant validation",
    "releaseNoteText": "**Consequence:** The FailJob command does not have support for tenant validation when a job is failed. This means that any user can fail a job, regardless of whether they have access to the tenant that owns the job.\n\n**Cause:** The JobFailProcessor does not check the accessibility of the tenant that owns the job when processing a FailJob command. This lack of tenant validation allows users to fail jobs belonging to tenants they do not have access to.\n\n**Fix:** The JobFailProcessor now checks the accessibility of the tenant that owns the job when processing a FailJob command. If the tenant is not accessible by the user making the request, the command is rejected.\n\n**Result:** With the fix in place, when a user tries to fail a job, the JobFailProcessor will validate whether the user has access to the tenant that owns the job. If the user does not have access, the command will be rejected, preventing unauthorized access to job failure. This ensures better security and control over the management of job failures."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13343",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\nThis ColumnFamily must contains **all** running process instances. We must create a migration script that initially fills this ColumnFamily.\r\n\r\n- Create the migration script\r\n    - Include tests!\r\n\r\n**Blocked by**\r\n#13340 \n",
    "title": "Migrate running process instance into `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily",
    "releaseNoteText": "**Consequence:** Running process instances were not being migrated into the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily.\n\n**Cause:** The ColumnFamily did not contain all running process instances, leading to incomplete migration.\n\n**Fix:** A migration script was created to populate the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily with all running process instances. Tests were included to ensure the integrity of the migration process.\n\n**Result:** After applying this fix, the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily now contains all running process instances, ensuring complete migration and accurate representation of process data."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13342",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\nWhen a process has reached an end state we must remove it from this ColumnFamily\r\n\r\n- Add a method to remove data from this ColumnFamily\r\n    - Include tests!\r\n- When a process is completed use this method to remove it from the ColumnFamily\r\n- When a process is terminated use this method to remove it from the ColumnFamily\r\n- When a call activity is completed / terminated use this method to remove it from the ColumnFamily\r\n\r\nPlease also change the `ElementInstanceStateTest#shouldNotLeakMemoryOnRemoval`. We must change the processInstanceRecord that's generated here to be of element type PROCESS.\r\n\r\n**Blocked by**\r\n#13340 \n",
    "title": "Remove from `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily",
    "releaseNoteText": "**Consequence:**\n\nThe `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily was not being updated correctly when a process reached an end state. This resulted in outdated and unnecessary data in the ColumnFamily.\n\n**Cause:**\n\nThe underlying cause of this issue was the lack of a method to remove data from the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily. This omission prevented the system from properly cleaning up after a process reached an end state.\n\n**Fix:**\n\nA method has been added to remove data from the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily. This method has been thoroughly tested to ensure its correctness.\n\nAdditionally, the `ElementInstanceStateTest#shouldNotLeakMemoryOnRemoval` has been updated to change the generated processInstanceRecord to be of element type PROCESS.\n\n**Result:**\n\nNow, when a process reaches an end state, it is correctly removed from the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily. This ensures that the system does not retain outdated and unnecessary data, leading to improved performance and reliability."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13341",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\nIn #13340 we have created this ColumnFamily. We now need to fill it with data.\r\n\r\n- Add a method to insert data into this ColumnFamily\r\n    - Include tests!\r\n- Use this method to insert data when starting a new process instance\r\n    - Also when starting a process anywhere\r\n- Use this method to insert data upon calling a CallActivity\r\n\r\n**Blocked by**\r\n#13340 \n",
    "title": "Insert into the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily",
    "releaseNoteText": "**Consequence:** The `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily is empty, resulting in missing data when starting a new process instance or calling a CallActivity.\n\n**Cause:** The missing data in the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily is caused by the lack of a method to insert data into this ColumnFamily.\n\n**Fix:** A new method has been added to insert data into the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily. Tests have also been included to ensure the correctness of the insertion process.\n\n**Result:** With this fix, the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily will now be populated with the necessary data. This means that starting a new process instance and calling a CallActivity will properly insert data into the ColumnFamily, ensuring the availability of the required information."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13340",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\nWe will need a way to find out if there are any process instances for a definition key. We need this so when a process instance is terminated/completed we can check if there are other process instances still running. \r\nIf this is not the case, and the deployment is pending deletion we can write the followup events to fully delete the resource.\r\n\r\n- Create the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY ` in `ZbColumnFamililes`\r\n- Create this ColumnFamily in the `DbElementInstanceState`\r\n\r\n**Out of scope:**\r\n- Doing any actions on this ColumnFamily. Inserting and deleting data will happen in:\r\n    - #13341 \r\n    - #13342\r\n    - #13343\n",
    "title": "Add `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily",
    "releaseNoteText": "**Consequence:** Users were unable to find out if there were any process instances for a definition key, which impacted the ability to check if there were other process instances still running when a process instance is terminated/completed. This caused pending deployments to not be fully deleted.\n\n**Cause:** The `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily was not created in `ZbColumnFamilies` and the creation of this ColumnFamily was missing in the `DbElementInstanceState`.\n\n**Fix:** The `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily has been created in `ZbColumnFamilies` and has been added to `DbElementInstanceState` to ensure its creation.\n\n**Result:** Users can now check if there are any process instances for a definition key. This allows for proper handling of terminated/completed process instances and ensures that pending deployments are fully deleted when there are no other process instances running."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13337",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nSupport multi-tenancy for timer start events and timer boundary/intermediate events.\r\n\r\n:information_source: For timers, the Gateway and Client code doesn't need to be adjusted. Timers are always processed in the engine, and inherit the `tenantId` from the process definition or process instance.\r\n\r\n```[tasklist]\r\n### Tasks\r\n- [x] The `TimerRecord` class contains a `tenantId` property\r\n- [x] The `TimerInstance` class contains a `tenantId` property\r\n- [ ] The `TimerInstanceState` has a `tenantId` key and can store and get instances by it\r\n- [x] Elasticsearch/Opensearch `timer-template.json` files have a `tenantId` property\r\n- [x] Timer start events inherit the `tenantId` from the process definition in the `DeploymentCreateProcessor`\r\n- [x] `TriggerTimerProcessor` propagates the `tenantId` from the `TimerRecord` to any follow-up records.\r\n- [x] `DueDateChecker` propagates the `tenantId` from the `TimerInstance` to any follow-up records.\r\n- [ ] Documentation on timer start events is expanded.\r\n- [ ] Add E2E multi-tenancy tests for timer events\r\n```\r\n\r\nFor the release candidate, we've disabled the timer events feature for non-default tenants.\r\n\r\nIt's easy to-re-enable:\r\n- remove TIMER [here](https://github.com/camunda/zeebe/blob/58ed10ab83e56c7970025911e8dd694aa054f3d9/engine/src/main/java/io/camunda/zeebe/engine/processing/deployment/model/validation/UnsupportedMultiTenantFeaturesValidator.java#L33-L34)\r\n- replace the tests cases [here](https://github.com/camunda/zeebe/blob/58ed10ab83e56c7970025911e8dd694aa054f3d9/engine/src/test/java/io/camunda/zeebe/engine/processing/multitenancy/TenantAwareTimerEventTest.java#L69-L237)\r\n- un-ignore the test case [here](https://github.com/camunda/zeebe/blob/80d849648b7de82f55ee1de624e3f797df0ce3bc/engine/src/test/java/io/camunda/zeebe/engine/processing/incident/TimerIncidentTest.java#L237)\r\n\r\n### Blocked by\r\n- https://github.com/camunda/zeebe/issues/13238\n\n koevskinikola: Update: All (impacted) Elasticsearch/Opensearch record templates have been updated to include a `tenantId` property with https://github.com/camunda/zeebe/issues/13520.\n korthout: I moved this to in progress, because I wanted to start with it, but ended up working on other thing instead. \n romansmirnov: > The `TimerInstanceState` has a `tenantId` key and can store and get instances by it\r\n\r\nAs in other cases, I decided against implementing the `TimerInstanceState` to be tenant -aware.",
    "title": "Support multi-tenancy for BPMN timer events",
    "releaseNoteText": "**Consequence:** Timer events in BPMN did not support multi-tenancy, causing issues for non-default tenants. As a result, the feature was disabled for non-default tenants in the release candidate.\n\n**Cause:** The absence of a `tenantId` property and implementation of `TimerInstanceState` prevented timer events from being multi-tenant aware.\n\n**Fix:** The `TimerRecord` and `TimerInstance` classes were updated to include a `tenantId` property. Additionally, the Elasticsearch/Opensearch record templates were also updated to include a `tenantId` property.\n\n**Result:** With the fix applied, timer start events and timer boundary/intermediate events now support multi-tenancy. Timers are processed in the engine and inherit the `tenantId` from the process definition or process instance. This ensures proper functionality and behavior of timer events for all tenants in the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13335",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\n\r\n- Add a new `Intent` to the `ResourceDeletionIntent` class named `DELETING`\r\n- In the `ResourceDeletionProcessor` write the `DELETING` event\r\n    - Currently only the DMN resource are part of this processor.\r\n    - We are writing a `DELETED` event to the log, and are writing this as a response to the client.\r\n    - Before this we should write the `DELETING` event to the log.\r\n    - The response to the client should be changed to be `DELETING` as wel\r\n    - After the response we must still write the `DELETED` tot he log.\n",
    "title": "Add Resource Deleting intent",
    "releaseNoteText": "**Consequence:** The system did not have a specific intent for resource deletion, resulting in inconsistent behavior and a lack of clarity for users. The system only had a `DELETED` event written to the log and sent as a response to the client, without logging the `DELETING` event or updating the response message.\n\n**Cause:** The `ResourceDeletionIntent` class did not have a designated `DELETING` intent, causing a gap in the processing flow. The processor responsible for resource deletion in the system only handled the `DELETED` event and did not log the `DELETING` event or update the response message.\n\n**Fix:** A new `Intent` named `DELETING` was added to the `ResourceDeletionIntent` class. The `ResourceDeletionProcessor` was modified to write the `DELETING` event to the log before sending the response to the client. The response message was also changed to reflect the `DELETING` event. Finally, the `DELETED` event is still logged after the response is sent.\n\n**Result:** With this fix, when a resource is being deleted, the system now properly recognizes the `DELETING` intent. The `DELETING` event is logged before the response is sent to the client, providing a clear indication of the ongoing deletion process. The response message is also updated to reflect the `DELETING` event, ensuring consistent and informative communication with the user. Finally, the `DELETED` event continues to be logged after the response, allowing for complete tracking and auditing of the deletion process."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13320",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nThe engine should support working with multi-tenancy during deployments.\r\n\r\n### AC\r\n- [x] The `RecordValueWithTenant` interface (see #13235) is implemented by the following classes:\r\n       - [ ] `DeploymentRecord`\r\n       - [ ] `ProcessMetadata`\r\n       - [ ] `ProcessRecord`\r\n       - [ ] `DecisionRequirementsMetadataRecord`\r\n       - [ ] `DecisionRequirementsRecord`\r\n       - [ ] `DecisionRecord`\r\n- [x] `DeploymentRecord` implements the `RecordValueWithTenantPermissions` interface (see #13235)\r\n- [ ] ~~`DeploymentCreateProcessor` rejects the command if the `tenantId` isn't found in the list of `tenantIds` available to the user.~~\r\n- [x] The `DeploymentCreateProcessor`/`BpmnResourceTransformer`/`DmnResourceTransformer` propagates the `tenantId` to the BPMN/DMN resources and their appropriate Records.\r\n- [x] The `DeploymentState`, `ProcessState`, and `DecisionState`  include a `tenantId` key when an appropriate resource is added.\r\n- [x] The unit test coverage is extended to cover the new tenant properties.\r\n\r\n\r\n\r\n### Blocked by\r\n- #13235\n\n koevskinikola: :question: Do we need to tackle the `DeploymentDistributionRecord` as well? \r\n\r\nThe record is deprecated, so it would only be triggered by existing \"in-progress\" deployments. We can classify this as old data, and map this deployment to the `<default>` tenant.\n remcowesterhoud: > DeploymentCreateProcessor rejects the command if the tenantId isn't found in the list of tenantIds available to the user.\r\n\r\nThis will happen in the gateway, so I won't do this as part of this issue.\n remcowesterhoud: > ❓ Do we need to tackle the DeploymentDistributionRecord as well?\r\n\r\nI don't think this record needs it. This record doesn't know any details about a process. It was used to store a key + partition id in the state. This kept track of what deployment needed to be distributed to what partition. But the actual command that got distributed was the `Deployment.DISTRIBUTE`, which is actually the `DeploymentRecord`.\n koevskinikola: Closing this issue as all the sub-tasks have been completed.",
    "title": "Engine can process multi-tenant DeploymentRecord commands",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13319",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nThe Gateway supports receiving and forwarding (to the Broker) DeployResource RPC calls with a `tenantId`.\r\n\r\n### AC\r\n- [x] The following gRPC messages contain a new `tenantId` property:\r\n       - [ ] `DeployResourceRequest`\r\n       - [ ] `ProcessMetadata`\r\n       - [ ] `DecisionMetadata`\r\n       - [ ] `DecisionRequirementsMetadata`\r\n- [x] The `deployResource(...)` Gateway endpoint passes the gRPC `DeployResourceRequest#tenantId` property to the `BrokerDeployResourceRequest`. The following scenarios are possible as well:\r\n      - If multi-tenancy is disabled (see #13237), the `BrokerDeployResourceRequest#tenantId` is set to `<default>`.\r\n      - If multi-tenancy is enabled, and `DeployResourceRequest#tenantId` is `null`, the deployment is rejected.\r\n- [ ] ~The `BrokerDeployResourceRequest#tenantIds` is set to the list of user accessible tenant ids provided through a gRPC context by the Identity SDK.~\r\n       -  The user's authorization list will be set in the `RecordMetadata`. It will be implemented through #13989 and #13237 \r\n\r\n### Blocked by\r\n- #13320\n",
    "title": "Gateway supports multi-tenancy in deployment RPCs",
    "releaseNoteText": "**Consequence**: The Gateway now has the capability to support multi-tenancy in deployment RPCs. When receiving and forwarding DeployResource RPC calls, the Gateway now includes a `tenantId` property.\n\n**Cause**: The issue was caused by the need to support multi-tenancy in the Gateway's deployment RPCs. This required adding the `tenantId` property to various gRPC messages, such as `DeployResourceRequest`, `ProcessMetadata`, `DecisionMetadata`, and `DecisionRequirementsMetadata`. \n\n**Fix**: The fix involved modifying the `deployResource(...)` Gateway endpoint to pass the `DeployResourceRequest#tenantId` property to the `BrokerDeployResourceRequest`. Depending on the multi-tenancy configuration, the `BrokerDeployResourceRequest#tenantId` was set to either `<default>` or the provided `DeployResourceRequest#tenantId`. \n\n**Result**: As a result of this fix, the Gateway is now able to receive and forward DeployResource RPC calls with the appropriate `tenantId` property. This allows for proper multi-tenancy support in the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13318",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nTo support multi-tenancy when polling or pushing jobs, a job should be activated when the `jobType` **AND** `tenantId` match.\r\nWe also want to be able to poll or push jobs for multiple tenants at once.\r\n\r\nAs a result, a list of `tenantIds` needs to be included in the `JobBatchRecord`s and `JobActivationProperties` classes, to ensure that filtering jobs by `jobType` **AND** `tenantId` is possible.\r\n\r\n\r\n```[tasklist]\r\n### AC\r\n- [x] The `JobBatchRecord` class provides a `tenantIds` property (List\\<String\\>).\r\n- [x] The `JobActivationProperties` class provides a `tenantIds` property (List\\<String\\>).\r\n- [x] ~The `JobBatchRecord` class implements the `RecordValueWithTenantPermissions` interface~ (see #13989 and #14283)\r\n- [x] ~The `JobActivationProperties` class implements the `RecordValueWithTenantPermissions` interface~ (see #13989 and #14283)\r\n- [x] ~The Gateway adds a list of tenant ids assigned to a user to the`JobBatchRecord` or `JobActivationProperties` instances for each `ActivateJobs`/`StreamJobs` RPC call.~ (see #14283)\r\n```\n",
    "title": "`JobBatchRecord` and `JobActivationProperties` provide tenant information",
    "releaseNoteText": "**Consequence:** Prior to this fix, the `JobBatchRecord` and `JobActivationProperties` classes did not provide any information about the tenant. This limitation prevented the filtering of jobs based on both the `jobType` and `tenantId`, making it difficult to support multi-tenancy when polling or pushing jobs. \n\n**Cause:** The missing tenant information in the `JobBatchRecord` and `JobActivationProperties` classes was due to the absence of a `tenantIds` property. Without this property, it was not possible to include the list of tenant IDs in instances of these classes.\n\n**Fix:** This fix introduced a `tenantIds` property to the `JobBatchRecord` and `JobActivationProperties` classes. This property allows for the inclusion of a list of tenant IDs, enabling the filtering of jobs based on both the `jobType` and `tenantId`.\n\n**Result:** With this fix applied, users are now able to filter jobs by `jobType` **AND** `tenantId`. The addition of the `tenantIds` property in the `JobBatchRecord` and `JobActivationProperties` classes ensures that multi-tenancy is supported when polling or pushing jobs, allowing for more efficient and targeted job management across multiple tenants."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13317",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nJob polling and pushing should filter jobs by `jobType` and `tenantId` before the jobs are activated.\r\n\r\n\r\n```[tasklist]\r\n### Tasks\r\n- [x] ~`JobBatchCollector` queries jobs by `jobType` and `tenantIds` (multiple tenantIds are possible)~ Done in #13345\r\n- [x] The JobStreamer API provides a new method `streamFor(final DirectBuffer jobType, final DirectBuffer tenantId)` that returns a `JobStream` for a given `jobType` and `tenantId`. Done with #14397\r\n- [x] `BpmnJobActivationBehavior` [queries for](https://github.com/camunda/zeebe/blob/main/engine/src/main/java/io/camunda/zeebe/engine/processing/bpmn/behavior/BpmnJobActivationBehavior.java#L67) `JobStream` by `jobType` and `tenantId`.\r\n```\r\n\r\n### Additional context\r\n* Once the `JobStreamer` API is adjusted, the [`RemoteJobStreamer` class](https://github.com/camunda/zeebe/blob/49a7709a3dc947d5579aff3c19c7d166c8a00167/transport/src/main/java/io/camunda/zeebe/transport/stream/impl/RemoteStreamerImpl.java#L62-L85) will need to be adjusted to filter the set of job streams for a given `jobType` by the `tenantIds` they specify (in the `JobActivationProperties` as well.\r\n* The ZDP team will need to be involved here, to expand the `JobStreamer` API to provide a new method `streamFor(final DirectBuffer jobType, final DirectBuffer tenantId)` that returns a `JobStream` for a given `jobType` and `tenantId`. \n\n koevskinikola: Closing as PR #14464 resolves this issue.",
    "title": "Jobs are pushed/polled only for requested tenants",
    "releaseNoteText": "**Consequence:** Jobs were being pushed or polled without considering the requested `tenantId`, resulting in potential data leakage between tenants.\n\n**Cause:** The job polling and pushing logic did not include filtering by `jobType` and `tenantId`, causing all jobs to be activated regardless of the requested tenant.\n\n**Fix:** Adjusted the `JobBatchCollector` to query jobs by `jobType` and `tenantIds`, and added a new method `streamFor(final DirectBuffer jobType, final DirectBuffer tenantId)` to the `JobStreamer` API, which returns a `JobStream` for a given `jobType` and `tenantId`.\n\n**Result:** Jobs are now only pushed or polled for the requested tenants, ensuring data isolation and preventing potential data leakage between tenants. This fix enhances the security and privacy of the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13316",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nSupport tenant validation when a job is completed.\r\n\r\n:information_source: No changes on the Client side, as the tenant ownership is clear from the `JobRecord`, and the user's tenant id list is obtained from the gateway.\r\n\r\n```[tasklist]\r\n### Task breakdown\r\n- [x] The `JobCompleteProcessor` completes a job only if the tenant that owns the job is accessible by the user making the request.\r\n- [x] The `JobCompleteProcessor` rejects a command if the tenant that owns the job is **not** accessible by the user making the request.\r\n- [x] ~Gateway provides a user's tenant id list with the broker request when a CompleteJob RPC call is made~ (Done with #13989 and #14283)\r\n```\r\n\r\n### Blocked by\r\n- #13345\n\n koevskinikola: PR https://github.com/camunda/zeebe/pull/14466 resolved this issue.",
    "title": "A CompleteJob request/command supports tenant validation",
    "releaseNoteText": "**Consequence:** The CompleteJob request/command did not support tenant validation, which meant that any user could complete a job regardless of their access to the tenant owning the job.\n\n**Cause:** The JobCompleteProcessor did not check if the user making the request had access to the tenant that owns the job.\n\n**Fix:** The JobCompleteProcessor was updated to perform tenant validation by checking if the user making the request had access to the tenant that owns the job.\n\n**Result:** With this fix, when a CompleteJob request/command is made, the JobCompleteProcessor will now ensure that the user making the request has access to the tenant owning the job. This ensures that only users with the proper access rights can complete jobs."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13288",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\n\nAdd multi-tenancy support for the following process instance operations:\n* [Process instance modification](https://docs.camunda.io/docs/next/apis-tools/grpc/#modifyprocessinstance-rpc)\n* [Cancel process instance](https://docs.camunda.io/docs/next/apis-tools/grpc/#cancelprocessinstance-rpc)\n* [Set variables](https://docs.camunda.io/docs/next/apis-tools/grpc/#setvariables-rpc)\n\nTo support each operation, the following changes need to be applied:\n- The engine validates tenant access in the appropriate processors (`ProcessInstanceModificationProcessor`, `ProcessInstanceCommandProcessor`, `UpdateVariableDocumentProcessor`).\n- The Gateway propagates tenant data to the Broker.\n- No client changes are needed for these operations. The tenant access is validated by comparing the following information: \n  - The tenant ownership is acquired from the State. \n  - The list of tenants the user is assigned to is acquired from the Identity SDK in the Gateway and forwarded to the engine through the command record (modification, cancel, set variables).\n\n#### Expected behaviors\n\nFor each of the process instance operations, the following applies:\n* A process instance operation is only possible if the user/token has access to the tenant owning the process instance.\n\n```[tasklist]\n### Task breakdown\n- [ ] https://github.com/camunda/zeebe/issues/14550\n- [ ] https://github.com/camunda/zeebe/issues/14520\n- [ ] https://github.com/camunda/zeebe/issues/13388\n```\n\n### Blocked by\n- #13279\n",
    "title": "Support multi-tenancy for process instance operations (modify, cancel, set variables)",
    "releaseNoteText": "**Consequence**: Users were unable to perform process instance operations such as modification, cancellation, and setting variables on multi-tenant environments.\n\n**Cause**: The process instance operations did not support multi-tenancy. The validation of tenant access was not implemented in the appropriate processors, and the Gateway did not propagate tenant data to the Broker. \n\n**Fix**: The engine now validates tenant access in the appropriate processors. The Gateway has been updated to propagate tenant data to the Broker. No client changes are required for these operations. The tenant access is validated by comparing the tenant ownership acquired from the State and the list of tenants the user is assigned to acquired from the Identity SDK in the Gateway.\n\n**Result**: Users can now perform process instance operations (modify, cancel, set variables) on multi-tenant environments. The operations are only possible if the user/token has access to the tenant owning the process instance."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13279",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nThis issue covers multi-tenancy support for the core operations of BPMN processes in Zeebe. The following should be covered, i.e. any command/event records produced by the following actions should contain a `tenantId`:\r\n* Starting a process instance\r\n* Starting a process instance with results\r\n* Starting a CallActivitiy\r\n* Evaluating a Decision through a Business Rule task.\r\n* Child instance creation.\r\n\r\n#### Expected behaviors\r\nThe following behavior should be supported:\r\n* A process instance is always owned by a tenant. A process instance can’t be shared across tenants.\r\n* All child instances are owned by the same tenant as the process instance. That means that any `ProcessInstanceRelated` records are owned by the same tenant as the process instance.\r\n* A `CallActivity` is started from a process definition with the following criteria:\r\n   1. The process definition is owned by the same tenant as the parent process instance.\r\n   2. If nothing is found, an `Incident` is raised.\r\n* A `Decision` is evaluated if it has the following criteria:\r\n   1. The Decision is owned by the same tenant as the parent process instance.\r\n   2. If nothing is found, an `Incident` is raised.\r\n* Filtering process definitions by version remains the same as the non-multi-tenant approach, i.e. if no version is specified, the latest version of the process definition owned by the tenant is used. (provided by #13238)\r\n* A process instance can be started only if a tenant id is provided with the client request. The following scenarios are possible:\r\n   * A tenant id is provided:\r\n      1. A process instance is started if the user making the request has access to the tenant owning the process definition.\r\n      2. The request/command is rejected if the user doesn't have correct access to the tenant.\r\n   * A tenant id isn't provided:\r\n      1. A process instance is started if the user only has access to a single tenant, and a single process definition can be found that can be mapped to that tenant.\r\n      2. The request/command is rejected if the user has access to multiple tenants or no process definition is found.\r\n\r\n:information_source: In the implementation, first focus on implementing the simple path, with a `tenantId` explicitly provided in the client request/command record. The goal is to deliver an MVP as soon as possible.\r\n\r\n```[tasklist]\r\n### Task breakdown\r\n- [x] Engine can process multi-tenant ProcessInstanceCreationRecord commands with tenant id\r\n- [x] Engine propagates tenant id to process instance elements.\r\n- [x] Engine can start CallActivity by tenant id\r\n- [ ] https://github.com/camunda/zeebe/issues/14211\r\n- [ ] https://github.com/camunda/zeebe/issues/13536\r\n- [x] Notify Operate and Optimize teams that core BPMN process events are available from Zeebe\r\n- [ ] ~CreateProcessInstance* RPCs documentation is expanded with multi-tenancy~\r\n- [ ] ~Engine can evaluate Business Rule task Decision by tenant id~ (moved to DMN)\r\n- [ ] ~Multi-tenancy data migration is provided for `ElementInstanceState`, `BannedInstanceState`~ WON'T DO\r\n```\r\n\r\n\r\n### Blocked by\r\n- https://github.com/camunda/zeebe/issues/13238\n\n koevskinikola: The task `Engine can evaluate Business Rule task Decision by tenant id` was moved to the https://github.com/camunda/zeebe/issues/13282 issue, as it fits better in the DMN scope of this Epic.\n koevskinikola: The `ElementInstanceState` and `BannedInstanceState` processing state won't be made tenant-aware, i.e. no `tenantId` properties will be added, and no data migration will be performed for them.\r\n\r\nWhy:\r\n* The data and operations within `ElementInstanceState` are internal to the engine, and aren't exposed to User requests. Furthermore, `ElementInstanceState` operations work with data KEYS (processDefinition, processInstance) which are unique across all tenants. As such, we see little value in introducing the additional overhead of introducing tenant-based data isolation for this state.\r\n  * On the other hand, we save up on implementation effort and time by not making this state tenant-aware.\r\n  * For operations like `CancelProcessInstance` and `DeleteResource` where we interact with the `ElementInstanceState` we can already validate the tenant against the `ProcessState` (either in the processor or within the state), ensuring that no unauthorized operations reach the `ElementInstanceState`.\r\n* The data and operations within `BannedInstanceState` are only accessible through the management/admin API which by its nature requires full access to the data. Furthermore, the `BannedInstanceState` only works with `processInstanceKey`s which are unique across tenants. As a result, we see little value in introducing tenant-based data isolation in these states.\r\n  * In addition, we plan to migrate away from using `BannedProcessInstances` with https://github.com/camunda/product-hub/issues/686.\n koevskinikola: The task `CreateProcessInstance* RPCs documentation is expanded with multi-tenancy` has been transfered to https://github.com/camunda/camunda-platform-docs/issues/2561.\n koevskinikola: The task `Engine can start CallActivity by tenant id` was already completed in https://github.com/camunda/zeebe/pull/14328.\n korthout: While we don't migrate `ElementInstanceState` and `BannedInstanceState` to be `TenantAware` right now, we could always add this later. The benefit would be further isolation of data. While keys are unique, the data is accessible when the engine makes a mistake due to a bug. By further isolating the data, we'd avoid such cases. But this can be done in a future iteration.",
    "title": "Support multi-tenancy for core BPMN process operations",
    "releaseNoteText": "**Consequence:** The core operations of BPMN processes in Zeebe did not support multi-tenancy. Starting a process instance, starting a process instance with results, starting a CallActivitiy, evaluating a Decision through a Business Rule task, and child instance creation did not include a `tenantId`. \n\n**Cause:** The lack of multi-tenancy support was due to a missing implementation in the core engine of Zeebe.\n\n**Fix:** The engine was updated to process multi-tenant ProcessInstanceCreationRecord commands with a tenant id, propagate the tenant id to process instance elements, and start a CallActivity by tenant id.\n\n**Result:** With the fix implemented, the core operations of BPMN processes in Zeebe now support multi-tenancy. Process instances are owned by a tenant, child instances are owned by the same tenant as the process instance, and the CallActivity and Decision evaluations are done by checking ownership against the tenant of the parent process instance."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13253",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nCritical issues are defined as a \"stop the world\" issue. This means they require immediate attention. Currently, when an issue is labeled as critical there's no way of knowing, unless the person who added the label notifies the team. If this doesn't happen, there's a good chance we won't notice until the next triage.\r\n\r\n**Describe the solution you'd like**\r\nWe can easily add a workflow that checks if an issue gets labelled as critical. If this is the case we could send a notification to the Zeebe slack channel to notify the engineers that attention is required.\r\n\r\n**Describe alternatives you've considered**\r\nPeople could ping us manually when they do this. But that's a manual step and could be forgotten.\r\n\r\n**Additional context**\r\nN/A\r\n\n\n remcowesterhoud: ZPA Triage:\n- Helps us identify critical bugs more quickly\n- Removes a manual step that could be forgotten otherwise\n- Marking it as `upcoming` as we want to be aware of critical bugs asap\n- We already send slack message from our CI. Should be easy to take inspiration from these other places.\n- Low effort, potentially high impact.\n megglos: ZDP-Triage:\n- could help us being faster to react\n- need to clarify who reacts on it\n- right now we would assume this is done by zeebe engineers who can also escalate directly when identifying a critical issue\n- marking as later for now",
    "title": "Notify Zeebe Team in Slack when an issue is labelled as critical",
    "releaseNoteText": "**Consequence:** Previously, critical issues in the Zeebe system would go unnoticed unless someone manually notified the team. This could lead to delays in addressing these urgent problems.\n\n**Cause:** The lack of an automatic notification system for critical issues meant that the team relied on manual reminders to be aware of these urgent problems.\n\n**Fix:** A workflow has been implemented that checks if an issue is labelled as critical. If a critical label is added, a notification is automatically sent to the Zeebe Slack channel, alerting the team to the need for immediate attention.\n\n**Result:** With this fix in place, the Zeebe team will now receive immediate notifications in Slack when an issue is labelled as critical. This will ensure that urgent problems are promptly addressed and prevent any delays in resolving critical issues."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13238",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\nThis issue covers multi-tenancy support for the Deployment functionality of Zeebe. The following behavior should be supported:\r\n\r\n* If multi-tenancy is enabled:\r\n   * If no tenant id is provided, the deployment is rejected.\r\n      * Reason: Zeebe will support shared deployments in the future that might be mapped to this call.\r\n   * If a tenant id is provided, the resources are owned by a single tenant.\r\n   * Resource versioning is separated by tenant, i.e. process definitions/decisions owned by different tenants with the same `processId` have separate version counts in each tenant.\r\n* If multi-tenancy is disabled:\r\n   * All deployments and their resources are mapped to the `<default>` tenant.\r\n\r\n:warning: Note: the user/m2m token must have access to the tenant. Otherwise, the deployment command is rejected.\r\nA user/token shouldn’t be allowed to deploy to a tenant they don’t have access to.\r\n\r\n#### Task breakdown\r\n\r\n```[tasklist]\r\n### Tasks\r\n- [ ] https://github.com/camunda/zeebe/issues/13320\r\n- [ ] https://github.com/camunda/zeebe/issues/13322\r\n- [ ] https://github.com/camunda/zeebe/issues/13319\r\n- [ ] https://github.com/camunda/zeebe/issues/13321\r\n- [x] Notify Web/Desktop Modeler and Operate teams that multi-tenancy Deployments are available in Zeebe\r\n- [ ] https://github.com/camunda/zeebe/issues/14019\r\n- [ ] https://github.com/camunda/zeebe/issues/13315\r\n```\r\n\r\n\r\n### Blocked by\r\n- #13235\r\n- #13237 (soft blocker, functionality can be implemented, excluding user tenant id list)\n\n koevskinikola: :question: We should consider if the deprecated `DeployProcess` RPC should be adjusted as well.\n koevskinikola: Documentation on this component is tracked in https://github.com/camunda/camunda-platform-docs/issues/2561. As a result, I'm removing `DeployResource RPC documentation is expanded with multi-tenancy` from the tasklist in this issue.",
    "title": "Support multi-tenancy for deployments",
    "releaseNoteText": "**Consequence:**\nMulti-tenancy support has been added for the Deployment functionality of Zeebe. This means that deployments can now be associated with specific tenants.\n\n**Cause:**\nPreviously, Zeebe did not support multi-tenancy for deployments. All deployments and their resources were mapped to the default tenant.\n\n**Fix:**\nWith this fix, if multi-tenancy is enabled, the following behavior is supported:\n- If no tenant id is provided, the deployment is rejected. This is because Zeebe will support shared deployments in the future that might be mapped to this call.\n- If a tenant id is provided, the resources are owned by a single tenant.\n- Resource versioning is separated by tenant, meaning that process definitions/decisions owned by different tenants with the same processId have separate version counts in each tenant.\n\n**Result:**\nNow, with multi-tenancy support for deployments in place, users can specify a tenant id when performing a deployment. This allows for better tracking and organization of resources, as well as ensuring that deployments are associated with the correct tenants. Additionally, resource versioning is now separated by tenant, providing a more accurate reflection of the status of process definitions and decisions for each individual tenant."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13040",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nAs a programmer writing Zeebe Job Handlers, I want to complete a job returning only a single variable value.\r\n\r\nThe `newCompleteCommand` offers only a parameter `.variables()` where I have to wrap the value in a Map beforehand.\r\n\r\nThis is overhead.\r\n\r\n**Describe the solution you'd like**\r\n`client.newCompleteCommand(job).variable(\"name\", value)...`\r\n\r\n**Describe alternatives you've considered**\r\nPromote the Java feature with `Map.of(\"name\", value)` giving examples in the docs and Zeebe examples. \r\n\r\nOlder programmers, that learned Java with versions older than 8 are not aware of this shortcut.\r\n\r\n**Additional context**\r\nIt came up in a migration Workshop with a customer.\n\n Gireesh2002: Acdording to [javadoc.io]https://javadoc.io/doc/io.camunda/zeebe-client-java/1.2.1/index-all.html\r\nMy Suggesstion\r\nbefor\r\njobClient.newCompleteCommand(job.getKey()).variables(variables);\r\nafter\r\nSystem.out.println(job.getElementId());   // returns element Id (or) null\r\n                  Map variables = job.getVariablesAsMap();  // Get variables\r\njobClient.newCompleteCommand(job.getKey()).variables((Map.of(\"newVariable\",\"VariableFromClient\"));\n korthout: ZPA triage:\n- seems like a reasonable request\n- should be a good first issue\n- @aleksander-dytko please consider if you want the team to work on this from PM perspective\n aleksander-dytko: @korthout thanks for the mention. I don't see it as a priority at the moment - changing priority level to `Later`.\n megglos: ZDP-Triage:\n- not affecting ZDP",
    "title": "As a Zeebe Java user, I want to complete a Job with a single variable",
    "releaseNoteText": "**Consequence:**\nCompleting a job in Zeebe Java required wrapping the variable value in a Map before passing it to the `newCompleteCommand` method. This led to unnecessary overhead and additional code complexity for developers.\n\n**Cause:**\nThe underlying cause of this issue was the limited functionality of the `newCompleteCommand` method, which only offered a `.variables()` parameter that required a Map input.\n\n**Fix:**\nThe fix for this issue involved introducing a new feature that allows developers to complete a job with a single variable value. The `newCompleteCommand` method now accepts a `variable(key, value)` parameter, allowing developers to directly pass the variable value without the need for a Map wrapper.\n\n**Result:**\nAfter applying this fix, Zeebe Java users can now complete a job using the simplified syntax `client.newCompleteCommand(job).variable(\"name\", value)`. This removes the overhead of wrapping the value in a Map and provides a more intuitive and streamlined approach for completing jobs. Additionally, by promoting this feature and providing examples in the documentation, the Zeebe team aims to ensure that all developers, including those not familiar with newer Java versions, are aware of this shortcut."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12975",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nWhen configuring async message TTL checking, I need to know the health of my system. A key metric here is the number of \"buffered\" messages. When this number is steadily climbing, I might need to configure the TTL checker differently to ensure that messages expire fast enough and don't accumulate.\r\n\r\n**Describe the solution you'd like**\r\nExport more message related metrics. Similar to the banned instance metrics, these should restore on recovery so that the reported counts are accurate even when brokers restart.\r\n\r\n**Describe alternatives you've considered**\r\nNot exposing metrics for this directly, instead relying on the exported record stream. This is more complicated and requires a separate application continously running.\r\n\r\n**Additional context**\r\n\r\nrelated to https://jira.camunda.com/browse/SUPPORT-17177\r\n\n\n Zelldon: :bulb: Just as a thought we might want to consider this a general metric for all column families. \r\n\n megglos: ZDP-Planning:\n- low hanging fruit that can be done shortly\n- solution needs to be discussed still though\n megglos: @rodrigo-lourenco-lopes @oleschoenburg  we likely would need to backport this (at least back to 8.1) for a particular customer to make use of, is that feasible? \n rodrigo-lourenco-lopes: > @rodrigo-lourenco-lopes @oleschoenburg we likely would need to backport this (at least back to 8.1) for a particular customer to make use of, is that feasible?\r\n\r\nYes, it should be possible :) ",
    "title": "Export number of buffered messages as metrics",
    "releaseNoteText": "**Consequence:**\nThe user was unable to obtain the number of buffered messages as a metric, making it difficult to assess the health of the system and identify any potential issues.\n\n**Cause:**\nThe product did not have a feature in place to export the number of buffered messages as metrics, limiting the user's visibility into the system's performance.\n\n**Fix:**\nA fix was implemented to export additional message-related metrics, including the number of buffered messages. These metrics are now accurately reported even after broker restarts, providing a comprehensive view of the system's health.\n\n**Result:**\nWith this fix, users now have access to the number of buffered messages as a metric, allowing them to monitor the system's performance more effectively. This enables them to identify any issues related to message accumulation and adjust the TTL checker configuration accordingly."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12942",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "In our case，the directory of `/usr/local/zeebe/data/raft-partition/partitions/1` has about 60 .log files with each one has 128MB size data,and we have 3 patitions,so all the files‘s size almost reach 66GB.When we do backup api,it always failed by throwing exception like below:\r\n![image](https://github.com/camunda/zeebe/assets/12196018/ea47d5f7-9fe6-40dc-9d62-92ece0445266)\r\n\r\n`\r\n{'backupId': 1685672012, 'status': 'FAILED', 'failureReason': \"Backup on partition 3 failed due to java.util.concurrent.CompletionException: software.amazon.awssdk.core.exception.SdkClientException: Unable to execute HTTP request: Acquire operation took longer than the configured maximum time. This indicates that a request cannot get a connection from the pool within the specified maximum time. This can be due to high request rate.\\nConsider taking any of the following actions to mitigate the issue: increase max connections, increase acquire timeout, or slowing the request rate.\\nIncreasing the max connections can increase client throughput (unless the network interface is already fully utilized), but can eventually start to hit operation system limitations on the number of file descriptors used by the process. If you already are fully utilizing your network interface or cannot further increase your connection count, increasing the acquire timeout gives extra time for requests to acquire a connection before timing out. If the connections doesn't free up, the subsequent requests will still timeout.\\nIf the above mechanisms are not able to fix the issue, try smoothing out your requests so that large traffic bursts cannot overload the client, being more efficient with the number of times you need to call AWS, or by increasing the number of hosts sending requests.}\r\n`\r\nI also see the code source,S3BackupStore.java create NettyNioAsyncHttpClient with static number 45s of ConnectionAcquireTimeout,and other configuration used the default value.There is no any chance to customizing httpClient configurtation data .\r\nI'm not sure if i have missing somthing important document.Could you help me with this problem?  Thanks.\r\n@oleschoenburg \r\n![image](https://github.com/camunda/zeebe/assets/12196018/fd472cf0-be41-4d2f-ae4e-222a4b1725c9)\r\n\r\n\n\n oleschoenburg: Thanks for reporting @codingman1990 :+1: \r\n\r\nPlease correct me if I'm wrong but the situation is basically this:\r\nWhen a partition has a lot of log segments, uploading them in parallel will cause some uploads to wait for longer than 45 seconds on an available connection. This results in an exception and failure to complete the backup.\r\n\r\nWe should definitely improve this, although I'm not sure if just configuring the connection acquisition timeout is the right way to go about it.\r\n\r\nIn my opinion, the issue is rather that there is no limiting of parallel uploads. This is an issue here, with 66 log segments. It will also become an issue for large states, especially if #12483 is merged. So I would suggest to rather limit the number of concurrent uploads. The limit can be configurable of course.\r\n\r\n\r\n@codingman1990 Slightly unrelated but I was a bit surprised when I read that you have >60 log segments. I'd have expected much lower numbers. Did you adjust the snapshot interval or know any other reasons why your brokers keep more segments than usual?\r\n\r\n\n codingman1990: > Thanks for reporting @codingman1990 👍\r\n> \r\n> Please correct me if I'm wrong but the situation is basically this: When a partition has a lot of log segments, uploading them in parallel will cause some uploads to wait for longer than 45 seconds on an available connection. This results in an exception and failure to complete the backup.\r\n> \r\n> We should definitely improve this, although I'm not sure if just configuring the connection acquisition timeout is the right way to go about it.\r\n> \r\n> In my opinion, the issue is rather that there is no limiting of parallel uploads. This is an issue here, with 66 log segments. It will also become an issue for large states, especially if #12483 is merged. So I would suggest to rather limit the number of concurrent uploads. The limit can be configurable of course.\r\n> \r\n> @codingman1990 Slightly unrelated but I was a bit surprised when I read that you have >60 log segments. I'd have expected much lower numbers. Did you adjust the snapshot interval or know any other reasons why your brokers keep more segments than usual?\r\n\r\nIn my situation,there are abount 200 running instances at the same time.I almost had not edit any configuration except something like ES,BackupStore.The full configuration data is like below(Some sensitive data are marsked):\r\n\r\n`\r\n{\r\n    \"network\": {\r\n        \"host\": \"0.0.0.0\",\r\n        \"portOffset\": 0,\r\n        \"maxMessageSize\": \"4MB\",\r\n        \"advertisedHost\": \"***\",\r\n        \"commandApi\": {\r\n            \"host\": \"0.0.0.0\",\r\n            \"port\": 26501,\r\n            \"advertisedHost\": \"***\",\r\n            \"advertisedPort\": 26501,\r\n            \"address\": \"0.0.0.0:26501\",\r\n            \"advertisedAddress\": \"***\"\r\n        },\r\n        \"internalApi\": {\r\n            \"host\": \"0.0.0.0\",\r\n            \"port\": 26502,\r\n            \"advertisedHost\": \"***\",\r\n            \"advertisedPort\": 26502,\r\n            \"address\": \"0.0.0.0:26502\",\r\n            \"advertisedAddress\": \"***\"\r\n        },\r\n        \"security\": {\r\n            \"enabled\": false,\r\n            \"certificateChainPath\": null,\r\n            \"privateKeyPath\": null\r\n        },\r\n        \"maxMessageSizeInBytes\": 4194304\r\n    },\r\n    \"cluster\": {\r\n        \"initialContactPoints\": [\r\n            \"***\",\r\n            \"***\",\r\n            \"***\"\r\n        ],\r\n        \"partitionIds\": [\r\n            1,\r\n            2,\r\n            3\r\n        ],\r\n        \"nodeId\": 1,\r\n        \"partitionsCount\": 3,\r\n        \"replicationFactor\": 3,\r\n        \"clusterSize\": 3,\r\n        \"clusterName\": \"camunda-zeebe\",\r\n        \"heartbeatInterval\": \"PT0.25S\",\r\n        \"electionTimeout\": \"PT2.5S\",\r\n        \"membership\": {\r\n            \"broadcastUpdates\": false,\r\n            \"broadcastDisputes\": true,\r\n            \"notifySuspect\": false,\r\n            \"gossipInterval\": \"PT0.25S\",\r\n            \"gossipFanout\": 2,\r\n            \"probeInterval\": \"PT1S\",\r\n            \"probeTimeout\": \"PT0.1S\",\r\n            \"suspectProbes\": 3,\r\n            \"failureTimeout\": \"PT10S\",\r\n            \"syncInterval\": \"PT10S\"\r\n        },\r\n        \"raft\": {\r\n            \"enablePriorityElection\": true\r\n        },\r\n        \"messageCompression\": \"NONE\"\r\n    },\r\n    \"threads\": {\r\n        \"cpuThreadCount\": 3,\r\n        \"ioThreadCount\": 3\r\n    },\r\n    \"data\": {\r\n        \"directory\": \"/usr/local/zeebe/data\",\r\n        \"logSegmentSize\": \"128MB\",\r\n        \"snapshotPeriod\": \"PT5M\",\r\n        \"logIndexDensity\": 100,\r\n        \"diskUsageMonitoringEnabled\": true,\r\n        \"diskUsageReplicationWatermark\": 0.87,\r\n        \"diskUsageCommandWatermark\": 0.85,\r\n        \"diskUsageMonitoringInterval\": \"PT1S\",\r\n        \"backup\": {\r\n            \"store\": \"S3\",\r\n            \"s3\": {\r\n                \"bucketName\": \"zeebe-proc\",\r\n                \"endpoint\": \"***\",\r\n                \"region\": \"cn-hangzhou\",\r\n                \"accessKey\": \"***\",\r\n                \"secretKey\": \"***\",\r\n                \"apiCallTimeout\": \"PT3M\"\r\n            }\r\n        },\r\n        \"logSegmentSizeInBytes\": 134217728,\r\n        \"freeDiskSpaceCommandWatermark\": 5052946022,\r\n        \"freeDiskSpaceReplicationWatermark\": 4379219886\r\n    },\r\n    \"exporters\": {\r\n        \"elasticsearch\": {\r\n            \"jarPath\": null,\r\n            \"className\": \"io.camunda.zeebe.exporter.ElasticsearchExporter\",\r\n            \"args\": {\r\n                \"index\": {\r\n                    \"prefix\": \"zeebe-record\"\r\n                },\r\n                \"authentication\": {\r\n                    \"password\": \"***\",\r\n                    \"username\": \"***\"\r\n                },\r\n                \"url\": \"***\"\r\n            },\r\n            \"external\": false\r\n        }\r\n    },\r\n    \"gateway\": {\r\n        \"network\": {\r\n            \"host\": \"0.0.0.0\",\r\n            \"port\": 26500,\r\n            \"minKeepAliveInterval\": \"PT30S\"\r\n        },\r\n        \"cluster\": {\r\n            \"initialContactPoints\": [\r\n                \"0.0.0.0:26502\"\r\n            ],\r\n            \"requestTimeout\": \"PT15S\",\r\n            \"clusterName\": \"zeebe-cluster\",\r\n            \"memberId\": \"gateway\",\r\n            \"host\": \"0.0.0.0\",\r\n            \"advertisedHost\": \"0.0.0.0\",\r\n            \"port\": 26502,\r\n            \"advertisedPort\": 26502,\r\n            \"membership\": {\r\n                \"broadcastUpdates\": false,\r\n                \"broadcastDisputes\": true,\r\n                \"notifySuspect\": false,\r\n                \"gossipInterval\": \"PT0.25S\",\r\n                \"gossipFanout\": 2,\r\n                \"probeInterval\": \"PT1S\",\r\n                \"probeTimeout\": \"PT0.1S\",\r\n                \"suspectProbes\": 3,\r\n                \"failureTimeout\": \"PT10S\",\r\n                \"syncInterval\": \"PT10S\"\r\n            },\r\n            \"security\": {\r\n                \"enabled\": false,\r\n                \"certificateChainPath\": null,\r\n                \"privateKeyPath\": null\r\n            },\r\n            \"messageCompression\": \"NONE\"\r\n        },\r\n        \"threads\": {\r\n            \"managementThreads\": 1\r\n        },\r\n        \"security\": {\r\n            \"enabled\": false,\r\n            \"certificateChainPath\": null,\r\n            \"privateKeyPath\": null\r\n        },\r\n        \"longPolling\": {\r\n            \"enabled\": true\r\n        },\r\n        \"interceptors\": [],\r\n        \"initialized\": true,\r\n        \"enable\": false\r\n    },\r\n    \"backpressure\": {\r\n        \"enabled\": true,\r\n        \"algorithm\": \"VEGAS\",\r\n        \"aimd\": {\r\n            \"requestTimeout\": \"PT1S\",\r\n            \"initialLimit\": 100,\r\n            \"minLimit\": 1,\r\n            \"maxLimit\": 1000,\r\n            \"backoffRatio\": 0.9\r\n        },\r\n        \"fixed\": {\r\n            \"limit\": 20\r\n        },\r\n        \"vegas\": {\r\n            \"alpha\": 3,\r\n            \"beta\": 6,\r\n            \"initialLimit\": 20\r\n        },\r\n        \"gradient\": {\r\n            \"minLimit\": 10,\r\n            \"initialLimit\": 20,\r\n            \"rttTolerance\": 2.0\r\n        },\r\n        \"gradient2\": {\r\n            \"minLimit\": 10,\r\n            \"initialLimit\": 20,\r\n            \"rttTolerance\": 2.0,\r\n            \"longWindow\": 600\r\n        }\r\n    },\r\n    \"experimental\": {\r\n        \"maxAppendsPerFollower\": 2,\r\n        \"maxAppendBatchSize\": \"32KB\",\r\n        \"disableExplicitRaftFlush\": false,\r\n        \"rocksdb\": {\r\n            \"columnFamilyOptions\": {},\r\n            \"enableStatistics\": false,\r\n            \"memoryLimit\": \"512MB\",\r\n            \"maxOpenFiles\": -1,\r\n            \"maxWriteBufferNumber\": 6,\r\n            \"minWriteBufferNumberToMerge\": 3,\r\n            \"ioRateBytesPerSecond\": 0,\r\n            \"disableWal\": false\r\n        },\r\n        \"raft\": {\r\n            \"requestTimeout\": \"PT5S\",\r\n            \"maxQuorumResponseTimeout\": \"PT0S\",\r\n            \"minStepDownFailureCount\": 3,\r\n            \"preferSnapshotReplicationThreshold\": 100,\r\n            \"preallocateSegmentFiles\": true\r\n        },\r\n        \"partitioning\": {\r\n            \"scheme\": \"ROUND_ROBIN\",\r\n            \"fixed\": []\r\n        },\r\n        \"queryApi\": {\r\n            \"enabled\": false\r\n        },\r\n        \"consistencyChecks\": {\r\n            \"enablePreconditions\": false,\r\n            \"enableForeignKeyChecks\": false,\r\n            \"settings\": {\r\n                \"enablePreconditions\": false,\r\n                \"enableForeignKeyChecks\": false\r\n            }\r\n        },\r\n        \"engine\": {\r\n            \"messages\": {\r\n                \"ttlCheckerBatchLimit\": 2147483647,\r\n                \"ttlCheckerInterval\": \"PT1M\"\r\n            }\r\n        },\r\n        \"features\": {\r\n            \"enableYieldingDueDateChecker\": false,\r\n            \"enableActorMetrics\": false,\r\n            \"enableBackup\": true,\r\n            \"enableMessageTtlCheckerAsync\": false\r\n        },\r\n        \"maxAppendBatchSizeInBytes\": 32768\r\n    },\r\n    \"executionMetricsExporterEnabled\": false,\r\n    \"processing\": {\r\n        \"maxCommandsInBatch\": 1\r\n    }\r\n}\r\n`\r\n@oleschoenburg Could you please point out the problem.\n megglos: ZDP-Triage:\n- to be discussed at planning as flagged by Ole\n megglos: ZDP-Planning:\r\n- will become more likely with sst partitioning enabled",
    "title": "Support S3 backup httpclient custom configuration.",
    "releaseNoteText": "**Consequence:** The backup API fails when attempting to upload multiple log segments in parallel. This results in a `java.util.concurrent.CompletionException` and the backup process is marked as \"FAILED\". \n\n**Cause:** The issue arises due to the lack of limiting parallel uploads when there are a large number of log segments. In this specific case, with 66 log segments, some uploads wait for longer than the default connection acquisition timeout of 45 seconds, leading to backup failure.\n\n**Fix:** The fix involves introducing a limit on the number of concurrent uploads to prevent the issue of long wait times for available connections. The limit will be configurable, allowing users to adjust according to their specific requirements.\n\n**Result:** After applying the fix, the backup API will successfully upload log segments without exceeding the connection acquisition timeout. The number of concurrent uploads can be controlled by adjusting the configuration, ensuring smooth and efficient backup operations."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12878",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nIn the BPMN model API, a `send task` or `message throw event` should be able to reference a message definition.\r\n\r\n* a `send task` or `message throw event` can have an extension element `messageDefinition` with\r\n  * an attribute `messageName` that references the name of the message\r\n    * has either a static value or an expression\r\n    * must not be empty\r\n  * an attribute `correlationKey` that references the correlation key of the message\r\n    * has either a static value or an expression\r\n    * must not be empty\r\n  * an attribute `messageId` that references the id of the message\r\n    * has either a static value or an expression\r\n  * an attribute `timeToLive` that references the time to live of the message\r\n    * has either a static value or an expression\r\n*  a `send task` or `message throw event` must have either an extension element `messageDefinition` or `taskDefinition` but not both\r\n\r\n\r\nSample XML with static attributes:\r\n\r\n```\r\n<bpmn:sendTask id=\"send_message\">\r\n  <bpmn:extensionElements>\r\n    <zeebe:messageDefinition \r\n        messageName=\"foo\" \r\n        correlationKey=\"abc\" \r\n        messageId=\"abc\" \r\n        timeToLive=\"PT10S\"/>\r\n  </bpmn:extensionElements>  \r\n</bpmn:sendTask> \r\n\r\n\r\n<bpmn:intermediateThrowEvent id=\"Event_1nqoocy\">\r\n  <bpmn:extensionElements>\r\n    <zeebe:messageDefinition \r\n        messageName=\"foo\" \r\n        correlationKey=\"abc\" \r\n        messageId=\"abc\" \r\n        timeToLive=\"PT10S\"/>\r\n  </bpmn:extensionElements>\r\n  <bpmn:messageEventDefinition id=\"MessageEventDefinition_1wizp0b\" />\r\n</bpmn:intermediateThrowEvent>\r\n```\r\n\r\nSample XML with expression attributes:\r\n\r\n```\r\n<bpmn:sendTask id=\"send_message\">\r\n  <bpmn:extensionElements>\r\n    <zeebe:messageDefinition \r\n        messageName=\"=messageName\" \r\n        correlationKey=\"=correlationKey\" \r\n        messageId=\"=messageId\" \r\n        timeToLive=\"=timeToLive\"/>\r\n  </bpmn:extensionElements>  \r\n</bpmn:sendTask> \r\n\r\n<bpmn:intermediateThrowEvent id=\"Event_1nqoocy\">\r\n  <bpmn:extensionElements>\r\n    <zeebe:messageDefinition \r\n        messageName=\"=messageName\" \r\n        correlationKey=\"=correlationKey\" \r\n        messageId=\"=messageId\" \r\n        timeToLive=\"=timeToLive\"/>\r\n  </bpmn:extensionElements>\r\n  <bpmn:messageEventDefinition id=\"MessageEventDefinition_1wizp0b\" />\r\n</bpmn:intermediateThrowEvent>\r\n```\r\n\n\n korthout: Hi @lzgabel 👋 Sorry for the delay. I wanted to look at your [pull request](https://github.com/camunda/zeebe/pull/12879), but I want to understand the big picture before reviewing it.\r\n\r\nAs I understand, this issue is one of several parts of:\r\n- #1021?\r\n\r\nIn order to allow a user to model a process where a Send Task (or Messsage Throw Event) sends a message, we need to bind it to some Message Definition. Which is what this issue is about.\r\n\r\nIn your suggestion, the binding is achieved through a new `messageDefinition` extension element in the zeebe namespace, informing the message name, correlation key, message id and time-to-live of the message that should be published.\r\n\r\n❓ How did you come to this design? Was there a specific reason why you chose this over any other?\r\n\r\nFor example, there already exists a `MessageEventDefinition` in the BPMN spec that we could attempt to re-use for this. Here is an example of pure BPMN.\r\n\r\n```xml\r\n<bpmn:definitions ...>\r\n  <bpmn:process id=\"Process_02p7q4p\" isExecutable=\"true\">\r\n    ...\r\n    <bpmn:intermediateThrowEvent id=\"Event_145tbfx\">\r\n      <bpmn:messageEventDefinition id=\"MessageEventDefinition_0inu3y4\" messageRef=\"Message_1nb8aa6\" />\r\n    </bpmn:intermediateThrowEvent>\r\n    ...\r\n  </bpmn:process>\r\n  <bpmn:message id=\"Message_1nb8aa6\" name=\"order_placed\" />\r\n  ...\r\n</bpmn:definitions>\r\n```\n lzgabel: Hi @korthout. Thanks for you reply.  I'm sorry for starting the implementation without agreeing on this issue. :bow:\r\n\r\n> How did you come to this design? Was there a specific reason why you chose this over any other?\r\n\r\nActually, what I first thought was that this behavior is considered to be an internal implementation of the Zeebe engine, and should be create a new `messageDefinition` extension element in the zeebe namespace. 😄 \r\n\r\n> there already exists a MessageEventDefinition in the BPMN spec that we could attempt to re-use for this. Here is an example of pure BPMN.\r\n\r\nYes. You are absolutely right. 👍  I've checked the BPMN spec again, I will take your suggestion.\r\n\r\n---\r\n🤔 I plan to add an extension element under message, WDYT? :\r\n\r\n```xml\r\n<bpmn:message id=\"Message_1nb8aa6\" name=\"order_placed\">\r\n  <bpmn:extensionElements>\r\n    <zeebe:publish correlationKey=\"= orderId\" timeTolive=\"PT10S\" messageId=\"= uuid()\" />\r\n  </bpmn:extensionElements>\r\n</bpmn:message>\r\n```\r\n\r\n--- \r\nAt the same time, I think we can also support the `message end event`. 🚀 \r\n\r\n\r\n\r\n\n korthout: > I plan to add an extension element under message\r\n\r\n@lzgabel Thanks for checking with me. I don't know what is best. \r\n\r\nAn argument for putting this under the message is that this allows several tasks/events to publish the same message with the same details (correlation key, TTL, message id).\r\n\r\nBut the counter-argument is that this does not allow changing any of these for tasks/events that want to publish the same message but with different details.\r\n\r\nI'll check with our Modeling experts to see what fits best. I'll attempt to make a decision on this early next week. Is that okay for you, @lzgabel?\n lzgabel: > I'll check with our Modeling experts to see what fits best. I'll attempt to make a decision on this early next week. Is that okay for you, @lzgabel?\r\n\r\nYes. Looking forward to the conclusion.\n nikku: @korthout to mirror my internal comment here:\r\n\r\n* I'd love to clearly separate a message from \"stuff that is being done with the message, in the context of a flow / process execution\".\r\n* To support this I'd keep what is relevant in the _execution flow_ on the flow element (intermediate catch event in this case)\n barmac: Hi, I'd like to add my two cents. First, it's great that you are looking into this feature, and I appreciate it a lot that you pulled in Modeler devs :)\r\n\r\nLet's examine how we solve modeling problems with each of the solutions:\r\n\r\n| Problem | Properties defined on shared `bpmn:Message` | Properties defined on individual events via `bpmn:MessageEventDefinition`|\r\n|-|-|-|\r\n| I want to add another event which publishes message of given name with the same correlation key, TTL etc. | Create a new event and select shared message | Copy existing event |\r\n| I want to modify properties for a specific event | Create a new message with the same name but different propertie | Modify properties on the event |\r\n| I want to modify properties on all events | Modify in single place | Modify on each event |\r\n| I want to create an event with a different variable as correlation key | Create a new message with the same name | Change on individual event |\r\n\r\nSetting different properties for the same message name leads to redundancy in the diagram (multiple messages of the same name but different extension elements). I believe this is not what we want, therefore I'd support setting event-related properties on the event definition.\r\n\r\nNote that all of that can be also applied to catch event. \r\n\r\n---\r\n\r\nI started writing this before Nico's comment but we discussed this in the morning and have 100% agreement :)\r\n\r\nProposal:\r\n\r\n```xml\r\n<bpmn:endEvent id=\"Message_1nb8aa6\" name=\"order_placed\">\r\n  <bpmn:messageEventDefinition>\r\n    <bpmn:extensionElements>\r\n      <zeebe:publish correlationKey=\"= orderId\" timeTolive=\"PT10S\" />\r\n    </bpmn:extensionElements>\r\n  </bpmn:messageEventDefinition>\r\n</bpmn:endEvent>\r\n```\n barmac: Question: What would `messageId` be used for?\n nikku: > Question: What would `messageId` be used for?\r\n\r\n@barmac I asked myself the same question. It is another way to ensure idempotent message delivery ([internal ref](https://camunda.slack.com/archives/CSQ2E3BT4/p1686047258957109)).\n nikku: @barmac and as we discussed, based on https://github.com/camunda/zeebe/issues/12878#issuecomment-1578613868, imagine this (attaching subscription information to the message definition, not the `bpmn:Message` as it is currently the case):\r\n\r\n```xml\r\n<bpmn:intermediateCatchEvent id=\"Message_1nb8aa6\" name=\"order_processed\">\r\n  <bpmn:messageEventDefinition messageRef=\"order_processed_message\">\r\n    <bpmn:extensionElements>\r\n      <zeebe:subscribe correlationKey=\"= orderId\" />\r\n    </bpmn:extensionElements>\r\n  </bpmn:messageEventDefinition>\r\n</bpmn:intermediateCatchEvent>\r\n```\n lzgabel: 👋 Hi Guys. Will this [#12878 (comment)](https://github.com/camunda/zeebe/issues/12878#issuecomment-1578613868) be the final conclusion?\n korthout: Hi @lzgabel I've asked our internal Modeling experts. @nikku and @barmac gave great input, and we'll likely take that route. However, I don't want to take the opportunity away for others to voice their opinions. I set Monday, June 12th, as the deadline for input, so let's await that. I'll update this issue on Tuesday with the outcome.\n jonathanlukas: I would also prefer the approach @nikku proposes. While a message correlation surely is related to the message itself, the implementation details are more part of the event definition of each bpmn element.\r\n\r\nAlso, I think in Camunda 7, the message throw event was able to select a message element to refer to which still leads to confusion...\n korthout: Thanks everyone for your input 👏 \r\n\r\nNo opinions against the proposal were raised. Let's move forward with the following:\r\n\r\n- a new extension element that can be added to the `bpmn:messageEventDefinition` (of Intermediate Throw Event, or End Event) and/or to the `bpmn:sendTask` to specify the details of the message to publish (i.e. correlation key, time to live, message id)\r\n- let's name this extension element `zeebe:publishMessage` to make the XML easier to understand/read for humans\r\n- the message to publish can be referenced directly from the `bpmn:messageEventDefinition` and from the `bpmn:sendTask` according to BPMN spec using the `messageRef` attribute (the referenced message contains the message name).\r\n\r\nHere follows an example with three different elements all publishing the same message with different message details (i.e. correlation key, time to live, message id).\r\n\r\n```xml\r\n<bpmn:definitions ...>\r\n  <bpmn:process id=\"Process_02p7q4p\" isExecutable=\"true\">\r\n    ...\r\n    <bpmn:intermediateThrowEvent id=\"Event_145tbfx\">\r\n      <bpmn:messageEventDefinition id=\"MessageEventDefinition_0inu3y4\" messageRef=\"Message_1nb8aa6\">\r\n        <bpmn:extensionElements>\r\n          <zeebe:publishMessage correlationKey=\"= orderId\" timeTolive=\"PT10S\" messageId=\"= orderId\" />\r\n        </bpmn:extensionElements>\r\n      </bpmn:messageEventDefinition>\r\n    </bpmn:intermediateThrowEvent>\r\n    ...\r\n    <bpmn:sendTask id=\"Activity_1kgyl85\" messageRef=\"Message_1nb8aa6\">\r\n      <bpmn:extensionElements>\r\n        <zeebe:publishMessage correlationKey=\"= some_other_order_id\" />\r\n      </bpmn:extensionElements>\r\n    </bpmn:sendTask>\r\n    ...\r\n    <bpmn:endEvent id=\"Message_1nb8aa6\" name=\"order_placed\">\r\n      <bpmn:messageEventDefinition messageRef=\"Message_1nb8aa6\">\r\n        <bpmn:extensionElements>\r\n          <zeebe:publishMessage correlationKey=\"= orderId\" timeTolive=\"PT1H\" />\r\n        </bpmn:extensionElements>\r\n      </bpmn:messageEventDefinition>\r\n    </bpmn:endEvent>\r\n    ...\r\n  </bpmn:process>\r\n  <bpmn:message id=\"Message_1nb8aa6\" name=\"order_placed\" />\r\n  ...\r\n</bpmn:definitions>\r\n```\r\n\r\n\r\n\n lzgabel: @korthout. Thanks. ❤️ \r\nI'll be pushing the latest commit for this feature in the next few days.",
    "title": "Add message definition extension elements",
    "releaseNoteText": "**Consequence:**\nIn the BPMN model API, the `send task` or `message throw event` did not have the ability to reference a message definition. This hindered the ability to model processes where a Send Task or Message Throw Event sends a message.\n\n**Cause:**\nThe underlying cause of this issue was that there was no extension element available to specify the details of the message to be published, such as the correlation key, time to live, and message ID.\n\n**Fix:**\nA new extension element, `zeebe:publishMessage`, was added to the `bpmn:messageEventDefinition` of Intermediate Throw Event and End Event, as well as to the `bpmn:sendTask`. This allowed specifying the details of the message to be published.\n\n**Result:**\nWith this fix, users can now add the `zeebe:publishMessage` extension element to the relevant BPMN elements and specify the correlation key, time to live, and message ID of the message to be published. This enables the modeling of processes where Send Tasks and Message Throw Events send messages, and allows for more flexibility in specifying different details for the same message in different elements."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12796",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nWhen pushing a job fails, we should trigger the `JobYieldProcessor` to make the related job available for long polling.\r\n\r\n**Describe the solution you'd like**\r\n`JobIntent.YIELD` command should be appended when job push fails. This will be achieved through registering the error handler to `RemoteJobStreamErrorHandlerService` inside `JobStreamServiceStep`.\r\n\r\n**Describe alternatives you've considered**\r\n/\r\n\r\n**Additional context**\r\nBlocked by:\r\n* https://github.com/camunda/zeebe/issues/12541\r\n\n",
    "title": "Provide Error Handler implementation for Job Streamer",
    "releaseNoteText": "**Consequence:** When pushing a job failed, the `JobYieldProcessor` was not triggered, which resulted in the related job not being available for long polling.\n\n**Cause:** The error handler was not implemented for the Job Streamer, specifically the `RemoteJobStreamErrorHandlerService` inside `JobStreamServiceStep`.\n\n**Fix:** The error handler has been implemented and registered to the `RemoteJobStreamErrorHandlerService` inside `JobStreamServiceStep`. Now, when a job push fails, the `JobIntent.YIELD` command is appended.\n\n**Result:** When a job push fails, the `JobYieldProcessor` is now triggered, making the related job available for long polling."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12793",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\n\r\nWe can configure a raft request timeout that is applicable to all requests send between raft leaders and followers. InstallRequest send snapshot files which are considerably larger than other raft requests. As a result sometimes it takes longer to send the request and get a response, especially on networks with higher latency between brokers. This can result in timeout exception and the snapshot replication will have to restart from the beginning (See #11496). \r\n\r\nTo workaround, we can increase raft request timeout. But increasing this can affect how fast failures are detected, how fast requests are retried etc. So it would be better to be able to set a higher request timeout just of InstallRequest.\r\n\r\n**Describe the solution you'd like**\r\n\r\nExpose a configuration to set request timeout for InstallRequest. This should be different from the existing raft request timeout configuration.\r\n\r\n**Describe alternatives you've considered**\r\n\r\n- Fix #11496. This will not solve the issue, but reduces the impact if the request timesout.\r\n- Instead of sending the files as it is, split into to smaller requests. This would be a better solution, but requires more effort. https://github.com/camunda/zeebe/issues/12795 \r\n\r\n**Additional context**\r\nRelated to https://jira.camunda.com/browse/SUPPORT-16901 \r\n\n\n megglos: Triage:\n- it allows us to provide a better workaround in cases where the snapshot transmission takes a longer time (depending on the size of a single snapshot chunk up to 64MB) => we could just increase the timeout for installs but not everything\n- me moved it into ready already to get to it asap for the next patch (we have to do it next week)",
    "title": "Allow configuring request timeout for InstallRequest",
    "releaseNoteText": "**Consequence**: Increased timeout for InstallRequest to prevent timeout exceptions during snapshot replication, especially on networks with higher latency between brokers.\n\n**Cause**: Previously, the default request timeout for all raft requests was being used for InstallRequest as well. However, since InstallRequest sends larger snapshot files, it sometimes took longer to send the request and get a response, leading to timeout exceptions.\n\n**Fix**: Exposed a new configuration option to set a separate request timeout specifically for InstallRequest. This allows users to increase the timeout only for InstallRequest without affecting the timeout for other raft requests.\n\n**Result**: Users can now configure a higher request timeout for InstallRequest, reducing the chances of timeout exceptions during snapshot replication. This improves the reliability and efficiency of the snapshot replication process, especially on networks with higher latency between brokers."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12696",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\n\r\nSome process models can sometimes cause whole partitions to become completely bricked. For example, process models with a combination of loops, large multi-instance collection spawning activities, or straight-through processing (e.g. FEEL script tasks, undefined tasks, manual tasks, etc.) can cause us to write a large amount of follow up records, much faster than we can process or export. This can lead to:\r\n\r\n- The exported system being overloaded, e.g. Operate cannot import fast enough, Elasticsearch runs out of disk space, etc.\r\n- Runaway process instances cannot be canceled because the command is either not accepted, or written so far down the log that it takes hours to get there, at which point we've written millions more follow up records.\r\n- Node is out of disk space, leading to all partitions stopping.\r\n\r\n**Describe the solution you'd like**\r\n\r\nWhile preventing such situations is ideal, as a first step, we would like to provide an escape hatch for when they arise by allowing users to forcefully terminate a process without having to wait for all events to be processed.\r\n\r\nThis means providing a way to bypass the causality chain of the partition by directly modifying the state projection.\r\n\r\nWe would have to:\r\n\r\n1. Provide an API endpoint (whether a management or client endpoint is to be determined) where users can forcefully terminate a PI by key\r\n2. The leader of the partition for this PI must then modify the state to mark that PI as forcefully terminated. **It may not be possible to write to disk anymore, so we may have to simply modify the state**.\r\n3. The state modification must be replicated to all followers, much like the exporter state. This is to ensure consistency between all nodes should an election occur.\r\n4. Once a PI is forcefully terminated, then all records associated it still have to be read but can be skipped/not processed/replayed. **We cannot delete them from the log - this not only breaks the append-only contract, but is much more complicated**\r\n5. The records must also be skipped by the exporters. However, we may want to notify the external systems that the PI has been forcefully terminated somehow, so we may need some solution here. Maybe we still need to write a record at the end of the log specifying it was forcefully terminated.\r\n\r\nIn terms of UX, it would be great to be able to differentiate between normal termination and forceful termination, if only for monitoring and auditing purposes.\r\n\r\n**Describe alternatives you've considered**\r\n\r\n- Static analysis of process models to detect endless loops. This may not always be possible, and there may very well be valid use cases for such. Unclear how easy that is, especially things like a process with a call activity where the called ID is a variable which could point to the process itself.\r\n- Fair/weighted process scheduling. A rather complex solution, where the processor ensures that one PI cannot overwhelm the partition at the cost of other PIs or external user commands. This could be achieved by performing more look-ahead and determining whether it's safe to continue processing a given PI or if it should yield, and writing deferred computation records to resume it.\r\n\r\n**Additional context**\r\n\r\nSeveral incidents occurred recently which were the results of users deploying looping, straight-through processes, which were producing too writing more than the system could process/export. One was writing an exponential number of follow up records, which led to an unrecoverable out of disk space, where there was nothing to compact yet, so processing could never resume.\r\n\n\n korthout: @npepinpe I worry that termination of the process instance is considered a loss of data.\r\n\r\nInstead of termination of the process instance, I'd rather see something like suspension of the process instance. For example, by creating a process instance-level [Incident](https://docs.camunda.io/docs/components/concepts/incidents/) with a new `ErrorType` that prevents any further progress unless they are attempts to repair the instance, e.g. Modification, SetVariable, Migration, etc. This aids in visibility and allows users to repair the instance.\r\n\r\nSuch a solution is also what we are investigating in relation to:\r\n- #5121 \r\n- https://github.com/camunda/product-hub/issues/686\r\n\r\nI understand if that would not make it into the first iteration, but I'd hope we can replace the termination logic with what I've described here in some future iteration.\n Zelldon: Yesterday I was also thinking about suspending process instances just for the same use case, from the operations point of view, to get my system healthy again or avoid an imbalance of exporting and processing. \r\n\r\nNot sure whether the proposed solutions above are optimal, due to the state changes without processing a command. \r\n\r\n----\r\n\r\nWhat I thought we could do is have a runtime transient toggle that can be flipped for every process instance. If we see that a process instance makes problems we could flip it and the process instance will be ignored. Kind of similar to being blacklisted, but not persistent. If we restart the instance can be executed again. \r\n\r\n**What does ignored mean?** \r\n\r\nThere could be two things either, completely skip all commands, but then you will lose the progress. OR you simply append the current command for the process instance at the end of the log, without processing it. This allows to make progress still on other instances, and later again on the PI if it is no longer suspended. (BTW IMHO commands shouldn't be exported anyway (https://github.com/camunda/zeebe/issues/6749))\r\n\r\nWe need to allow certain commands like canceling still be processed for such PI. This allows to clean up, similar to we should allow for blacklisted instances.\r\n\r\n_______\r\n\r\n\r\nTo develop this further I could imagine that we could at some point have this even automatically, like normal OS scheduling or as we know in K8. If you have X process instances and Y CPUs than a PI is only allowed to take Y/X CPU time, if more we could suspend it for a time frame (append the current command at the end such it will be processed later).\r\n\n korthout: 🤔 Interesting ideas @Zelldon. \r\n\r\nYielding the commands (appending them to the end of the log instead of processing them) would solve parts of these issues, but not the fork bomb case where the log keeps expanding. Consider a fork bomb that's been running for a few minutes unnoticed. There are likely already thousands if not millions of unprocessed commands for that instance on the log. These will all be yielded to back of the log continuously. New commands will take at least the time that it takes to yield all those commands.\r\n\r\nI'm personally more in favor of skipping the commands. This puts the process instance in a state of limbo just like a blacklisted process instance. All we need is a way to start up processing again, and to allow the user to cancel the instance. \r\n\r\n- We could persist the skipped commands. As soon as we want to continue, we write these back to the log and continue processing. This has two downsides: disk usage and fork bomb would continue. So I don't think we should do this.\r\n- We could ignore the skipped commands. As soon as we want to continue, the user can usePI modification / migration to re-start the processing, or they can simply cancel the instance. \r\n\r\nLastly, I was wondering how you'd keep track of a runtime flag for each process instance? If a user has large state (millions of running PI) then this would consume a lot of memory. Why not persist it like blacklisting?\n deepthidevaki: > Not sure whether the proposed solutions above are optimal, due to the state changes without processing a command.\r\n\r\nI think it is possible to do this correctly. When we force \"suspend\" (for lack of a better word) a process, we should bypass the normal stream processing process. That means, we do not write the command to the logstream, but submit to StreamProcessor actor as a task to be processed immediately. When processing this task, it should generate followup events  to suspend the process. The followup event could be also the error record/incident record which @korthout mentioned. The follow up event should somehow refer to the invisible command that should have been at the current processingPosition (yet to figure out how to do it.). The event get's replicated and the followers will have the same state as the leader. Send the response only after this follow up event is committed, so that we know for sure that the process is suspended.\r\n\r\nSide note:- In general, it would make sense to not write user commands to the logstream. Only follow up command and events are required for deterministic replay. \n megglos: ZDP-Triage:\n- may be a topic that affects both teams\n- if such incidents occur we can't recover in an easy way (you would need to manually update the rocksDb sate to e.g. blacklist the instance)\n\n@abbasadel would be curious on the triage outcome of your team, as this would be crucial to mitigate incidents where we experience malicious processes\n megglos: @felix-mueller this is the feature I mentioned in the stakeholder round today in order to allow us to better handle incidents that are caused by malicious processes\n megglos: ZDP-Planning:\n- ZDP is picking this up to assess solutions for this asap to allow engineers to handle incidents better going forward\n remcowesterhoud: ZPA Triage:\r\n- The solution of bypassing the log is unrelated to ZPA, as we only build things on top of the log stream.\r\n- We are interested in what happens to the process instance. The state of this PI is our responsibility\r\n- Please reach out to us when you need involvement on deciding what happens with the PI! We are happy to support 🙂 \r\n- We're not planning to work on this unless we need to support the ZDP team. As a result we will remove it from our project board.\n Zelldon: First of all, I want to thank you all for your ideas and brain dumps you have done here. :bow: \r\n\r\nI thought about this for a while and also re-read all your comments. If you're interested, I used the following [gdoc](https://docs.google.com/document/d/1NDKjq7osuzYoFd5Jtj9Xhe5_buw8rVU6nLIqBDQwJ74/edit) to summarize and better assess potential solutions. \r\n\r\nI have a potential solution in mind, let me shortly explain it to you.\r\n\r\n## Proposal:\r\n\r\nI think we can combine several ideas together to get the best out of it. \r\n\r\nIt makes sense to have a two-step process implemented, which allows to first ban the instance and secondly (if wished) to cancel the instance.\r\n\r\nWe provide an API (potentially actuator) where we can send “ban instance X” request. We access the ZeebeDB with a separate context, as we do with the Exporters, and mark the process instance as banned. I think this should bring us a quick win and feels potentially easy to implement. \r\n\r\nThe ban column family is less frequented, which should allows us to already stop the processing for a specific PI, without conflicts. It might make sense to think about the blacklisting cache again, whether we really need it or we just delete it (otherwise we have to update flag).\r\n\r\nThe banning of the instance should be confirmed with an event on the log, this allows to replicate the state change, and followers can do the same.\r\n\r\nPlease be aware as soon as the instance is banned, there is right now no way to recover. [The process instance is in limbo, as ](https://github.com/camunda/zeebe/issues/12696#:~:text=I%27m%20personally%20more%20in%20favor%20of%20skipping%20the%20commands.%20This%20puts%20the%20process%20instance%20in%20a%20state%20of%20limbo%20just%20like%20a%20blacklisted%20process%20instance.%20All%20we%20need%20is%20a%20way%20to%20start%20up%20processing%20again%2C%20and%20to%20allow%20the%20user%20to%20cancel%20the%20instance.)[Nico Korthout](mailto:nico.korthout@camunda.com) said so nicely. To handle this differently I see this out of the scope of this feature since we expect it to use only for severe issues where the cluster is otherwise not recoverable (and/or can make no progress).\r\n\r\nYou may ask why is that actually the case. Because we likely skip commands which are then lost. Furthermore, there is right now no way to remove some PI from the banned instances list. A potential follow-up could be to allow removing instances from that list and applying PI modification (to repair instances).\r\n\r\nWe could think of [persisting the skipped records into the state](https://github.com/camunda/zeebe/issues/12696#:~:text=We%20could%20persist%20the%20skipped%20commands.%20As%20soon%20as%20we%20want%20to%20continue%2C%20we%20write%20these%20back%20to%20the%20log%20and%20continue%20processing.%20This%20has%20two%20downsides%3A%20disk%20usage%20and%20fork%20bomb%20would%20continue.%20So%20I%20don%27t%20think%20we%20should%20do%20this.), but I would rather not do that, since it could cause severe other issues due to limited space, etc. It is also not clear to me whether the gains are outweighed by the costs since we can also easily run process instance modification to recover such.\r\n\r\nThe approach above is a combination of multiple others mentioned before, and I feel it is best to not make it too easy to erase customer data, which is why cancelation is/can be the second step. \r\n\r\nGenerally, cancelation involves more work, due to the child's cancelation, etc. This should happen in the processing, with a separate command. Since it might fail as well, due to large nested instances, etc, but with this approach it is fine since it is already banned in this case. \r\n\r\nIn order to allow the cancelation we need to make sure that we implement the [cancelation for banned instances.](https://github.com/camunda/zeebe/issues/12772)\r\n\r\n## Next\r\n\r\nAs a next step, I would like to spend some time doing a POC and play a bit around with that. Please feel free to raise any concerns with this approach. I'm open for discussions of course.\r\n",
    "title": "Forcefully terminate a process instance",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12655",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\nAfter a release we must send a list of fixed issues related to support tickets. For this we look at the `support` label on issues. It's easy to forget to add this label.\n\n**Describe the solution you'd like**\nIntroduce a GitHub action that checks new issues and comments in issues if the text contains `SUPPORT-XXXX`. If it find any the action should add the `support` label.\n\n**Describe alternatives you've considered**\nN/A\n\n**Additional context**\nN/A\n\n\n remcowesterhoud: @abbasadel fyi 🙂 \n megglos: ZDP-Triage:\n- would be great to be done for all C8 teams actually\n- @megglos will take this over as part of the support/engineering collaboration\n",
    "title": "Automatically add `support` label to support related issues",
    "releaseNoteText": "**Consequence:** Issues related to support were not being labeled automatically, leading to manual effort in identifying support-related issues for release notes.\n\n**Cause:** The previous system did not have a mechanism in place to automatically add the `support` label to support-related issues.\n\n**Fix:** Introduced a GitHub action that checks new issues and comments for the text `SUPPORT-XXXX`. If found, the action automatically adds the `support` label to the respective issue.\n\n**Result:** Support-related issues are now automatically labeled with the `support` label, reducing the manual effort required to identify and track these issues. This ensures that the list of fixed issues related to support tickets can be easily generated after each release."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12575",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\n\r\nI think that it is not a good idea to collect snapshot files in the list. \r\nCheck it here: https://github.com/camunda/zeebe/blob/02d1df8ac1b6f5484f1e9d4c1f19c8a5712176b1/snapshot/src/main/java/io/camunda/zeebe/snapshots/impl/SnapshotChecksum.java#L41. \r\nBecause, after a while, the amount of snapshot files should grow. So, we could instead call the `.forEachOrdered` method to calculate the snapshot. I will provide the PR soon to see this in action and benchmark this.\r\n\r\n**Describe the solution you'd like**\r\nWe should call `Stream.forEachOrdered` instead of collecting snapshots `File`s in the list.\r\n\r\n**Describe alternatives you've considered**\r\nWe could use `Stream.forEach` but as I can understand the order is important, so we shouldn't do this.\n\n aivinog1: @Zelldon Hello 👋 \nI think that this could be addressed to the ZDP Team (sorry if I am wrong 😅)\nCould we assign this to someone, please? In our load test environment, we see a little degradation in creating snapshots and we are suspecting this part of the code.\nThank you in advance 🙂\n Zelldon: Looks like @deepthidevaki will look into your PR, thanks for providing it :)",
    "title": "Improve the traversing of snapshot files",
    "releaseNoteText": "**Consequence:**\nThe traversal of snapshot files in the system was inefficient as it involved collecting the files in a list, leading to performance degradation.\n\n**Cause:**\nThe inefficient traversal of snapshot files was caused by the usage of the `.forEachOrdered` method in the code, which accumulated the snapshot files in a list.\n\n**Fix:**\nThe code has been updated to use the `Stream.forEachOrdered` method instead, eliminating the need to collect the snapshot files in a list and improving performance.\n\n**Result:**\nAs a result of this fix, the traversal of snapshot files has been optimized and the system now has improved performance in creating snapshots. Users will experience faster processing times, especially in load test environments. This enhancement will also prevent potential degradation as the number of snapshot files increases over time."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12548",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nI understand that [the Actor metrics](https://github.com/camunda/zeebe/blob/a3d2002a0eb74cc701572b4e02abfeb16d6a48a8/scheduler/src/main/java/io/camunda/zeebe/scheduler/ActorMetrics.java) are in the experimental phase, but it would be nice if there exists the Actor dashboard out of the box 🙂\r\n\r\n**Describe the solution you'd like**\r\nA separate file, called, for example, actor.json in [the Grafana dashboards](https://github.com/camunda/zeebe/tree/a3d2002a0eb74cc701572b4e02abfeb16d6a48a8/monitor/grafana) folder.\r\n\r\n**Describe alternatives you've considered**\r\nAdd these metrics to the existing dashboard, for example, [zeebe.json](https://github.com/camunda/zeebe/blob/a3d2002a0eb74cc701572b4e02abfeb16d6a48a8/monitor/grafana/zeebe.json). But I think that it would be more convenient if we have this in a separate dashboard (some users may decide to not enable these metrics thus the dashboard is not needed for them)\n\n megglos: ZDP-Triage:\r\n- seems like a good first issue to pickup during an upcoming onboarding",
    "title": "Provide Grafana Dashboards for the Actor metrics",
    "releaseNoteText": "**Consequence:**\nPreviously, there was no out-of-the-box Grafana dashboard for the Actor metrics in the experimental phase.\n\n**Cause:**\nThe absence of a separate Grafana dashboard for the Actor metrics was due to the experimental nature of these metrics and the need to keep them separate from the existing dashboard.\n\n**Fix:**\nA new file called \"actor.json\" has been added to the Grafana dashboards folder to provide a separate dashboard for the Actor metrics.\n\n**Result:**\nUsers can now access a dedicated Grafana dashboard for the Actor metrics, providing better visibility and convenience for monitoring the experimental phase of these metrics. Users who do not enable these metrics can choose to not use the dashboard, ensuring flexibility in configuration."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12541",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThe following `Processor` classes are able to push an activated job to a client.\r\n\r\n- [JobWorkerTaskProcessor](https://github.com/camunda/zeebe/blob/319813a684325c97556fb59013eb8b88ea27b3f2/engine/src/main/java/io/camunda/zeebe/engine/processing/bpmn/task/JobWorkerTaskProcessor.java#L54) / [BpmnJobBehavior](https://github.com/camunda/zeebe/blob/4d46a4947e6d3ac72cb4e0af324f4c978b591989/engine/src/main/java/io/camunda/zeebe/engine/processing/bpmn/behavior/BpmnJobBehavior.java#L81-L92)\r\n- [JobTimeOutProcessor](https://github.com/camunda/zeebe/blob/ad1d5c92a3d4d6009da3a9d968238b83b9dd5c5c/engine/src/main/java/io/camunda/zeebe/engine/processing/job/JobTimeOutProcessor.java#L20)\r\n- [JobFailProcessor](https://github.com/camunda/zeebe/blob/813c86f780f54496fcd089810704c1a4e4958141/engine/src/main/java/io/camunda/zeebe/engine/processing/job/JobFailProcessor.java)\r\n- [JobRecurProcessor](https://github.com/camunda/zeebe/blob/813c86f780f54496fcd089810704c1a4e4958141/engine/src/main/java/io/camunda/zeebe/engine/processing/job/JobRecurProcessor.java)\r\n- [ResolveIncidentProcessor](https://github.com/camunda/zeebe/blob/18dd3c5e8df9bd2164e4d0fc73f5429c9d38b05c/engine/src/main/java/io/camunda/zeebe/engine/processing/incident/ResolveIncidentProcessor.java)\r\n\r\n**Describe the solution you'd like**\r\nThe `BpmnJobActivationBehavior` class is able to use the `JobStreamer` API to push jobs. The following steps need to be performed:\r\n\r\n- [x] For an available `JobStream` get `JobActivationProperties`\r\n- [x] Set `deadline` for `JobRecord` (using `JobActivationProperties`)\r\n- [x] Set `variables` for `JobRecord` (using `JobActivationProperties`) \r\n- [x] Set `worker` for `JobRecord` (using `JobActivationProperties`)\r\n- [x] Activate job using a `JobBatchRecord`/`JobBatchIntent.ACTIVATE`\r\n- [x] Push `JobRecord` on the `JobStream` through a `SideEffectProducer`\r\n\r\n**Describe alternatives you've considered**\r\n/\r\n\r\n**Additional context**\r\nBlocked by:\r\n* https://github.com/camunda/zeebe/issues/12083\r\n\n",
    "title": "Jobs are pushed from relevant processors",
    "releaseNoteText": "**Consequence:**\n\nJobs were not being pushed from relevant processors, resulting in a lack of activation of jobs for clients.\n\n**Cause:**\n\nThe `Processor` classes responsible for pushing activated jobs were not properly configured to utilize the `JobStreamer` API and perform the necessary steps for job activation.\n\n**Fix:**\n\nThe `BpmnJobActivationBehavior` class was modified to incorporate the use of the `JobStreamer` API for pushing jobs. The following steps were implemented:\n- Obtaining `JobActivationProperties` for an available `JobStream`\n- Setting the `deadline` for the `JobRecord` using `JobActivationProperties`\n- Setting the `variables` for the `JobRecord` using `JobActivationProperties`\n- Setting the `worker` for the `JobRecord` using `JobActivationProperties`\n- Activating the job using a `JobBatchRecord` with the intent `JobBatchIntent.ACTIVATE`\n- Pushing the `JobRecord` onto the `JobStream` through a `SideEffectProducer`\n\n**Result:**\n\nAfter the fix, the relevant processors are now able to properly push the activated jobs to the clients. This ensures that the jobs are correctly activated and processed within the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12539",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\n\r\nCreate a processor to process `ProcessInstanceBatch.TERMINATE` commands.\r\n\r\n- Add a method to the BpmnStateBehavior to get a defined amount of child elements, starting at a specific key.\r\n- Create the processor\r\n- Write a `ProcessInstance.TERMINATE` command for each of the element instance keys in the list\r\n- If the `index` is available:\r\n    - Get the next `BATCH_SIZE` + 1 child instances of the `batchElementInstanceKey`\r\n    - Create a new `ProcessInstanceBatch` record\r\n    - Write a new `ProcessInstanceBatch.TERMINATE` command\r\n- If no `index` is available:\r\n    - Have a 🍪 Nothing else to do\n",
    "title": "Create `ProcessInstanceBatch.TERMINATE` processor",
    "releaseNoteText": "**Consequence:**\n\nPreviously, there was no processor available to handle the `ProcessInstanceBatch.TERMINATE` commands. As a result, these commands could not be processed, causing a delay in terminating process instances.\n\n**Cause:**\n\nThe absence of a `ProcessInstanceBatch.TERMINATE` processor was the underlying cause of this issue. The BpmnStateBehavior did not have a method to retrieve a specific number of child elements starting from a given key, which prevented the creation of this processor.\n\n**Fix:**\n\nThis issue has been resolved by introducing a new processor called `ProcessInstanceBatch.TERMINATE`. To enable this, a method has been added to the BpmnStateBehavior that allows retrieving a specified number of child elements starting from a specific key. Additionally, a new `ProcessInstanceBatch` record is created when the `index` is available, and a `ProcessInstanceBatch.TERMINATE` command is written for each element instance key in the list.\n\n**Result:**\n\nWith this fix in place, the `ProcessInstanceBatch.TERMINATE` commands can now be properly processed by the system. When the `index` is available, the processor retrieves the next `BATCH_SIZE` + 1 child instances of the `batchElementInstanceKey`, creates a new `ProcessInstanceBatch` record, and writes a corresponding `ProcessInstanceBatch.TERMINATE` command. In the absence of an `index`, the system continues as usual with no additional actions required."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12538",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\n\r\nContainer elements (process, subprocess, etc.) all terminate their child instances when `onTerminate` is called. For all these elements this method calls the `BpmnStateTransitionBehavior#terminateChildInstances` method. This method needs to be changed to make use of the new `ProcessInstanceBatch` command with the `TERMINATE` intent.\r\n\r\n- Modify the `BpmnStateTransitionBehavior#terminateChildInstances `:\r\n    - Create a `ProcessInstanceBatch` record\r\n        - `batchElementInstanceKey` will be the key of the container element\r\n        - `index` will be empty as this is the first batch command\r\n    - Write the a `ProcessInstanceBatch.TERMINATE` command using the created record\n",
    "title": "Use the `ProcessInstanceBatch Command` when terminating container elements",
    "releaseNoteText": "**Consequence:** The system did not correctly terminate container elements when the `onTerminate` method was called. This resulted in child instances of these elements not being properly terminated.\n\n**Cause:** The issue was caused by the usage of the outdated `BpmnStateTransitionBehavior#terminateChildInstances` method. This method did not utilize the new `ProcessInstanceBatch` command with the `TERMINATE` intent.\n\n**Fix:** The `BpmnStateTransitionBehavior#terminateChildInstances` method has been modified. It now creates a `ProcessInstanceBatch` record for the container element and writes a `ProcessInstanceBatch.TERMINATE` command using the created record.\n\n**Result:** With the fix in place, container elements now correctly terminate their child instances when the `onTerminate` method is called. The new `ProcessInstanceBatch` command with the `TERMINATE` intent ensures that child instances are properly terminated, improving the overall behavior and reliability of the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12537",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\nThe `ProcessInstanceBatch` record and intent will be used to perform certain actions (terminating/activating children) on a process instance in batches.\r\n\r\nThe record will contain the following data:\r\n- `batchElementInstanceKey` - The element instance key of the element that the batch is executed. E.g., the key of a subprocess which will terminate all its children.\r\n- ~~`childElementInstanceKeys` - The element instance keys of all elements for which the batch action needs to be performed. E.g., all child instance for which a `TERMINATE` command needs to be written.~~\r\n- `index` - The index to keep track of where we are in the batch. Depending on the Intent this index can be something different. For `TERMINATE` this will be the element instance key of the first child in the next batch.\r\n\r\nThere will be only 1 intent for now:\r\n- `TERMINATE`\n",
    "title": "Create `ProcessInstanceBatch` Record and Intent",
    "releaseNoteText": "**Consequence**: The system lacked the ability to perform batch actions on process instances, such as terminating or activating children. This limitation affected the efficiency and flexibility of managing large sets of process instances. \n\n**Cause**: The absence of a `ProcessInstanceBatch` record and intent prevented users from executing batch actions on process instances. \n\n**Fix**: The `ProcessInstanceBatch` record and intent have been implemented to enable users to perform batch actions on process instances. The record now includes the `batchElementInstanceKey`, which represents the element instance key of the element that the batch is executed on. In addition, the `index` field has been added to keep track of the current position in the batch.\n\n**Result**: With this fix, users can now utilize the `ProcessInstanceBatch` record and intent to efficiently perform batch actions on process instances. They have the option to terminate or activate batches of child instances, enhancing their ability to manage and control process execution."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12416",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nIt is hard to tune the JVM if I cannot override the default `-Xms128m`. I can't override it via `JAVA_OPTS` because it is used at first, and then goes explicit `-Xms128m` (and finally applied):\r\n```\r\nexec \"$JAVACMD\" $JAVA_OPTS -Xms128m -XX:+ExitOnOutOfMemoryError -Dfile.encoding=UTF-8...\r\n```\r\n\r\n**Describe the solution you'd like**\r\nI would like to move all JVM options into the default Docker environment variable (`JAVA_OPTS`) so that users could override it easily (`-Xms` included).\r\n\r\n**Describe alternatives you've considered**\r\nWe could just remove `-Xms128m` from the default options, but I can't predict the consequences.\r\n\r\n**Additional context**\r\nSee the details [here](https://camunda-platform.slack.com/archives/C6WGNHV2A/p1681141205701259).\r\nI would like to see the changes backported in 8.0, 8.1, 8.2 as well 🙂\n\n remcowesterhoud: @megglos Do you know which team should be responsible for this one? I feel like this is more of a shared responsibility and I don't see it listed in the team split document.\n megglos: this affects the dist module (which is shared? 😅 ), the script is generated via the `appassembler-maven-plugin`, which has a feature request for exactly this open since 2016 😅 \r\nhttps://github.com/mojohaus/appassembler/issues/48\r\n\r\nJust synced with @Zelldon on this, the best would probably be to remove the Xms flag from the plugin config and stick to jvm defaults\r\nhttps://github.com/camunda/zeebe/blob/main/dist/pom.xml#L290\n remcowesterhoud: Oh right, I don't know how I missed that in the split document 🤦 \r\n\r\nI'll assign it to both our project in that case\n aivinog1: Hey @remcowesterhoud! If this is not occupied I could provide the PR to fix it:\r\n> Just synced with @Zelldon on this, the best would probably be to remove the Xms flag from the plugin config and stick to jvm defaults\n remcowesterhoud: @aivinog1 Thanks! It's not occupied so go ahead. I'll assign the issue to you 👍 ",
    "title": "Remove the default un-overridable `-Xms128m` value",
    "releaseNoteText": "**Consequence:** Users were unable to override the default `-Xms128m` value when tuning the JVM. This caused difficulty in customizing the JVM options for optimal performance.\n\n**Cause:** The default `-Xms128m` value was hardcoded in the script generated by the `appassembler-maven-plugin`. Attempts to override it via `JAVA_OPTS` were ineffective as the explicit `-Xms128m` flag was applied after.\n\n**Fix:** The `-Xms` flag was removed from the plugin configuration, allowing the JVM to use its default value. This change was implemented by @aivinog1.\n\n**Result:** Users can now easily override the default `-Xms` value by modifying the `JAVA_OPTS` environment variable. The JVM will use the appropriate value specified, providing greater flexibility in tuning the system for optimal performance."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12382",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nCurrently the zeebe process is run by the root user in the zeebe docker image:\r\n```\r\nroot@5ce6a5346a36:/usr/local/zeebe# ps aux\r\nUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\r\nroot         1  0.3  0.0   1940   448 ?        Ss   14:26   0:00 tini -- /usr/local/bin/startup.sh\r\nroot         7  197  1.4 4216156 295668 ?      Sl   14:26   0:05 /opt/java/openjdk/bin/java -Xms128m -XX:+ExitOnOutOfMemoryError -Dfile.encoding=UTF-8 -classpath /usr/local/zeebe/config:/usr/local/zeebe/lib/* -Dapp.name=broker -Dapp.pid=7 -Dapp.repo=/usr/local/zeebe\r\nroot        42  1.0  0.0   6880  3364 pts/0    Ss   14:26   0:00 /bin/bash\r\nroot        55  0.0  0.0   8476  2808 pts/0    R+   14:26   0:00 ps aux\r\n```\r\n\r\nTo harden the security of the docker image it should **default** to run it with an unprivileged user instead, [see the OWASP recommendation](https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-2-set-a-user).\r\n\r\n**Describe the solution you'd like**\r\nThe zeebe image **should by default run with an unprivileged user.** [There is already a zeebe user setup with uid 1000](https://github.com/camunda/zeebe/blob/main/Dockerfile#L105-L111) it's just not used.\r\n\r\n**Additional context**\r\nRelates to https://github.com/camunda/product-hub/issues/717\r\nsupport for running with an unprivileged user was added with https://github.com/camunda/zeebe/issues/11784\r\nRelates to https://jira.camunda.com/browse/SUPPORT-15969\n\n mvawork: Please review my pull request.\r\n\r\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code class=\"notranslate\"># ps -aux\r\nUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\r\nzeebe        1  0.1  0.0   2504   580 pts/0    Ss   12:28   0:00 tini -- /usr/local/bin/startup.sh /bin/bash\r\nzeebe       12 35.8  1.7 12603416 455936 pts/0 Sl+  12:28   1:18 /opt/java/openjdk/bin/java -XX:+ExitOnOutOfMemoryError -Dfile.encoding=UTF-8 -classpath /usr/local/zeebe/config:/usr/local/zeeb\r\nroot       100  0.0  0.0   2612   536 pts/1    Ss   12:28   0:00 /bin/sh\r\nroot       109  0.0  0.0   8892  3308 pts/1    R+   12:32   0:00 ps -aux\r\n# \r\n</code></pre><div class=\"zeroclipboard-container position-absolute right-0 top-0\">\r\n    <clipboard-copy aria-label=\"Copy\" class=\"ClipboardButton btn js-clipboard-copy m-2 p-0 tooltipped-no-delay\" data-copy-feedback=\"Copied!\" data-tooltip-direction=\"w\" value=\"root@5ce6a5346a36:/usr/local/zeebe# ps aux\r\nUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\r\nroot         1  0.3  0.0   1940   448 ?        Ss   14:26   0:00 tini -- /usr/local/bin/startup.sh\r\nroot         7  197  1.4 4216156 295668 ?      Sl   14:26   0:05 /opt/java/openjdk/bin/java -Xms128m -XX:+ExitOnOutOfMemoryError -Dfile.encoding=UTF-8 -classpath /usr/local/zeebe/config:/usr/local/zeebe/lib/* -Dapp.name=broker -Dapp.pid=7 -Dapp.repo=/usr/local/zeebe\r\nroot        42  1.0  0.0   6880  3364 pts/0    Ss   14:26   0:00 /bin/bash\r\nroot        55  0.0  0.0   8476  2808 pts/0    R+   14:26   0:00 ps aux\" tabindex=\"0\" role=\"button\" style=\"display: inherit;\">\r\n      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-copy js-clipboard-copy-icon m-2\">\r\n    <path d=\"M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z\"></path><path d=\"M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z\"></path>\r\n</svg>\r\n      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-check js-clipboard-check-icon color-fg-success m-2 d-none\">\r\n    <path d=\"M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z\"></path>\r\n</svg>\r\n    </clipboard-copy>\r\n  </div></div>\r\n\n jessesimpson36: I proposed an [alternative PR ](https://github.com/camunda/zeebe/pull/13418)to accomplish the same goal, without gosu.  I believe there was a concern that if you set `USER zeebe` about whether you could exec into the container as root, and I did verify manually that you can run the process as root via `docker run --user root ...`  as well as `docker exec -it --user root ...`.\r\n\r\nThis change will require a helm chart change, but I'm on the team that works on that, so coordinating that change shouldn't be much of a challenge.",
    "title": "Docker: Run the zeebe process with an unprivileged user by default",
    "releaseNoteText": "**Consequence:** The zeebe process in the Docker image is currently run by the root user. This poses a security risk according to the OWASP recommendation.\n\n**Cause:** The zeebe user with UID 1000, which is already set up in the Docker image, is not used by default.\n\n**Fix:** The zeebe image now runs with an unprivileged user by default. The Dockerfile has been updated to utilize the existing zeebe user.\n\n**Result:** With this fix, the zeebe process will be run with an unprivileged user by default, improving the security of the Docker image."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12283",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nI want to be able to open processes modeled with `zeebe-bpmn-model` in the Desktop Modeler as a C8 process by default. Currently, I need to set a lot of attributes by hand.\r\n\r\n```java\r\nDefinitions definitions = modelInstance.newInstance(Definitions.class);\r\n            definitions.setTargetNamespace(BPMN20_NS);\r\n            //definitions.setExporter(\"Camunda Modeler\");\r\n            //definitions.setExporterVersion(\"5.8.0\");\r\n            definitions.setAttributeValueNs(\"http://camunda.org/schema/modeler/1.0\",\"modeler:executionPlatform\",\"Camunda Cloud\");\r\n            modelInstance.setDefinitions(definitions);\r\n```\r\n\r\n**Describe the solution you'd like**\r\nProvide reasonable defaults for these attributes.\r\n\r\n**Describe alternatives you've considered**\r\nNone, the workaround above works as well.\r\n\r\n**Additional context**\r\nRequested by @superbeagle \n\n korthout: ZPA triage:\n\n- @remcowesterhoud mentioned that we should indicate somewhere in these attributes that this was generated with the zeebe-bpmn-model\n- @koevskinikola reproduced the issue already and it wasn't very pleasant\n- @koevskinikola the values should be defaults and can be changed by the user via the API (at least like above, or perhaps with an improved API)\n- @remcowesterhoud we should also add the other missing attributes\n- our own priority for this is low, but we think it's a good first issue for new contributors\n\n",
    "title": "Zeebe BPMN Model should provide reasonable defaults for definition attributes",
    "releaseNoteText": "**Consequence:**\nPreviously, when using the `zeebe-bpmn-model` to model processes, users had to manually set multiple attributes by hand in order to open the processes in the Desktop Modeler as a C8 process. This process was cumbersome and time-consuming.\n\n**Cause:**\nThe lack of reasonable defaults for the definition attributes in the `zeebe-bpmn-model` caused users to manually set the necessary attributes. This was a technical limitation of the product.\n\n**Fix:**\nIn this release, we have added reasonable defaults for the definition attributes in the `zeebe-bpmn-model`. This allows users to avoid manually setting these attributes, saving time and effort.\n\n**Result:**\nWith this fix applied, users will no longer have to manually set the attributes when modeling processes with the `zeebe-bpmn-model`. The default values will be automatically applied, making the process more efficient and user-friendly."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12085",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\nA job pushed to the `JobStreamer` API may fail to be handed over to the client due to client failure.\r\n\r\n## AC\r\n* A new `JobIntent.YIELD` intent is added.\r\n* A new `JobYield` processor is added. The processor will perform logic similar to the `JobFailProcessor`, i.e. make a job `ACTIVATABLE` again.\r\n\r\n## Additional context\r\n* The implementation of this issue should then be used in the job push `ErrorHandler` implementation.\n",
    "title": "Job Yield Processor is implemented to be used for Job Push fallback",
    "releaseNoteText": "**Consequence:** Previously, when a job pushed to the `JobStreamer` API failed to be handed over to the client due to client failure, there was no fallback mechanism in place. \n\n**Cause:** The lack of a fallback mechanism in the `JobStreamer` API caused jobs to be permanently stuck in a failed state when a client failure occurred.\n\n**Fix:** A new `JobYield` processor has been added to handle the fallback scenario when a job fails to be handed over to the client. This processor performs similar logic to the existing `JobFailProcessor`, allowing the job to become activatable again.\n\n**Result:** With this fix, jobs that fail to be handed over to the client due to client failure will now be processed by the new `JobYield` processor, which sets the job to an `ACTIVATABLE` state. This enables the job to be retried or processed by other fallback mechanisms, ensuring a smoother and more reliable job processing experience. The implementation of this fix should also be used in the job push `ErrorHandler` implementation for comprehensive error handling."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12000",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThis is part of the break-down of the following epic https://github.com/camunda/product-hub/issues/120 .\r\n\r\n**Describe the solution you'd like**\r\nIn order to add native support for oauth token authentication in Camunda 8 SM the Zeebe Gateway needs to get extended to perform oauth authentication token validation. The scope of this feature is just authentication, authorisation is to be added in a follow-up. Ideally the solution is making use of the identity-sdk for token verification.\r\n\r\n**Describe alternatives you've considered**\r\nAn alternative would be adding an auth proxy component to the Camunda 8 SM stack, however this adds more complexity to the infrastructure.\r\n\r\n**Additional context**\r\n- https://github.com/camunda-community-hub/zeebe-keycloak-interceptor Community project\r\n- Identity interceptor prototype https://github.com/npepinpe/zeebe-identity-interceptor/blob/main/src/main/java/org/camunda/community/zeebe/interceptors/identity/Interceptor.java\r\n\r\n\r\nRelates to:\r\n- https://jira.camunda.com/browse/SUPPORT-15807\n",
    "title": "OAuth Auth Token authentication support in Zeebe Gateway",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11920",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "- ## Adjust `EndEventProcessor` to broadcast a signal on activation\r\n\t- Introduce a new `SignalEndEventBehavior`\r\n\t- When Signal End Event activates:\r\n\t\t- Apply input mappings\r\n\t\t- Transition to activated\r\n\t\t- Write `Signal:Broadcast` command\r\n\t\t- Apply output mappings\r\n\t\t- Transition to complete the element\n",
    "title": "Support Broadcast signal for Signal End Events",
    "releaseNoteText": "**Consequence:** The broadcast signal for Signal End Events was not supported, resulting in the inability to trigger a signal when the Signal End Event activates.\n\n**Cause:** The `EndEventProcessor` did not have the capability to broadcast a signal on activation.\n\n**Fix:** To address this issue, a new feature called `SignalEndEventBehavior` was introduced. When a Signal End Event activates, the system now applies input mappings, transitions to the activated state, writes a `Signal:Broadcast` command, applies output mappings, and finally transitions to complete the element.\n\n**Result:** With this fix, users can now utilize Signal End Events to trigger signals and achieve the expected behavior in the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11919",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "- ## Adjust `IntermediateThrowEventProcessor` to broadcast a signal on activation\r\n\t- When Signal Intermediate Throw Event activates:\r\n\t\t- Apply input mappings\r\n\t\t- Transition to activated\r\n\t\t- Write `Signal:Broadcast` command\r\n\t\t- Apply output mappings\r\n\t\t- Transition to complete the element\r\n\r\n\n",
    "title": "Support Broadcast signal for Signal Intermediate Throw Events",
    "releaseNoteText": "**Consequence:** The signal intermediate throw events were not broadcasting a signal when activated, resulting in an incomplete communication flow.\n\n**Cause:** The `IntermediateThrowEventProcessor` did not have the functionality to broadcast a signal upon activation.\n\n**Fix:** We made adjustments to the `IntermediateThrowEventProcessor` to incorporate broadcasting of a signal when the signal intermediate throw event is activated. This includes applying input mappings, transitioning to the activated state, writing a `Signal:Broadcast` command, applying output mappings, and transitioning to complete the element.\n\n**Result:** With this fix, the signal intermediate throw events can now properly broadcast a signal when activated. This ensures a seamless communication flow and allows for effective synchronization between different parts of the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11708",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\n\r\nIn order to register job streams to the gateway, we will need to add a new gRPC API for this. This API will be a unidirectional stream (from server to client). \r\n\r\nIt should take in as parameter the same activation properties as the job worker (minus anything related to long polling and the likes), but it will be a long-living stream. Later, when implementing it on the client side and on the gateway, it's important that its keep-alive be configured properly.\r\n\r\nIt returns a stream of single `ActivatedJob`. We can discuss if we want to keep the batching properties, as this could be a useful optimization, but I would propose to ignore it for the alpha target and measure its impact once we have the complete end-to-end pipeline.\n",
    "title": "Add gRPC job stream API",
    "releaseNoteText": "**Consequence**: The system did not have a gRPC API for registering job streams to the gateway, which resulted in a limited functionality for job management. Users were not able to have a long-living stream for job activation properties.\n\n**Cause**: The absence of a gRPC API for job streams was due to the lack of implementation in the system. \n\n**Fix**: A new gRPC API for job streams was added to support registering job streams to the gateway. The API was designed as a unidirectional stream from the server to the client, accepting the same activation properties as the job worker.\n\n**Result**: With the addition of the new gRPC job stream API, users can now register job streams to the gateway and receive a long-living stream of single `ActivatedJob` objects. The system's job management functionality is enhanced, allowing for improved end-to-end pipeline performance."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/10031",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThe BPMN symbol for the inclusive gateway is supported with\r\n- #9747 \r\n\r\nHowever, only the diverging behavior was added. Users may want to use the converging behavior as well.\r\n\r\n**Describe the solution you'd like**\r\n- remove the validation that restricts the number of incoming sequence flows to an inclusive gateway to max 1.\r\n- only activate the inclusive gateway when (see discussion https://github.com/camunda/zeebe/pull/9747#discussion_r925770530):\r\n  - there are no active children of its flow scope instance (we can check this already),\r\n  - or if all incoming sequence flows were taken at least once (we can check this already),\r\n  - or if no path can be found from any of the active children to the inclusive gateway (but we can't check this yet).\r\n\r\n**Describe alternatives you've considered**\r\nUsers can use a combination of parallel and exclusive gateways to build similar joining logic.\r\n\r\n**Additional context**\r\nSpec: https://www.omg.org/spec/BPMN/2.0.2/PDF\r\n\r\nProduct-Hub:\r\n- https://github.com/camunda/product-hub/issues/364\r\n\n\n lzgabel: Hi @remcowesterhoud. I'm working on this recently, please assign this task to me. Thanks. ❤️\n lzgabel: Hi @remcowesterhoud. Sorry to bother you, :bow:  because I'm currently implementing this feature, and I need to get all child instances in the scope, but when I rebase the changes on top of  `main`, I found that you removed this `BpmnStateBehavior#getChildInstances` method by this [commit](https://github.com/camunda/zeebe/pull/12604/commits/0f3f458221a72b2fae27e9a1f5bc7dad890c4395), so I will add it back.\n remcowesterhoud: > Hi @remcowesterhoud. Sorry to bother you, :bow:  because I'm currently implementing this feature, and I need to get all child instances in the scope, but when I rebase the changes on top of  `main`, I found that you removed this `BpmnStateBehavior#getChildInstances` method by this [commit](https://github.com/camunda/zeebe/pull/12604/commits/0f3f458221a72b2fae27e9a1f5bc7dad890c4395), so I will add it back.\n\nIt was unused 😄\n\nOf course, go ahead and put it back if you need it!\n remcowesterhoud: @koevskinikola assigning you on the issue, so it's clear on our board you are reviewing it\n aisong: I'm really looking forward to this feature. I wonder when it will be released?\n lzgabel: Hi @aisong. This feature is available after version `8.3.0-alpha2`. It also brings another problem, which I am currently solving: #13070 \r\n\r\nBTW. `8.3.0` will be released this October.\n aisong: > Hi @aisong. This feature is available after version `8.3.0-alpha2`. It also brings another problem, which I am currently solving: #13070\r\n> \r\n> BTW. `8.3.0` will be released this October.\r\n\r\nOK,thank you. I'll be waiting for your good product.\n jschulenklopper: Looking forwards to this. This is related to the error message in the Modeler that reads \"An <Inclusive Gateway> with more than one incoming <Sequence Flow> is not supported by Camunda 8.2\", right? And delivery of this feature will deprecate the warning that's on https://docs.camunda.io/docs/components/modeler/bpmn/inclusive-gateways/?\r\n\r\nAnd for the modeller, the missing feature will be delivered per https://github.com/camunda/camunda-modeler/issues/3613 ?\r\n\r\n\n korthout: Great to hear you're all excited about this feature 🚀 \r\n\r\n@jschulenklopper That's all correct! You can already tryout the feature in `8.3.0-alpha2`, but Modeler does not take this into account yet (which is why it raises the warning). As @lzgabel mentioned, this is planned to be fully available in `8.3.0` (release planned for October 2023)\n korthout: Due to several limitations, we had to revert the implementation of the converging inclusive gateway:\r\n- https://github.com/camunda/zeebe/issues/13640\r\n\r\nTherefore, I'm re-opening this issue.\n lzgabel: Hi @korthout. I'll continue to look into this issue in the next few days. If you have a better solution, please let me know. 🙇 \n korthout: @lzgabel Sorry, I haven't spent time on this topic in a while, and I don't yet have ideas other than those that I wrote [here](https://github.com/camunda/zeebe/issues/13070#issuecomment-1649387794). So, we need some way to store in the state that a sequence flow is active and expect to process a command to activate the target element at some point in the future. If you want to work on this feature, I think we need to tackle this part first.",
    "title": "Support BPMN Converging Inclusive Gateway",
    "releaseNoteText": "**Consequence:**\nBPMN converging inclusive gateway behavior is missing from the supported features. This means that users have been unable to utilize the converging behavior of inclusive gateways.\n\n**Cause:**\nThe validation in the system restricted the number of incoming sequence flows to an inclusive gateway to a maximum of one. This limitation prevented the implementation of converging inclusive gateway behavior.\n\n**Fix:**\nThe fix involved removing the validation that restricted the number of incoming sequence flows to an inclusive gateway. Additionally, the fix introduced conditions for activating the inclusive gateway, which included checking for active children of the flow scope instance and ensuring that all incoming sequence flows were taken at least once. However, there was still a limitation in checking if no path could be found from any active child to the inclusive gateway.\n\n**Result:**\nWith this fix, users can now utilize the converging behavior of inclusive gateways. The system no longer restricts the number of incoming sequence flows to an inclusive gateway, allowing for more flexible and comprehensive handling of process flows. This enhancement provides users with a more powerful and versatile tool for designing their BPMN workflows."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14509",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nAfter restoring from backup, some partitions encountered [following error](https://console.cloud.google.com/logs/query;cursorTimestamp=2023-09-26T11:26:03.072149905Z;endTime=2023-09-26T11:56:36.088Z;pinnedLogId=2023-09-26T11:26:01.585896295Z%2Fh1uykcs15coafgsq;query=logName:%22stdout%22%0Aresource.type%3D%22k8s_container%22%0Aresource.labels.container_name%3D%22zeebe%22%0Aresource.labels.namespace_name%3D%22eab43606-a898-467f-967f-21a757fc69b7-zeebe%22%0Aresource.labels.location%3D%22europe-west1%22%0Aresource.labels.project_id%3D%22camunda-saas-int-chaos%22%0Aresource.labels.cluster_name%3D%22worker-chaos-1%22%0Aresource.labels.pod_name%3D%22zeebe-1%22%0AjsonPayload.context.partitionId%3D%2226%22%0Atimestamp%3D%222023-09-26T11:26:01.585896295Z%22%0AinsertId%3D%22h1uykcs15coafgsq%22;startTime=2023-09-26T10:56:36.088Z;summaryFields=jsonPayload%252Fcontext%252FpartitionId,resource%252Flabels%252Fpod_name:false:32:beginning?project=camunda-saas-int-chaos).\r\n\r\n```\r\njava.lang.IllegalStateException: Expected to delete index after 3982, but it is lower than the commit index 3983. Deleting committed entries can lead to inconsistencies and is prohibited.\r\n\tat io.atomix.raft.storage.log.RaftLog.deleteAfter(RaftLog.java:168) ~[zeebe-atomix-cluster-8.2.13.jar:8.2.13]\r\n\tat io.atomix.raft.roles.PassiveRole.tryToAppend(PassiveRole.java:565) ~[zeebe-atomix-cluster-8.2.13.jar:8.2.13]\r\n\tat io.atomix.raft.roles.PassiveRole.appendEntries(PassiveRole.java:511) ~[zeebe-atomix-cluster-8.2.13.jar:8.2.13]\r\n\tat io.atomix.raft.roles.PassiveRole.handleAppend(PassiveRole.java:367) ~[zeebe-atomix-cluster-8.2.13.jar:8.2.13]\r\n```\r\n\r\nWhat happened:\r\n\r\nFirst Zeebe-1 votes for Zeebe-2 and Zeebe-2 becomes leader.\r\n```\r\nINFO 2023-09-26T11:25:57.870482436Z [jsonPayload.context.partitionId: 26] [resource.labels.podName: zeebe-1] RaftServer{raft-partition-partition-26}{role=FOLLOWER} - Accepted PollRequest{term=0, candidate=2, lastLogIndex=3982, lastLogTerm=2}: candidate's log is up-to-date\r\nINFO 2023-09-26T11:25:57.887139637Z [jsonPayload.context.partitionId: 26] [resource.labels.podName: zeebe-1] RaftServer{raft-partition-partition-26}{role=FOLLOWER} - Accepted VoteRequest{term=1, candidate=2, lastLogIndex=3982, lastLogTerm=2}: candidate's log is up-to-date\r\nINFO 2023-09-26T11:25:57.892834402Z [jsonPayload.context.partitionId: 26] [resource.labels.podName: zeebe-1] RaftServer{raft-partition-partition-26} - Found leader 2\r\n```\r\nSee above. `term` is 1, but the lastLogTerm is 2. This is because after restore, raft metastore is empty. So it restarts the term from 1. This is ok so far. But after Zeebe-2 becomes the leader and commits its InitialEntry at index `3983`, Zeebe-0 starts election probably because it did not receive any heartbeat from the leader `Zeebe-2`.\r\n\r\n```\r\nINFO 2023-09-26T11:26:01.569385207Z [jsonPayload.context.partitionId: 26] [resource.labels.podName: zeebe-1] RaftServer{raft-partition-partition-26}{role=FOLLOWER} - Accepted PollRequest{term=1, candidate=0, lastLogIndex=3982, lastLogTerm=2}: candidate's log is up-to-date\r\nINFO 2023-09-26T11:26:01.577404370Z [jsonPayload.context.partitionId: 26] [resource.labels.podName: zeebe-1] RaftServer{raft-partition-partition-26}{role=FOLLOWER} - Accepted VoteRequest{term=2, candidate=0, lastLogIndex=3982, lastLogTerm=2}: candidate's log is up-to-date\r\nINFO 2023-09-26T11:26:01.582069421Z [jsonPayload.context.partitionId: 26] [resource.labels.podName: zeebe-1] RaftServer{raft-partition-partition-26} - Found leader 0\r\n```\r\nZeebe-1 accepts poll request from Zeebe-0 because Zeebe-0's lastLogTerm > Zeebe-1's current term 1. This leads to inconsistency. \r\n\r\n**Impact**\r\nIf it happens immediately after restore before writing any new data, it should be safe to restart the brokers. Due to the above error, the new leader Zeebe-0 cannot commit anything. So, as a result there won't be any actual data inconsistency. If we wait until all nodes are healthy before restarting the traffic to the cluster, this will not lead to any actual data inconsistency because no user requests has been processed yet. However, if there are new user data that has been processed, then there is a possibility for partial data loss. **Note that the data recovered from the backup won't be lost if this happens. Only new data will be affected.**\r\n\r\n**To Reproduce**\r\n\r\nFollowing scenario can lead to this error:\r\n1. A, B, C restored from the same backup, where the term of the last entry in the log is > 1\r\n2. A, B forms the quorum and A became the leader\r\n3. A and B committed new entries \r\n4. C does not know about the new leader. So it starts new election.\r\n\r\n**Expected behavior**\r\n\r\nAfter, restoring from a backup raft can continue working in all scenarios.\r\n\r\n**Environment:**\r\n- Zeebe Version: Observed in 8.2.13\r\n\n",
    "title": "Potential inconsistency in raft after restoring from a backup",
    "releaseNoteText": "**Consequence:**\nAfter restoring from a backup, some partitions encountered an error where the system attempted to delete an index that was lower than the commit index, resulting in potential inconsistencies. This caused the leader to be unable to commit any new entries.\n\n**Cause:**\nThe issue occurred because, after restoring from a backup, the raft metastore was empty, causing the system to restart the term from 1. When the new leader committed its initial entry, the previous leader started a new election because it did not receive any heartbeats from the current leader. This led to an inconsistency where the follower accepted a poll request from the candidate with a higher lastLogTerm.\n\n**Fix:**\nThe code has been updated to handle the situation after restoring from a backup. The system now ensures that the lastLogTerm is properly initialized, preventing inconsistencies in the leader election process.\n\n**Result:**\nAfter the fix, the system is able to properly handle the situation after restoring from a backup. The leader election process works correctly, ensuring that the leader can commit new entries without encountering errors. This prevents potential data inconsistencies and ensures the stability of the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14486",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nWhen writing to the checksum file of a snapshot fails, we silently ignore `IOException`s:\r\n\r\nhttps://github.com/camunda/zeebe/blob/6d3a703a34677809e08335e0f19c81814d59673c/snapshot/src/main/java/io/camunda/zeebe/snapshots/impl/SfvChecksumImpl.java#L74-L88\r\n\r\n`PrintWriter#flush` does not throw `IOException`s but only set's an internal error flag. The documentation recommends to use `PrintWriter#checkError` to flush and check for errors at the same time.\r\n\r\n**To Reproduce**\r\n\r\nProbably difficult to reproduce but can happen for any IO failure while writing the checksum file.\r\n\r\n**Expected behavior**\r\n\r\nThe exception is thrown so that the snapshot store can handle it as a failed attempt to commit a snapshot.\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\n<STACKTRACE>\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.3.0-alpha6, possibly all versions\r\n\r\n\n",
    "title": "Failure to write snapshot checksum is silently ignored",
    "releaseNoteText": "**Consequence:** The system silently ignores any `IOException` that occurs when writing to the checksum file of a snapshot. This means that if there is a failure in writing the checksum, it is not reported or handled as a failed attempt to commit a snapshot.\n\n**Cause:** The issue arises from the use of `PrintWriter#flush` which does not throw `IOException`s but only sets an internal error flag. The recommended approach is to use `PrintWriter#checkError` which flushes and checks for errors simultaneously.\n\n**Fix:** The fix involves replacing the usage of `PrintWriter#flush` with `PrintWriter#checkError` in order to correctly handle `IOException`s when writing the checksum file.\n\n**Result:** With this fix, any `IOException` that occurs when writing to the checksum file will be properly thrown and handled as a failed attempt to commit a snapshot. This ensures that any IO failure during the snapshot process is correctly reported and addressed."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14406",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nKudos to @romansmirnov for identifying this issue!\r\n\r\nWith https://github.com/camunda/zeebe/issues/13763, already executed migration tasks are marked as finished in the runtime state, to avoid rerunning already executed migrations with every restart.\r\n\r\nThis can lead to inconsistent states of snapshots across the brokers when a rolling update is performed. This is due to the fact that during the update brokers running an old version of zeebe are create a different state when replaying events than it would be the case when running the new version already.\r\n\r\n**To Reproduce/Scenario**\r\n\r\n_Given_\r\nWith 8.3 a new column family `PROCESS_INSTANCE_KEY_BY_DEFINITION_KEY` has been introduced. It maps a process instance key to its process definition. This new column family is used when trying to delete a process definition. However, in 8.2 that specific column family `PROCESS_INSTANCE_KEY_BY_DEFINITION_KEY` does not exist.\r\n\r\n_When_\r\nThe user upgrades from 8.2 to 8.3, and the upgrade happens in a rolling update manner, broker by broker.\r\n\r\nDuring that update, it may happen that a broker `x` (already updated to 8.3) will replicate a (already migrated) snapshot `s1` to a broker `y` which is still on version 8.2.\r\n\r\nAs the snapshot `s1` was already migrated by broker `x`, it contains an entry marking the migration task as finished.\r\n\r\nAfter broker `y` installed the received snapshot `s1`, it will start to replay the events.\r\n\r\nWhile replaying, it won't put anything into the column family `PROCESS_INSTANCE_KEY_BY_DEFINITION_KEY` because broker `y` is still running the previous version where this column family didn't exist.\r\n\r\nIn the meantime, broker `y` takes a new snapshot `s2`. Eventually, broker `y` gets updated to 8.3\r\n\r\n_Then_\r\n, it will recover from snapshot `s2` but it won't run the migration task anymore because the respective migration task was already marked as finished in snapshot `s1`. Thus the snapshot state of broker `y` will not be consistent with the one of broker `x`. If broker `y` eventually becomes leader, the corrupted state will be used going forward.\r\n\r\n**Expected behavior**\r\n\r\nMigrations must be executed in a way to ensure all brokers have eventually a consistent snapshot state.\r\n\r\n**Environment:**\r\n- OS: <!-- [e.g. Linux] -->\r\n- Zeebe Version: 8.3.x\r\n- Configuration: <!-- [e.g. exporters etc.] -->\r\n\n\n korthout: For completeness, I'll share [my thoughts](https://camunda.slack.com/archives/C037W9NMATG/p1695278745025669?thread_ts=1695219026.075349&cid=C037W9NMATG) here as well:\r\n\r\nI see two options:\r\n1. Followers should not replay events created by brokers on a higher version because we can't guarantee that these will be applied the same. This would result in slower failover during rolling update.\r\n2. Events should always be replayed in the same way, even between broker versions. This can be achieved by increasing the semantic `recordVersion` property of the event in the newer broker version. For backward compatibility, this requires separate event appliers (i.e. versioned event appliers).\r\n\r\nOption 2 would only work if we carefully create versioned event appliers for:\r\nevents that write to `PROCESS_INSTANCE_KEY_BY_DEFINITION_KEY`\r\nevents that interact with states that were migrated with migration tasks for multi-tenancy.\r\n\r\nIMO, option 1 is much easier to implement, and its negative effect may be limited.\n megglos: Thread with further discussions https://camunda.slack.com/archives/C037W9NMATG/p1695618000621469\n megglos: ZDP-Planning:\n- following-up with the ZPA team next if they have capacity in the next week\n megglos: ZPA-Planning outcome:\r\n\r\n@korthout & @remcowesterhoud  raised that removing https://github.com/camunda/zeebe/issues/13763 is the simplest solution as of now and it will be suffice for the 8.3 release, as all migrations now result in new CFs created.\r\n\r\nThus a migration will be run based on a condition if the deprecated CF still exists and contains data, if so the records in it will get migrated when a broker got updated eventually. This covers the edge case of an old broker receiving a snapshot from a new broker, as if it replays events it will write them to the deprecated CF , on update it will migrate those records to the new CF\r\n\r\nThank you 🙌 \n korthout: @remcowesterhoud I think we can close this issue right?\n remcowesterhoud: Yes we can",
    "title": "Migration could result in inconsistent data on rolling update",
    "releaseNoteText": "**Consequence**: The rolling update process can result in inconsistent data snapshots across brokers. This occurs when a broker running an older version replays events and does not have a specific column family that is present in the new version. As a result, the snapshot state of the broker does not align with other brokers, leading to a corrupted state if the affected broker becomes the leader.\n\n**Cause**: The issue arises from the fact that during the rolling update, already executed migration tasks are marked as finished in the runtime state to prevent them from being rerun. However, when a broker running an older version replays events without the necessary column family, the state of the snapshot becomes inconsistent.\n\n**Fix**: The team has decided to remove the feature that marked already executed migration tasks as finished in order to ensure consistent snapshot states during rolling updates. Now, if an old broker receives a snapshot from a new broker, it will replay the events and write them to the necessary column family. When the updated broker migrates, it will also migrate these records to the new column family.\n\n**Result**: With this fix, the rolling update process will now ensure that all brokers have a consistent snapshot state, even when running on different versions. This prevents the occurrence of corrupted states and ensures the integrity of the system's data."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14367",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nError group https://console.cloud.google.com/errors/detail/CLXT_uWi3KaNSQ;service=zeebe;time=P7D?project=camunda-saas-int-chaos\r\n\r\nWe recently reworked how we store replicated snapshots, and added some error cases that should fail when we for example receive a duplicate snapshot.\r\n\r\nThis seemed to happen in one of our recent chaos tests. The impact is quite high since due to the exception, which is thrown in this case, the node transitions to inactive. It will no longer take part of the corresponding partition.\r\n\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n**To Reproduce**\r\n\r\nNot clear yet.\r\n\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\n**Expected behavior**\r\n\r\nNo duplicate snapshot replication\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\nio.camunda.zeebe.snapshots.SnapshotException$SnapshotAlreadyExistsException: Expected to receive snapshot with id 225-1-225-240, but was already persisted. This shouldn't happen.\r\n\tat io.camunda.zeebe.snapshots.impl.FileBasedSnapshotStore.newReceivedSnapshot(FileBasedSnapshotStore.java:381) ~[zeebe-snapshots-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.snapshots.impl.FileBasedSnapshotStore.newReceivedSnapshot(FileBasedSnapshotStore.java:51) ~[zeebe-snapshots-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.atomix.raft.roles.PassiveRole.onInstall(PassiveRole.java:190) ~[zeebe-atomix-cluster-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.atomix.raft.roles.FollowerRole.onInstall(FollowerRole.java:83) ~[zeebe-atomix-cluster-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.atomix.raft.impl.RaftContext.lambda$registerHandlers$8(RaftContext.java:303) ~[zeebe-atomix-cluster-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.atomix.raft.impl.RaftContext.lambda$runOnContext$27(RaftContext.java:322) ~[zeebe-atomix-cluster-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.atomix.utils.concurrent.SingleThreadContext$WrappedRunnable.run(SingleThreadContext.java:178) [zeebe-atomix-utils-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) [?:?]\r\n\tat java.util.concurrent.FutureTask.run(Unknown Source) [?:?]\r\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source) [?:?]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]\r\n\tat java.lang.Thread.run(Unknown Source) [?:?]\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- OS: <!-- [e.g. Linux] -->\r\n- Zeebe Version: snapshot 8.3\r\n- Configuration: <!-- [e.g. exporters etc.] -->\r\n\n\n deepthidevaki: The threads are not running at the same time. Thread 40 logs are until `01:37` and Thread 39 logs are from `01:43`. Was there a restart in between?\n Zelldon: @deepthidevaki I think you are right, \r\n[Here](\r\nhttps://console.cloud.google.com/logs/query;query=logName:%22stdout%22%0Aresource.type%3D%22k8s_container%22%0Aresource.labels.location%3D%22europe-west1%22%0Aresource.labels.namespace_name%3D%222042c610-7a76-4f45-bfb3-b5bd2e8a9083-zeebe%22%0Aresource.labels.project_id%3D%22camunda-saas-int-chaos%22%0Aresource.labels.container_name%3D%22zeebe%22%0Aresource.labels.cluster_name%3D%22worker-chaos-1%22%0Aresource.labels.pod_name%3D%22zeebe-0%22;pinnedLogId=2023-09-20T01:43:33.274205286Z%2Fizrpouifho13c9fq;summaryFields=labels%252F%2522k8s-pod%252Fstatefulset_kubernetes_io%252Fpod-name%2522,jsonPayload%252Fcontext%252FthreadId:false:32:beginning;cursorTimestamp=2023-09-20T01:43:40.387554808Z;startTime=2023-09-20T01:08:25.576Z;endTime=2023-09-20T02:08:25.576Z?project=camunda-saas-int-chaos) is the restart\r\n\r\nSomehow overseen the timestamp gaps 👍🏼 \r\n\n Zelldon: Keeping it as critical for now since it is a regression and causing the node to transition to inactive when receiving duplicate snapshot\n npepinpe: From the logs, it looks like the leader tried sending it the same snapshot twice in a row.\r\n\r\n> 2023-09-20 03:37:52.583 CEST RaftServer{raft-partition-partition-2}{role=FOLLOWER} - Delete existing log (lastIndex '0') and replace with received snapshot (index '225'). First entry in the log will be at index 226\r\n>\r\n> 2023-09-20 03:37:55.560 CEST Committed new snapshot 225-1-225-240\r\n>\r\n> 2023-09-20 03:37:55.573 CEST RaftServer{raft-partition-partition-2}{role=FOLLOWER} - Committed snapshot FileBasedSnapshot{directory=/usr/local/zeebe/data/raft-partition/partitions/2/snapshots/225-1-225-240, checksumFile=/usr/local/zeebe/data/raft-partition/partitions/2/snapshots/225-1-225-240.checksum, checksum=1740324580, snapshotId=FileBasedSnapshotId{index=225, term=1, processedPosition=225, exporterPosition=240}, metadata=FileBasedSnapshotMetadata[version=1, processedPosition=225, exportedPosition=240, lastFollowupEventPosition=240]}\r\n>\r\n> 2023-09-20 03:37:55.576 CEST RaftServer{raft-partition-partition-2} - An uncaught exception occurred, transition to inactive role\r\n\r\nNow, the exception occurs is the following:\r\n\r\n```\r\nio.camunda.zeebe.snapshots.SnapshotException$SnapshotAlreadyExistsException: Expected to receive snapshot with id 225-1-225-240, but was already persisted. This shouldn't happen.\r\n\tat io.camunda.zeebe.snapshots.impl.FileBasedSnapshotStore.newReceivedSnapshot(FileBasedSnapshotStore.java:381) ~[zeebe-snapshots-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.snapshots.impl.FileBasedSnapshotStore.newReceivedSnapshot(FileBasedSnapshotStore.java:51) ~[zeebe-snapshots-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.atomix.raft.roles.PassiveRole.onInstall(PassiveRole.java:190) ~[zeebe-atomix-cluster-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.atomix.raft.roles.FollowerRole.onInstall(FollowerRole.java:83) ~[zeebe-atomix-cluster-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.atomix.raft.impl.RaftContext.lambda$registerHandlers$8(RaftContext.java:303) ~[zeebe-atomix-cluster-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.atomix.raft.impl.RaftContext.lambda$runOnContext$27(RaftContext.java:322) ~[zeebe-atomix-cluster-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.atomix.utils.concurrent.SingleThreadContext$WrappedRunnable.run(SingleThreadContext.java:178) [zeebe-atomix-utils-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) [?:?]\r\n\tat java.util.concurrent.FutureTask.run(Unknown Source) [?:?]\r\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source) [?:?]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]\r\n\tat java.lang.Thread.run(Unknown Source) [?:?]\r\n```\r\n\r\nAs we can see, it occurs because it received an install request from the leader. So nothing too weird is happening here - or rather, nothing that would lead to data loss or corruption or something :smile: \r\n\r\nThe question now is: did it make sense for the leader to send the snapshot install request again? If yes, then we should log a warning when this happens, but not fail, and return an appropriate response to the leader (i.e. we have this snapshot). If not, then we should fix it on the leader side.\r\n\r\nMy hypothesis is this is a race condition. Since the snapshot listener is invoked asynchronously after persisting, the snapshot reference of the follow may not be updated yet when the leader pushes out an append request (after it thinks the install operation is finished), which leads to it trying to send the same snapshot again.\n npepinpe: OK let's work through the hypothesis, since it's difficult to replicate this in a controlled way.\r\n\r\nThe follower's `onInstall` method will persist the snapshot. Meaning the Raft thread is blocked. And until we reply, the leader will not send another request to us. Now, in `onInstall`, we call the `persist()` method of the pending snapshot, and join on the resulting future. By the time this returns, the snapshot listeners have been invoked. This listener is asynchronous, so it won't execute immediately, but it will enqueue a job on the Raft thread. _Then we reply, and then the leader can send us another request_. \r\n\r\nSince all operations on the Raft thread are sequenced, the snapshot listener will be executed before the next leader request is processed. So the snapshot reference will be updated before the next request is processed, and thus the leader shouldn't be sending us the same snapshot again.\r\n\r\nOf course, this is all expected behavior, and maybe there's a bug somewhere in the code :smile: But according to that, my hypothesis is then wrong.\n npepinpe: Hm, I think the analysis above is wrong. In `onInstall`, we check if our current snapshot index is `>=` to the request's own index. So clearly, in this case, we decided this was false...\r\n\r\n```java\r\n    // If the snapshot already exists locally, do not overwrite it with a replicated snapshot.\r\n    // Simply reply to the request successfully.\r\n    final var latestIndex = raft.getCurrentSnapshotIndex();\r\n    if (latestIndex >= request.index()) {\r\n      abortPendingSnapshots();\r\n\r\n      return CompletableFuture.completedFuture(\r\n          logResponse(InstallResponse.builder().withStatus(RaftResponse.Status.OK).build()));\r\n    }\r\n```\r\n\r\nSo what might have happened is for some reason the leader decided to send another install request before the snapshot listener was handled. Are we possibly retrying install requests and ended up sending one too many? The default install request timeout is 2.5 seconds. If there's any I/O stalls, for example, and we take longer, then we'll retry the request. \r\n\r\nIndeed, on the leader, zeebe-1, we see the install request timed out.\r\n\r\n> 2023-09-20 03:37:54.894 CEST RaftServer{raft-partition-partition-2} - InstallRequest{currentTerm=1, leader=1, index=225, term=1, version=1, chunkId=HeapByteBuffer{position=0, remaining=14, limit=14, capacity=14, mark=java.nio.HeapByteBuffer[pos=0 lim=14 cap=14], hash=-1428579103}, nextChunkId=null, data=HeapByteBuffer{position=0, remaining=159, limit=159, capacity=159, mark=java.nio.HeapByteBuffer[pos=0 lim=159 cap=159], hash=559971236}, initial=false, complete=true} to 0 failed: java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException: Request ProtocolRequest{id=6017, subject=raft-partition-partition-2-install, sender=zeebe-1.zeebe-broker-service.2042c610-7a76-4f45-bfb3-b5bd2e8a9083-zeebe.svc.cluster.local:26502, payload=byte[]{length=308, hash=-186090835}} to zeebe-0.zeebe-broker-service.2042c610-7a76-4f45-bfb3-b5bd2e8a9083-zeebe.svc.cluster.local:26502 **timed out in PT2.5S**\r\n\r\nSo the request was simply retried, and it was then received and queued before the snapshot listener was processed.\r\n\r\n![mic drop](https://media.giphy.com/media/15BuyagtKucHm/giphy.gif)\r\n",
    "title": "Duplicate snapshot replication causing to fail raft follower",
    "releaseNoteText": "**Consequence:**\nThe issue caused the duplicate replication of snapshots, leading to the failure of the raft follower. This resulted in the node transitioning to an inactive state and no longer participating in the corresponding partition, impacting the system's functionality.\n\n**Cause:**\nThe issue was caused by a race condition in the code. The snapshot listener was invoked asynchronously after persisting, but the snapshot reference of the follower may not have been updated when the leader pushed out an append request. This led to the leader trying to send the same snapshot again.\n\n**Fix:**\nThe fix addressed the race condition by ensuring that the snapshot reference was updated before processing the next leader request. This prevented the leader from sending duplicate snapshot install requests.\n\n**Result:**\nAfter applying the fix, the system no longer experiences duplicate snapshot replication. The follower correctly handles the install request and avoids transitioning to an inactive state. The issue of the raft follower failure is resolved, maintaining the system's stability and functionality."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14309",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n\r\n\r\nThe issue seemed to happen while processing. The exporter could not process the record as the DeployedProcess.getVersion() returns null, because ProcessState.getLatestProcessVersionByProcessId()\" is null. \r\n\r\n\r\n\r\n\r\n```\r\nExpected to process record \r\n'TypedRecordImpl{metadata=RecordMetadata{recordType=COMMAND, valueType=DEPLOYMENT, intent=CREATE, authorization=UNKNOWN}, value={\"resources\":[],\"processesMetadata\":[{\"bpmnProcessId\":\"Process_Connect_Intra\",\"version\":4,\"processDefinitionKey\":2251799813740831,\"resourceName\":\"diagram_connect_intra.bpmn\",\"checksum\":\"vE026arBq5Hh09ilkSj4Cw==\",\"isDuplicate\":false,\"tenantId\":\"<default>\"}],\"decisionRequirementsMetadata\":[],\"decisionsMetadata\":[],\"tenantId\":\"<default>\"}}' \r\nwithout errors, but exception occurred with message \r\n'Cannot invoke \"io.camunda.zeebe.engine.state.deployment.DeployedProcess.getVersion()\" \r\nbecause the return value of \"io.camunda.zeebe.engine.state.immutable.ProcessState.getLatestProcessVersionByProcessId(org.agrona.DirectBuffer)\" is null'.\r\n```\r\n\r\n**To Reproduce**\r\n\r\nDont know as of moment, the previous behavior in the [logs](https://console.cloud.google.com/logs/query%3Bquery=logName:%22stdout%22%0Aresource.type=%22k8s_container%22%0Aresource.labels.container_name=%22zeebe%22%0Aresource.labels.project_id=%22camunda-cloud-240911%22%0Aresource.labels.namespace_name=%22179a54e4-45b8-4850-8ff8-7caca6407a47-zeebe%22%0Aresource.labels.cluster_name=%22prod-worker-4%22%0Aresource.labels.location=%22australia-southeast1%22%0Aresource.labels.pod_name=%22zeebe-1%22%0A%3BpinnedLogId=2023-09-14T00:20:12.977565274Z/az5svd4vg7gfo3zz%3BcursorTimestamp=2023-09-14T00:21:43.373199577Z%3BstartTime=2023-09-13T23:50:42.977Z%3BendTime=2023-09-14T00:50:42.977Z?project=camunda-cloud-240911) seems normal at first glance.\r\n\r\n\r\n\r\n\r\n\r\n**Log/Stacktrace**\r\n\r\n```\r\njava.lang.NullPointerException: Cannot invoke \"io.camunda.zeebe.engine.state.deployment.DeployedProcess.getVersion()\" because the return value of \"io.camunda.zeebe.engine.state.immutable.ProcessState.getLatestProcessVersionByProcessId(org.agrona.DirectBuffer)\" is null\r\n\tat io.camunda.zeebe.engine.processing.deployment.StartEventSubscriptionManager.isLatestProcess(StartEventSubscriptionManager.java:68) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.deployment.StartEventSubscriptionManager.tryReOpenStartEventSubscription(StartEventSubscriptionManager.java:58) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.deployment.DeploymentCreateProcessor.processDistributedCommand(DeploymentCreateProcessor.java:108) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.streamprocessor.DistributedTypedRecordProcessor.processRecord(DistributedTypedRecordProcessor.java:31) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.Engine.process(Engine.java:127) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.batchProcessing(ProcessingStateMachine.java:353) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$2(ProcessingStateMachine.java:269) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run(ZeebeTransaction.java:98) ~[zeebe-db-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand(ProcessingStateMachine.java:269) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord(ProcessingStateMachine.java:228) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.readNextRecord(ProcessingStateMachine.java:204) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:109) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:204) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n```\r\n\r\n**Environment:**\r\n- OS: <!-- [e.g. Linux] -->\r\n- Zeebe Version: 8.3.0-alpha6\r\n- Configuration: <!-- [e.g. exporters etc.] -->\r\n\n\n megglos: ZDP-Triage:\n- happened on a trial cluster, the user deleted the cluster after\n- this seems to be a processing error that is gracefully handled and will result in a command rejection\n- it actually happened while processing not exporting\n- the causing error in the engine needs investigation as it is a potential regression\n- might be related to start event subscriptions\n- should be raised to ZPA for investigation on the actual cause, could be critical if it's indeed a regression\n megglos: @koevskinikola could this be a regression related to Multi Tenancy?\n koevskinikola: @megglos from the logs this doesn't seem related to multi-tenancy (MT) because:\r\n\r\nThe MT-related changes to the `ProcessState` haven't been merged with `main`, they are kept on the `12653_epic_multi-tenancy` branch. We refactored the signature of `ProcessState#getLatestProcessVersionByProcessId(DirectBuffer processId)` to include a `tenantId` input argument. The logs show the old version of this method.\r\n\r\nHowever,  this might be related to the resource deletion Epic, as there were some recent changes in the referenced code though [this PR](https://github.com/camunda/zeebe/pull/13935) (although nothing that significantly impacts the logic).\n korthout: ZPA Triage:\r\n- should be investigated before the 8.3 release\r\n- let's mark this as critical, because this appears to block deploying and can lead to partition inconsistency\r\n- #14366 appears to have the same root cause\n korthout: I had a look into this yesterday. I was not yet able to reproduce or even to understand how this could happen, but I'll share my findings so far.\r\n\r\n🧺 I started by collecting similar issues:\r\n- https://github.com/camunda/zeebe/issues/14366\r\n- https://github.com/camunda/zeebe/issues/14309\r\n- https://github.com/camunda/zeebe/issues/14055 (3 weeks ago)\r\n- https://github.com/camunda/zeebe/issues/11734 (Feb 17th)\r\n- https://github.com/camunda/zeebe/issues/11414 (Jan 16th)\r\n\r\n🔍 All these issues have `getLatestProcessVersionByProcessId` in their stacktrace, but they come from different places:\r\n- [Cancel process instance](https://github.com/camunda/zeebe/issues/14366) on 8.3.0-alpha6\r\n- [Create deployment](https://github.com/camunda/zeebe/issues/14055#issue-1873096848) (distributed to another partition using command distribution) on 8.3.0-alpha4\r\n- [Call activity](https://github.com/camunda/zeebe/issues/14055#issuecomment-1700454899) on 8.2.12\r\n- [DeploymentDistributeProcessor](https://github.com/camunda/zeebe/issues/14055#issuecomment-1700454899) on 8.2.12\r\n\r\n🤔 While similar, the last two on 8.2.12 may have a different root cause, as 8.3.0 introduces several related topics:\r\n- Create deployment over command distribution\r\n- Delete process using resource deletion\r\n\r\n🕵️ After collecting this, I wondered: could a newer process version be distributed to another partition before the older version is distributed to that same partition?\r\n- Yes, individual commands sent to another partition are unreliable, which is why we've built command distribution to provide a reliable mechanism for distributing commands\r\n- However, this does not lead to the failure\r\n  - I adjusted the engine not to send the deployment of the first version of a process to other partitions\r\n  - and wrote a multi-partition test case that deploys two versions of processes\r\n  - the exception is not thrown because version 2 that is distributed is [first persisted to state](https://github.com/camunda/zeebe/blob/09f444092d8f42e402449eb2cde0a546b781a4c0/engine/src/main/java/io/camunda/zeebe/engine/processing/deployment/DeploymentCreateProcessor.java#L109-L111) before it is looked up as being the last version, and so it doesn't return null\r\n\r\n🔬 Looking more closely [at the code](https://github.com/camunda/zeebe/blob/09f444092d8f42e402449eb2cde0a546b781a4c0/engine/src/main/java/io/camunda/zeebe/engine/processing/deployment/StartEventSubscriptionManager.java#L57-L58), I wondered: could it be that a non-executable process would be distributed but not stored in state and so could not be found?\r\n- No, even when pooled together with executable processes, the non-executable process is not distributed. It is not extracted into the process metadata.\r\n\r\nAnd this is where I stopped. 🔚 \r\n\r\n🤞 One idea is to look at the data from https://github.com/camunda/zeebe/issues/14055. Perhaps we can use it, together with a test and some debugging to see if we can reproduce that case.\r\n> Further data for analysis is [here](https://drive.google.com/drive/folders/1g6HlGdY3RwJO1yd1zl3Ld3T7r_CJUxto)\n Zelldon: @korthout happened on PROd again. Might be worth to investigate the running cluster.\r\n\r\nhttps://console.cloud.google.com/errors/detail/CMHx49mjl8qiRQ;service=zeebe;time=P7D?project=camunda-cloud-240911\n nicpuppa: @Zelldon can we get a snapshot of the state ?\n Zelldon: @nicpuppa see slack please handle [it their](https://app.slack.com/client/T0PM0P1SA/C05TBCABJUS) I have to other medic duties still to do\n nicpuppa: data directory can be found [here](https://drive.google.com/drive/folders/16VnPRGYisXY-0weDKmMLRIhyXlZd5TPw)\n remcowesterhoud: ## Root-cause\r\n\r\nIt's an issue with the caching of process versions. There is a scenario in which different keys in the cache reference the same value object. Because of this versions of different processes can influence each other.\r\n\r\n1. If we deploy a process we store it in the state using a shared object (nextVersion) (this name is still bad btw)\r\n2. If get the version info we search in the cache. If we can't find it we get it from the state and store this in the cache.\r\n3. Since getting it from the state wraps the values in the same object it means the entries in the cache all reference this same object. So if the object changes it changes for all entries.\r\n\r\nWe can fix it by making sure we copy the value from the state before storing it in the cache.\r\n\r\n**Reproducible test case**\r\n\r\n```java\r\n  @Test\r\n  public void x() {\r\n    // given\r\n    final var process1V1 = creatingProcessRecord(processingState, \"process1\").setVersion(1);\r\n    final var process1V2 = creatingProcessRecord(processingState, \"process1\").setVersion(2);\r\n    final var process2V1 = creatingProcessRecord(processingState, \"process2\").setVersion(1);\r\n    final var process2V2 = creatingProcessRecord(processingState, \"process2\").setVersion(2);\r\n    final var process2V3 = creatingProcessRecord(processingState, \"process2\").setVersion(3);\r\n\r\n    // when\r\n    processState.putProcess(process1V1.getKey(), process1V1);\r\n    processState.putProcess(process2V1.getKey(), process2V1);\r\n    processState.putProcess(process2V2.getKey(), process2V2);\r\n    processState.clearCache();\r\n    processState.putProcess(process1V2.getKey(), process1V2);\r\n    processState.putProcess(process2V3.getKey(), process2V3);\r\n\r\n    // then\r\n    assertThat(processState.getNextProcessVersion(\"process1\")).isEqualTo(2);\r\n  }\r\n```",
    "title": "Cannot process record due to process deployed version being null.",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14275",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n\r\n- When we write something from the StreamProcessor we write it as a batch of records.\r\n- That is written to the Sequencer, wrapped as SequencedBatch. \r\n- The LogstorageAppender is writing this to the AtomixLogStorage.\r\n- The AtomixLogstorage writes to the LogAppender (LeaderRole).\r\n- The LeaderRole schedules[ a job to append](https://github.com/camunda/zeebe/blob/main/atomix/cluster/src/main/java/io/atomix/raft/roles/LeaderRole.java#L592), [the SequencedBatch is now wrapped as UnserializedApplicationEntry](https://github.com/camunda/zeebe/blob/main/atomix/cluster/src/main/java/io/atomix/raft/roles/LeaderRole.java#L595)\r\n- The LeaderRole appends the entry, [wrapped again as RaftLogEntry to the RaftLog.](https://github.com/camunda/zeebe/blob/main/atomix/cluster/src/main/java/io/atomix/raft/roles/LeaderRole.java#L617)\r\n- The RaftLog returns the IndexedRaftLogEntry (wrapped again)\r\n- LeaderRole [starts to replicate, with the IndexedRaftLogEntry](https://github.com/camunda/zeebe/blob/main/atomix/cluster/src/main/java/io/atomix/raft/roles/LeaderRole.java#L628)\r\n- LeaderAppender#appendEntries [creates futures for each index](https://github.com/camunda/zeebe/blob/main/atomix/cluster/src/main/java/io/atomix/raft/roles/LeaderAppender.java#L530)\r\n- LeaderRole registers async completion listeners (lambdas),[ which reference the IndexedRaftLogEntry](https://github.com/camunda/zeebe/blob/main/atomix/cluster/src/main/java/io/atomix/raft/roles/LeaderRole.java#L635)\r\n\r\nThe appendFutures and async lambda completion listeners in consequence reference the IndexedRaftEntries the whole lifetime. \r\n\r\nWhen a commit is not possible for a longer period of time, e.g. due to network issues, disk issues, etc. We accumulate more and more futures, and records. This is especially problematic if we have a lot of internal load, like scheduled timers that trigger all at once, and maybe even repeatedly since they are committed or processed not fast enough. \r\n\r\nThis can reach a certain point where the Broker goes out of memory.\r\n\r\nCame up in a support case: https://jira.camunda.com/browse/SUPPORT-18291\r\n\r\n\r\nRelated heap dump extract:\r\n\r\n![LeaderAppender-appendfutures](https://github.com/camunda/zeebe/assets/2758593/c309b99c-b715-4cdf-9310-1d099771513e)\r\n![sequencebatch](https://github.com/camunda/zeebe/assets/2758593/6c29b4dd-48d2-4fb2-9a28-7ddfe86384c8)\r\n![timerrecord-ref](https://github.com/camunda/zeebe/assets/2758593/057ae361-0d0a-49c2-b7d0-76249cae0d88)\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n**To Reproduce**\r\n\r\nCause some issue that commit is not possible have several timer due, which should be triggered (and triggered and triggered again) such that this causes a lot of load on the system. Eventually, the broker will go out of memory.\r\n\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\n**Expected behavior**\r\n\r\nWe don't keep the reference to the instances, we don't need all the information in the listeners. Mostly we only [need to commit indexes etc.](https://github.com/camunda/zeebe/blob/main/atomix/cluster/src/main/java/io/atomix/raft/roles/LeaderRole.java#L644-L648)\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n\r\n**Environment:**\r\n- OS: <!-- [e.g. Linux] -->\r\n- Zeebe Version: 8.2.x\r\n- Configuration: <!-- [e.g. exporters etc.] -->\r\n\n\n Zelldon: Related to https://github.com/camunda/zeebe/issues/13870, but instead of Jobs we had issues with timer triggers which caused this.",
    "title": "IndexedRaftLogEntry are kept in memory until committed, which can lead to OOM",
    "releaseNoteText": "**Consequence:**\nIndexedRaftLogEntry were kept in memory until committed, leading to Out of Memory (OOM) issues. This was observed when the system encountered longer periods of commits being not possible, such as due to network issues or disk problems. As a result, the system accumulated more futures and records, especially when there was a high internal load with multiple scheduled timers triggering simultaneously and repeatedly. This caused the Broker to eventually run out of memory.\n\n**Cause:**\nThe issue occurred because the appendFutures and async lambda completion listeners in the LeaderRole were referencing the IndexedRaftLogEntry throughout their entire lifetime. As a result, when commits were not possible, the system continued to accumulate more and more futures and records, leading to memory exhaustion.\n\n**Fix:**\nThe fix involved modifying the LeaderRole to no longer keep the reference to the instances of IndexedRaftLogEntry. The system no longer needed all the information in the listeners, and only required the commit indexes and other relevant information. This prevented the accumulation of unnecessary data and reduced the memory consumption.\n\n**Result:**\nWith this fix, the system no longer keeps references to IndexedRaftLogEntry instances in the appendFutures and async lambda completion listeners. As a result, when commits are not possible for longer periods, the system no longer accumulates excessive futures and records, preventing Out of Memory (OOM) issues. Users will experience improved memory management and stability in scenarios with high internal load and prolonged commit unavailability."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14055",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n```\r\nExpected to process record 'TypedRecordImpl{metadata=RecordMetadata{recordType=COMMAND, valueType=DEPLOYMENT, intent=CREATE}, value={...}' without errors, but exception occurred with message 'Expected to find executable process in persisted process with key '2251799813685388', but after transformation no such executable process could be found.'.\r\n\r\n\r\njava.util.NoSuchElementException: Expected to find executable process in persisted process with key '2251799813685388', but after transformation no such executable process could be found.\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.lambda$updateInMemoryState$2(DbProcessState.java:231) ~[zeebe-workflow-engine-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat java.util.Optional.orElseThrow(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.updateInMemoryState(DbProcessState.java:228) ~[zeebe-workflow-engine-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.lookupProcessByIdAndPersistedVersion(DbProcessState.java:375) ~[zeebe-workflow-engine-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.getLatestProcessVersionByProcessId(DbProcessState.java:271) ~[zeebe-workflow-engine-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.engine.processing.deployment.StartEventSubscriptionManager.isLatestProcess(StartEventSubscriptionManager.java:64) ~[zeebe-workflow-engine-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.engine.processing.deployment.StartEventSubscriptionManager.tryReOpenStartEventSubscription(StartEventSubscriptionManager.java:55) ~[zeebe-workflow-engine-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.engine.processing.deployment.DeploymentCreateProcessor.processDistributedCommand(DeploymentCreateProcessor.java:105) ~[zeebe-workflow-engine-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.engine.processing.streamprocessor.DistributedTypedRecordProcessor.processRecord(DistributedTypedRecordProcessor.java:31) ~[zeebe-workflow-engine-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.engine.Engine.process(Engine.java:126) ~[zeebe-workflow-engine-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.batchProcessing(ProcessingStateMachine.java:353) ~[zeebe-stream-platform-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$2(ProcessingStateMachine.java:269) ~[zeebe-stream-platform-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run(ZeebeTransaction.java:98) ~[zeebe-db-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand(ProcessingStateMachine.java:269) ~[zeebe-stream-platform-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord(ProcessingStateMachine.java:228) ~[zeebe-stream-platform-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.readNextRecord(ProcessingStateMachine.java:204) ~[zeebe-stream-platform-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) [zeebe-scheduler-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:109) [zeebe-scheduler-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:204) [zeebe-scheduler-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\r\n```\r\n\r\nThe error is repeatedly occurring in partitions 2 and 3. But didn't see it in Partition 1. The error is logged every time, partition 1 retries to distribute the deployment. It looks like the deployment was created successfully in partition 1, but it cannot distributed it to partitions 2 and 3. So the users would assume the deployment was successful, but they cannot create instances on other partitions.\r\n\r\n**To Reproduce**\r\n\r\nNot sure. Further data for analysis is [here](https://drive.google.com/drive/folders/1g6HlGdY3RwJO1yd1zl3Ld3T7r_CJUxto)\r\n\r\n**Expected behavior**\r\n\r\nA deployment successfully created in partition 1 should be accepted by other partitions.\r\n\r\n**Log/Stacktrace**\r\n\r\n[Link to logs](https://console.cloud.google.com/logs/query;cursorTimestamp=2023-08-29T12:56:18.601486031Z;endTime=2023-08-30T07:23:27.338991Z;pinnedLogId=2023-08-29T12:56:18.601486031Z%2Fenfqgfrxijsl5m8x;query=logName:%22stdout%22%0Aresource.type%3D%22k8s_container%22%0Aresource.labels.container_name%3D%22zeebe%22%0Aresource.labels.project_id%3D%22camunda-cloud-240911%22%0Aresource.labels.namespace_name%3D%2225697942-1bb8-47f8-9c2e-2d7e6b348a75-zeebe%22%0Aresource.labels.location%3D%22australia-southeast1%22%0Aresource.labels.cluster_name%3D%22prod-worker-4%22%0Atimestamp%3D%222023-08-29T12:56:18.601486031Z%22%0AinsertId%3D%22enfqgfrxijsl5m8x%22;startTime=2023-08-25T12:33:36.208Z;summaryFields=resource%252Flabels%252Fpod_name,jsonPayload%252Fcontext%252FpartitionId:false:32:beginning?project=camunda-cloud-240911)\r\n\r\n\r\n```\r\n\r\nDEBUG 2023-08-29T12:56:18.012401245Z [resource.labels.podName: zeebe-2] [jsonPayload.context.partitionId: 1] Ignoring unknown BPMN element 'http://camunda.org/schema/1.0/bpmn:inputOutput'\r\nDEBUG 2023-08-29T12:56:18.490833810Z [resource.labels.podName: zeebe-2] [jsonPayload.context.partitionId: 3] Ignoring unknown BPMN element 'http://camunda.org/schema/1.0/bpmn:inputOutput'\r\nDEBUG 2023-08-29T12:56:18.492924797Z [resource.labels.podName: zeebe-2] [jsonPayload.context.partitionId: 3] Ignoring unknown BPMN element 'http://camunda.org/schema/1.0/bpmn:inputOutput'\r\nDEBUG 2023-08-29T12:56:18.493531262Z [resource.labels.podName: zeebe-2] [jsonPayload.context.partitionId: 3] Ignoring unknown BPMN element 'http://camunda.org/schema/1.0/bpmn:inputOutput'\r\nDEBUG 2023-08-29T12:56:18.493878660Z [resource.labels.podName: zeebe-2] [jsonPayload.context.partitionId: 3] Ignoring unknown BPMN element 'http://camunda.org/schema/1.0/bpmn:inputOutput'\r\nERROR 2023-08-29T12:56:18.601486031Z [resource.labels.podName: zeebe-2] [jsonPayload.context.partitionId: 3] Expected to process record 'TypedRecordImpl{metadata=RecordMetadata{recordType=COMMAND, valueType=DEPLOYMENT, intent=CREATE}, value={\"resources\":...}' without errors, but exception occurred with message 'Expected to find executable process in persisted process with key '2251799813685388', but after transformation no such executable process could be found.'.\r\n\r\n\r\n```\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.3.0-alpha4\r\n\n\n korthout: ZPA Triage:\r\n- potential regression to 8.2 due to generalized distribution for deployments\r\n- possibly something with a race condition between different versions of a process\r\n- size medium, because unclear for now\r\n- priority `upcoming` because this could be quite severe\n deepthidevaki: There were two similar errors, for which I think the root cause might be the same even though it occurred when processing different records.\r\n\r\n\r\n[Logs](https://console.cloud.google.com/errors/detail/CL3C3rOOkMXdRw;service=zeebe;time=P7D?project=camunda-cloud-240911)\r\n```\r\nExpected to process record 'TypedRecordImpl{metadata=RecordMetadata{recordType=COMMAND, valueType=JOB, intent=COMPLETE}, value={\"deadline\":-1,\"worker\":\"\",\"retries\":-1,\"retryBackoff\":0,\"recurringTime\":-1,\"type\":\"\",\"customHeaders\":[packed value (length=1)],\"variables\":\"...\",\"errorMessage\":\"\",\"errorCode\":\"\",\"bpmnProcessId\":\"\",\"processDefinitionVersion\":-1,\"processDefinitionKey\":-1,\"processInstanceKey\":-1,\"elementId\":\"\",\"elementInstanceKey\":-1}}' without errors, but exception occurred with message 'No value present'.\r\n\r\njava.util.NoSuchElementException: No value present\r\n\tat java.util.Optional.orElseThrow(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.updateInMemoryState(DbProcessState.java:178) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.lookupProcessByIdAndPersistedVersion(DbProcessState.java:319) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.getLatestProcessVersionByProcessId(DbProcessState.java:215) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnStateBehavior.getLatestProcessVersion(BpmnStateBehavior.java:120) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.container.CallActivityProcessor.getProcessForProcessId(CallActivityProcessor.java:170) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.util.Either$Right.flatMap(Either.java:366) ~[zeebe-util-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.container.CallActivityProcessor.onActivate(CallActivityProcessor.java:66) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.container.CallActivityProcessor.onActivate(CallActivityProcessor.java:29) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.lambda$processEvent$2(BpmnStreamProcessor.java:144) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.util.Either$Right.ifRightOrLeft(Either.java:381) ~[zeebe-util-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.processEvent(BpmnStreamProcessor.java:143) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.lambda$processRecord$0(BpmnStreamProcessor.java:92) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.util.Either$Right.ifRightOrLeft(Either.java:381) ~[zeebe-util-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.processRecord(BpmnStreamProcessor.java:89) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.engine.Engine.process(Engine.java:140) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.batchProcessing(ProcessingStateMachine.java:352) ~[zeebe-stream-platform-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$2(ProcessingStateMachine.java:269) ~[zeebe-stream-platform-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run(ZeebeTransaction.java:84) ~[zeebe-db-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand(ProcessingStateMachine.java:269) ~[zeebe-stream-platform-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord(ProcessingStateMachine.java:228) ~[zeebe-stream-platform-8.2.12.jar:8.2.12]\r\n```\r\n\r\n[Logs](https://console.cloud.google.com/errors/detail/CLe7o6_LmPTh4AE;service=zeebe;time=P7D?project=camunda-cloud-240911)\r\n\r\n```\r\nExpected to process record 'TypedRecordImpl{metadata=RecordMetadata{recordType=COMMAND, valueType=DEPLOYMENT, intent=DISTRIBUTE}, value={\"resources\":[{\"resourceName\":\"xxx.bpmn\",\"resource\":\"...}' without errors, but exception occurred with message 'No value present'.\r\n\r\njava.util.NoSuchElementException: No value present\r\n\tat java.util.Optional.orElseThrow(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.updateInMemoryState(DbProcessState.java:178) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.lookupProcessByIdAndPersistedVersion(DbProcessState.java:319) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.getLatestProcessVersionByProcessId(DbProcessState.java:215) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.engine.processing.deployment.StartEventSubscriptionManager.isLatestProcess(StartEventSubscriptionManager.java:64) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.engine.processing.deployment.StartEventSubscriptionManager.tryReOpenStartEventSubscription(StartEventSubscriptionManager.java:55) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.engine.processing.deployment.distribute.DeploymentDistributeProcessor.processRecord(DeploymentDistributeProcessor.java:46) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.engine.Engine.process(Engine.java:140) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.batchProcessing(ProcessingStateMachine.java:352) ~[zeebe-stream-platform-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$2(ProcessingStateMachine.java:269) ~[zeebe-stream-platform-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run(ZeebeTransaction.java:84) ~[zeebe-db-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand(ProcessingStateMachine.java:269) ~[zeebe-stream-platform-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord(ProcessingStateMachine.java:228) ~[zeebe-stream-platform-8.2.12.jar:8.2.12]\r\n\r\n```\r\n\n korthout: Thanks @deepthidevaki That's useful input!\n nicpuppa: Started to look at it last week 👀 Didn't figured out yet how to reproduce.\r\n\r\n~~Could be related to this [PR](https://github.com/camunda/zeebe/pull/13505), but idk~~",
    "title": "No executable process found while processing Deployment:Create command",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14044",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nObserved 100% backpressure on one partition. On further investigation, we found that there was back-to-back role transition.\r\n\r\nThe node was leader for partition 2. It transitioned to follower. The transition was cancelled in between because it became leader again.\r\n\r\nBecause of [this fix](https://github.com/camunda/zeebe/pull/13541), command api is not notified that it became follower. So when it transitioned to leader again, it reuse the limiter from previous leader role because it was not removed. https://github.com/camunda/zeebe/blob/615c751216c3fdc99493792d8f1c19644633d275/broker/src/main/java/io/camunda/zeebe/broker/transport/backpressure/PartitionAwareRequestLimiter.java#L147\r\n\r\nThe partition started processing when a new leader was elected after a few hours.\r\n\r\n**To Reproduce**\r\n\r\nNot easy to reproduce. Leader -> Follower -> Leader transition should happen where the follower transition is cancelled.\r\n\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.2.12\r\n\n",
    "title": "Backpressure queue is not reset when back-to-back role transitions",
    "releaseNoteText": "**Consequence:**\n\nThe backpressure queue was not properly reset when there were back-to-back role transitions in the system. This resulted in 100% backpressure on one partition, preventing it from processing tasks.\n\n**Cause:**\n\nThe issue was caused by a bug in which the command API was not properly notified when a node transitioned from leader to follower. This bug was introduced in a previous fix that resulted in the reuse of the limiter from the previous leader role, even when the transition was cancelled. As a result, the backpressure queue was not reset correctly.\n\n**Fix:**\n\nIn order to fix the issue, the code was modified to ensure that the command API is properly notified when a role transition occurs. Specifically, the code in the `PartitionAwareRequestLimiter` class was updated to remove the limiter when a transition is cancelled.\n\n**Result:**\n\nWith this fix, the backpressure queue is now correctly reset when there are back-to-back role transitions. This ensures that the system can process tasks in a timely manner and prevents any unnecessary backpressure on the partitions."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13936",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nIt seems that an Inclusive Gateway with a single outgoing Sequence Flow ignores the Condition\r\n\r\n<img width=\"974\" alt=\"Screenshot 2023-08-17 at 09 52 07\" src=\"https://github.com/camunda/zeebe/assets/3511026/57eb2d2d-8646-4947-b7c6-3aa05472521a\">\r\n\r\nThe BPMN spec is quite clear about the expected behavior:\r\n\r\n> A default path can optionally be identified, to be taken in the event that none of the conditional `Expressions` evaluate to `true`. If a default path is not specified and the **Process** is executed such that none of the conditional `Expressions` evaluates to `true`, a runtime exception occurs. - [10.6.3 Inclusive Gateway](https://www.omg.org/spec/BPMN/2.0.2/PDF#10.6.3%20Inclusive%20Gateway)\r\n\r\nFor C8, such a runtime exception should be represented by [an incident](https://docs.camunda.io/docs/next/components/concepts/incidents/) at the inclusive gateway.\r\n\r\nOriginally reported on the forums: https://forum.camunda.io/t/strange-behavior-with-only-one-sequence-flow-with-condition/46938\r\n\r\n**To Reproduce**\r\n\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\n- deploy a process with an inclusive gateway that has one outgoing sequence flow with the condition `= false`\r\n- create an instance of it\r\n- notice that the sequence flow is taken and that no incident is raised\r\n\r\n**Expected behavior**\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\nAn incident is raised at the inclusive gateway\n\n korthout: Mid severity, because there is a workaround: use two outgoing sequence flows - set the second one to `= false` and let it flow to a none end event\n lzgabel: 👋 Hi @korthout. Please assign this task to me. I'll take a look.",
    "title": "Condition ignored on Inclusive Gateway with singular outgoing Sequence Flow",
    "releaseNoteText": "**Consequence:**\nAn Inclusive Gateway with a single outgoing Sequence Flow was ignoring the Condition. The expected behavior, as specified in the BPMN spec, was for a runtime exception to occur if none of the conditional expressions evaluated to true. However, in this case, the Sequence Flow was being taken without raising an incident.\n\n**Cause:**\nThe underlying cause of this issue was the improper handling of the Condition in the Inclusive Gateway logic. The system was not correctly evaluating the Condition expression and was allowing the Sequence Flow to be taken even when the Condition evaluated to false.\n\n**Fix:**\nThe fix for this issue involved updating the logic of the Inclusive Gateway to properly evaluate the Condition expression. The system now correctly checks the Condition and only allows the Sequence Flow to be taken if the Condition evaluates to true. If none of the conditional expressions evaluate to true, a runtime exception is raised, as expected.\n\n**Result:**\nWith this fix, an Inclusive Gateway with a single outgoing Sequence Flow correctly considers the Condition. If the Condition evaluates to false, a runtime exception is raised, indicating that none of the conditional expressions evaluated to true. This ensures that the system behaves according to the BPMN spec, providing a more accurate and reliable process flow."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13233",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "We've introduced a regression on deploying large payloads on multi-partition clusters with\r\n- #11661 \r\n\r\nBefore that pull request, a deployed resource was written in two follow-up events:\r\n- `Deployment:CREATED`\r\n- `Process:CREATED` (or `DecisionRequirements:CREATED` depending on resource)\r\n\r\nThe specialized `DeploymentDistribution` would take the resource from the `Deployment:CREATED` to distribute it to the other partitions.\r\n\r\nWith #11456, a new event `CommandDistribution:STARTED` was introduced to store the command for distribution. For deployments, this contains the entire Deployment incl. the resource. This event is only appended on multi-partition clusters. But, when it is appended, it further reduces the maximum payload size because the follow-up events reach the `MAX_BATCH_SIZE` restriction with a lower payload size.\r\n\r\n> **Note** The `MAX_BATCH_SIZE` is a limitation that originated from the `MAX_MESSAGE_SIZE` configuration setting but nowadays is only defined by the `LogStreamBuilder`'s `MAX_FRAGMENT_SIZE`: 4MB.\r\n\r\nThis regression lowers the maximum payload size of deployments from ~2MB down to ~1.4MB.\r\n\r\nThe regression does not exist on single partition clusters as these do not require the distribution of the deployment.\r\n\r\n---\r\n\r\nSuggested solution:\r\n- don't write the resource in the `Deployment:CREATED` event, only in the `Process:CREATED` and the `DecisionRequirements:CREATED` events (estimate: x-small)\r\n\r\nOf course, we'd need to inform users that we're no longer writing the resource for that event in the Update Guide. As this would be breaking user space. The documentation should clarify that the resource is available in the `Process:CREATED` and the `DecisionRequirements:CREATED` instead.\n\n korthout: There are several solution ideas:\n- https://github.com/camunda/zeebe/issues/11513\n  - Does not seem trivial to implement\n  - Mostly a ZDP effort\n- don't write the resource in the `Deployment:CREATED` event, only in the `Process:CREATED` and the `DecisionRequirements:CREATED` events (estimate: x-small)\n  - would require alignment with Operate and Optimize to make sure they consume the resource from `Process:CREATED` \n korthout: @sdorokhova Can you verify for me that Operate consumes the `Process:CREATED` event to get the BPMN model and not the `Deployment:CREATED` event? Likewise, does it consume the `DecisionRequirements:CREATED` event for DMN models?\r\n\r\n@RomanJRW Can you verify the same as above, but for Optimize?\n sdorokhova: Hi @korthout ,\r\nwe read processes from `process` index and decision requirements from `decision-requirements` index. You can check [here](https://github.com/camunda/operate/blob/168d22f5352c66bfd56c47678e92655a82414194/common/src/main/java/io/camunda/operate/zeebe/ZeebeESConstants.java) all the indices we're reading from. Is this what you were asking about? \n korthout: @sdorokhova Perfect! That's exactly what I mean. I should've mentioned that it concerns the ES indices `zeebe-record-process` and `zeebe-record-decision-requirements`. \r\n\r\nI was hoping you weren't reading the BPMN/DMN resources from the `zeebe-record-deployment` index.\n megglos: @korthout do I understand this correctly that this regression is only present in 8.3? And the changes that happened in 8.2 like https://github.com/camunda/zeebe/issues/11660 have not caused a regression?\n korthout: @megglos Correct, the regression only exists on `main` and if left unfixed will only be present in the upcoming 8.3. The reason is that deployment distribution has been switched over only for 8.3(-alpha*), not for 8.2.\n RomanJRW: Hey @korthout - apologies for slow response, I have had FTO. I can confirm the same for Optimize, that we read from `zeebe-record-process` and not the deployment index\n korthout: Now that both Operate and Optimize have confirmed that they don't access the resource from the `zeebe-record-deployment` index, I think it's fine to take this solution:\r\n- don't write the resource in the `Deployment:CREATED` event, only in the `Process:CREATED` and the `DecisionRequirements:CREATED` events (estimate: x-small)\r\n\r\nOf course, we'd need to inform users that we're no longer writing the resource for that event in the Update Guide. As this would be breaking user space. The documentation should clarify that the resource is available in the `Process:CREATED` and the `DecisionRequirements:CREATED` instead.",
    "title": "Regression in deploying large payloads",
    "releaseNoteText": "**Consequence**: Deploying large payloads on multi-partition clusters introduced a regression that impacted the maximum payload size of deployments. Before the regression, a deployed resource was written in two follow-up events (`Deployment:CREATED` and `Process:CREATED` or `DecisionRequirements:CREATED`). However, with the regression, a new event (`CommandDistribution:STARTED`) was introduced to store the command for distribution, leading to a reduction in the maximum payload size. This regression lowered the maximum payload size of deployments from approximately 2MB to around 1.4MB. The regression did not affect single partition clusters as they do not require the distribution of the deployment.\n\n**Cause**: The regression was caused by the introduction of the `CommandDistribution:STARTED` event, which stored the entire deployment including the resource for distribution. This event was only appended on multi-partition clusters, and it further reduced the maximum payload size due to the `MAX_BATCH_SIZE` restriction.\n\n**Fix**: To address this issue, the fix involved not writing the resource in the `Deployment:CREATED` event but only in the `Process:CREATED` and `DecisionRequirements:CREATED` events.\n\n**Result**: After applying the fix, the maximum payload size of deployments on multi-partition clusters is restored to its original size of approximately 2MB. The resource is now available in the `Process:CREATED` and `DecisionRequirements:CREATED` events, allowing for proper distribution without impacting the payload size. It is important to note that this fix requires alignment with Operate and Optimize to ensure that they consume the resource from the correct events. The documentation has been updated to inform users that the resource is no longer written in the `Deployment:CREATED` event and is available in the `Process:CREATED` and `DecisionRequirements:CREATED` events instead."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13093",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nRunning `RandomizedRaftTest.consistencyTestWithSnapshot` on 8.0.17 leads to the following exception:\r\n```\r\njava.lang.IllegalStateException: Expected to delete index after 404, but it is lower than the commit index 405. Deleting committed entries can lead to inconsistencies and is prohibited. at io.atomix.raft.storage.log.RaftLog.deleteAfter(RaftLog.java:186) at io.atomix.raft.roles.PassiveRole.replaceExistingEntry(PassiveRole.java:623) at io.atomix.raft.roles.PassiveRole.tryToAppend(PassiveRole.java:562) at io.atomix.raft.roles.PassiveRole.appendEntries(PassiveRole.java:514) at io.atomix.raft.roles.PassiveRole.handleAppend(PassiveRole.java:370) at io.atomix.raft.roles.ActiveRole.onAppend(ActiveRole.java:50) at io.atomix.raft.roles.FollowerRole.onAppend(FollowerRole.java:187) at io.atomix.raft.impl.RaftContext.lambda$registerHandlers$13(RaftContext.java:263) at \r\n...\r\n```\r\n\r\nSee [Test results for unit tests(1).zip](https://github.com/camunda/zeebe/files/11734753/Test.results.for.unit.tests.1.zip) from test run https://github.com/camunda/zeebe/actions/runs/5255172343/jobs/9494733250\r\n\r\n**To Reproduce**\r\n| jqwick | explanation |\r\n|--------|--------|\r\n| tries = 10                    | # of calls to property |\r\n| checks = 10                   | # of not rejected calls |\r\n| generation = RANDOMIZED       | parameters are randomly generated |\r\n| after-failure = PREVIOUS_SEED | use the previous seed |\r\n| when-fixed-seed = ALLOW       | fixing the random seed is allowed |\r\n| edge-cases#mode = NONE        | edge cases are not explicitly generated |\r\n| seed = 2349382887260487435    | random seed to reproduce generated values |\r\n\r\n**Expected behavior**\r\n\r\nTest does not fail.\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.lang.IllegalStateException: Expected to delete index after 404, but it is lower than the commit index 405. Deleting committed entries can lead to inconsistencies and is prohibited. at io.atomix.raft.storage.log.RaftLog.deleteAfter(RaftLog.java:186) at io.atomix.raft.roles.PassiveRole.replaceExistingEntry(PassiveRole.java:623) at io.atomix.raft.roles.PassiveRole.tryToAppend(PassiveRole.java:562) at io.atomix.raft.roles.PassiveRole.appendEntries(PassiveRole.java:514) at io.atomix.raft.roles.PassiveRole.handleAppend(PassiveRole.java:370) at io.atomix.raft.roles.ActiveRole.onAppend(ActiveRole.java:50) at io.atomix.raft.roles.FollowerRole.onAppend(FollowerRole.java:187) at io.atomix.raft.impl.RaftContext.lambda$registerHandlers$13(RaftContext.java:263) at io.atomix.raft.impl.RaftContext.lambda$runOnContext$20(RaftContext.java:274) at io.atomix.raft.DeterministicSingleThreadContext$WrappedRunnable.run(DeterministicSingleThreadContext.java:129) at org.jmock.lib.concurrent.DeterministicScheduler$CallableRunnableAdapter.call(DeterministicScheduler.java:176) at org.jmock.lib.concurrent.DeterministicScheduler$ScheduledTask.run(DeterministicScheduler.java:251) at org.jmock.lib.concurrent.DeterministicScheduler.runNextPendingCommand(DeterministicScheduler.java:66) at io.atomix.raft.ControllableRaftContexts.runNextTask(ControllableRaftContexts.java:246) at io.atomix.raft.RaftOperation.lambda$getDefaultRaftOperations$0(RaftOperation.java:62) at io.atomix.raft.RaftOperation.run(RaftOperation.java:42) at io.atomix.raft.RandomizedRaftTest.consistencyTest(RandomizedRaftTest.java:123) at io.atomix.raft.RandomizedRaftTest.consistencyTestWithSnapshot(RandomizedRaftTest.java:88) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:568) at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725) at org.junit.platform.commons.support.ReflectionSupport.invokeMethod(ReflectionSupport.java:198) at net.jqwik.engine.execution.CheckedPropertyFactory.lambda$createRawFunction$1(CheckedPropertyFactory.java:84) at net.jqwik.engine.execution.CheckedPropertyFactory.lambda$createRawFunction$2(CheckedPropertyFactory.java:91) at net.jqwik.engine.properties.CheckedFunction.execute(CheckedFunction.java:17) at net.jqwik.api.lifecycle.AroundTryHook.lambda$static$0(AroundTryHook.java:57) at net.jqwik.engine.execution.lifecycle.HookSupport.lambda$wrap$2(HookSupport.java:48) at net.jqwik.engine.hooks.lifecycle.TryLifecycleMethodsHook.aroundTry(TryLifecycleMethodsHook.java:57) at net.jqwik.engine.execution.lifecycle.HookSupport.lambda$wrap$3(HookSupport.java:53) at net.jqwik.engine.execution.CheckedPropertyFactory.lambda$createTryExecutor$0(CheckedPropertyFactory.java:60) at net.jqwik.engine.execution.lifecycle.AroundTryLifecycle.execute(AroundTryLifecycle.java:23) at net.jqwik.engine.properties.GenericProperty.testPredicate(GenericProperty.java:166) at net.jqwik.engine.properties.GenericProperty.check(GenericProperty.java:68) at net.jqwik.engine.execution.CheckedProperty.check(CheckedProperty.java:67) at net.jqwik.engine.execution.PropertyMethodExecutor.executeProperty(PropertyMethodExecutor.java:90) at net.jqwik.engine.execution.PropertyMethodExecutor.executeMethod(PropertyMethodExecutor.java:69) at net.jqwik.engine.execution.PropertyMethodExecutor.lambda$execute$0(PropertyMethodExecutor.java:49) at net.jqwik.api.lifecycle.AroundPropertyHook.lambda$static$0(AroundPropertyHook.java:46) at net.jqwik.engine.execution.lifecycle.HookSupport.lambda$wrap$0(HookSupport.java:26) at net.jqwik.engine.hooks.lifecycle.PropertyLifecycleMethodsHook.aroundProperty(PropertyLifecycleMethodsHook.java:57) at net.jqwik.engine.execution.lifecycle.HookSupport.lambda$wrap$1(HookSupport.java:31) at net.jqwik.engine.execution.lifecycle.HookSupport.lambda$wrap$0(HookSupport.java:26) at net.jqwik.engine.hooks.statistics.StatisticsHook.aroundProperty(StatisticsHook.java:37) at net.jqwik.engine.execution.lifecycle.HookSupport.lambda$wrap$1(HookSupport.java:31) at net.jqwik.engine.execution.lifecycle.HookSupport.lambda$wrap$0(HookSupport.java:26) at net.jqwik.engine.hooks.lifecycle.AutoCloseableHook.aroundProperty(AutoCloseableHook.java:13) at net.jqwik.engine.execution.lifecycle.HookSupport.lambda$wrap$1(HookSupport.java:31) at net.jqwik.engine.execution.PropertyMethodExecutor.execute(PropertyMethodExecutor.java:47) at net.jqwik.engine.execution.PropertyTaskCreator.executeTestMethod(PropertyTaskCreator.java:166) at net.jqwik.engine.execution.PropertyTaskCreator.lambda$createTask$1(PropertyTaskCreator.java:51) at net.jqwik.engine.execution.lifecycle.CurrentDomainContext.runWithContext(CurrentDomainContext.java:28) at net.jqwik.engine.execution.PropertyTaskCreator.lambda$createTask$2(PropertyTaskCreator.java:50) at net.jqwik.engine.execution.pipeline.ExecutionTask$1.lambda$execute$0(ExecutionTask.java:31) at net.jqwik.engine.execution.lifecycle.CurrentTestDescriptor.runWithDescriptor(CurrentTestDescriptor.java:17) at net.jqwik.engine.execution.pipeline.ExecutionTask$1.execute(ExecutionTask.java:31) at net.jqwik.engine.execution.pipeline.ExecutionPipeline.runToTermination(ExecutionPipeline.java:82) at net.jqwik.engine.execution.JqwikExecutor.execute(JqwikExecutor.java:46) at net.jqwik.engine.JqwikTestEngine.executeTests(JqwikTestEngine.java:70) at net.jqwik.engine.JqwikTestEngine.execute(JqwikTestEngine.java:53) at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107) at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88) at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54) at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67) at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52) at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114) at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86) at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86) at org.apache.maven.surefire.junitplatform.LazyLauncher.execute(LazyLauncher.java:55) at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:234) at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133) at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:228) at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:175) at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:131) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:456) at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:169) at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:595) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:581) \r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.0.17\r\n\r\n\r\n\n\n megglos: ZDP-Triage:\n- unexpected breaking behavior\n- needs to be investigated\n megglos: ZDP-Planning:\n- this indicates a high severity bug and we need to investigate this asap\n deepthidevaki: @npepinpe You might not be able to reproduce this easily. When I was investigating sometime ago, I was wondering why it is not reproducible with the seed, and it looks like https://github.com/camunda/zeebe/blob/bcfe82dcb3be17b6f4a1ee732d40574d374cc1d8/atomix/cluster/src/main/java/io/atomix/raft/cluster/impl/RaftMemberContext.java#L137 this is the reason. I didn't debug further, but relying on System time will be non-deterministic. So most likely this is the reason. This doesn't help in your investigation, but just a hint in case you failed to reproduce it  :smile: \n npepinpe: We have `System.currentTimeMillis()` in other parts of our system as well :sweat: For example, with heartbeat related code, member context, quorum timeout calculation, etc.\r\n\r\nI guess the next step would be to use some clock for full reproducibility :+1: \n npepinpe: Observations:\r\n\r\n1. Expected to delete all entries after 404, but the commit index is 405.\r\n1. We tried to delete after 404 because the replicated entry did not have the same term as the existing entry\r\n1. The mismatched entries index was 405, hence why we want to delete it and anything after (so we say after 404, exclusive)\r\n1. The existing entry was committed\r\n\r\nSo the issue is either:\r\n\r\n1. The entry should not have been committed\r\n1. The terms should not have been mismatched\n npepinpe: A little above, we see the following line:\r\n\r\n```\r\n12:08:19.021 [] WARN  io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - AppendRequest{term=23, leader=1, prevLogIndex=404, prevLogTerm=19, entries=145, commitIndex=839} to 0 failed: java.util.concurrent.TimeoutException\r\n```\r\n\r\nHad to scroll way up higher to find when the commit index was not 405 on any of these nodes. So 405 was appended on 2 as:\r\n\r\n```\r\n12:08:18.230 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=405, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@5d2e7a6a}\r\n```\r\n\r\nIts term was 19.\r\n\r\nIt was appended on 0 as:\r\n\r\n```\r\n12:08:18.229 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=405, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@2dc97eb8}\r\n```\r\n\r\nAlso term 19. Alright. Let's check then when we try to re-append it, what the term was. Cool, we don't have that in the logs :)\r\n\r\nThis is the request we get which triggers the error:\r\n\r\n```\r\n12:08:19.023 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received AppendRequest{term=23, leader=1, prevLogIndex=404, prevLogTerm=19, entries=145, commitIndex=839}\r\n```\r\n\r\nSo the prevLogTerm seems correct, but unfortunately we don't log the mismatched entries, so I can't check it.\r\n\r\nBy the time the error occurs, the term is 23.\r\n\r\nOne weird thing, it seems the leader sends this request to 0:\r\n\r\n```\r\n12:08:19.021 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Sending AppendRequest{term=23, leader=1, prevLogIndex=873, prevLogTerm=23, entries=0, commitIndex=839} to 0\r\n```\r\n\r\nThere is no log for when the response is received from 0, no time out or anything; and then it suddenly tries to send starting at 405 :thinking: Unclear to me what caused it to do that. Possibly it's backed up and has many messages buffered?\r\n\r\nCorrect. Way above, we see:\r\n\r\n```\r\n12:08:19.021 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Received AppendResponse{status=OK, term=23, succeeded=false, lastLogIndex=578, lastSnapshotIndex=404} from 0\r\n12:08:19.021 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Reset next index for RaftMemberContext{member=0, term=23, configIndex=0, snapshotIndex=404, nextSnapshotIndex=0, nextSnapshotChunk=null, matchIndex=0, heartbeatTime=1686658099013, appending=0, appendSucceeded=false, appendTime=1686658099018, configuring=false, installing=false, failures=0} to 579\r\n12:08:19.021 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Sending AppendRequest{term=23, leader=1, prevLogIndex=404, prevLogTerm=19, entries=145, commitIndex=839} to 0\r\n```\r\n\r\nBut if the last log index on 0 is 578, why aren't we sending starting at 579?? Why are we sending starting at 405?\r\n\r\nAh, then we get \r\n\r\n```\r\n12:08:19.021 [] WARN  io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - AppendRequest{term=23, leader=1, prevLogIndex=404, prevLogTerm=19, entries=145, commitIndex=839} to 0 failed: java.util.concurrent.TimeoutException\r\n```\r\n\r\nFollowed quickly by:\r\n\r\n```\r\n12:08:19.021 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Sending AppendRequest{term=23, leader=1, prevLogIndex=873, prevLogTerm=23, entries=0, commitIndex=839} to 0\r\n```\r\n\r\nUnclear why a timeout would lead to the prevLogIndex to be reset...\r\n\r\nAnyway, that second one is never really processed by 0.\n npepinpe: If we keep going further back in time, we see the last append that was processed by 0 was:\r\n\r\n```\r\n12:08:19.018 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Sending AppendRequest{term=23, leader=1, prevLogIndex=860, prevLogTerm=23, entries=0, commitIndex=405} to 0\r\n12:08:19.020 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received AppendRequest{term=23, leader=1, prevLogIndex=860, prevLogTerm=23, entries=0, commitIndex=405}\r\n12:08:19.020 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Rejected AppendRequest{term=23, leader=1, prevLogIndex=860, prevLogTerm=23, entries=0, commitIndex=405}: Previous index (860) is greater than the local log's last index (578)\r\n12:08:19.021 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending AppendResponse{status=OK, term=23, succeeded=false, lastLogIndex=578, lastSnapshotIndex=404}\r\n```\r\n\r\nThis correlates with the reset we saw on 1 earlier, when it reset to 579 (presumably - apparently not!).\r\n\r\nIn fact, we see many of these. Due to time outs, 1 keeps trying to send the same append request, and 0 keeps rejecting it because the previous index is greater than the local log's last index. So let's try to find the last successful append (579).\r\n\r\n```\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received AppendRequest{term=19, leader=1, prevLogIndex=560, prevLogTerm=19, entries=18, commitIndex=405}\r\n12:08:18.284 [] INFO  io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Found leader 1\r\n12:08:18.284 [] TRACE io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Set leader 1\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=561, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@7a75a78e}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=562, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@5058a8e2}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=563, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@1187855a}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=564, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@1f7952dd}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=565, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@4867cf6b}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=566, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@5d6d613d}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=567, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@706d929f}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=568, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@2557d9f5}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=569, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@533d56a3}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=570, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@656fb170}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=571, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@790a2e28}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=572, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@6209d88d}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=573, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@5b381221}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=574, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@43c4dc07}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=575, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@6b17d139}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=576, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@47f6a166}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=577, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@5c441aa3}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=578, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@3985d1c8}\r\n12:08:18.284 [] TRACE io.atomix.raft.storage.system.MetaStore - Store last flushed index 578\r\n12:08:18.284 [] TRACE io.atomix.raft.storage.system.MetaStore - Skip storing same last flushed index 578\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending AppendResponse{status=OK, term=19, succeeded=true, lastLogIndex=578, lastSnapshotIndex=404}\r\n12:08:18.284 [] TRACE io.atomix.raft.impl.RandomizedElectionTimer - Election timeout scheduled for PT3.733S\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received AppendRequest{term=19, leader=1, prevLogIndex=578, prevLogTerm=19, entries=0, commitIndex=405}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending AppendResponse{status=OK, term=19, succeeded=true, lastLogIndex=578, lastSnapshotIndex=404}\r\n```\r\n\r\nSo we can see all the other entries were also during term 19. So it looks like after term 19, 0 never appended anything.\r\n\r\nThen we see 0 soon after trying to get elected for term 20:\r\n\r\n```\r\n12:08:18.285 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 0\r\n12:08:18.285 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 0\r\n12:08:18.285 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - No heartbeat from 1 since 1ms\r\n12:08:18.285 [] TRACE io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Set leader null\r\n12:08:18.285 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending poll requests to all active members: [DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.453Z}, DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.453Z}]\r\n12:08:18.285 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.453Z} for next term 20\r\n12:08:18.285 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.453Z} for next term 20\r\n12:08:18.285 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick 50ms on 0\r\n12:08:18.285 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 0\r\n12:08:18.285 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Failed to poll a majority of the cluster in PT2.5S\r\n```\r\n\r\nThis fails. So let's try and find all the successful elections between 19 and 23.\r\n\r\nThere seems to be no successful election for term 20:\r\n\r\n```\r\n12:08:18.289 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Rejected AppendRequest{term=19, leader=1, prevLogIndex=578, prevLogTerm=19, entries=0, commitIndex=405}: request term is less than the current term (20)\r\n12:08:18.289 [] TRACE io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Sending AppendResponse{status=OK, term=20, succeeded=false, lastLogIndex=578, lastSnapshotIndex=404}\r\n12:08:18.290 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 0\r\n12:08:18.290 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - No heartbeat from a known leader since 6ms\r\n```\r\n\r\nBy the way, during this time, 1 still thinks it's the leader and tries to send append requests.\r\n\r\n```\r\n12:08:18.289 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Received AppendRequest{term=19, leader=1, prevLogIndex=559, prevLogTerm=19, entries=0, commitIndex=405}\r\n12:08:18.289 [] INFO  io.atomix.raft.impl.RaftContext - RaftServer{2-partition-1} - Found leader 1\r\n12:08:18.289 [] TRACE io.atomix.raft.impl.RaftContext - RaftServer{2-partition-1} - Set leader 1\r\n12:08:18.289 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Sending AppendResponse{status=OK, term=19, succeeded=true, lastLogIndex=559, lastSnapshotIndex=404}\r\n12:08:18.289 [] TRACE io.atomix.raft.impl.RandomizedElectionTimer - Election timeout scheduled for PT2.743S\r\n```\r\n\r\nThen later on:\r\n\r\n```\r\n12:08:18.290 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received PollRequest{term=19, candidate=2, lastLogIndex=559, lastLogTerm=19}\r\n12:08:18.290 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Rejected PollRequest{term=19, candidate=2, lastLogIndex=559, lastLogTerm=19}: candidate's term is less than the current term\r\n12:08:18.290 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending PollResponse{status=OK, term=20, accepted=false}\r\n12:08:18.290 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 1\r\n12:08:18.290 [] WARN  io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - AppendRequest{term=19, leader=1, prevLogIndex=559, prevLogTerm=19, entries=0, commitIndex=405} to 2 failed: java.util.concurrent.TimeoutException\r\n12:08:18.290 [] WARN  io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - AppendRequest{term=19, leader=1, prevLogIndex=578, prevLogTerm=19, entries=0, commitIndex=405} to 0 failed: java.util.concurrent.TimeoutException\r\n12:08:18.290 [] TRACE io.atomix.raft.roles.LeaderRole - RaftServer{1-partition-1}{role=LEADER} - Appended IndexedRaftLogEntryImpl{index=613, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=PersistedJournalRecord[metadata=RecordMetadata[checksum=3177859851, length=77], record=RecordData[index=613, asqn=0, data=UnsafeBuffer{addressOffset=140583371101078, capacity=49, byteArray=null, byteBuffer=java.nio.DirectByteBuffer[pos=9159 lim=10240 cap=10240]}]]}\r\n12:08:18.290 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Sending AppendRequest{term=19, leader=1, prevLogIndex=559, prevLogTerm=19, entries=0, commitIndex=405} to 2\r\n12:08:18.290 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Sending AppendRequest{term=19, leader=1, prevLogIndex=578, prevLogTerm=19, entries=0, commitIndex=405} to 0\r\n12:08:18.290 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 0\r\n12:08:18.290 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - No heartbeat from a known leader since 6ms\r\n12:08:18.290 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending poll requests to all active members: [DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.453Z}, DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.453Z}]\r\n12:08:18.290 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.453Z} for next term 21\r\n12:08:18.290 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.453Z} for next term 21\r\n```\r\n\r\nThen we get this weird one about term 1??\r\n\r\n```\r\n12:08:18.290 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received AppendRequest{term=19, leader=1, prevLogIndex=578, prevLogTerm=19, entries=0, commitIndex=405}\r\n12:08:18.290 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Rejected AppendRequest{term=19, leader=1, prevLogIndex=578, prevLogTerm=19, entries=0, commitIndex=405}: request term is less than the current term (20)\r\n12:08:18.290 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending AppendResponse{status=OK, term=20, succeeded=false, lastLogIndex=578, lastSnapshotIndex=404}\r\n12:08:18.290 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Expected heartbeat from null in term 20, but received one from 1 in term 1, ignoring it\r\n```\r\n\r\nThen rapidly 0 will keep doing elections, bumping the term from 20 to 21, then 22:\r\n\r\n```\r\n12:08:18.291 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Received PollRequest{term=20, candidate=0, lastLogIndex=578, lastLogTerm=19}\r\n12:08:18.291 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Accepted PollRequest{term=20, candidate=0, lastLogIndex=578, lastLogTerm=19}: candidate's log is up-to-date\r\n12:08:18.291 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Sending PollResponse{status=OK, term=20, accepted=true}\r\n12:08:18.291 [] INFO  io.atomix.raft.RandomizedRaftTest - Receive next message on 2\r\n[...]\r\n12:08:18.292 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received accepted poll from DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.453Z}\r\n12:08:18.292 [] INFO  io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Transitioning to CANDIDATE\r\n12:08:18.292 [] INFO  io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Starting election\r\n12:08:18.292 [] TRACE io.atomix.raft.storage.system.MetaStore - Store term 21\r\n12:08:18.292 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote null\r\n12:08:18.292 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Set term 21\r\n12:08:18.292 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote 0\r\n12:08:18.292 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Voted for 0\r\n12:08:18.292 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Requesting votes for term 21\r\n12:08:18.292 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Requesting vote from DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.453Z} for term 21\r\n12:08:18.292 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Requesting vote from DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.453Z} for term 21\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 1\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Run next task on 2\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Run next task on 2\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Run next task on 0\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Run next task on 2\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick heartbeatTimeout on 2\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 0\r\n12:08:18.292 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Election timed out. Restarting election.\r\n12:08:18.292 [] TRACE io.atomix.raft.storage.system.MetaStore - Store term 22\r\n12:08:18.292 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote null\r\n12:08:18.292 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Set term 22\r\n12:08:18.292 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote 0\r\n12:08:18.292 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Voted for 0\r\n12:08:18.292 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Requesting votes for term 22\r\n12:08:18.292 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Requesting vote from DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.453Z} for term 22\r\n12:08:18.292 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Requesting vote from DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.453Z} for term 22\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Run next task on 2\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Append on leader on 0\r\n12:08:18.292 [] INFO  TEST - Appending on leader 1\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Append on leader on 0\r\n12:08:18.292 [] INFO  TEST - Appending on leader 1\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Take snapshot on 2\r\n12:08:18.292 [] INFO  TEST - Snapshot taken at index 404. Current commit index is 405\r\n12:08:18.292 [] DEBUG io.camunda.zeebe.journal.file.SegmentsManager - No segments can be deleted with index < 404 (first log index: 313)\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Receive next message on 1\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Run next task on 0\r\n12:08:18.292 [] WARN  io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - null\r\n```\r\n\r\nThen we get this cool warning:\r\n\r\n```\r\n12:08:18.292 [] WARN  io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - null\r\n```\r\n\r\n:smile:\r\n\r\n2 will vote for 0 for term 22:\r\n\r\n```\r\n12:08:18.292 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Received VoteRequest{term=22, candidate=0, lastLogIndex=578, lastLogTerm=19}\r\n12:08:18.292 [] TRACE io.atomix.raft.storage.system.MetaStore - Store term 22\r\n12:08:18.292 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote null\r\n12:08:18.292 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{2-partition-1} - Set term 22\r\n12:08:18.292 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Accepted VoteRequest{term=22, candidate=0, lastLogIndex=578, lastLogTerm=19}: candidate's log is up-to-date\r\n12:08:18.292 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote 0\r\n12:08:18.292 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{2-partition-1} - Voted for 0\r\n12:08:18.292 [] TRACE io.atomix.raft.impl.RandomizedElectionTimer - Election timeout scheduled for PT3.103S\r\n12:08:18.292 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Sending VoteResponse{status=OK, term=22, voted=true}\r\n```\r\n\r\nStill, no election succeeded yet. Then 2 bumps term to 23:\r\n\r\n```\r\n12:08:18.293 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 2\r\n12:08:18.293 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - No heartbeat from a known leader since 1ms\r\n12:08:18.293 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Sending poll requests to all active members: [DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.455Z}, DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.455Z}]\r\n12:08:18.293 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.455Z} for next term 23\r\n12:08:18.293 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.455Z} for next term 23\r\n```\r\n\r\nThen after all this time, 1 processes the message from 0 about term increasing to 21 (yes, 1 was still on term 19 trying to append stuff to the followers):\r\n\r\n```\r\n12:08:18.293 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 1\r\n12:08:18.293 [] TRACE io.atomix.raft.storage.system.MetaStore - Store term 21\r\n12:08:18.293 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote null\r\n12:08:18.293 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{1-partition-1} - Set term 21\r\n12:08:18.293 [] INFO  io.atomix.raft.roles.LeaderRole - RaftServer{1-partition-1}{role=LEADER} - Received greater term from 0\r\n12:08:18.293 [] INFO  io.atomix.raft.impl.RaftContext - RaftServer{1-partition-1} - Transitioning to FOLLOWER\r\n12:08:18.296 [] TRACE io.atomix.raft.roles.LeaderRole - RaftServer{1-partition-1}{role=LEADER} - Cancelling append timer\r\n12:08:18.296 [] TRACE io.atomix.raft.impl.RandomizedElectionTimer - Election timeout scheduled for PT4.57S\r\n12:08:18.296 [] TRACE io.atomix.raft.roles.LeaderRole - RaftServer{1-partition-1}{role=LEADER} - Received VoteRequest{term=21, candidate=0, lastLogIndex=578, lastLogTerm=19}\r\n12:08:18.296 [] DEBUG io.atomix.raft.roles.LeaderRole - RaftServer{1-partition-1}{role=LEADER} - Rejected VoteRequest{term=21, candidate=0, lastLogIndex=578, lastLogTerm=19}: candidate's last log entry (578) is at a lower index than the local log (621)\r\n12:08:18.296 [] TRACE io.atomix.raft.roles.LeaderRole - RaftServer{1-partition-1}{role=LEADER} - Sending VoteResponse{status=OK, term=21, voted=false}\r\n[...]\r\n12:08:18.297 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Second round of election timed out. Transitioning to follower.\r\n12:08:18.297 [] INFO  io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Transitioning to FOLLOWER\r\n12:08:18.297 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Cancelling election\r\n12:08:18.297 [] TRACE io.atomix.raft.impl.RandomizedElectionTimer - Election timeout scheduled for PT3.95S\r\n12:08:18.297 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 1\r\n12:08:18.297 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - No heartbeat from a known leader since 4ms\r\n12:08:18.297 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Sending poll requests to all active members: [DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.454Z}, DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.454Z}]\r\n12:08:18.297 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.454Z} for next term 22\r\n12:08:18.297 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.454Z} for next term 22\r\n[...]\r\n12:08:18.298 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Failed to poll a majority of the cluster in PT2.5S\r\n12:08:18.298 [] TRACE io.atomix.raft.impl.RandomizedElectionTimer - Election timeout scheduled for PT3.769S\r\n12:08:18.298 [] INFO  io.atomix.raft.RandomizedRaftTest - Drop next message on 2\r\n12:08:18.298 [] INFO  TEST: - Dropped a message to 2\r\n12:08:18.298 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick heartbeatTimeout on 2\r\n12:08:18.298 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick 50ms on 1\r\n12:08:18.298 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 1\r\n12:08:18.298 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Failed to poll a majority of the cluster in PT2.5S\r\n12:08:18.298 [] TRACE io.atomix.raft.impl.RandomizedElectionTimer - Election timeout scheduled for PT4.267S\r\n[...]\r\n12:08:18.298 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Received VoteRequest{term=22, candidate=0, lastLogIndex=578, lastLogTerm=19}\r\n12:08:18.298 [] TRACE io.atomix.raft.storage.system.MetaStore - Store term 22\r\n12:08:18.298 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote null\r\n12:08:18.298 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{1-partition-1} - Set term 22\r\n12:08:18.298 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Rejected VoteRequest{term=22, candidate=0, lastLogIndex=578, lastLogTerm=19}: candidate's last log entry (578) is at a lower index than the local log (621)\r\n12:08:18.298 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Sending VoteResponse{status=OK, term=22, voted=false}\r\n[...]\r\n12:08:18.298 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Expected heartbeat from null in term 22, but received one from 1 in term 1, ignoring it\r\n12:08:18.298 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received PollRequest{term=22, candidate=2, lastLogIndex=559, lastLogTerm=19}\r\n12:08:18.298 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Rejected PollRequest{term=22, candidate=2, lastLogIndex=559, lastLogTerm=19}: candidate's last log entry (559) is at a lower index than the local log (578)\r\n12:08:18.298 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending PollResponse{status=OK, term=22, accepted=false}\r\n[...]\r\n12:08:18.299 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Sending poll requests to all active members: [DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.455Z}, DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.455Z}]\r\n12:08:18.299 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.455Z} for next term 23\r\n12:08:18.299 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.455Z} for next term 23\r\n[...]\r\n12:08:18.299 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received PollRequest{term=22, candidate=2, lastLogIndex=559, lastLogTerm=19}\r\n12:08:18.299 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Rejected PollRequest{term=22, candidate=2, lastLogIndex=559, lastLogTerm=19}: candidate's last log entry (559) is at a lower index than the local log (578)\r\n12:08:18.299 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending PollResponse{status=OK, term=22, accepted=false}\r\n[...]\r\n12:08:18.299 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Received PollRequest{term=22, candidate=2, lastLogIndex=559, lastLogTerm=19}\r\n12:08:18.299 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Rejected PollRequest{term=22, candidate=2, lastLogIndex=559, lastLogTerm=19}: candidate's last log entry (559) is at a lower index than the local log (621)\r\n12:08:18.299 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Sending PollResponse{status=OK, term=22, accepted=false}\r\n[...]\r\n12:08:18.299 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - No heartbeat from a known leader since 6ms\r\n12:08:18.299 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Sending poll requests to all active members: [DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.454Z}, DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.454Z}]\r\n12:08:18.299 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.454Z} for next term 23\r\n12:08:18.299 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.454Z} for next term 23\r\n```\r\n\r\nSo we see both 0 and 1 keep rejecting 2 as a candidate, and since 1 has the longest log, it should become elected eventually. Last we see it tries an election on term 23.\r\n\r\n```\r\n12:08:18.299 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received PollRequest{term=22, candidate=1, lastLogIndex=621, lastLogTerm=19}\r\n12:08:18.299 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Accepted PollRequest{term=22, candidate=1, lastLogIndex=621, lastLogTerm=19}: candidate's log is up-to-date\r\n12:08:18.299 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending PollResponse{status=OK, term=22, accepted=true}\r\n[...]\r\n12:08:18.299 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{1-partition-1} - Set term 23\r\n12:08:18.299 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote 1\r\n12:08:18.299 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{1-partition-1} - Voted for 1\r\n12:08:18.299 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{1-partition-1}{role=CANDIDATE} - Requesting votes for term 23\r\n12:08:18.299 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{1-partition-1}{role=CANDIDATE} - Requesting vote from DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.454Z} for term 23\r\n12:08:18.299 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{1-partition-1}{role=CANDIDATE} - Requesting vote from DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.454Z} for term 23\r\n[...]\r\n12:08:18.299 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received VoteRequest{term=23, candidate=1, lastLogIndex=621, lastLogTerm=19}\r\n12:08:18.299 [] TRACE io.atomix.raft.storage.system.MetaStore - Store term 23\r\n12:08:18.299 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote null\r\n12:08:18.299 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Set term 23\r\n12:08:18.300 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Accepted VoteRequest{term=23, candidate=1, lastLogIndex=621, lastLogTerm=19}: candidate's log is up-to-date\r\n12:08:18.300 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote 1\r\n12:08:18.300 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Voted for 1\r\n12:08:18.300 [] TRACE io.atomix.raft.impl.RandomizedElectionTimer - Election timeout scheduled for PT2.939S\r\n12:08:18.300 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending VoteResponse{status=OK, term=23, voted=true}\r\n[...]\r\n12:08:18.300 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - No heartbeat from a known leader since 8ms\r\n12:08:18.300 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Sending poll requests to all active members: [DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.455Z}, DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.455Z}]\r\n12:08:18.300 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.455Z} for next term 23\r\n12:08:18.300 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.455Z} for next term 23\r\n12:08:18.300 [] INFO  io.atomix.raft.RandomizedRaftTest - Append on leader on 0\r\n12:08:18.300 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick heartbeatTimeout on 0\r\n12:08:18.300 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 1\r\n12:08:18.300 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{1-partition-1}{role=CANDIDATE} - Received successful vote from DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.454Z}\r\n12:08:18.300 [] INFO  io.atomix.raft.impl.RaftContext - RaftServer{1-partition-1} - Transitioning to LEADER\r\n12:08:18.300 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{1-partition-1}{role=CANDIDATE} - Cancelling election\r\n```\r\n\r\nSo now we get to term 23. After that is a long time of just sending append requests trying to keep appending on the nodes, the log of which I already put above.\r\n\n npepinpe: So that the commit index is 405 is fine, and it would make sense that the entry is committed. We saw that both nodes received further appends (2 up to 559, and 0 up to 578), both requests which started at 406. So 405 should be committed, with term 19.\r\n\r\nSo possibly the problem is that during replication, 1 should have started sending entries from 579, but instead it started at 405? Or some other weird thing happened...but it's likely a replication issue.\n npepinpe: One thing to note, the index we delete after does not come from the entry we're trying to replace, but instead from the AppendRequest's prevLogIndex. It then gets incremented as we append...\r\n\r\nIs this correct? We saw that 0 replies that it's prevLogIndex is not 404, but instead 578.\r\n\r\nSee:\r\n\r\n```\r\n12:08:19.021 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Received AppendResponse{status=OK, term=23, succeeded=false, lastLogIndex=578, lastSnapshotIndex=404} from 0\r\n12:08:19.021 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Reset next index for RaftMemberContext{member=0, term=23, configIndex=0, snapshotIndex=404, nextSnapshotIndex=0, nextSnapshotChunk=null, matchIndex=0, heartbeatTime=1686658099013, appending=0, appendSucceeded=false, appendTime=1686658099018, configuring=false, installing=false, failures=0} to 579\r\n12:08:19.021 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Sending AppendRequest{term=23, leader=1, prevLogIndex=404, prevLogTerm=19, entries=145, commitIndex=839} to 0\r\n```\r\n\r\n\n npepinpe: From what I can see, it reset to 578, but then when building the next AppendRequest, it builds it from the previous entry which is...404? Is it because the snapshot is at 404?\r\n\r\nSo on the leader, if the previous entry (previous from what we should send) is not found, then we will send the prevLogEntry is the snapshot. Since 0 reports its last entry as 405, we try to get the prevLogEntry - which is not there, because 405 is the first entry in 1's log. So we send the snapshot - 404 - with term 19. But the first entry in the request is 579.\r\n\r\nSo on the follower (0), our last log entry is 578, greater than the reported `prevLogEntry` (404), so we think we have to replace some existing entries, starting at 405 (even if the first entry in the request is 578).\r\n\r\nI still don't get why the terms were mismatched though - it seems to me they should be the same, 19, so we shouldn't even have caught this bug :scream: \n npepinpe: @deepthidevaki - let's sync tomorrow on this\r\n\r\nI don't remember why we use the prevLogIndex as the start index instead of just using the entry's index themselves.\n deepthidevaki: > Is this correct? We saw that 0 replies that it's prevLogIndex is not 404, but instead 578.\r\n\r\nThis doesn't look like the expected behavior. Were there other AppendRequests in between, which would have truncated the entries in 0 back to 404? \n npepinpe: So it did compact right before:\r\n\r\n```\r\n12:08:19.021 [] INFO  io.atomix.raft.RandomizedRaftTest - Take snapshot on 1\r\n12:08:19.021 [] INFO  TEST - Snapshot taken at index 791. Current commit index is 839\r\n12:08:19.021 [] DEBUG io.camunda.zeebe.journal.file.SegmentsManager - atomix - Deleting log up from 313 up to 625 (removing 4 segments)\r\n```\n npepinpe: Alright, so the bug is that we have two snapshot listeners: one triggers compaction, and one updates the locale reference of the persisted snapshot in the Raft context. While both execute on the Raft thread, they may be executed in any order (since the listeners on the snapshot store are a set, iteration is not ordered).\r\n\r\nSo what happened here is:\r\n\r\n1. A new snapshot was taken up to entry 839\r\n2. The log is compacted up to 839\r\n3. 1 sent an append request for 839 to 0\r\n4. 0 rejected saying its last log index is 578\r\n5. 1 tries to reset 0 to 578, but the entry does not exist\r\n6. 1 sends a new append request using the local snapshot reference for prevLogIndex and prevLogTerm\r\n7. The snapshot reference is updated to 839\r\n\r\nSolution is to ensure that all the changes to the Raft context (updating the reference, compacting, etc.) are done in a single listener so we can control the ordering: update snapshot ref, then compact.\n npepinpe: This likely affect all versions. The severity is high in this case, as the partition will go inactive due to an uncaught exception. Workaround is to restart your broker.",
    "title": "`RandomizedRaftTest.consistencyTestWithSnapshot` fails with unexpected exception",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12780",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n```\r\nERROR 2023-05-16T00:30:04.104402170Z [resource.labels.containerName: zeebe] Unexpected error on writing CREATE command Failed to write request to logstream\r\n```\r\n\r\nThe error is from CommandAPIHandler when it tries to write a user request to the leader's logstream. This happened while the leader is transition to follower, and the logstream has already closed. Before this error we see that Sequencer rejects the record because it is closed. \r\n\r\nThis is a new error message introduced in https://github.com/camunda/zeebe/pull/12676. Previously this error was ignored. So we never got the error message.\r\n\r\n[logs](https://console.cloud.google.com/errors/detail/CNy21ZD_7pnW3AE;service=zeebe;time=P7D?project=zeebe-io)\r\n\r\n**Expected behavior**\r\n\r\n- Reduce the log level to warn/debug\r\n- logstream#tryWrite should return specific error code instead of -1, and use that to log more meaningful message. \r\n- If we can recognize that this is during the leader transition we can chose to not log the error. Instead return a PARTITION_LEADER_MISMATCH code back to the gateway so that it can retry the command with the new leader before sending an error to the client.\r\n\n\n megglos: ZDP-Triage:\n- mostly noise\n- it's expected and shouldn't be logged as error in the particular scenario\n- as it's new (last or next patch) it can be considered a regression => could be confusing after update\n megglos: ZDP-Planning:\n- we will look into it before the next release\n- also affects 8.2,8.1,8.0 due to a backporrt\n Zelldon: I feel this is not 100% resolved. [We see a lot of errors messages also in the gateway](https://console.cloud.google.com/logs/query;query=resource.type%3D%22k8s_container%22%0Aresource.labels.project_id%3D%22zeebe-io%22%0Aresource.labels.location%3D%22europe-west1-b%22%0Aresource.labels.cluster_name%3D%22zeebe-cluster%22%0Aresource.labels.namespace_name%3D%22medic-y-2023-cw-20-d2345cc-benchmark%22%0Alabels.k8s-pod%2Fapp%3D%22camunda-platform%22%0Alabels.k8s-pod%2Fapp_kubernetes_io%2Fcomponent%3D%22zeebe-gateway%22%0Alabels.k8s-pod%2Fapp_kubernetes_io%2Finstance%3D%22medic-y-2023-cw-20-d2345cc-benchmark%22%0Alabels.k8s-pod%2Fapp_kubernetes_io%2Fmanaged-by%3D%22Helm%22%0Alabels.k8s-pod%2Fapp_kubernetes_io%2Fname%3D%22zeebe-gateway%22%0Alabels.k8s-pod%2Fapp_kubernetes_io%2Fpart-of%3D%22camunda-platform%22;cursorTimestamp=2023-06-07T10:45:04.209515605Z?project=zeebe-io), which is also in this case a lot of noise.\r\n\r\nExample of a current medic benchmark\r\n```\r\nio.camunda.zeebe.gateway.cmd.BrokerErrorException: Received error from broker (INTERNAL_ERROR): Failed writing request: Failed to write request to logstream\r\n\tat io.camunda.zeebe.gateway.impl.broker.BrokerRequestManager.handleResponse(BrokerRequestManager.java:194) ~[zeebe-gateway-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.gateway.impl.broker.BrokerRequestManager.lambda$sendRequestInternal$2(BrokerRequestManager.java:143) ~[zeebe-gateway-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.future.FutureContinuationRunnable.run(FutureContinuationRunnable.java:28) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\"\r\n```\r\n\r\n[Where at this time a role change happens](https://grafana.dev.zeebe.io/d/NzsO1mUnk/zeebe-overview?orgId=1&var-DS_PROMETHEUS=Prometheus&var-cluster=All&var-namespace=medic-y-2023-cw-20-d2345cc-benchmark&var-pod=All&var-partition=All&from=1686132065352&to=1686136683285)\r\n![role](https://github.com/camunda/zeebe/assets/2758593/66f672f3-3af8-44f0-8cb3-7d3cb5029dd5)\r\n\n deepthidevaki: @Zelldon That is an old benchmark before the bug fix.\n Zelldon: Ups thanks @deepthidevaki you're right :+1: ",
    "title": "Failing to write to logstream during stepdown is logged as error",
    "releaseNoteText": "**Consequence:**\nPreviously, during a leader transition in the system, if the logstream closed before the leader could write a user request to it, an error was logged. This error message was introduced in a recent update, causing confusion for users. The error message was noisy and unnecessary, creating additional log entries.\n\n**Cause:**\nThe error was occurring due to a change introduced in a recent pull request. Previously, this error was ignored, but the update caused the error message to be logged. The logstream closed before the leader could complete the write operation, leading to the error.\n\n**Fix:**\nThe log level has been reduced to warn/debug to reduce the noise in the logs. Additionally, the `logstream#tryWrite` function has been updated to return specific error codes for better error handling. Now, when a leader transition occurs and the logstream is closed, the system recognizes this situation and returns a `PARTITION_LEADER_MISMATCH` code to the gateway, allowing it to retry the command with the new leader.\n\n**Result:**\nWith this fix, the unnecessary error message is no longer logged during leader transitions when the logstream is closed. The log level has been reduced to warn/debug, reducing noise in the logs. The system now handles the situation of a leader transition and closed logstream correctly, returning the appropriate error code to the gateway for retrying the command with the new leader. This improves the overall behavior and reliability of the system during leader transitions."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/7855",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nIt seems we are closing the resources not in the correct order. We still accept or want to send an response but the message service is closed concurrently and the sending fails and causes on error on closing the Broker.\r\n\r\nOccurred 10 times in two error groups within `1.2.0-alpha2`\r\n\r\n\r\nError groups:\r\n\r\n * https://console.cloud.google.com/errors/CJeh5tGzv8X1Rg?service=zeebe&time=P7D&refresh=off&project=camunda-cloud-240911\r\n * https://console.cloud.google.com/errors/CKKJ762u3J3ZUw?service=zeebe&time=P7D&refresh=off&project=camunda-cloud-240911\r\n\r\n\r\n**To Reproduce**\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n - when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\nI assume run the service and close the broker.\r\n\r\n**Expected behavior**\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nNo error, correct sequence of closing resources.\r\n\r\n**Log/Stacktrace**\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.lang.IllegalStateException: Messaging service is not running.\r\nat io.camunda.zeebe.transport.impl.AtomixClientTransportAdapter.tryToSend (AtomixClientTransportAdapter.java:105)\r\nat io.camunda.zeebe.transport.impl.AtomixClientTransportAdapter.lambda$handleResponse$7 (AtomixClientTransportAdapter.java:206)\r\nat io.camunda.zeebe.util.sched.ActorJob.invoke (ActorJob.java:76)\r\nat io.camunda.zeebe.util.sched.ActorJob.execute (ActorJob.java:39)\r\nat io.camunda.zeebe.util.sched.ActorTask.execute (ActorTask.java:122)\r\nat io.camunda.zeebe.util.sched.ActorThread.executeCurrentTask (ActorThread.java:94)\r\nat io.camunda.zeebe.util.sched.ActorThread.doWork (ActorThread.java:78)\r\nat io.camunda.zeebe.util.sched.ActorThread.run (ActorThread.java:191)\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n\r\n**Environment:**\r\n\r\n - Zeebe Version: 1.2.0-alpha2\r\n\r\n\n\n Zelldon: Error message introduce via https://github.com/camunda-cloud/zeebe/pull/7568\n Zelldon: related https://github.com/camunda-cloud/zeebe/issues/5521\n Zelldon: @pihme do you think this is solved due to recent refactorings?\n pihme: @Zelldon No I don't. The order of atomix and embedded gateway has not changed by the refactoring. One thing I wonder is whether this occurred in the Broker or in a Standalone Gateway. Do you know where it occurred?\r\n\r\nIn terms of macro-order of shutdown steps this should not happen.\r\n \r\nIt could happen if either shutdown of the actor does not cancel all subsequent planned tasks. Or it could happen if it is possible to schedule new tasks after an actor has been shutdown.\n pihme: Reoccurred recently:\r\n\r\nLogs:\r\n```\r\nD 2022-01-16T09:29:16.098476Z Closing Broker-0 [6/10]: embedded gateway closed in 15 ms \r\nI 2022-01-16T09:29:16.098659Z Closing Broker-0 [7/10]: cluster services \r\nD 2022-01-16T09:29:16.098802Z Closing Broker-0 [7/10]: cluster services closed in 0 ms \r\nI 2022-01-16T09:29:16.098978Z Closing Broker-0 [8/10]: subscription api \r\nD 2022-01-16T09:29:16.102018Z Closing Broker-0 [8/10]: subscription api closed in 2 ms \r\nI 2022-01-16T09:29:16.102259Z Closing Broker-0 [9/10]: command api transport and handler \r\nI 2022-01-16T09:29:16.232428Z Stopped \r\nD 2022-01-16T09:29:16.233107Z Closing Broker-0 [9/10]: command api transport and handler closed in 131 ms \r\nI 2022-01-16T09:29:16.233481Z Closing Broker-0 [10/10]: Migrated Startup Steps \r\nD 2022-01-16T09:29:16.235563Z Shutdown was called with context: io.camunda.zeebe.broker.bootstrap.BrokerStartupContextImpl@36539fa \r\nI 2022-01-16T09:29:16.235975Z Shutdown Cluster Services (Creation) \r\nI 2022-01-16T09:29:16.238984Z Stopped \r\nI 2022-01-16T09:29:16.240144Z Stopped \r\nI 2022-01-16T09:29:16.241539Z 0 - Member deactivated: Member{id=0, address=zeebe-0.zeebe-broker-service.0bbb3676-e3ad-445b-891e-0e1350800e41-zeebe.svc.cluster.local:26502, properties={brokerInfo=EADJAAAAAwAAAAAAAgAAAAMAAAADAAAAAAABCgAAAGNvbW1hbmRBcGlfAAAAemVlYmUtMC56ZWViZS1icm9rZXItc2VydmljZS4wYmJiMzY3Ni1lM2FkLTQ0NWItODkxZS0wZTEzNTA4MDBlNDEtemVlYmUuc3ZjLmNsdXN0ZXIubG9jYWw6MjY1MDEFAAIBAAAAAQIAAAABDAAABQAAADEuMi45BQACAQAAAAECAAAAAQ==, event-service-topics-subscribed=KIIDAGpvYnNBdmFpbGFibOU=}} \r\nI 2022-01-16T09:29:16.241947Z Stopped \r\nI 2022-01-16T09:29:16.242539Z Local node Member{id=0, address=zeebe-0.zeebe-broker-service.0bbb3676-e3ad-445b-891e-0e1350800e41-zeebe.svc.cluster.local:26502, properties={brokerInfo=EADJAAAAAwAAAAAAAgAAAAMAAAADAAAAAAABCgAAAGNvbW1hbmRBcGlfAAAAemVlYmUtMC56ZWViZS1icm9rZXItc2VydmljZS4wYmJiMzY3Ni1lM2FkLTQ0NWItODkxZS0wZTEzNTA4MDBlNDEtemVlYmUuc3ZjLmNsdXN0ZXIubG9jYWw6MjY1MDEFAAIBAAAAAQIAAAABDAAABQAAADEuMi45BQACAQAAAAECAAAAAQ==, event-service-topics-subscribed=KIIDAGpvYnNBdmFpbGFibOU=}} left the bootstrap servide \r\nI 2022-01-16T09:29:16.243398Z Stopped cluster membership service for member Member{id=0, address=zeebe-0.zeebe-broker-service.0bbb3676-e3ad-445b-891e-0e1350800e41-zeebe.svc.cluster.local:26502, properties={brokerInfo=EADJAAAAAwAAAAAAAgAAAAMAAAADAAAAAAABCgAAAGNvbW1hbmRBcGlfAAAAemVlYmUtMC56ZWViZS1icm9rZXItc2VydmljZS4wYmJiMzY3Ni1lM2FkLTQ0NWItODkxZS0wZTEzNTA4MDBlNDEtemVlYmUuc3ZjLmNsdXN0ZXIubG9jYWw6MjY1MDEFAAIBAAAAAQIAAAABDAAABQAAADEuMi45BQACAQAAAAECAAAAAQ==, event-service-topics-subscribed=KIIDAGpvYnNBdmFpbGFibOU=}} \r\nE 2022-01-16T09:29:18.270930Z Expected to handle gRPC request, but an unexpected error occurred \r\nE 2022-01-16T09:29:18.272130Z Expected to handle gRPC request, but an unexpected error occurred \r\nE 2022-01-16T09:29:18.273074Z Expected to handle gRPC request, but an unexpected error occurred \r\nI 2022-01-16T09:29:18.362961Z Stopped \r\nI 2022-01-16T09:29:18.363819Z Stopped \r\nI 2022-01-16T09:29:18.365121Z Shutdown monitoring services \r\nD 2022-01-16T09:29:18.366707Z Finished shutdown process \r\nD 2022-01-16T09:29:18.367117Z Closing Broker-0 [10/10]: Migrated Startup Steps closed in 2134 ms \r\nI 2022-01-16T09:29:18.367404Z Closing Broker-0 succeeded. Closed 10 steps in 2419 ms. \r\nI 2022-01-16T09:29:18.367629Z Broker shut down. \r\nD 2022-01-16T09:29:18.367920Z Closing actor thread ground 'Broker-0-zb-fs-workers' \r\nD 2022-01-16T09:29:18.368650Z Closing actor thread ground 'Broker-0-zb-actors' \r\nD 2022-01-16T09:29:18.369899Z Closing actor thread ground 'Broker-0-zb-actors': closed successfully \r\nD 2022-01-16T09:29:18.369957Z Closing actor thread ground 'Broker-0-zb-fs-workers': closed successfully \r\n```\n Zelldon: This happened again on our benchmark week 27\r\n\r\nhttps://console.cloud.google.com/errors/detail/CJ-difGB-LXsAg;service=zeebe;version=medic-cw-27-56ad2b36c8-benchmark;time=P7D?project=zeebe-io\n korthout: Happened again on benchmark week 27 ([newly reported error](https://console.cloud.google.com/errors/detail/CK6Khe-c7YzlFw;service=zeebe;version=medic-cw-27-56ad2b36c8-benchmark;time=P7D?project=zeebe-io)) with a different stacktrace. \r\n\r\nThis time because of `AtomixClientTransportAdapter.lambda$sendRequestInternal$2` instead of `AtomixClientTransportAdapter.lambda$handleResponse$7`.\n Zelldon: Last seen: 1 day ago.\n oleschoenburg: Happened again on 8.1.8: https://console.cloud.google.com/errors/detail/CK6Khe-c7YzlFw;service=zeebe;time=P7D?project=camunda-cloud-240911\n Zelldon: Happened in [zeebe:8.0.19-SNAPSHOT-stable-8.0-c326e93b](https://console.cloud.google.com/errors/detail/CKKJ762u3J3ZUw;service=zeebe;version=8.0.19-SNAPSHOT-stable-8.0-c326e93b;time=P7D?project=camunda-saas-int-chaos)\r\n\r\nhttps://console.cloud.google.com/errors/detail/CKKJ762u3J3ZUw;service=zeebe;time=P7D?project=camunda-saas-int-chaos\n deepthidevaki: Observed in 8.2.12\r\n\r\nRoot cause analysis:\r\n\r\nThe error message originates in `AtomixClientTransportAdapter`. This is created in Gateway BrokerClient\r\n\r\nhttps://github.com/camunda/zeebe/blob/18657c586a1974f6bb3ce3f86c1e16359458050d/gateway/src/main/java/io/camunda/zeebe/gateway/impl/broker/BrokerClientImpl.java#L64\r\n\r\nBut never closed by it\r\nhttps://github.com/camunda/zeebe/blob/0945c3088629e5a72b260778c104418afb113b08/gateway/src/main/java/io/camunda/zeebe/gateway/impl/broker/BrokerClientImpl.java#L70-L87\r\n\r\nFix:\r\nMessaging service is only closed after BrokerClient (I guess so because the lifecycle is managed by Spring, so the dependency order should be preserved). So closing `AtomixClientTransportAdapter` when closing `BrokerClientImpl` should ensure that no request are send after it is closed.\n oleschoenburg: Observed again on 8.1.16: https://console.cloud.google.com/errors/detail/CK6Khe-c7YzlFw;service=zeebe;time=P7D?project=camunda-saas-int-chaos\n megglos: ZDP-Triage:\n- mostly noise during shutdown\n- maybe the close is not sync",
    "title": "Messaging service is not running",
    "releaseNoteText": "**Consequence:** The messaging service was not running, causing an error when trying to send a response and closing the Broker.\n\n**Cause:** The issue occurred due to the incorrect order of resource closure. The messaging service was closed concurrently while still accepting or attempting to send a response, resulting in an error during the closure of the Broker.\n\n**Fix:** The `AtomixClientTransportAdapter` was not closed by the `BrokerClientImpl`. The fix involved closing the `AtomixClientTransportAdapter` when closing the `BrokerClientImpl` to ensure that no requests are sent after it is closed.\n\n**Result:** After the fix, the messaging service is running correctly, ensuring that requests are sent and responses are received without errors during the closure of the Broker."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/5209",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Description**\r\nCurrently we start zeebe partition services only after atomix is fully started. That means if a node has 3 partitions, we wait until all 3 partitions has successfully started their raft servers. In some benchmarks it was observed that sometimes 2 partitions in a node succesfuly starts and become the raft leaders. But the third partition did not join, or took long (hours) to join (due to #5208 ). During this time, this node is the raft leader for 2 partitions, but no processing is happening because we don't install stream processor and other leader services until the atomix start is completed. As a result, the service is unavailable or only partially available even though there are leaders for all partitions.\r\n\n\n Zelldon: I think this goes in hand with removing the complete atomix wrapper and bootstrap logic, which wanted to do anyway at some point. I think it makes sense to remove it and have independent starters for each partition.\n deepthidevaki: Is there an issue for it already? If so, let's link it here.\n Zelldon: Unfortunately I haven't found it in the backlog. That's why I just commented it :smile: \n npepinpe: Do we see any blockers to starting partitions in parallel without waiting for the others to be started?\n Zelldon: I see no blockers. Maybe @deepthidevaki has some thoughts?\r\n\n deepthidevaki: No real blockers that I know. We would have to refactor atomix interface. There should be a way to detect when an individual partition is started and ready. For a single partition, the steps in bootstrap are the following in order.\r\n1. RaftPartitionServer is created\r\n2. Become follower\r\n3. Leader election happens\r\n4. Join completes\r\n5. Catch up if needed\r\n6. Marked as Ready -> Startup complete.\r\n\r\nWe have to ensure that zeebe services are installed only after step 6. Currently, we do `atomix.start().join()`, this is guaranteed.\r\n  \n Zelldon: What we could to instead is:\r\n\r\n * start atomix transport\r\n * membership\r\n * start topology\r\n * start monitoring \r\n * start disk space\r\n * split up in partition starts with sub steps for installing processors etc.\n npepinpe: Would love to improve the bootstrapping logic next quarter - let's see if we can make it happen.\n deepthidevaki: We observed an e2e test failure as a consequence of this.\r\n\r\nBroker 2 was not becoming ready, because partition 1 was not able to receive heartbeats from leader (due to another bug). But it was receiving events for partitions 2 and 3. Since raft partition 1 is not ready, Zeebe services  (processing and snapshoting) were not started for any partitions. So partitions 2 and 3 starts accumulating events in the log, but no snapshot were taken.\r\n\r\nLater, the root issue was resolved, partition 1 startup was unblocked and broker 2 became ready. Immediately after that, broker 2 became the leader for partition 3. Since it was not replaying and taking snapshots before, it's last snapshot was from 12 hours ago. As a result, partition 3 had to replay events generated in the last 12 hours before it can start processing. The replay took almost 1 hour, and during that time partition 3 was essentially unavailable for processing. All instances that were created before the leader change, was completed only after 1 hour.\n megglos: planning: affects higher partition count setups more heavily, risk of incidents,\nlet's time-box the investigation to 2h\n oleschoenburg: I looked into this, mostly to understand the startup process.\r\nTo me it looks like we have two \"barriers\" that wait for all partitions before continuing the startup process:\r\n1. `RaftPartitionGroup#join` waits for all raft partitions [to become `READY`](https://github.com/camunda/zeebe/blob/f574c79929f01e51f33a3d5902fc651aa15c3d46/atomix/cluster/src/main/java/io/atomix/raft/impl/DefaultRaftServer.java#L191-L197). This is [called by the PartitionManger](https://github.com/camunda/zeebe/blob/6a454eedc03e14d4f7fa56e5560c4febfbe57f89/broker/src/main/java/io/camunda/zeebe/broker/partitioning/PartitionManagerImpl.java#L128-L129) before scheduling `ZeebePartition`s. \r\n2.  `PartitionManagerImpl#start` waits for all `ZeebePartition`s to start. This blocks at least the installation of `AdminApiRequestHandler`\r\n\r\nThe first one is the crux I think, we don't want to wait for the partition _group_ to start, we just want to wait for each partition individually.\r\n\r\nThe second one appears to be trivial to solve:\r\n\r\n```patch\r\ndiff --git a/broker/src/main/java/io/camunda/zeebe/broker/partitioning/PartitionManagerImpl.java b/broker/src/main/java/io/camunda/zeebe/broker/partitioning/PartitionManagerImpl.java\r\n--- a/broker/src/main/java/io/camunda/zeebe/broker/partitioning/PartitionManagerImpl.java\t(revision 4fa6c0dc0009f5faedb987b09a055f9135629495)\r\n+++ b/broker/src/main/java/io/camunda/zeebe/broker/partitioning/PartitionManagerImpl.java\t(revision 6a454eedc03e14d4f7fa56e5560c4febfbe57f89)\r\n@@ -127,7 +127,7 @@\r\n \r\n     return partitionService\r\n         .start()\r\n-        .thenApply(\r\n+        .thenApplyAsync(\r\n             ps -> {\r\n               LOGGER.info(\"Registering Partition Manager\");\r\n \r\n@@ -158,12 +158,10 @@\r\n                       topologyManager,\r\n                       brokerCfg.getExperimental().getFeatures().toFeatureFlags()));\r\n \r\n-              final var futures =\r\n-                  partitions.stream()\r\n-                      .map(partition -> CompletableFuture.runAsync(() -> startPartition(partition)))\r\n-                      .toArray(CompletableFuture[]::new);\r\n+              for (final var partition : partitions) {\r\n+                CompletableFuture.runAsync(() -> startPartition(partition));\r\n+              }\r\n \r\n-              CompletableFuture.allOf(futures).join();\r\n               return null;\r\n             });\r\n   }\r\n```\r\n\r\n",
    "title": "Startup failure of one raft partition server affects the availability of other healthy partition",
    "releaseNoteText": "**Consequence:** The availability of healthy partitions is affected by the startup failure of one raft partition server. This issue arises because the system currently waits for all partitions to successfully start their raft servers before starting the zeebe partition services. As a result, even though there are leaders for all partitions, the service may be unavailable or only partially available.\n\n**Cause:** The issue is caused by the current logic of waiting for all partitions to start before installing stream processors and other leader services. This logic is based on waiting for the atomix start to be completed.\n\n**Fix:** The fix involves removing the complete atomix wrapper and bootstrap logic. Instead, independent starters for each partition are implemented. The bootstrapping logic is improved by starting the atomix transport, membership, topology, monitoring, and disk space first. Then, the partition starts are split up into sub-steps for installing processors and other services.\n\n**Result:** After the fix is applied, the system will no longer wait for all partitions to start before installing stream processors and leader services. Each partition will be able to start independently, allowing for parallel startup of the partitions. This will improve the availability of healthy partitions and prevent the system from being partially or fully unavailable due to startup failures."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/2890",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nAt a multi-instance activity, I can define the input collection to iterate over. If the collection holds a large number of elements, the broker might fail to spawn the inner instances. Such a case can only be fixed, by manually decreasing the collection variable.\r\n\r\n**Describe the solution you'd like**\r\n* I can spawn as many instances as defined by the input collection\r\n* instead of spawning the instances all at once on activating the body (current behavior), spawn the instances step-wise until all instances are created (e.g. spawn 10 instances and write a record to spawn the next 10)\r\n* the instance spawning can be interrupted by an event or a terminate command\r\n\r\n**Describe alternatives you've considered**\r\n* increasing buffers and record max length - see #2880 \r\n* splitting the creation into multiple steps (e.g. loop or nested multi-instance) or control it externally -  \r\n\r\nRelates to\r\nhttps://jira.camunda.com/browse/SUPPORT-16653\r\nhttps://jira.camunda.com/browse/SUPPORT-16549\r\n\n\n saig0: Using Zeebe 0.21.0, we can execute a multi-instance activity with an input collection of ~ 5.000 elements (depending on the variables). By increasing the max message size, we could process even larger collections. \r\n\r\nSince this is ok for now, we will postpone the issue.\n npepinpe: Just a side note, let's postpone at least until the new engine is done (i.e. at least Q2 2021). I can see us working on reducing the max message size next year, so we will need to implement this kind of \"chunking\" - obviously can't guarantee, but it sounds plausible to me.\n saig0: **Update:** I tested it again with version `1.0.0-alpha7`. We can now iterate over an input collection with up to ~12.000 elements :tada: \n korthout: Marking priority as `later` because 'multi-instance for large collections' is not the main concern for the process automation team right now. However, we should probably work on this when we work on\r\n- #8687\r\n\r\nNote that the impact of this bug is reduced once we've resolved\r\n- #5221\r\n\r\nPlease comment if you think this should have a higher priority.\n epollum: Hello team, I want to mention that we have a customer who requires the ability to use large multi-instance activities. Please see https://jira.camunda.com/browse/SUPPORT-16499\r\nAnd https://github.com/camunda/zeebe/issues/11355\r\n\r\nThank you!\n daniel-ewing: Hi team, here is another one: https://jira.camunda.com/browse/SUPPORT-16653\n npepinpe: And another: https://jira.camunda.com/browse/SUPPORT-16549\n abbasadel: Team meeting: we changed the priority to \"Upcoming\" to pick this up when we have time\n felix-mueller: Hey @remcowesterhoud \r\nI saw you worked on this item, there is one more item in the backlog: https://github.com/camunda/zeebe/issues/8687\r\n\r\nCould you perhaps elaborate if this is fixed now as well or is there a case which is not covered? \r\nCould you perhaps explain which case is not covered for #8687?\r\n\r\nThanks\r\nFelix\n remcowesterhoud: @felix-mueller from what I understand after reading #8687 it is a different issue. This one had to do with instance banning and the original author of the issue [explicitly mentioned](https://github.com/camunda/zeebe/issues/8687#issuecomment-1026023644) his instances weren't getting banned.\r\n\r\nWhether this is fixed, I am not sure as I'm struggling to understand what is happening in the other issue 😅 @korthout do you have any ideas? It reads to me like it's more about job activation than multi-instance input collections.\r\n\r\nIf it's stale we can consider closing it and seeing if it ever occurs again.\n korthout: @remcowesterhoud I've had a look. IMO, that issue is not specifically related to multi-instance, but rather to activating many jobs with large variables in the Go client.\r\n\r\nI've provided my thoughts on that issue [here](https://github.com/camunda/zeebe/issues/8687#issuecomment-1665950474)",
    "title": "I can spawn inner instances for a large input collection",
    "releaseNoteText": "**Consequence:**\nThe broker fails to spawn inner instances when the input collection for a multi-instance activity contains a large number of elements. This requires manual adjustment of the collection variable to fix the issue.\n\n**Cause:**\nThe current behavior of the multi-instance activity is to spawn all instances at once, which can overload the system when dealing with large input collections.\n\n**Fix:**\nThe instances are now spawned step-wise, allowing for a controlled creation process. For example, a set number of instances are spawned first, followed by the spawning of the next set of instances after a specific event or terminate command.\n\n**Result:**\nUsers can now spawn as many instances as defined by the input collection, even for large collections. The instances are spawned in a controlled manner, ensuring the system can handle the process without overloading."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14236",
      "component": "Zeebe",
      "subcomponent": "Gateway",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nWhen calling any of the `StreamObserver` methods (including `#onNext`) concurrently, the underlying stream will send garbled messages, and you end up with errors like \r\n\r\n```\r\njava.lang.IllegalStateException: sendHeaders has already been called \r\n```\r\n\r\nOr \r\n\r\n```\r\nio.grpc.StatusRuntimeException: INTERNAL: Connection closed after GOAWAY. HTTP/2 error code: INTERNAL_ERROR, debug data: Stream 7 sent too many headers EOS: false\r\n```\r\n\r\nReading the documentation, it's quite clearly stated in `StreamObserver.java`:\r\n\r\n> Separate StreamObservers do not need to be synchronized together; incoming and outgoing directions are independent. Since individual StreamObservers are not thread-safe, if multiple threads will be writing to a StreamObserver concurrently, the application must synchronize calls.\r\n\r\nIn the `ClientStreamAdapter`, we call `onNext` using a thread pool executor, so concurrent calls are possible.\r\n\r\n**To Reproduce**\r\n\r\nEasily reproducible with [Camunda 8 Benchmark](https://github.com/camunda-community-hub/camunda-8-benchmark), since there we have a single client, so it's more likely for us to receive all the jobs.\r\n\r\n**Expected behavior**\r\n\r\nCalls to a `StreamObserver` are serialized. Either all consumers become an actor (what is the overhead there?), or we use gRPC's `SerializingExecutor` to wrap around our thread pool.\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.3.0-alpha5, 8.3.0-alpha6\r\n\n\n npepinpe: Open questions:\r\n\r\n- Can we make all `ClientStreamConsumerImpl` just be actors? I mean, that's our synchronization/serialization mechanism, and it lends itself quite well here. What's the overhead of creating potentially so many actors? Just that they're all on the same pool? That they will be performing I/O?\r\n- As a quick fix, we could use gRPC's `SerializingExecutor`, but that seems to be an internal utility, so maintenance wise it could be an issue in the end. In fact, they have a TODO comment: `// TODO(madongfly): figure out a way to not expose it or move it to transport package.`\n npepinpe: For job push to be usable in 8.3, this needs to be fixed by then\n Zelldon: I think this is related to https://github.com/camunda/zeebe/issues/14047\n\nWe have also observed the go away errors when testing with Immi.\n npepinpe: So maybe for 8.3 (in a few weeks) we quickly fix it using `SerializingExecutor` (literally a one line fix), but commit to a better solution using actors which could take a bit longer (definitely not small :smile:)",
    "title": "Prevent concurrent StreamObserver calls on the same instance",
    "releaseNoteText": "**Consequence:** When calling `StreamObserver` methods concurrently, such as `onNext`, the underlying stream would send garbled messages and result in errors like `java.lang.IllegalStateException: sendHeaders has already been called` or `io.grpc.StatusRuntimeException: INTERNAL: Connection closed after GOAWAY. HTTP/2 error code: INTERNAL_ERROR, debug data: Stream 7 sent too many headers EOS: false`.\n\n**Cause:** The issue was caused by the `ClientStreamAdapter` calling `onNext` using a thread pool executor, which allowed for concurrent calls to `StreamObserver` methods.\n\n**Fix:** The team implemented a fix by using gRPC's `SerializingExecutor` to wrap around the thread pool, ensuring that calls to `StreamObserver` are serialized.\n\n**Result:** With the fix in place, calls to `StreamObserver` methods are now properly serialized, preventing garbled messages and errors. This ensures the correct and reliable functioning of the underlying stream."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14176",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nThe constructor of the `ActivateJobsCommandImpl` is trying to set the `workerName` during construction and expects the `config.getDefaultJobWorkerName())` to be non null. This leads to a `NullPointerException` as the underlying builder doesnt support a null value. This is an issue as clients are able to set the `workerName` using the builder pattern after constructing the command.\r\n\r\n```\r\npublic ActivateJobsCommandImpl(\r\n      final GatewayStub asyncStub,\r\n      final ZeebeClientConfiguration config,\r\n      final JsonMapper jsonMapper,\r\n      final Predicate<Throwable> retryPredicate) {\r\n    this.asyncStub = asyncStub;\r\n    this.jsonMapper = jsonMapper;\r\n    this.retryPredicate = retryPredicate;\r\n    builder = ActivateJobsRequest.newBuilder();\r\n    requestTimeout(config.getDefaultRequestTimeout());\r\n    timeout(config.getDefaultJobTimeout());\r\n    workerName(config.getDefaultJobWorkerName());\r\n  }\r\n```\r\n\r\n**To Reproduce**\r\n\r\nCreate an implementation of the `ZeebeClientProperties` that returns `null` for the `getDefaultJobWorkerName`\r\n\r\n**Expected behavior**\r\n\r\nThe builder should fallback to the default worker name only if it wasnt set by a client when constructing the final command.\r\n\r\n**Log/Stacktrace**\r\n\r\n```\r\njava.lang.NullPointerException: null\r\n\tat io.camunda.zeebe.gateway.protocol.GatewayOuterClass$ActivateJobsRequest$Builder.setWorker(GatewayOuterClass.java:2169)\r\n\tat io.camunda.zeebe.client.impl.command.ActivateJobsCommandImpl.workerName(ActivateJobsCommandImpl.java:83)\r\n\tat io.camunda.zeebe.client.impl.command.ActivateJobsCommandImpl.<init>(ActivateJobsCommandImpl.java:60)\r\n```\r\n\r\n**Environment:**\r\n- Zeebe Version:8.3.0-alpha4\r\n\r\nLooks like it happens as the command builder is used instead of the previously used request builder since: https://github.com/camunda/zeebe/commit/359a402b5bbee7247749385a458c5b3f3aba7e78\n\n korthout: @npepinpe This sounds like a regression introduced with #12888. Could you please have a look?\n npepinpe: We had a look together, and AFAIK this is the same behavior as ever. We were always calling `workerName` in the constructor of the command, and it makes sense to me to follow Simon's suggestion and only apply defaults in the build method, not in the constructor.\r\n\r\nEDIT: here's the commit which introduced setting the worker name as the default configuration: https://github.com/camunda/zeebe/blob/06f4e463efd3ca1be67056660f12abd4ad7867d8/clients/java/src/main/java/io/zeebe/client/impl/job/ActivateJobsCommandImpl.java#L46\r\n\r\nThat's from 2018, so it looks to me like this was always the behavior. The only thing I can imagine would be that _maybe_ the gRPC behavior changed, where they simply don't allow null values anymore, but I doubt it. I suspect if we check, it was always like this.\n npepinpe: > Looks like it happens as the command builder is used instead of the previously used request builder since: https://github.com/camunda/zeebe/commit/359a402b5bbee7247749385a458c5b3f3aba7e78\r\n\r\n@sbuettner - can you point out to me where the change from request to command builder happened? From the commit, the code is the same - we extracted an interface out of a class, but the implementation was not changed.\r\n\r\nFor example, in the \"old\" implementation:\r\n\r\n```java\r\n    final ActivateJobsCommandStep3 activateCommand =\r\n        jobClient\r\n            .newActivateJobsCommand()\r\n            .jobType(jobType)\r\n            .maxJobsToActivate(maxJobsToActivate)\r\n            .timeout(timeout)\r\n            .workerName(workerName);\r\n```\r\n\r\nAnd it was simply moved in the \"new\" one, but remains the same:\r\n\r\n```java\r\n    final ActivateJobsCommandStep3 activateCommand =\r\n        jobClient\r\n            .newActivateJobsCommand()\r\n            .jobType(jobType)\r\n            .maxJobsToActivate(maxJobsToActivate)\r\n            .timeout(timeout)\r\n            .workerName(workerName);\r\n```\r\n\r\nI'm also not sure what you mean by request vs command builder :thinking: \n sbuettner: @npepinpe It looks like it was changed here: https://github.com/camunda/zeebe/commit/11c548dd4d46f62119a600da0dc1abad7231c417#diff-7101968ae21254c317a1606e4a1969ec3864b538403541ecc7161351b2bd38ddL86\n npepinpe: Good catch! Seems unrelated to job push, but it is indeed a regression in that sense for 8.3.0. I'll defer to the ZPA team on what the priority for it is.\n korthout: Thanks @npepinpe and @sbuettner for the extra input.\r\n\r\n- We'll need to fix this before 8.3 as it regressed recently.\r\n- We consider this mid severity, as a workaround is available (don't set null as the default job worker name)\r\n- We wonder how quickly a user would run into this, do you have to set `null` explicitly, or does it also occur when the client doesn't define any default?\r\n- This would change the impact dramatically, and so our priority\r\n- We want to timebox whether that is the case\r\n- It might be worth it to fix immediately then (but please timebox it)",
    "title": "ActivateJobsCommandImpl throws NullPointerException when ZeebeClientProperties.getDefaultJobWorkerName is null",
    "releaseNoteText": "**Consequence:**\nThe constructor of the `ActivateJobsCommandImpl` throws a `NullPointerException` when `ZeebeClientProperties.getDefaultJobWorkerName` is null. This issue occurs because the builder used in the constructor does not support a null value for the worker name.\n\n**Cause:**\nThe issue is caused by a recent change where the builder used in the constructor was updated to not allow null values for the worker name. This change was introduced in a commit (https://github.com/camunda/zeebe/commit/359a402b5bbee7247749385a458c5b3f3aba7e78) and affects the behavior of setting the worker name during construction.\n\n**Fix:**\nThe issue was fixed by modifying the code to apply defaults to the worker name only in the build method, rather than in the constructor.\n\n**Result:**\nAfter the fix, the builder in the `ActivateJobsCommandImpl` will fallback to the default worker name only if it was not set by a client when constructing the final command. This ensures that a `NullPointerException` will no longer occur when `ZeebeClientProperties.getDefaultJobWorkerName` is null."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14496",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nRestoring a backup taken with `clusterSize=3` from a broker with `clusterSize=1` fails because not all backups can be found.\r\n\r\nThe `PartitionRestoreService`  builds known broker ids based on `clusterSize` and then attempts to find a partition backup by going through all broker ids. When `clusterSize=1`, it will only search for backups taken by broker 1, thus missing backups taken by broker 2 and 3.\r\n\r\n**To Reproduce**\r\n\r\n1. Take a backup with `clusterSize=3` where broker 1 is _not_ leader for all partitions\r\n2. Restore with `clusterSize=1`\r\n\r\n**Expected behavior**\r\n\r\nRestore finds backups from all brokers, not only those in the current configuration.\r\n\r\n**Context**\r\nSupport case: https://jira.camunda.com/browse/SUPPORT-18562\n",
    "title": "Restore with fewer brokers fails to find all backups",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14418",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nWe got alerted in SaaS, an incident was created which contains several information to it https://app.slack.com/client/T0PM0P1SA/C05TG4DLD5H/thread/C05TG4DLD5H-1695364924.679279\r\n\r\nWe can see that a role change happened and on the new leader the corruption was detected. The partition was marked as dead afterward!\r\n\r\n\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n**To Reproduce**\r\nNot clear yet.\r\n\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\n**Expected behavior**\r\n\r\nNot detection\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n**Log/Stacktrace**\r\nError group https://console.cloud.google.com/errors/detail/CKLTyeONkqLZCg;service=zeebe;time=P7D?project=camunda-cloud-240911\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\nio.camunda.zeebe.db.ZeebeDbInconsistentException: Key DbCompositeKey{first=DbLong{2251799813994826}, second=optimus_entity_created_message} in ColumnFamily PROCESS_SUBSCRIPTION_BY_KEY already exists\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.assertKeyDoesNotExist(TransactionalColumnFamily.java:286) ~[zeebe-db-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.lambda$insert$0(TransactionalColumnFamily.java:80) ~[zeebe-db-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.lambda$ensureInOpenTransaction$19(TransactionalColumnFamily.java:314) ~[zeebe-db-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.DefaultTransactionContext.runInTransaction(DefaultTransactionContext.java:31) ~[zeebe-db-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.ensureInOpenTransaction(TransactionalColumnFamily.java:313) ~[zeebe-db-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.insert(TransactionalColumnFamily.java:75) ~[zeebe-db-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.state.message.DbProcessMessageSubscriptionState.put(DbProcessMessageSubscriptionState.java:82) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.state.appliers.ProcessMessageSubscriptionCreatingApplier.applyState(ProcessMessageSubscriptionCreatingApplier.java:28) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.state.appliers.ProcessMessageSubscriptionCreatingApplier.applyState(ProcessMessageSubscriptionCreatingApplier.java:15) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.state.appliers.EventAppliers.applyState(EventAppliers.java:327) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.streamprocessor.writers.ResultBuilderBackedEventApplyingStateWriter.appendFollowUpEvent(ResultBuilderBackedEventApplyingStateWriter.java:56) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.streamprocessor.writers.ResultBuilderBackedEventApplyingStateWriter.appendFollowUpEvent(ResultBuilderBackedEventApplyingStateWriter.java:42) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.common.CatchEventBehavior.subscribeToMessageEvent(CatchEventBehavior.java:271) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.common.CatchEventBehavior.lambda$subscribeToMessageEvents$7(CatchEventBehavior.java:246) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat java.util.stream.ForEachOps$ForEachOp$OfRef.accept(Unknown Source) ~[?:?]\r\n\tat java.util.stream.ReferencePipeline$2$1.accept(Unknown Source) ~[?:?]\r\n\tat java.util.ArrayList$ArrayListSpliterator.forEachRemaining(Unknown Source) ~[?:?]\r\n\tat java.util.stream.AbstractPipeline.copyInto(Unknown Source) ~[?:?]\r\n\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(Unknown Source) ~[?:?]\r\n\tat java.util.stream.ForEachOps$ForEachOp.evaluateSequential(Unknown Source) ~[?:?]\r\n\tat java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(Unknown Source) ~[?:?]\r\n\tat java.util.stream.AbstractPipeline.evaluate(Unknown Source) ~[?:?]\r\n\tat java.util.stream.ReferencePipeline.forEach(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.engine.processing.common.CatchEventBehavior.subscribeToMessageEvents(CatchEventBehavior.java:246) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.common.CatchEventBehavior.lambda$subscribeToEvents$4(CatchEventBehavior.java:154) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.util.Either$Right.ifRight(Either.java:371) ~[zeebe-util-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.common.CatchEventBehavior.subscribeToEvents(CatchEventBehavior.java:152) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnEventSubscriptionBehavior.subscribeToEvents(BpmnEventSubscriptionBehavior.java:48) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.container.CallActivityProcessor.lambda$onActivate$0(CallActivityProcessor.java:64) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.util.Either$Right.flatMap(Either.java:366) ~[zeebe-util-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.container.CallActivityProcessor.onActivate(CallActivityProcessor.java:64) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.container.CallActivityProcessor.onActivate(CallActivityProcessor.java:29) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.lambda$processEvent$2(BpmnStreamProcessor.java:144) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.util.Either$Right.ifRightOrLeft(Either.java:381) ~[zeebe-util-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.processEvent(BpmnStreamProcessor.java:143) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.lambda$processRecord$0(BpmnStreamProcessor.java:92) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.util.Either$Right.ifRightOrLeft(Either.java:381) ~[zeebe-util-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.processRecord(BpmnStreamProcessor.java:89) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.incident.ResolveIncidentProcessor.lambda$attemptToContinueProcessProcessing$0(ResolveIncidentProcessor.java:105) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.util.Either$Right.ifRightOrLeft(Either.java:381) ~[zeebe-util-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.incident.ResolveIncidentProcessor.attemptToContinueProcessProcessing(ResolveIncidentProcessor.java:103) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.incident.ResolveIncidentProcessor.processRecord(ResolveIncidentProcessor.java:81) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.Engine.process(Engine.java:127) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.batchProcessing(ProcessingStateMachine.java:353) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$2(ProcessingStateMachine.java:269) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run(ZeebeTransaction.java:98) ~[zeebe-db-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand(ProcessingStateMachine.java:269) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord(ProcessingStateMachine.java:228) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.readNextRecord(ProcessingStateMachine.java:204) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) ~[zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:109) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:204) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- OS: <!-- [e.g. Linux] -->\r\n- Zeebe Version: 8.3.0-alpha6\r\n- Configuration: <!-- [e.g. exporters etc.] -->\r\n\n\n korthout: >Based on the metrics we can see that around 23:11:30 a leader change happened.\r\n>- Before Zeebe-1 was leader for all a partitions (also 1)\r\n>- Later Zeebe-0 becomes leader for partition one (Zeebe-1 is still leader!)\r\n>- Both leaders become unhealthy\r\n\r\nThis sounds like a raft issue. If both are leader they would be handing out the same key and inconsistency is expected.\r\n\r\nThis is a platform issue, so moving it to Zeebe Distributed Platform team board.\n Zelldon: @korthout please write in the incident channel\n korthout: After more research, we found out that this inconsistency was the cause of leader election and that there were not two leaders at the same time as initially thought.\r\n\r\nFurther investigation showed that the inconsistency occurred due to the following:\r\n\r\n- A message subscription and process message subscription are both created along with the raised incident on activating a call activity where the called process is not found\r\n- When the incident is resolved, the call activity is attempted to be activated again.\r\n- This time, the message subscription and process message subscription are recreated as part of the call activity activation.\r\n- This causes the inconsistency failure\r\n\r\nIt should be possible to reproduce this in a test case\n korthout: I've added a reproducing test case in https://github.com/camunda/zeebe/compare/8.3.0-alpha6...korthout-14418-test-inconsistency",
    "title": "Inconsitency detected in PROCESS_SUBSCRIPTION_BY_KEY",
    "releaseNoteText": "**Consequence**:\n\nA inconsistency was detected in the `PROCESS_SUBSCRIPTION_BY_KEY` column family, resulting in a key `DbCompositeKey{first=DbLong{2251799813994826}, second=optimus_entity_created_message}` already existing. This led to an `io.camunda.zeebe.db.ZeebeDbInconsistentException` being thrown.\n\n**Cause**:\n\nThe inconsistency was caused by the recreation of a message subscription and process message subscription during the activation of a call activity where the called process was not found. When the incident was resolved and the call activity was activated again, the message subscription and process message subscription were recreated, resulting in the duplication of the key.\n\n**Fix**:\n\nThe fix for this issue involved modifying the handling of the call activity activation to prevent the recreation of the message subscription and process message subscription that caused the inconsistency.\n\n**Result**:\n\nAfter applying the fix, the call activity activation no longer recreates the message subscription and process message subscription, eliminating the cause of the inconsistency. Users will no longer encounter the `io.camunda.zeebe.db.ZeebeDbInconsistentException` and the system behavior will be consistent when activating call activities."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14366",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nError group: https://console.cloud.google.com/errors/detail/CJLK28eniZi1JA;service=zeebe;time=P7D?project=camunda-cloud-240911 (occurred 12 times yesterday on alpha6)\r\n\r\nThe cancelation of a process fails because the process model can't be found or is not available and the processing then fails with a NPE.\r\n\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n**To Reproduce**\r\n\r\nCancel a process I guess? I'm not 100% sure how this could happen in the log we also don't see the reference key, unfortunately. \r\n\r\nI tried with zbctl to cancel negative or non-existing but it didn't worked\r\n\r\n```\r\n$ zbctl cancel instance \"-1\"\r\nError: unknown shorthand flag: '1' in -1\r\n$ zbctl cancel instance \"1\" --insecure\r\nError: rpc error: code = Unavailable desc = Expected to execute command on partition 0, but either it does not exist, or the gateway is not yet aware of it\r\n```\r\n\r\nFor me it looks like it has to be a real key otherwise it will not be forwarded from the gateway to the right partition and written to the log.\r\n🤷🏼 \r\n\r\n\r\nI tried to reproduce this on SaaS:\r\n\r\n1. Created a cluster with alpha6\r\n2. Deployed a simple model with a user task\r\n3. Created an instance\r\n4. Canceled the instance in operate\r\n5. Cancelation was successful\r\n\r\nNot sure how to reproduce this, but I think this is clearly a regression.\r\n\r\n\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\n**Expected behavior**\r\n\r\nWe can cancel a process instance without issues.\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n<pre>\r\njava.lang.NullPointerException: Cannot invoke \"io.camunda.zeebe.engine.state.deployment.DeployedProcess.getKey()\" because \"process\" is null\r\n\tat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnBufferedMessageStartEventBehavior.findNextMessageToCorrelate(BpmnBufferedMessageStartEventBehavior.java:95) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnBufferedMessageStartEventBehavior.correlateNextBufferedMessage(BpmnBufferedMessageStartEventBehavior.java:76) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnBufferedMessageStartEventBehavior.correlateMessage(BpmnBufferedMessageStartEventBehavior.java:66) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.container.ProcessProcessor.lambda$getPostTransitionAction$10(ProcessProcessor.java:212) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat java.util.Optional.ifPresent(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.container.ProcessProcessor.lambda$getPostTransitionAction$11(ProcessProcessor.java:210) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.util.Either$Right.ifRightOrLeft(Either.java:381) ~[zeebe-util-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.container.ProcessProcessor.transitionTo(ProcessProcessor.java:182) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.container.ProcessProcessor.onChildTerminated(ProcessProcessor.java:167) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.container.ProcessProcessor.onChildTerminated(ProcessProcessor.java:27) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnStateTransitionBehavior.lambda$onElementTerminated$5(BpmnStateTransitionBehavior.java:424) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnStateTransitionBehavior.invokeElementContainerIfPresent(BpmnStateTransitionBehavior.java:474) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnStateTransitionBehavior.onElementTerminated(BpmnStateTransitionBehavior.java:419) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.task.JobWorkerTaskProcessor.lambda$onTerminate$10(JobWorkerTaskProcessor.java:97) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat java.util.Optional.ifPresentOrElse(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.task.JobWorkerTaskProcessor.onTerminate(JobWorkerTaskProcessor.java:86) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.task.JobWorkerTaskProcessor.onTerminate(JobWorkerTaskProcessor.java:25) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.processEvent(BpmnStreamProcessor.java:153) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.lambda$processRecord$0(BpmnStreamProcessor.java:92) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.util.Either$Right.ifRightOrLeft(Either.java:381) ~[zeebe-util-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.processRecord(BpmnStreamProcessor.java:89) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.Engine.process(Engine.java:127) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.batchProcessing(ProcessingStateMachine.java:353) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$2(ProcessingStateMachine.java:269) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run(ZeebeTransaction.java:98) ~[zeebe-db-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand(ProcessingStateMachine.java:269) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord(ProcessingStateMachine.java:228) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.readNextRecord(ProcessingStateMachine.java:204) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:109) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:204) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\"\r\n</pre>\r\n</details>\r\n\r\n**Environment:**\r\n- OS: <!-- [e.g. Linux] -->\r\n- Zeebe Version: 8.3.0-alpha6\r\n- Configuration: <!-- [e.g. exporters etc.] -->\r\n\n\n Zelldon: I put the severity to high since I think we should take a look asap since it seems to be a regression due to our latest changes. I was not putting it as critical since I was not able to reproduce it easily, so there might be some weird circumstances that this happens not sure yet.\n nicpuppa: This error seems to be related to this method `processState.getLatestProcessVersionByProcessId()`. This method is somehow implicated also in those bugs #14309 and #14055 \r\n\r\n@Zelldon why do you think is related to process cancellation ?\n Zelldon: @nicpuppa because of the stacktrace see above and the related [log](https://console.cloud.google.com/logs/query;query=error_group%2528%22CJLK28eniZi1JA%22%2529%0AlogName:%22stdout%22%0Aresource.type%3D%22k8s_container%22%0Aresource.labels.namespace_name%3D%227832669d-388f-4728-8e2d-d4426c350cd4-zeebe%22%0Aresource.labels.cluster_name%3D%22prod-worker-4%22%0Aresource.labels.pod_name%3D%22zeebe-2%22%0Aresource.labels.container_name%3D%22zeebe%22%0Aresource.labels.location%3D%22australia-southeast1%22%0Aresource.labels.project_id%3D%22camunda-cloud-240911%22;cursorTimestamp=2023-09-19T11:42:39.114714745Z;startTime=2023-09-19T11:14:14.453Z;endTime=2023-09-19T12:14:14.453Z?project=camunda-cloud-240911)\r\n\r\n```\r\nERROR 2023-09-19T11:42:39.114714745Z [resource.labels.containerName: zeebe] Expected to process record 'TypedRecordImpl{metadata=RecordMetadata{recordType=COMMAND, valueType=PROCESS_INSTANCE, intent=CANCEL, authorization=UNKNOWN}, value={\"bpmnElementType\":\"UNSPECIFIED\",\"elementId\":\"\",\"bpmnProcessId\":\"\",\"version\":-1,\"processDefinitionKey\":-1,\"processInstanceKey\":-1,\"flowScopeKey\":-1,\"bpmnEventType\":\"UNSPECIFIED\",\"parentProcessInstanceKey\":-1,\"parentElementInstanceKey\":-1}}' without errors, but exception occurred with message 'Cannot invoke \"io.camunda.zeebe.engine.state.deployment.DeployedProcess.getKey()\" because \"process\" is null'.\r\n  {\r\n    \"insertId\": \"0d4cq88vzwi3t76q\",\r\n    \"jsonPayload\": {\r\n      \"serviceContext\": {\r\n        \"version\": \"8.3.0-alpha6\",\r\n        \"service\": \"zeebe\"\r\n      },\r\n      \"message\": \"Expected to process record 'TypedRecordImpl{metadata=RecordMetadata{recordType=COMMAND, valueType=PROCESS_INSTANCE, intent=CANCEL, authorization=UNKNOWN}, value={\\\"bpmnElementType\\\":\\\"UNSPECIFIED\\\",\\\"elementId\\\":\\\"\\\",\\\"bpmnProcessId\\\":\\\"\\\",\\\"version\\\":-1,\\\"processDefinitionKey\\\":-1,\\\"processInstanceKey\\\":-1,\\\"flowScopeKey\\\":-1,\\\"bpmnEventType\\\":\\\"UNSPECIFIED\\\",\\\"parentProcessInstanceKey\\\":-1,\\\"parentElementInstanceKey\\\":-1}}' without errors, but exception occurred with message 'Cannot invoke \\\"io.camunda.zeebe.engine.state.deployment.DeployedProcess.getKey()\\\" because \\\"process\\\" is null'.\",\r\n      \"@type\": \"type.googleapis.com/google.devtools.clouderrorreporting.v1beta1.ReportedErrorEvent\",\r\n      \"exception\": \"java.lang.NullPointerException: Cannot invoke \\\"io.camunda.zeebe.engine.state.deployment.DeployedProcess.getKey()\\\" because \\\"process\\\" is null\\n\\tat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnBufferedMessageStartEventBehavior.findNextMessageToCorrelate(BpmnBufferedMessageStartEventBehavior.java:95) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnBufferedMessageStartEventBehavior.correlateNextBufferedMessage(BpmnBufferedMessageStartEventBehavior.java:76) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnBufferedMessageStartEventBehavior.correlateMessage(BpmnBufferedMessageStartEventBehavior.java:66) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.engine.processing.bpmn.container.ProcessProcessor.lambda$getPostTransitionAction$10(ProcessProcessor.java:212) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat java.util.Optional.ifPresent(Unknown Source) ~[?:?]\\n\\tat io.camunda.zeebe.engine.processing.bpmn.container.ProcessProcessor.lambda$getPostTransitionAction$11(ProcessProcessor.java:210) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.util.Either$Right.ifRightOrLeft(Either.java:381) ~[zeebe-util-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.engine.processing.bpmn.container.ProcessProcessor.transitionTo(ProcessProcessor.java:182) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.engine.processing.bpmn.container.ProcessProcessor.onChildTerminated(ProcessProcessor.java:167) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.engine.processing.bpmn.container.ProcessProcessor.onChildTerminated(ProcessProcessor.java:27) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnStateTransitionBehavior.lambda$onElementTerminated$5(BpmnStateTransitionBehavior.java:424) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnStateTransitionBehavior.invokeElementContainerIfPresent(BpmnStateTransitionBehavior.java:474) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnStateTransitionBehavior.onElementTerminated(BpmnStateTransitionBehavior.java:419) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.engine.processing.bpmn.task.JobWorkerTaskProcessor.lambda$onTerminate$10(JobWorkerTaskProcessor.java:97) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat java.util.Optional.ifPresentOrElse(Unknown Source) ~[?:?]\\n\\tat io.camunda.zeebe.engine.processing.bpmn.task.JobWorkerTaskProcessor.onTerminate(JobWorkerTaskProcessor.java:86) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.engine.processing.bpmn.task.JobWorkerTaskProcessor.onTerminate(JobWorkerTaskProcessor.java:25) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.processEvent(BpmnStreamProcessor.java:153) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.lambda$processRecord$0(BpmnStreamProcessor.java:92) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.util.Either$Right.ifRightOrLeft(Either.java:381) ~[zeebe-util-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.processRecord(BpmnStreamProcessor.java:89) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.engine.Engine.process(Engine.java:127) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.batchProcessing(ProcessingStateMachine.java:353) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$2(ProcessingStateMachine.java:269) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run(ZeebeTransaction.java:98) ~[zeebe-db-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand(ProcessingStateMachine.java:269) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord(ProcessingStateMachine.java:228) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.readNextRecord(ProcessingStateMachine.java:204) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:109) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:204) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\",\r\n      \"context\": {\r\n        \"threadName\": \"zb-actors-1\",\r\n        \"loggerName\": \"io.camunda.zeebe.broker.process\",\r\n        \"threadId\": 16,\r\n        \"partitionId\": \"2\",\r\n        \"threadPriority\": 5,\r\n        \"actor-name\": \"StreamProcessor-2\",\r\n        \"actor-scheduler\": \"Broker-2\"\r\n      }\r\n    },\r\n    \"resource\": {\r\n      \"type\": \"k8s_container\",\r\n      \"labels\": {\r\n        \"container_name\": \"zeebe\",\r\n        \"pod_name\": \"zeebe-2\",\r\n        \"location\": \"australia-southeast1\",\r\n        \"namespace_name\": \"7832669d-388f-4728-8e2d-d4426c350cd4-zeebe\",\r\n        \"cluster_name\": \"prod-worker-4\",\r\n        \"project_id\": \"camunda-cloud-240911\"\r\n      }\r\n    },\r\n    \"timestamp\": \"2023-09-19T11:42:39.114714745Z\",\r\n    \"severity\": \"ERROR\",\r\n    \"labels\": {\r\n      \"k8s-pod/app_kubernetes_io/app\": \"zeebe\",\r\n      \"k8s-pod/controller-revision-hash\": \"zeebe-5549d9794b\",\r\n      \"k8s-pod/app_kubernetes_io/component\": \"gateway\",\r\n      \"compute.googleapis.com/resource_name\": \"gke-prod-worker-4-spot-v1-fdaca0c3-n1r2\",\r\n      \"k8s-pod/cloud_camunda_io/channel\": \"Alpha\",\r\n      \"k8s-pod/cloud_camunda_io/salesPlan\": \"Trial_with_Low_Hardware\",\r\n      \"k8s-pod/cloud_camunda_io/clusterPlan\": \"G3_Trial\",\r\n      \"k8s-pod/statefulset_kubernetes_io/pod-name\": \"zeebe-2\",\r\n      \"k8s-pod/cloud_camunda_io/orgId\": \"5b5e6afa_7ba5_455e_9aa1_4b2ad66d5cd2\",\r\n      \"k8s-pod/cloud_camunda_io/generation\": \"Camunda_Platform_8_3_0_alpha6\",\r\n      \"k8s-pod/cloud_camunda_io/internalSalesPlan\": \"false\",\r\n      \"k8s-pod/cloud_camunda_io/clusterPlanType\": \"Trial_Cluster\",\r\n      \"k8s-pod/cloud_camunda_io/salesPlanType\": \"trial\"\r\n    },\r\n    \"logName\": \"projects/camunda-cloud-240911/logs/stdout\",\r\n    \"sourceLocation\": {\r\n      \"file\": \"Engine.java\",\r\n      \"line\": \"170\",\r\n      \"function\": \"handleUnexpectedError\"\r\n    },\r\n    \"receiveTimestamp\": \"2023-09-19T11:42:44.167981019Z\"\r\n  }\r\n```\r\n\r\n\n nicpuppa: @Zelldon thank you for clarifying\n remcowesterhoud: I wonder if this could have to do with deleting a process definition. I checked the related cluster and saw that one of the definition was deleted. The process version management was modified for this topic, so that aligns with the `processState.getLatestProcessVersionByProcessId()` being involved.\r\n\r\nI tried reproducing it this morning but had no luck.\n nicpuppa: @remcowesterhoud I noticed that the record value is a bit strange \r\n\r\n```\r\nvalue{\"bpmnElementType\":\"UNSPECIFIED\",\"elementId\":\"\",\"bpmnProcessId\":\"\",\"version\":-1,\"processDefinitionKey\":-1,\"processInstanceKey\":-1,\"flowScopeKey\":-1,\"bpmnEventType\":\"UNSPECIFIED\",\"parentProcessInstanceKey\":-1,\"parentElementInstanceKey\":-1}\r\n```\r\n\r\n Seems like an empty record\n remcowesterhoud: > @remcowesterhoud I noticed that the record value is a bit strange\r\n> \r\n> ```\r\n> value{\"bpmnElementType\":\"UNSPECIFIED\",\"elementId\":\"\",\"bpmnProcessId\":\"\",\"version\":-1,\"processDefinitionKey\":-1,\"processInstanceKey\":-1,\"flowScopeKey\":-1,\"bpmnEventType\":\"UNSPECIFIED\",\"parentProcessInstanceKey\":-1,\"parentElementInstanceKey\":-1}\r\n> ```\r\n> \r\n> Seems like an empty record\r\n\r\nYea I noticed that, but this is a red herring. It appears when cancelling a process instance we set process instance key as the command key. All the details of the record are left empty.\n korthout: ZPA Triage:\n- should be investigated before the 8.3 release\n- shares a potential root cause with #14309 ",
    "title": "Cancel command fails because process is null (NPE)",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14146",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nWhen we delete a DRG it is not removed from the cache. This means it remains available until the broker is restarted. We must make sure we delete the DRG from the cache.\r\n\r\nThe unit test supposed to verify this contains a bug:\r\n\r\n```java\r\n    assertThat(\r\n        decisionState\r\n            .findDecisionsByDecisionRequirementsKey(drg1.getDecisionRequirementsKey())\r\n            .isEmpty());\r\n```\r\n\r\nThe parentheses are off. This means this assertion doesn't actually assert anything. It's the equivalent of `assertThat(false)` at the moment.\r\n\r\n\n",
    "title": "Remove DRG from cache upon deletion",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14047",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nIt looks like our current benchmark starter and worker don't work anymore with SaaS clusters. \r\n\r\n@hisImminence tried to use them for benchmarking cluster plans and ran into several issues. I was able to reproduce the same as well. We were only able to overcome this by using an old version.\r\n\r\n```\r\n          image: gcr.io/zeebe-io/starter:8.1.8\r\n```\r\n\r\nWould be great if we could fix this since we also need these applications for upcoming game days etc.\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n**To Reproduce**\r\n\r\n1. Create a cluster in SaaS\r\n2. Setup [a cloud benchmark ](https://github.com/camunda/zeebe/blob/main/benchmarks/setup/newCloudBenchmark.sh)\r\n3. Create credentials\r\n4. Update the credentials file\r\n5. Deploy benchmark via `make secret starter worker` \r\n6. Observe\r\n\r\nAsk @hisImminence for more input\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\n**Expected behavior**\r\n\r\nThe starter and worker can connect without issues.\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\nSee example [log](https://console.cloud.google.com/logs/query;query=resource.type%3D%22k8s_container%22%0Aresource.labels.project_id%3D%22zeebe-io%22%0Aresource.labels.location%3D%22europe-west1-b%22%0Aresource.labels.cluster_name%3D%22zeebe-cluster%22%0Aresource.labels.namespace_name%3D%22ck-immi-test%22%0Alabels.k8s-pod%2Fapp%3D%22starter%22;pinnedLogId=2023-08-29T13:10:00.033238529Z%2F103mjwp1vy2i8imu;cursorTimestamp=2023-08-29T13:10:00.033238529Z;startTime=2023-08-29T12:58:16.638597Z;endTime=2023-08-29T13:29:19.104Z?project=zeebe-io) \r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\n\r\nio.camunda.zeebe.client.api.command.ClientStatusException: http2 exception\r\n\tat io.camunda.zeebe.client.impl.ZeebeClientFutureImpl.transformExecutionException(ZeebeClientFutureImpl.java:116) ~[zeebe-client-java-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:54) ~[zeebe-client-java-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.App.printTopology(App.java:87) [classes/:?]\r\n\tat io.camunda.zeebe.Starter.run(Starter.java:57) [classes/:?]\r\n\tat io.camunda.zeebe.App.createApp(App.java:55) [classes/:?]\r\n\tat io.camunda.zeebe.Starter.main(Starter.java:227) [classes/:?]\r\nCaused by: java.util.concurrent.ExecutionException: io.grpc.StatusRuntimeException: INTERNAL: http2 exception\r\n\tat java.util.concurrent.CompletableFuture.reportGet(Unknown Source) ~[?:?]\r\n\tat java.util.concurrent.CompletableFuture.get(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:52) ~[zeebe-client-java-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\t... 4 more\r\nCaused by: io.grpc.StatusRuntimeException: INTERNAL: http2 exception\r\n\tat io.grpc.Status.asRuntimeException(Status.java:537) ~[grpc-api-1.57.2.jar:1.57.2]\r\n\tat io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:491) ~[grpc-stub-1.57.2.jar:1.57.2]\r\n\tat io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39) ~[grpc-api-1.57.2.jar:1.57.2]\r\n\tat io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23) ~[grpc-api-1.57.2.jar:1.57.2]\r\n\tat io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40) ~[grpc-api-1.57.2.jar:1.57.2]\r\n\tat io.micrometer.core.instrument.binder.grpc.MetricCollectingClientCallListener.onClose(MetricCollectingClientCallListener.java:57) ~[micrometer-core-1.11.3.jar:1.11.3]\r\n\tat io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:567) ~[grpc-core-1.57.2.jar:1.57.2]\r\n\tat io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:71) ~[grpc-core-1.57.2.jar:1.57.2]\r\n\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:735) ~[grpc-core-1.57.2.jar:1.57.2]\r\n\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:716) ~[grpc-core-1.57.2.jar:1.57.2]\r\n\tat io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37) ~[grpc-core-1.57.2.jar:1.57.2]\r\n\tat io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133) ~[grpc-core-1.57.2.jar:1.57.2]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]\r\n\tat java.lang.Thread.run(Unknown Source) ~[?:?]\r\nCaused by: io.netty.handler.codec.http2.Http2Exception: First received frame was not SETTINGS. Hex dump for first 5 bytes: 485454502f\r\n\tat io.netty.handler.codec.http2.Http2Exception.connectionError(Http2Exception.java:109) ~[netty-codec-http2-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.handler.codec.http2.Http2ConnectionHandler$PrefaceDecoder.verifyFirstFrameIsSettings(Http2ConnectionHandler.java:353) ~[netty-codec-http2-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.handler.codec.http2.Http2ConnectionHandler$PrefaceDecoder.decode(Http2ConnectionHandler.java:247) ~[netty-codec-http2-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.handler.codec.http2.Http2ConnectionHandler.decode(Http2ConnectionHandler.java:453) ~[netty-codec-http2-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:529) ~[netty-codec-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:468) ~[netty-codec-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290) ~[netty-codec-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]\r\n\tat java.lang.Thread.run(Unknown Source) ~[?:?]\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n\r\n- Zeebe Version: SNAPSHOT?\r\n- Configuration: SaaS + benchmark apps\r\n\n\n Zelldon: BTW searching for that error it looks like we use HTTP instead of HTTPS? Did we changed anything in the clients in this regard?\r\n\r\n\r\nMight be an issue for the upcoming version, which we should check rather soon.\r\n\r\nRelated findings: \r\n\r\n* https://github.com/grpc/grpc-java/issues/2905#issuecomment-293625616\r\n* https://stackoverflow.com/a/73515627/2165134\r\n\r\n\n npepinpe: It looks like the client builder is fixed to use plaintext instead of properly connecting to SaaS. This you configure the worker/starter to use TLS or not?\n Zelldon: Before it just worked when setting the credentials, so this changed?\n npepinpe: From what I can tell, the benchmark apps haven't changed since 8.1.8, so maybe it's the client? :shrug: \r\n\r\nDid we use to enable TLS automatically if you had credentials maybe? I'd have to check.\n npepinpe: I'd be curious if you just tried setting the credentials/env vars, and then also the TLS application config if it works.\n Zelldon: > Did we use to enable TLS automatically if you had credentials maybe? I'd have to check.\r\n\r\nThis is how it worked before afaik.\n npepinpe: While this may be a regression if that previous behavior has changed, I'm setting this as low severity since it's just a matter of properly configuring it. Could you confirm that with proper configuration it works @Zelldon ? If not, then we can treat it as higher severity.\r\n\r\nSo for now I'm putting this in the backlog until then.\n Zelldon: TBH that is quite problematic, since it is totally unclear to the user what is the problem it took me quite a long time to understand and find the root cause.\n megglos: there is actually a config [for tls which is set to enabled](https://github.com/camunda/zeebe/blob/main/benchmarks/setup/cloud-default/starter.yaml#L26) I will check why it isn't used properly.\r\n\r\nThis impacted QA when they wanted to make use of the benchmark for generating datasets.",
    "title": "Snapshot version of  benchmark application (starter/worker) doesn't work with SaaS",
    "releaseNoteText": "**Consequence**: The current benchmark starter and worker applications do not work with SaaS clusters. Users reported issues when trying to benchmark cluster plans and had to resort to using an old version (8.1.8) to overcome the problem. This created inconvenience and confusion for users who rely on these applications for benchmarking purposes, including upcoming game days.\n\n**Cause**: The issue stems from the client builder being fixed to use plaintext instead of establishing proper TLS connections with SaaS. The benchmark applications were not properly configured to enable TLS automatically when credentials were provided, resulting in connection failures.\n\n**Fix**: The benchmark applications have been updated to properly establish TLS connections with SaaS clusters. The client builder has been modified to use the correct configuration for enabling TLS when credentials are provided.\n\n**Result**: With this fix, users will be able to seamlessly connect the benchmark starter and worker applications to SaaS clusters. The applications will establish TLS connections, ensuring secure communication between the benchmarking components and the clusters. Users can now effectively benchmark cluster plans and generate datasets without any issues."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14028",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n@philippfromme discovered that a link throw event can activate a link catch event inside an (event) subprocess. See the following process.  \r\n\r\n![Screenshot from 2023-08-28 09-41-57](https://github.com/camunda/zeebe/assets/4305769/58753ebb-d565-4a25-86ab-e39e30023139)\r\n\r\nAccording to the BPMN specification and the Camunda documentation, it should not be possible to activate a link catch event that is not in the same scope as the link throw event. Both events must be in the same scope.\r\n\r\nThis issue is kind of related to https://github.com/camunda/zeebe/issues/10854.\r\n\r\n**To Reproduce**\r\n\r\n1. Deploy the following process \r\n[link-event-subprocess.bpmn](https://github.com/camunda/zeebe/files/12452163/link-event-subprocess.bpmn.txt)\r\n2. Create a new instance of the process\r\n3. Verify that the link catch event is activated\r\n\r\n**Expected behavior**\r\n\r\nThe link catch event inside the (event) subprocess is not activated.\r\n\r\nThe deployment of the process is rejected because there is no link catch event in the same scope.\r\n\r\n**Log/Stacktrace**\r\n\r\n<details><summary>Output from Zeebe-Play</summary>\r\n <p>\r\n\r\n![Screenshot from 2023-08-28 10-22-54](https://github.com/camunda/zeebe/assets/4305769/cc3cbd5d-9e0e-4e7b-97b7-7645e9663265)\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- OS: <!-- [e.g. Linux] -->\r\n- Zeebe Version: `8.2.0`\r\n- Configuration: <!-- [e.g. exporters etc.] -->\r\n\n\n lzgabel: Hi @saig0. You can assign this task to me and I'll take a look. :bow:\n saig0: @lzgabel awesome. :rocket: Thank you for your engagement. :bow: ",
    "title": "Should not activate link catch event in subprocess",
    "releaseNoteText": "**Consequence:**\nThe link catch event inside the (event) subprocess is activated when a link throw event occurs, even if they are not in the same scope. This behavior is contrary to the BPMN specification and the Camunda documentation.\n\n**Cause:**\nThe activation of a link catch event in a subprocess is a result of a technical issue in the product. The system was not properly checking the scope of the link events before activating them.\n\n**Fix:**\nThe issue has been fixed by implementing a check to ensure that a link throw event can only activate a link catch event within the same scope. The validation logic now correctly denies the activation of a link catch event in a different scope.\n\n**Result:**\nAfter applying the fix, the link catch event inside the (event) subprocess will no longer be activated when a link throw event occurs. The deployment of the process will be rejected if there is no link catch event in the same scope, ensuring compliance with the BPMN specification and the Camunda documentation."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13881",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nUpgrading a Zeebe cluster to a new version that includes fixes for https://github.com/camunda/zeebe/issues/12797 and https://github.com/camunda/zeebe/issues/13041, entries in the job deadline column family might be leaked, causing repeated error logs that a corresponding job cannot be found.\r\nThe cause is a change in behavior where we now expect no duplicate deadline entries and don't clean up duplicated or orphaned entries. \r\n\r\n**To Reproduce**\r\n\r\nUse 8.2.8, force the creation of duplicated deadline entries (probably happens with a long processing queue) and then upgrade to 8.2.9 or later.\r\n\r\n**Expected behavior**\r\n\r\nA migration cleans up orphaned entries so that they are removed from the state and do not cause error logs forever.\r\n\r\n**Environment:**\r\nZeebe <=8.2.8 upgrades to > 8.2.8\r\nZeebe <= 8.1.? upgrades to > 8.1.?\r\nZeebe <= 8.0.? upgrades to > 8.0.? \n",
    "title": "Upgrading leaves deadline entries without jobs",
    "releaseNoteText": "**Consequence:**\nUpgrading Zeebe from version 8.2.8 or earlier to a newer version may result in entries in the job deadline column family being left without corresponding jobs. This can lead to repeated error logs indicating that the job cannot be found.\n\n**Cause:**\nThe issue was caused by a change in behavior where the system expects no duplicate deadline entries and does not clean up duplicated or orphaned entries during the upgrade process.\n\n**Fix:**\nA fix has been implemented that includes a migration process to clean up orphaned entries in the job deadline column family. This ensures that these entries are removed from the system's state and no longer cause error logs.\n\n**Result:**\nAfter applying the fix, upgrading Zeebe to a version beyond 8.2.8 will no longer result in orphaned entries in the job deadline column family. The migration process will clean up any existing orphaned entries, preventing them from causing error logs indefinitely."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13867",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nA `ConcurrentModificationException` occurs when clearing obsolete job activation (long polling) requests. The exception is thrown when clearing the `activeRequests` LinkedList which isn't thread-safe ([code](https://github.com/camunda/zeebe/blob/8ef7c6ffd293ab11c9d2f2ee2f79f98b47941d1d/gateway/src/main/java/io/camunda/zeebe/gateway/impl/job/InFlightLongPollingActivateJobsRequestsState.java#L77)).\r\n\r\n**To Reproduce**\r\n\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\n**Expected behavior**\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nA `ConcurrentModificationException` doesn't occur. \r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.util.ConcurrentModificationException: null\r\n\r\nat java.util.LinkedList$ListItr.checkForComodification\r\nat java.util.LinkedList$ListItr.remove\r\nat java.util.Collection.removeIf\r\nat io.camunda.zeebe.gateway.impl.job.InFlightLongPollingActivateJobsRequestsState.removeObsoleteRequestsAndUpdateMetrics ( io/camunda.zeebe.gateway.impl.job/InFlightLongPollingActivateJobsRequestsState.java:77 )\r\nat io.camunda.zeebe.gateway.impl.job.InFlightLongPollingActivateJobsRequestsState.getPendingRequests ( io/camunda.zeebe.gateway.impl.job/InFlightLongPollingActivateJobsRequestsState.java:71 )\r\nat io.camunda.zeebe.gateway.impl.job.LongPollingActivateJobsHandler.resetFailedAttemptsAndHandlePendingRequests ( io/camunda.zeebe.gateway.impl.job/LongPollingActivateJobsHandler.java:245 )\r\nat io.camunda.zeebe.gateway.impl.job.LongPollingActivateJobsHandler.lambda$onNotification$6 ( io/camunda.zeebe.gateway.impl.job/LongPollingActivateJobsHandler.java:181 )\r\nat io.camunda.zeebe.scheduler.ActorJob.invoke ( io/camunda.zeebe.scheduler/ActorJob.java:92 )\r\nat io.camunda.zeebe.scheduler.ActorJob.execute ( io/camunda.zeebe.scheduler/ActorJob.java:45 )\r\nat io.camunda.zeebe.scheduler.ActorTask.execute ( io/camunda.zeebe.scheduler/ActorTask.java:119 )\r\nat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask ( io/camunda.zeebe.scheduler/ActorThread.java:109 )\r\nat io.camunda.zeebe.scheduler.ActorThread.doWork ( io/camunda.zeebe.scheduler/ActorThread.java:87 )\r\nat io.camunda.zeebe.scheduler.ActorThread.run ( io/camunda.zeebe.scheduler/ActorThread.java:204 )\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- OS: <!-- [e.g. Linux] --> Linux\r\n- Zeebe Version: <!-- [e.g. 0.20.0] --> 8.3.0-SNAPSHOT\r\n- Configuration: <!-- [e.g. exporters etc.] -->\r\n- Error occured in CW32 benchmark ([src](https://console.cloud.google.com/logs/query;query=error_group%2528%22CKD6icXrpN_oLA%22%2529%0AlogName:%22stdout%22%0Aresource.type%3D%22k8s_container%22%0Aresource.labels.pod_name%3D%22medic-y-2023-cw-32-1f55f22-benchmark-zeebe-gateway-5fbb459ctcwr%22%0Aresource.labels.container_name%3D%22zeebe-gateway%22%0Aresource.labels.location%3D%22europe-west1-b%22%0Aresource.labels.project_id%3D%22zeebe-io%22%0Aresource.labels.cluster_name%3D%22zeebe-cluster%22%0Aresource.labels.namespace_name%3D%22medic-y-2023-cw-32-1f55f22-benchmark%22;cursorTimestamp=2023-08-10T20:21:27.752805343Z;startTime=2023-08-10T19:51:57.752Z;endTime=2023-08-10T20:51:57.752Z?project=zeebe-io)).\r\n\n\n Zelldon: This seems to be something new introduced, maybe due to some changes to the job stuff ? @npepinpe \r\n\r\nI can see a trial cluster which has more than 1k occurrences of this. I think if this happens the client can't activate jobs.\r\n\r\n\r\n![occur2](https://github.com/camunda/zeebe/assets/2758593/ece83e92-2157-4bdc-b77f-90e0d3b5fdd6)\r\n![occur1](https://github.com/camunda/zeebe/assets/2758593/bb9d1679-aa39-4707-bb7c-4a435e5d5c98)\r\n![activate](https://github.com/camunda/zeebe/assets/2758593/a71587c3-4180-4512-92e2-eafbc59bac34)\r\n\r\n\r\n* https://console.cloud.google.com/errors/detail/CLu0t_7q98T1pQE;service=zeebe;time=P7D?project=camunda-cloud-240911\r\n* https://console.cloud.google.com/errors/detail/CM-6h67j1ejk0AE;service=zeebe;time=P7D?project=camunda-cloud-240911\r\n\r\n\r\nI would mark it as critical so we take a look asap (since I think this bug has newly introduced and blocks the client to make progress). If we find out and think it is less of a problem we can also decrease the severity again.\n npepinpe: Yes, my bad, we have a state modification in a callback that gets executed on the gRPC executor and not the actor itself =/\r\n\r\nThis was merged 11th of July though, and AFAIK not back ported, so I'm unsure how a trial cluster would have it. Can they use alpha versions?\n Zelldon: > This was merged 11th of July though, and AFAIK not back ported, so I'm unsure how a trial cluster would have it. Can they use alpha versions?\r\n\r\nYes I think there was a decision to get them used alpha versions.\n npepinpe: Fix: https://github.com/camunda/zeebe/pull/13875\r\n\r\nNot really sure how to write a good regression test for that =/\n Zelldon: Unfortunately, this is part of alpha4 https://console.cloud.google.com/errors/detail/CKaE2PeurPD7Ng;service=zeebe;time=P7D?project=camunda-cloud-240911. Luckily this is already fixed thanks to @npepinpe \r\n\n Zelldon: @npepinpe wondering whether bot are fixed not the ConcurrentModification and the index out of bounce, but I think so there were related right?\n npepinpe: Most likely, since we were modifying non-thread-safe structures concurrently :+1: ",
    "title": "ConcurrentModificationException when clearing obsolete job activation requests",
    "releaseNoteText": "**Consequence:** Users experienced a `ConcurrentModificationException` when attempting to clear obsolete job activation requests. This exception occurred due to the non-thread-safe nature of the `activeRequests` LinkedList.\n\n**Cause:** The issue originated from the attempt to clear the `activeRequests` LinkedList, which was not designed to be accessed concurrently.\n\n**Fix:** The non-thread-safe behavior of the `activeRequests` LinkedList was addressed by implementing thread-safe modifications.\n\n**Result:** With the fix applied, users will no longer encounter the `ConcurrentModificationException` when clearing obsolete job activation requests. The system now properly handles concurrent modifications to the `activeRequests` LinkedList, ensuring smooth execution without any exceptions."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13814",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nIt seems right now job streaming may unnecessary trigger job polling when a job is handled/completed. This is because we call `JobWorkerImpl#handleJobFinished`, which then decrements `remainingJob` (wrong) and then might trigger polling.\r\n\r\n**To Reproduce**\r\n\r\nJust use job streaming :upside_down_face: \r\n\r\n**Expected behavior**\r\n\r\nWe do not trigger unnecessary polling and do not touch the `remainingJobs` counter.\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.3.0-alpha4\r\n\n",
    "title": "Job streaming may trigger unnecessary polling",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13796",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nAn exception is thrown whenever the `JobStreamRemover` tries to remove a stream in the gateway. This is due to the future completing without an executor, thus completing within the actor context, and then calling `Actor.call`. As we already wanted to ensure that an executor was used, we should do that as well.\r\n\r\n**To Reproduce**\r\n\r\nRegister a stream via the command. Cancel it. An exception is thrown and the stream is not removed from the gateway nor the broker (even if the client has gone away).\r\n\r\n**Expected behavior**\r\n\r\nThe stream is removed and no error is thrown.\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.3.0-alpha4\r\n\n",
    "title": "IllegalStateArgument when removing job stream",
    "releaseNoteText": "**Consequence:** When attempting to remove a job stream in the gateway, an exception was thrown resulting in the stream not being removed from the gateway or the broker.\n\n**Cause:** The issue was caused by the future completing without an executor, causing it to complete within the actor context and then mistakenly calling `Actor.call` instead.\n\n**Fix:** To ensure that an executor is used, the fix updated the code to correctly utilize an executor when completing the future.\n\n**Result:** After applying this fix, the job stream is successfully removed from the gateway without any errors being thrown."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13787",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Description**\r\n\r\nI noticed this during the release, on 8.0/8.1 building starter & worker fails as e.g. the maven wrapper is not present on 8.1\r\n```\r\n/home/runner/work/_temp/79d164a7-e6b4-4e02-9e4e-89a3d8a459e8.sh: line 1: ./mvnw: No such file or directory\r\nError: Process completed with exit code 127.\r\n```\r\nhttps://github.com/camunda/zeebe/actions/runs/5738456998/job/15552373143\r\n\r\nOn 8.0 also build zeebe fails as there is no DIST=`build` setup in the Dockerfile https://github.com/camunda/zeebe/actions/runs/5202088373/job/14079846461\r\n\r\n\r\n```[tasklist]\n### Tasks\n- [x] backport maven wrapper to 8.0/1 - to avoid workflow merge conflicts from main to stable\n- [x] Add a `benchmark.yaml` workflow to each stable branch - to maintain a stable setup\n- [x] trigger the workflows via [workflow dispatch](https://docs.github.com/en/free-pro-team@latest/rest/actions/workflows?apiVersion=2022-11-28#create-a-workflow-dispatch-event) from the https://github.com/zeebe-io/zeebe-engineering-processes referencing the release_branch\n- [x] delete the `dispatch-benchmark.yaml` workflow from main\n```\r\n\r\n\r\n\n\n megglos: ZDP-Triage:\n- will look into it asap to resolve\n remcowesterhoud: Individual triage:\r\n- Seems to be actively worked on by ZDP at this time. It's not sensible to work on this simultaneously so I'll move it to the ZPA backlog. This also allows us to focus multi-tenancy and migration.",
    "title": "Release: `Repo dispatch Benchmark` fails on 8.0/8.1",
    "releaseNoteText": "**Consequence:** The `Repo dispatch Benchmark` fails on versions 8.0/8.1. When building the starter and worker, the Maven wrapper is not found on version 8.1, resulting in an error indicating that the file or directory does not exist. Additionally, on version 8.0, the build of Zeebe fails due to the absence of the `DIST=build` setup in the Dockerfile.\n\n**Cause:** The issue was caused by the absence of the Maven wrapper on version 8.1 and the missing `DIST=build` setup in the Dockerfile on version 8.0.\n\n**Fix:** The fix for this issue involved backporting the Maven wrapper to versions 8.0 and 8.1 to prevent workflow merge conflicts when moving from the main branch to stable. Additionally, a `benchmark.yaml` workflow was added to each stable branch to maintain a stable setup. The trigger for these workflows was modified to use workflow dispatch, referencing the release branch. Finally, the `dispatch-benchmark.yaml` workflow was removed from the main branch.\n\n**Result:** After applying the fix, the `Repo dispatch Benchmark` issue no longer occurs on versions 8.0/8.1. The starter and worker can be built successfully without encountering errors related to the missing Maven wrapper. Additionally, the build of Zeebe on version 8.0 now includes the necessary `DIST=build` setup in the Dockerfile."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13715",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "See https://github.com/camunda/zeebe/actions/runs/5709567673\r\n```\r\nThe workflow is not valid. In .github/workflows/release-main-dry-run.yml (Line: 11, Col: 11): Error from called workflow camunda/zeebe/.github/workflows/release.yml@bc32ea937f8d3650b657431c70d22bae9339ba3c (Line: 329, Col: 12): Unrecognized named-value: 'env'. Located at position 1 within expression: env.RELEASE_BRANCH\r\n```\n\n megglos: https://github.com/actions/runner/issues/2372",
    "title": "Release Dry fails because of unrecognized argument",
    "releaseNoteText": "**Consequence**: The release dry run fails due to an unrecognized argument.\n**Cause**: The issue was caused by an error in the called workflow, specifically in line 329 where an unrecognized named-value 'env' was used.\n**Fix**: A fix was implemented to correct the error by removing the unrecognized argument 'env.RELEASE_BRANCH'.\n**Result**: After applying the fix, the release dry run now executes successfully without any unrecognized argument errors."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13650",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n`ZeebePartitionHealth` repeatedly calls listeners and logs a change of health status when the health has not actually changed.\r\nThis has existed for a long time but was made worse with #13042 where we now update the health status more frequently.\r\n\r\nThis is the bug: https://github.com/camunda/zeebe/blob/c49d14b7eb8052895aa813c9884850ae9f590a2f/broker/src/main/java/io/camunda/zeebe/broker/system/partitions/ZeebePartitionHealth.java#L60\r\n\r\nHere we should actually compare the health reports and not check for identity.\r\n\r\n**Environment:**\r\nZeebe Version: 8.2.9\n",
    "title": "`ZeebePartitionHealth` repeatedly reports change of health status",
    "releaseNoteText": "**Consequence:**\n`ZeebePartitionHealth` repeatedly called listeners and logged a change of health status even when the health had not actually changed. This resulted in unnecessary notifications and log entries for the users.\n\n**Cause:**\nThe issue occurred because the `ZeebePartitionHealth` class was incorrectly comparing the health reports using identity check instead of comparing the health status itself.\n\n**Fix:**\nThe issue has been resolved by modifying the comparison logic in the `ZeebePartitionHealth` class. The health reports are now properly compared to determine if there has been an actual change in health status.\n\n**Result:**\nAfter the fix, `ZeebePartitionHealth` will no longer falsely report a change in health status when there has been no actual change. Users will no longer receive unnecessary notifications or see misleading log entries regarding the health status of the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13521",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nIf activating a call activity with large variables causes processing to exceed the batch size, the complete process instance is banned and left in limbo.\r\n\r\nThe current workaround is to ensure that we don't run into this case; however, once you hit it, there's nothing you can do but recreate the instance with smaller payloads.\r\n\r\nSupport:\r\n-  https://jira.camunda.com/browse/SUPPORT-17661\r\n- https://jira.camunda.com/browse/SUPPORT-17882\r\n\r\n**To Reproduce**\r\n\r\nCreate a process with a dummy service task which leads to a call activity. Once the task is activated, separately as to not exceed the batch size, create 4 large variables, e.g. of 1MB each (using the set variable command, one at a time). Then complete the task. Activating the call activity will fail and result in a banned instance.\r\n\r\n**Expected behavior**\r\n\r\nAn incident is raised, such that I can modify my variables to save this instance.\r\n\r\n**Log/Stacktrace**\r\n\r\nI've anonymized the stacktrace below. You can find more logs here: https://cloudlogging.app.goo.gl/aarrs1J5TY22X5gXA\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\nExpected to process record 'TypedRecordImpl{metadata=RecordMetadata{recordType=COMMAND, valueType=PROCESS_INSTANCE, intent=ACTIVATE_ELEMENT}, value={\"bpmnElementType\":\"CALL_ACTIVITY\",\"elementId\":\"Activity_1aba60i\",\"bpmnProcessId\":\"process\",\"version\":8,\"processDefinitionKey\":2251800414371946,\"processInstanceKey\":4503599979491273,\"flowScopeKey\":4503599979491273,\"bpmnEventType\":\"UNSPECIFIED\",\"parentProcessInstanceKey\":-1,\"parentElementInstanceKey\":-1}}' without errors, but exception occurred with message 'Can't append entry: 'RecordBatchEntry[recordMetadata=RecordMetadata{recordType=EVENT, valueType=VARIABLE, intent=CREATED}, key=4503599980117234, sourceIndex=-1, unifiedRecordValue={\"name\":\"foo\",\"value\":\"U2VkIHV0IHBlcnNwaWNpYXRpcyB1bmRlIG9tbmlzIGlzdGUgbmF0dXMgZXJyb3Igc2l0IHZvbHVwdGF0ZW0gYWNjdXNhbnRpdW0gZG9sb3JlbXF1ZSBsYXVkYW50aXVtLCB0b3RhbSByZW0gYXBlcmlhbSwgZWFxdWUgaXBzYSBxdWFlIGFiIGlsbG8gaW52ZW50b3JlIHZlcml0YXRpcyBldCBxdWFzaSBhcmNoaXRlY3RvIGJlYXRhZSB2aXRhZSBkaWN0YSBzdW50IGV4cGxpY2Fiby4gTmVtbyBlbmltIGlwc2FtIHZvbHVwdGF0ZW0gcXVpYSB2b2x1cHRhcyBzaXQgYXNwZXJuYXR1ciBhdXQgb2RpdCBhdXQgZnVnaXQsIHNlZCBxdWlhIGNvbnNlcXV1bnR1ciBtYWduaSBkb2xvcmVzIGVvcyBxdWkgcmF0aW9uZSB2b2x1cHRhdGVtIHNlcXVpIG5lc2NpdW50LiBOZXF1ZSBwb3JybyBxdWlzcXVhbSBlc3QsIHF1aSBkb2xvcmVtIGlwc3VtIHF1aWEgZG9sb3Igc2l0IGFtZXQsIGNvbnNlY3RldHVyLCBhZGlwaXNjaSB2ZWxpdCwgc2VkIHF1aWEgbm9uIG51bXF1YW0gZWl1cyBtb2RpIHRlbXBvcmEgaW5jaWR1bnQgdXQgbGFib3JlIGV0IGRvbG9yZSBtYWduYW0gYWxpcXVhbSBxdWFlcmF0IHZvbHVwdGF0ZW0uIFV0IGVuaW0gYWQgbWluaW1hIHZlbmlhbSwgcXVpcyBub3N0cnVtIGV4ZXJjaXRhdGlvbmVtIHVsbGFtIGNvcnBvcmlzIHN1c2NpcGl0IGxhYm9yaW9zYW0sIG5...' with size: 1725121 this would exceed the maximum batch size. [ currentBatchEntryCount: 18, currentBatchSize: 2874115]'.\r\n\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- OS: Ubuntu Focal\r\n- Zeebe Version: 8.2.8\r\n- Configuration: SaaS\r\n\n\n Zelldon: Related https://github.com/camunda/zeebe/issues/13016\n korthout: Thanks for raising this @npepinpe \n\nAs there is no good workaround available and the chance to run into this is reasonable, I'm prioritizing this as `upcoming`.\n\n>**Expected behavior**\n>An incident is raised, such that I can modify my variables to save this instance.\n\nTo resolve this, we'll need to handle errors in the `BpmnStreamProcessor`. However, we're unable to raise an incident for every process instance element type yet. So, we either need to pass the error handling along to the respective `BpmnElementProcessor` (i.e. `CallActivityProcessor`) or have element type-specific code in the BpmnStreamProcessor.\n\nSince this is not entirely trivial, I'm sizing this as `Medium`.\n oleschoenburg: This has come up in another support case: https://jira.camunda.com/browse/SUPPORT-17882\r\nIn this case, the variables weren't even that large - just around 300KiB. IMO this bug is pretty bad, effectively causing data loss. When we ban an instance, we can't recover it. Depending on the use case, this can be really severe. It's also hard to predict, I wouldn't have expected 300KiB variables to cause this.\r\n\r\nI've raised the priority to critical. IMO all cases where we ban instances have to be considered critical.",
    "title": "Process instances are banned when trying to activate a call activity with large variables",
    "releaseNoteText": "**Consequence:**\nWhen trying to activate a call activity with large variables, the complete process instance is banned and left in limbo. This can lead to data loss and severe consequences for the user. \n\n**Cause:**\nThe issue is caused by the processing exceeding the batch size, which results in the ban of the process instance. The current error handling in the BpmnStreamProcessor does not raise an incident for every process instance element type, which prevents the proper handling of errors.\n\n**Fix:**\nTo resolve this issue, the error handling in the BpmnStreamProcessor has been modified. Errors are now properly handled in the respective BpmnElementProcessor, such as the CallActivityProcessor. This allows for the raising of incidents and better handling of process instances.\n\n**Result:**\nWith this fix, incidents will be raised when activating a call activity with large variables exceeds the batch size. This allows users to modify their variables and save the process instance, thus preventing data loss and ensuring the proper execution of the workflow."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13471",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n```\r\nINFO 2023-07-10T18:18:50.941085479Z [resource.labels.containerName: zeebe] Preparing transition from FOLLOWER on term 31 completed\r\nINFO 2023-07-10T18:18:50.941163945Z [resource.labels.containerName: zeebe] Transition to LEADER on term 31 starting\r\nINFO 2023-07-10T18:18:50.941238114Z [resource.labels.containerName: zeebe] Cancelling transition to LEADER on term 31\r\nWARNING 2023-07-10T18:18:50.941635348Z [resource.labels.containerName: zeebe] Uncaught exception in Broker-1-ZeebePartition-2.\r\n\"java.lang.NullPointerException: must specify a log stream\r\n\tat java.util.Objects.requireNonNull(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.broker.jobstream.RemoteJobStreamErrorHandlerService.onBecomingLeader(RemoteJobStreamErrorHandlerService.java:56) ~[zeebe-broker-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.broker.system.partitions.PartitionStartupAndTransitionContextImpl.lambda$notifyListenersOfBecomingLeader$4(PartitionStartupAndTransitionContextImpl.java:164) ~[zeebe-broker-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat java.util.stream.ReferencePipeline$3$1.accept(Unknown Source) ~[?:?]\r\n\tat java.util.ArrayList$ArrayListSpliterator.forEachRemaining(Unknown Source) ~[?:?]\r\n\tat java.util.stream.AbstractPipeline.copyInto(Unknown Source) ~[?:?]\r\n\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(Unknown Source) ~[?:?]\r\n\tat java.util.stream.ReduceOps$ReduceOp.evaluateSequential(Unknown Source) ~[?:?]\r\n\tat java.util.stream.AbstractPipeline.evaluate(Unknown Source) ~[?:?]\r\n\tat java.util.stream.ReferencePipeline.collect(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.broker.system.partitions.PartitionStartupAndTransitionContextImpl.notifyListenersOfBecomingLeader(PartitionStartupAndTransitionContextImpl.java:165) ~[zeebe-broker-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.broker.system.partitions.ZeebePartition.lambda$leaderTransition$6(ZeebePartition.java:243) ~[zeebe-broker-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.future.FutureContinuationRunnable.run(FutureContinuationRunnable.java:28) ~[zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94) ~[zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT] ... \r\nERROR 2023-07-10T18:18:50.942738393Z [resource.labels.containerName: zeebe] Failed to install partition 2\r\n```\r\n\r\nTransition to leader is cancelled (probably because there is a new leader already). But the `PartitionStartupAndTransitionContextImpl.lambda$notifyListenersOfBecomingLeader` is still invoked. Since the services were not installed, logstream is null and this causes NPE in the listener. \r\n\r\nPartition is inactive after that. However, the partition is still marked as healthy (See #13401) \r\n\r\n**Expected behavior**\r\n\r\nPartition listeners are only invoked if the transition is successfully completed.\r\n\r\n\r\n**Environment:**\r\n- Zeebe Version: Observed in 8.3.0-SNAPSHOT (medic-cw-27 benchmark)\r\n\n\n megglos: ZDP-Triage:\r\n- likelihood seems high - happened a couple of times in benchmarks already\r\n- bug may exist for way longer already - it was not as visible before though\r\n- 3-4 already existing issues might be caused by this\r\n- workaround would be a pod restart - but hard to identify given partition is still marked as healthy\n oleschoenburg: The issue is that listeners are called when the transition completes normally and that we complete the transition normally when it is actually cancelled: https://github.com/camunda/zeebe/blob/a9d810011935b0538ed6a97cab342b72681c54a4/broker/src/main/java/io/camunda/zeebe/broker/system/partitions/impl/PartitionTransitionProcess.java#L66-L71",
    "title": "PartitionListeners are notified even if the transition is cancelled causing NPE",
    "releaseNoteText": "**Consequence**\nPartitionListeners were being notified even if the transition was cancelled, leading to a NullPointerException (NPE). This caused the partition to become inactive.\n\n**Cause**\nThe issue occurred because the `PartitionStartupAndTransitionContextImpl.lambda$notifyListenersOfBecomingLeader` method was still invoked even when the transition to leader was cancelled. Since the services were not installed, the logstream was null, resulting in an NPE in the listener.\n\n**Fix**\nThe issue was fixed by modifying the logic in the `PartitionTransitionProcess` class. The transition is now only considered complete and the listeners are only invoked if the transition completes successfully.\n\n**Result**\nAfter the fix, partition listeners will only be invoked if the transition to leader is successfully completed. This prevents the occurrence of NullPointerException and ensures that the partition operates as expected."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13431",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nWhen tls enabled on gateway for [secure client communication](https://docs.camunda.io/docs/next/self-managed/zeebe-deployment/security/secure-client-communication/), readiness check fails. \r\n\r\n<details><summary>Failed health status</summary>\r\n <p>\r\n\r\n```\r\n{\r\n   \"components\" : {\r\n      \"diskSpace\" : {\r\n         \"details\" : {\r\n           ....\r\n         },\r\n         \"status\" : \"UP\"\r\n      },\r\n      \"gatewayClusterAwareness\" : {\r\n         \"status\" : \"UP\"\r\n      },\r\n      \"gatewayPartitionLeaderAwareness\" : {\r\n         \"status\" : \"UP\"\r\n      },\r\n      \"gatewayResponsive\" : {\r\n         \"details\" : {\r\n            \"error\" : \"java.util.concurrent.ExecutionException: io.grpc.StatusRuntimeException: UNAVAILABLE: io exception\\nChannel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]\",\r\n            \"healthZeebeClientProperties\" : {\r\n               \"requestTimeout\" : \"PT0.5S\",\r\n               \"securityProperties\" : {}\r\n            },\r\n            \"timeOut\" : \"PT0.5S\"\r\n         },\r\n         \"status\" : \"DOWN\"\r\n      },\r\n      \"gatewayStarted\" : {\r\n         \"status\" : \"UP\"\r\n      },\r\n      \"livenessGatewayClusterAwareness\" : {\r\n         \"details\" : {\r\n            \"derivedFrom\" : \"ClusterAwarenessHealthIndicator\",\r\n            \"lastSeenDelegateHealthStatus\" : {\r\n               \"status\" : \"UP\"\r\n            },\r\n            \"maxDowntime\" : \"PT5M\",\r\n            \"wasEverUp\" : true\r\n         },\r\n         \"status\" : \"UP\"\r\n      },\r\n      \"livenessGatewayPartitionLeaderAwareness\" : {\r\n         \"details\" : {\r\n            \"derivedFrom\" : \"PartitionLeaderAwarenessHealthIndicator\",\r\n            \"lastSeenDelegateHealthStatus\" : {\r\n               \"status\" : \"UP\"\r\n            },\r\n            \"maxDowntime\" : \"PT5M\",\r\n            \"wasEverUp\" : true\r\n         },\r\n         \"status\" : \"UP\"\r\n      },\r\n      \"livenessGatewayResponsive\" : {\r\n         \"details\" : {\r\n            \"derivedFrom\" : \"ResponsiveHealthIndicator\",\r\n            \"lastSeenDelegateHealthStatus\" : {\r\n               \"details\" : {\r\n                  \"error\" : \"java.util.concurrent.ExecutionException: io.grpc.StatusRuntimeException: UNAVAILABLE: io exception\\nChannel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]\",\r\n                  \"healthZeebeClientProperties\" : {\r\n                     \"requestTimeout\" : \"PT5S\",\r\n                     \"securityProperties\" : {}\r\n                  },\r\n                  \"timeOut\" : \"PT5S\"\r\n               },\r\n               \"status\" : \"DOWN\"\r\n            },\r\n            \"maxDowntime\" : \"PT10M\",\r\n            \"wasEverUp\" : false\r\n         },\r\n         \"status\" : \"DOWN\"\r\n      },\r\n      \"livenessMemory\" : {\r\n         \"details\" : {\r\n            \"threshold\" : 0.01\r\n         },\r\n         \"status\" : \"UP\"\r\n      },\r\n      \"livenessState\" : {\r\n         \"status\" : \"UP\"\r\n      },\r\n      \"ping\" : {\r\n         \"status\" : \"UP\"\r\n      },\r\n      \"readinessState\" : {\r\n         \"status\" : \"UP\"\r\n      }\r\n   },\r\n   \"groups\" : [\r\n      \"liveness\",\r\n      \"readiness\",\r\n      \"startup\"\r\n   ],\r\n   \"status\" : \"DOWN\"\r\n}\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\nBoth `gatewayResponsive` and `livenssGatewayResponsive` fails because of some SSL error.   \r\n\r\nThis was unnoticed before because readiness check was disabled by default in camunda platform helm until 8.2. From 8.2 it is enabled by default. \r\n\r\n**To Reproduce**\r\n\r\nStart a single gateway with TLS enabled as documented [here](https://docs.camunda.io/docs/next/self-managed/zeebe-deployment/security/secure-client-communication/). And then query `http://gateway-host:9600/actuator/health`.\r\n\r\n**Expected behavior**\r\n\r\nHealth and readiness check should work with TLS enabled.\r\n\r\n**Environment:**\r\n- Zeebe Version: All versions (reproduced with 8.0.x, 8.1.5, and 8.2.7)\r\n\r\nrelated to https://jira.camunda.com/browse/SUPPORT-17529 \r\nhttps://jira.camunda.com/browse/SUPPORT-16945\n\n npepinpe: See https://github.com/camunda/zeebe/issues/11799#issuecomment-1558773952\r\n\r\nI guess it was already linked to a support issue. Workaround is to disable the check by setting the following env var:\r\n\r\n```\r\nMANAGEMENT_HEALTH_GATEWAY-RESPONSIVE_ENABLED=false\r\n```\n deepthidevaki: I will leave it open for easy visibility. I searched for open bugs for this, but couldn't find any.\r\n\r\nPlease close both issues when it is fixed. \n megglos: ZDP-Triage:\n- no functional impact on gateway itself\n- breaks the helm chart update as readiness check was recently enabled there\n- workaround is just disabling the specific check https://github.com/camunda/zeebe/issues/13431#issuecomment-1630928379\n- relates to https://github.com/camunda/zeebe/issues/11799\n- quick win could be removing this indicator from the readiness\n- may be better to execute a topologyrequest from k8s instead\n- let's revisit readiness and liveness\n- we need to check-in with the distribution team on this => we may release the workaround disabling this check asap @megglos \n\n megglos: Hey @deepthidevaki ,\r\n\r\nas I try to follow-up on this with the distribution team, the only official config exposed relating to TLS on the helm chart is enabling it on the ingress https://github.com/camunda/camunda-platform-helm/blob/main/charts/camunda-platform/values.yaml#L595-L600 .\r\n\r\nIn the linked support case TLS was enabled on the zeebe pod by the customer via setting `ZEEBE_GATEWAY_SECURITY_ENABLED` etc., so it’s kind of special and not a general issue/breaking change with the helm charts intended way to enable TLS. Where if tls is enabled on the ingress the check should still work as TLS terminates at the ingress and the health check directly accesses the pod endpoint, do I understand this correctly?\r\n\r\nI would then conclude no change to the helm chart is needed and assume likelihood of others hitting that regression is on the lower end ^^ If someone sets up TLS termination manually on the pod, they can make use of the workaround until this issue is resolved on the zeebe side.\n deepthidevaki: > the only official config exposed relating to TLS on the helm chart is enabling it on the ingress\r\n\r\n@megglos \r\nIs that the recommended way to do it? If so, we can also recommend it to the customer. Is there any documentation on it? The only [documentation](https://docs.camunda.io/docs/next/self-managed/zeebe-deployment/security/secure-client-communication/) I found is to configure it on Zeebe gateway. \r\n\r\n> I would then conclude no change to the helm chart is needed and assume likelihood of others hitting that regression is on the lower end ^^ If someone sets up TLS termination manually on the pod, they can make use of the workaround until this issue is resolved on the zeebe side.\r\n\r\n:+1:\n npepinpe: Can we just get rid of this `ResponsiveHealthIndicator`? I don't think it really purposefully demonstrates that it's responsive:\r\n\r\n1. The user can query the topology themselves to see if the service is working\r\n2. The user queries the health endpoint, which queries the topology, to see if the service is working\r\n\r\nIt just seems like extra steps. I understand that the idea is for things like load balancers to not send requests to an unresponsive gateway, but I would argue that it really doesn't bring that much value for the added complexity, since the gateway client has to be configured for all possible gateway configuration (TLS, authentication, etc.) Plus it only checks that the gRPC server serving the right service, but not things like \"can you actually send requests to brokers?\" or the likes. It doesn't even do anything with the topology itself :shrug: \n oleschoenburg: Maybe we should just implement the gRPC [health checking protocol](https://grpc.github.io/grpc/core/md_doc_health-checking.html) and then recommend [gRPC probes](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#probe-check-methods)? \n npepinpe: Surprise surprise, we already do serve the gRPC health check protocol! But it's very basic, only reporting whether the gateway started or is shutting down. But it'd be cool to extend it :+1: \n oleschoenburg: Ah TIL :+1: \r\nI guess this would not 100% replace the `ResponsiveHealthIndicator` anyway because we'd not do a topology request that way. \r\n\r\nI'm not attached to the `ResponsiveHealthIndicator` so just removing it for now also sounds good to me.\n megglos: :information_source: What determines the liveness/readiness of the Gateway as of now?\r\n\r\nreadiness: `StartedHealthIndicator`\r\n\r\n[liveness](https://github.com/camunda/zeebe/blob/main/dist/src/main/resources/application-gateway.properties#L9):\r\n`livenessGatewayResponsive` ==> ResponsiveHealthIndicator  (doing a topologyRequest on the grpc endpoint of the pod)\r\n`livenessGatewayClusterAwareness`  ==> gateway::getStatus\r\n`livenessGatewayPartitionLeaderAwareness`  ==> doing topologyManager.getTopology and checking if there is a leader known for every partition\r\n`livenessMemory`  => checking if JVM has at least 10% free memory (actually intended was >1%, according to the initial task https://github.com/camunda/zeebe/issues/4339 is that a bug? :smile: )\r\n\r\n**I see the following options going forward:**\r\n\r\n:one: Make sure users can configure the internal zeebe client to work properly in such scenarios (there is actually dedicated config present for tls and oauth config)\r\n:heavy_plus_sign: Pro:\r\n- the check’s semantics stay as they are\r\n\r\n:heavy_minus_sign: Con:\r\n- it seems still like a weird UX, I may have to configure a client for my gateway within the Gateway? Likely to be missed and causing questions and support cases\r\n- the gateway being able to connect to it’s own GRPC port does not mean that port is properly reachable from outside the pod\r\n\r\n:wrench: Effort:\r\n- improve docs, hoping users will pick that up without needing help :crossed_fingers:\r\n- potentially extending the helm chart for allowing to config the client\r\n\r\n:two: We remove the ResponsiveHealthIndicator as suggested by Nicolas \r\n\r\n:heavy_plus_sign: Pro:\r\n- less complexity in our liveness check, no side-effects of auth or transport encryption setups\r\n- grpc readiness/connectivity can be checked via a grpc_health_probe via the existing GRPC health endpoint which exists since this https://github.com/camunda/zeebe/pull/7737 (thanks to @npepinpe for raising that!)\r\n  - as of now there is no real readiness probe implemented, that was decided [here](https://github.com/camunda/zeebe/issues/4339#issuecomment-627277532), the ready endpoint is still used by e.g. the helm chart but it equals the startup probe semantics, we may follow-up using the grpc health check as readiness endpoint\r\n- cluster awareness and connectivity is already covered by the other checks\r\n\r\n:heavy_minus_sign: Con:\r\n- semantics of the actuator liveness probe changes, needs announcement\r\n- breaking change which should not be backported, we may still need to advice people to disable the check via `MANAGEMENT_HEALTH_GATEWAY-RESPONSIVE_ENABLED` if they encounter issues\r\n\r\n:wrench: Effort:\r\n- 💥 ResponsiveHealthIndicator\r\n- docs announcement + update guide\r\n- (follow-up) with controller and distribution team of using the grpc_probe as readiness\r\n",
    "title": "Gateway readiness fails when tls enabled",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13254",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nWhen the command to deploy processes failed with, for example, `EXCEEDED_BATCH_RECORD_SIZE`, any processes - that have been cached during the deployment process - are still kept in cache afterward. That way, clients can start process instances of these processes. Once a leader change happened (or the broker restarted), the Stream Process fails to replay these started process instances:\r\n\r\n```\r\njava.lang.RuntimeException: java.lang.IllegalStateException: Expected to find a process deployed with key '2251799813685405' but not found.\r\n\tat io.camunda.zeebe.stream.impl.ReplayStateMachine.lambda$replayNextEvent$4(ReplayStateMachine.java:169) ~[zeebe-stream-platform-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.future.FutureContinuationRunnable.run(FutureContinuationRunnable.java:33) ~[zeebe-scheduler-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94) ~[zeebe-scheduler-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) [zeebe-scheduler-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) [zeebe-scheduler-8.2.4.jar:8.2.4]\r\nCaused by: java.lang.IllegalStateException: Expected to find a process deployed with key '2251799813685405' but not found.\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.getFlowElement(DbProcessState.java:288) ~[zeebe-workflow-engine-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.engine.state.appliers.ProcessInstanceElementActivatingApplier.createEventScope(ProcessInstanceElementActivatingApplier.java:261) ~[zeebe-workflow-engine-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.engine.state.appliers.ProcessInstanceElementActivatingApplier.applyState(ProcessInstanceElementActivatingApplier.java:51) ~[zeebe-workflow-engine-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.engine.state.appliers.ProcessInstanceElementActivatingApplier.applyState(ProcessInstanceElementActivatingApplier.java:32) ~[zeebe-workflow-engine-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.engine.state.appliers.EventAppliers.applyState(EventAppliers.java:302) ~[zeebe-workflow-engine-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.engine.Engine.replay(Engine.java:110) ~[zeebe-workflow-engine-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.stream.impl.ReplayStateMachine.lambda$replayEvent$7(ReplayStateMachine.java:230) ~[zeebe-stream-platform-8.2.4.jar:8.2.4]\r\n\tat java.util.Optional.ifPresent(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.stream.impl.ReplayStateMachine.replayEvent(ReplayStateMachine.java:230) ~[zeebe-stream-platform-8.2.4.jar:8.2.4]\r\n\tat java.util.Iterator.forEachRemaining(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.stream.impl.ReplayStateMachine.lambda$tryToReplayBatch$5(ReplayStateMachine.java:208) ~[zeebe-stream-platform-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run(ZeebeTransaction.java:84) ~[zeebe-db-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.stream.impl.ReplayStateMachine.tryToReplayBatch(ReplayStateMachine.java:206) ~[zeebe-stream-platform-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.stream.impl.ReplayStateMachine.lambda$replayNextEvent$3(ReplayStateMachine.java:165) ~[zeebe-stream-platform-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.retry.ActorRetryMechanism.run(ActorRetryMechanism.java:28) ~[zeebe-scheduler-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.retry.RecoverableRetryStrategy.run(RecoverableRetryStrategy.java:48) ~[zeebe-scheduler-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) ~[zeebe-scheduler-8.2.4.jar:8.2.4]\r\n\t... 5 more\r\n```\r\n\r\nBasically, when processing a deployment command the following happens\r\n\r\n1. `DeploymentCreateProcessor#processRecord()` transforms the given resources accordingly\r\nhttps://github.com/camunda/zeebe/blob/e4c88b2a20f48e764118caefa5052199e12ad15e/engine/src/main/java/io/camunda/zeebe/engine/processing/deployment/DeploymentCreateProcessor.java#L129-L135\r\n2. If succeeded, the engine will create timers for any timer start event\r\nhttps://github.com/camunda/zeebe/blob/e4c88b2a20f48e764118caefa5052199e12ad15e/engine/src/main/java/io/camunda/zeebe/engine/processing/deployment/DeploymentCreateProcessor.java#L137-L142\r\n3. When executing the `#createTimerIfTimerStartEvent()` it will try to get the process definition by key:\r\nhttps://github.com/camunda/zeebe/blob/e4c88b2a20f48e764118caefa5052199e12ad15e/engine/src/main/java/io/camunda/zeebe/engine/processing/deployment/DeploymentCreateProcessor.java#L193-L203\r\n4. Getting the process definition by key will eventually put the process definition into a map `processesByProcessIdAndVersion` for caching purposes:\r\nhttps://github.com/camunda/zeebe/blob/e4c88b2a20f48e764118caefa5052199e12ad15e/engine/src/main/java/io/camunda/zeebe/engine/state/deployment/DbProcessState.java#L201-L211\r\n5. Once `#createTimerIfTimerStartEvent()` finishes, the engine will write a follow-up event to indicate the deployment has been created\r\nhttps://github.com/camunda/zeebe/blob/e4c88b2a20f48e764118caefa5052199e12ad15e/engine/src/main/java/io/camunda/zeebe/engine/processing/deployment/DeploymentCreateProcessor.java#L146\r\n6. Writing this follow-up event might fail with `EXCEEDED_BATCH_RECORD_SIZE`\r\n\r\nAs a consequence, the RocksDB transaction gets rolled back, meaning, the process definitions are not stored in RocksDB. But the not successful deployed process definitions are still present in the map `processesByProcessIdAndVersion`. That way, when a client starts a process by `bpmnProcessId`, the engine will get the actually non-existing process definition from the map `processesByProcessIdAndVersion` and start a process instance eventually. And as long as the leader keeps the same and Zeebe does not restart, process instances of the non-existing process definition can get started.\r\n\r\nAdditionally, the `ProcessVersionManager#versionCache` caches the latest version of a process definition. The same applies here, when the deployment fails, the cached latest version has been already increased and stays present in the cache. Basically, since `processesByProcessIdAndVersion` contains a non-existing process definition (for example, with version `12`) and the `versionCache` points to the non-existing version `12` of that process definition, the engine will return the process definition from `processesByProcessIdAndVersion`\r\n\r\nhttps://github.com/camunda/zeebe/blob/e4c88b2a20f48e764118caefa5052199e12ad15e/engine/src/main/java/io/camunda/zeebe/engine/state/deployment/DbProcessState.java#L214-L231\r\n\r\nBefore `versionCache` have been introduced, the engine would get the latest version from RocksDB which would not be `12` for sure. That way, the engine ensured that it always returns a process definition that actually exists.\r\n\r\n**To Reproduce**\r\n1. Deploy many processes at once (containing a process with `bpmnProcessId=foo` )\r\n2. Ensure that the deployment fails with `EXCEEDED_BATCH_RECORD_SIZE`\r\n3. Start a process instance by `bpmnProcessId=foo`  => The process instance gets started accordingly.\r\n4. Stop the broker\r\n5. Start the broker\r\n\r\n**Expected behavior**\r\n* Whenever a deployment fails, the client cannot start any process instance of any of those process definitions.\r\n* To be more concrete, any process definitions that get cached when processing the deployment command must be removed when the deployment failed.\r\n* Or as an alternative, don't cache any process definitions during deployment.\r\n* The same applies to `ProcessVersionManager#versionCache`\r\n\r\n**What is the impact of the issue?**\r\n* Any follower cannot replay any events of process instances belonging to a non-existing process definition. That way, the stream processor will stop replaying anything. The follower won't create any new snapshots, and so on.\r\n* If there is a failover, the new leader will fail to replay the events when recovering from the last snapshot. It will again stop the stream processor. No snapshots are taken anymore. The back pressure goes up to 100%. The partition gets unhealthy.\r\n* It may result in process definitions that are available on partition >= 2 but not on partition 1. For example:\r\n  1. Deploy the first version of a process `foo` that succeeds\r\n  2. Deploy a new version of the process `foo` that fails with `EXCEEDED_BATCH_RECORD_SIZE` (try doing multiple times)\r\n  3. Then redeploy the first version once again (and let it succeed)\r\n  4. => It will result in a process definition with version `x` (`x > 1`) which gets distributed to other partitions but on partition 1 this process definition won't get persisted in RocksDB.\r\n  5. So on partition > 1, there are running process instances referencing process definition with version `x`. But these process instances cannot be opened in Operate, because the process definition was never exported and imported.\r\n* There is no way to recover from that. Only purging the clusters resolves the issue.\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.2.4\r\n\r\n---\r\n\r\nrelated to #5723 \r\nrelated to [SUPPORT-17483](https://jira.camunda.com/browse/SUPPORT-17483)\r\nrelated to #12873\n\n Zelldon: **Sidenote**: this might affect also other places where we have a transient state (cached data in the state) since we never roll back such cached data on errors. \n Zelldon: Small assessment from my side:\n\nRight now, it is hard to detect from the engine perspective when a transaction is rolled back. \n\nWe could better handle the exceeding batch size, of course, but even then, it might be already part of our cache.\n\nI haven't checked the code yet. It might make sense to avoid the cache here, but I'm not sure whether this is possible.\n\nAlternatively, we could split the caches and store it in a second cache during processing, and when we are at the end of the command processing, we can put everything in the actual one.\n\nSimilarly, we could do this on the stream platform side. We have a primary cache object and a secondary cache object which is a copy which we give as an argument to the engine. On rollback, the secondary cache is cleared on success we persist the cache in our primary cache. \n\nThis involves a lot of copying which might be suboptimal if you have cached a lot of data but would resolve our issue in a more general fashion which we can use also for other places.\n\nRight now, i have no other or better idea yet. Except remove all caches.\n\n korthout: So far, we know the following solutions may work:\r\n1. only cache the processes when deployment successful\r\n  - requires reworking the logic\r\n2. when an exception is thrown, clear cache\r\n  - may slow down regular processing slightly as cache needs to be rebuild\r\n  - this also happens on leader changes\r\n3. allowing the stream processor to invalidate new cache entries similar to state transaction rollbacks\r\n  - more involved, but likely the most elegant solution\r\n  - will allow us to add caches more easily in the future\r\n\r\nAs a quick solution to overcome this problem fast, I suggest using option 2 for the next patch release. \r\n\r\n@Zelldon @romansmirnov What do you think?\n romansmirnov: @korthout, option 2 sounds like a good approach to get it fixed quickly. I share your assessment.\n Zelldon: Agree :+1: \n korthout: I was thinking a bit about how this case could've happened. We have several guarantees on our persisted state (e.g. all state changes must be covered by an event on the log), including some relational consistency checks. We do not have a foreign key relation from process instance to the process definition it belongs to. \r\n\r\nIf we had such a check, the corrupted state would've been avoided. The affected partition would've been `DEAD` but recoverable after a patch. Such a check would avoid all errors (including the unknown ones) related to process instance creation where the process definition is actually not stored in state.\r\n\r\nKeeping track of this foreign key relation would come at the cost of performance (two additional IO operations on process instance creation: one to store the additional entry in a new column family, and one to check the foreign key relation when the instance is being created in the state). \r\n\r\nIn the past, we've made decisions to go for safety over speed, but here we'd have to consider that decision again.\n korthout: Closing as fixed by:\r\n- #13256 \r\n- #13327 \r\n- #13280 \r\n- #13328 \r\n\r\nHowever, we should follow up with:\r\n- https://github.com/camunda/zeebe/issues/13259\r\n- https://github.com/camunda/zeebe/issues/13254#issuecomment-1617550522",
    "title": "When the deployment of a process definition failed, clients can still start new process instances of the actually non-existing process definition",
    "releaseNoteText": "**Consequence:**\nWhen the deployment of a process definition failed, clients were still able to start new process instances of the non-existing process definition. This caused issues with replaying events for followers, lead to failures during failover, and resulted in process definitions being available on some partitions but not others.\n\n**Cause:**\nThe issue was caused by the cached process definitions remaining in memory even after the deployment failed. The process definitions were stored in a map for caching purposes, and the failure to write the follow-up event did not remove them from the cache.\n\n**Fix:**\nThe fix involved clearing the cached process definitions when a deployment failed, ensuring that only successfully deployed process definitions were stored in memory.\n\n**Result:**\nAfter applying the fix, clients can no longer start process instances of non-existing process definitions. The cached process definitions are correctly cleared when a deployment fails, preventing issues with event replay, failover, and partition consistency."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13164",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nA segfault can occur when one of the experimental feature flags is enabled and the other disabled:\r\n- `ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLEMESSAGETTLCHECKERASYNC`\r\n- `ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLETIMERDUEDATECHECKERASYNC`\r\n\r\nThis same error does not occur when both are enabled, or both are disabled.\r\n\r\n**To Reproduce**\r\n\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\nEnable either of, and disable the other:\r\n- `ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLEMESSAGETTLCHECKERASYNC`\r\n- `ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLETIMERDUEDATECHECKERASYNC`\r\n\r\n**Expected behavior**\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\nNo segfault occurs.\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\nCurrent thread (0x00007f4dbd280db0):  JavaThread \"-zb-actors-7\" [_thread_in_native, id=15747, stack(0x00007f4d1222d000,0x00007f4d1232e000)]\r\n\r\nStack: [0x00007f4d1222d000,0x00007f4d1232e000],  sp=0x00007f4d1232c340,  free space=1020k\r\nNative frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)\r\nC  [librocksdbjni14514272448621065586.so+0x5e2a98]  rocksdb::Arena::~Arena()+0x188\r\nC  [librocksdbjni14514272448621065586.so+0x877aa6]  rocksdb::WriteBatchWithIndex::Rep::ClearIndex()+0x16\r\nC  [librocksdbjni14514272448621065586.so+0x851e59]  rocksdb::TransactionBaseImpl::Clear()+0x349\r\nC  [librocksdbjni14514272448621065586.so+0x83afca]  rocksdb::OptimisticTransaction::Rollback()+0x2a\r\nC  [librocksdbjni14514272448621065586.so+0x2def2e]  Java_org_rocksdb_Transaction_rollback+0x1e\r\nJ 13789  org.rocksdb.Transaction.rollback(J)V (0 bytes) @ 0x00007f4dad4fa05b [0x00007f4dad4f9fa0+0x00000000000000bb]\r\nJ 14913 c1 org.rocksdb.Transaction.rollback()V (60 bytes) @ 0x00007f4da5bf2fbc [0x00007f4da5bf2e00+0x00000000000001bc]\r\nJ 14285 c2 io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.ensureInOpenTransaction(Lio/camunda/zeebe/db/impl/rocksdb/transaction/TransactionConsumer;)V (29 bytes) @ 0x00007f4dad398614 [0x00007f4dad398280+0x0000000000000394]\r\nJ 21410 c1 io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.forEach(Ljava/util/function/BiConsumer;)V (24 bytes) @ 0x00007f4da5fb5a94 [0x00007f4da5fb56a0+0x00000000000003f4]\r\nj  io.camunda.zeebe.engine.state.distribution.DbDistributionState.foreachPendingDistribution(Lio/camunda/zeebe/engine/state/immutable/DistributionState$PendingDistributionVisitor;)V+49\r\nj  io.camunda.zeebe.engine.processing.distribution.CommandRedistributor.runRetryCycle()V+30\r\nj  io.camunda.zeebe.engine.processing.distribution.CommandRedistributor$$Lambda$727+0x00000008015c5c08.run()V+4\r\nJ 18846 c1 io.camunda.zeebe.stream.api.scheduling.SimpleProcessingScheduleService.lambda$runAtFixedRate$0(Ljava/lang/Runnable;Ljava/time/Duration;)V (49 bytes) @ 0x00007f4da57d5274 [0x00007f4da57d5160+0x0000000000000114]\r\nJ 18845 c1 io.camunda.zeebe.stream.api.scheduling.SimpleProcessingScheduleService$$Lambda$720+0x00000008015c4698.run()V (18 bytes) @ 0x00007f4da5f4ecac [0x00007f4da5f4eb40+0x000000000000016c]\r\nJ 15127 c2 io.camunda.zeebe.scheduler.ActorJob.execute(Lio/camunda/zeebe/scheduler/ActorThread;)V (351 bytes) @ 0x00007f4dad5606ac [0x00007f4dad560540+0x000000000000016c]\r\nJ 18627 c2 io.camunda.zeebe.scheduler.ActorTask.execute(Lio/camunda/zeebe/scheduler/ActorThread;)Z (350 bytes) @ 0x00007f4dad8730f4 [0x00007f4dad872840+0x00000000000008b4]\r\nJ 22040 c2 io.camunda.zeebe.scheduler.ActorThread.doWork()V (285 bytes) @ 0x00007f4dadbb264c [0x00007f4dadbb2240+0x000000000000040c]\r\nJ 11375 c2 io.camunda.zeebe.scheduler.ActorThread.run()V (103 bytes) @ 0x00007f4dacf579e0 [0x00007f4dacf578e0+0x0000000000000100]\r\nv  ~StubRoutines::call_stub\r\nV  [libjvm.so+0x825045]  JavaCalls::call_helper(JavaValue*, methodHandle const&, JavaCallArguments*, JavaThread*)+0x315\r\nV  [libjvm.so+0x82683b]  JavaCalls::call_virtual(JavaValue*, Handle, Klass*, Symbol*, Symbol*, JavaThread*)+0x1cb\r\nV  [libjvm.so+0x8f14f3]  thread_entry(JavaThread*, JavaThread*)+0xa3\r\nV  [libjvm.so+0xe62564]  JavaThread::thread_main_inner()+0x184\r\nV  [libjvm.so+0xe65c10]  Thread::call_run()+0xc0\r\nV  [libjvm.so+0xc1aa21]  thread_native_entry(Thread*)+0xe1\r\n```\r\n\r\n</p>\r\n</details>\n\n korthout: ❗ Since merging #13008, this segfault can also occur when both feature flags are enabled. We should fix this soon! Luckily the fix is easy and already coming up.\r\n\r\nThe segfault occurs because of the following situation:\r\n- the stream processor has its own transaction context\r\n- the scheduled tasks have their own shared transaction context\r\n- the scheduled tasks can be run on the stream processor's actor, or async on their scheduled tasks actor\r\n- the problem occurs when two scheduled tasks run on different actors while reusing the shared transaction context\n korthout: Since this issue only exists in a few places and those don't affect users, we'll look into it after the company retreat:\r\n- on released versions (`8.2.7-` and `8.1.13-`, but not on any `8.3` alphas) when only one of these experimental feature flags are enabled and the other is disabled:\r\n  - `ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLEMESSAGETTLCHECKERASYNC`\r\n  - `ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLETIMERDUEDATECHECKERASYNC`\r\n- on `main` branch when one or both feature flags are enabled\r\n- on `main` branch in tests (because both feature flags are enabled)\r\n\r\nWe should fix this before the next alpha release: `8.3.0-alpha3`.",
    "title": "Segfault on enabling async scheduled task",
    "releaseNoteText": "**Consequence:**\n\nA segfault could occur when enabling one experimental feature flag and disabling the other (`ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLEMESSAGETTLCHECKERASYNC` and `ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLETIMERDUEDATECHECKERASYNC`). This issue could also occur when both feature flags were enabled. The segfault would result in unexpected behavior for the user.\n\n**Cause:**\n\nThe segfault was caused by a specific situation where the stream processor had its own transaction context and the scheduled tasks had their own shared transaction context. The issue arose when two scheduled tasks ran on different actors while reusing the shared transaction context.\n\n**Fix:**\n\nThe fix for this issue involved addressing the transactional context conflict. The shared transaction context is now properly managed to prevent the segfault from occurring.\n\n**Result:**\n\nAfter applying the fix, the system no longer experiences a segfault when enabling or disabling the experimental feature flags. Users will observe consistent and expected behavior without any unexpected crashes. This fix has been implemented in the latest release, `8.3.0-alpha3`, ensuring a more stable and reliable system for all users."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13123",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nCurrently, the `JobTimeOutProcessor` assumes that the job it is supposed to time out is in the state. This is wrong, as it's possible to have the following sequence on the log: Job.COMPLETE, Job.TIME_OUT. The first command will remove the job from the state, and when processing the time out command, the state will not return any job.\r\n\r\n**To Reproduce**\r\n\r\nNew test in `JobTimeOutTest`:\r\n\r\n```java\r\n@Test\r\n  public void shouldRejectIfJobDoesNotExist() {\r\n    // given\r\n    final var jobKey = ENGINE.createJob(jobType, PROCESS_ID).getKey();\r\n    final var job = ENGINE.getProcessingState().getJobState().getJob(jobKey);\r\n    final var partitionId = Protocol.decodePartitionId(jobKey);\r\n\r\n    // when\r\n    ENGINE.pauseProcessing(partitionId);\r\n    ENGINE.writeRecords(\r\n        RecordToWrite.command().job(JobIntent.COMPLETE, job),\r\n        RecordToWrite.command().job(JobIntent.TIME_OUT, job));\r\n    ENGINE.resumeProcessing(partitionId);\r\n    Awaitility.await(\"until everything processed\").until(ENGINE::hasReachedEnd);\r\n\r\n    // then activated again\r\n    final List<Record<JobRecordValue>> jobEvents =\r\n        jobRecords()\r\n            .withRecordKey(jobKey)\r\n            .withIntent(JobIntent.TIME_OUT)\r\n            .withRecordType(RecordType.COMMAND_REJECTION)\r\n            .limit(1)\r\n            .toList();\r\n    assertThat(jobEvents).isNotEmpty();\r\n  }\r\n```\r\n\r\n**Expected behavior**\r\n\r\nThe command is rejected when the job to time out does not exist.\r\n\r\n**Log/Stacktrace**\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.lang.NullPointerException: Cannot invoke \"io.camunda.zeebe.protocol.impl.record.value.job.JobRecord.getDeadline()\" because \"job\" is null\r\n\tat io.camunda.zeebe.engine.processing.job.JobTimeOutProcessor.processRecord(JobTimeOutProcessor.java:53) ~[zeebe-workflow-engine-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.engine.Engine.process(Engine.java:143) ~[zeebe-workflow-engine-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.batchProcessing(ProcessingStateMachine.java:347) ~[zeebe-stream-platform-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$2(ProcessingStateMachine.java:268) ~[zeebe-stream-platform-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run(ZeebeTransaction.java:84) ~[zeebe-db-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand(ProcessingStateMachine.java:268) ~[zeebe-stream-platform-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord(ProcessingStateMachine.java:227) ~[zeebe-stream-platform-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.readNextRecord(ProcessingStateMachine.java:203) ~[zeebe-stream-platform-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:203) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n```\r\n\r\n</p>\r\n</details>\r\n\n\n koevskinikola: Just adding for transparency:\r\n\r\n*Observed behavior*:\r\nThe process instance related to the job is banned, making it unrecoverable.\r\n\r\n*Workarounds*:\r\n- There are no workarounds to fix this issue if it has already happened. As a result, this bug has a high severity.",
    "title": "NPE when processing Job.TIME_OUT command",
    "releaseNoteText": "**Consequence:**\nA NullPointerException occurs when processing the Job.TIME_OUT command. This results in the process instance related to the job becoming banned and unrecoverable.\n\n**Cause:**\nThe JobTimeOutProcessor incorrectly assumes that the job to be timed out is present in the state. However, there is a possibility that the job has already been removed from the state due to previous commands.\n\n**Fix:**\nThe fix involves updating the JobTimeOutProcessor to handle cases where the job being timed out no longer exists in the state. This prevents the NullPointerException from occurring.\n\n**Result:**\nAfter applying the fix, the Job.TIME_OUT command will be rejected when the job being timed out does not exist. This ensures that the process instance remains in a valid state and can be recovered if needed."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13061",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nThere is currently a potential race condition which would result in a remote stream existing server side, even though the client stream has gone away.\r\n\r\nSince we register remote streams asynchronously, a remove request may be submitted client side, which will immediately remove it there. Then asynchronous removal requests are sent to the server. However, this can be interleaved with the asynchronous registration, resulting in a stream existing server side.\r\n\r\nThe impact is additional latency during a push, or possible unnecessary job activation if it was the last stream for this type. However, the stream will eventually get removed appropriately.\r\n\r\n**Expected behavior**\r\n\r\nRegistration/removal of remote streams is sequenced, such that a removal request would cancel registration attempts, and queue the removal after whatever in-flight requests were sent are finished.\r\n\r\nThere is still a slight edge case around time outs, of course, but I think this is acceptable for now. The other option would be introducing even more coordination in the protocol, and I'd rather avoid this.\n\n npepinpe: The idea is to associate a `ClientStreamRegistration` state machine to each stream, which manages the current state of the remote registration. States would be:\r\n\r\n```mermaid\r\ngraph LR;\r\n    Initial-->Adding;\r\n    Adding-->Added;\r\n    Initial-->Removing;\r\n    Adding-->Removing;\r\n    Added-->Removing;\r\n    Removing-->Removed;\r\n```\r\n\r\nWhen a client first connects, it creates a registration state machine. The initial state is `INITIAL`, before any request is sent. In this state, nothing is happening until the first request occurs. Once an add request is submitted, it transitions to `ADDING`. It may also transition to `REMOVING` immediately; for simplicity, we'll send a REMOVE request anyway at the moment. \r\n\r\nIn `ADDING`, it will send registration requests to all known brokers. It transitions to `ADDED` when it has been registered to all known brokers. If the client disconnects in this stage, it transitions to `REMOVING`.\r\n\r\nIn the `ADDED` state, it does nothing. When a new broker is discovered, it transitions to `ADDING`, and again only transitions back to `ADDED` when it is registered to all known brokers. If, however, a client disconnects in the `ADDED` state, it transitions to `REMOVING`.\r\n\r\nIn the `REMOVING` state, it first cancels any in-flight registration requests, if possible. It will also discard any responses from previous registration requests at that point. Then it sends a request to all known brokers to remove this stream. When it is disconnected from all brokers, it transitions to `REMOVED`. If a new broker is discovered in this state, nothing should happen.\r\n\r\nIn the `REMOVED` state, nothing happens, and ideally the registration is garbage collected. Any incoming responses should be discarded.\n npepinpe: The above is not quite enough to catch all edge cases however. Since requests can be processed out of order, for every request to a broker per stream, we have to wait until previous requests are finished before sending the next one. Now, we still have the time out edge case - when a request to a broker times out, we can't tell if it never made it, or if it will eventually be processed. But the likelihood of a request being processed out of order in this case is very low I think. Say you send an ADD request, which times out, then a REMOVE request. It's very unlikely that the REMOVE request will somehow be received and processed before the ADD by the remote server. Whereas if you send an ADD, then immediately a REMOVE (without waiting for the ADD), the likelihood is much higher.\n deepthidevaki: If we are not able to prevent the edge case due to REMOVE processed before ADD, we can handle it safely by ensuring that gateway sends a `NoStreamExistsException`, and the broker proactively closes the stream if it gets this error response. \n npepinpe: I think that's already the case. In a way, this issue is generally an optimization one, to avoid unnecessarily activating and pushing a job when there is nothing to receive it. It's a bit hard for me to evaluate the likelihood of this happening. I see two cases:\r\n\r\n1. A client disconnects quickly while registration is still on-going.\r\n2. A client disconnects around the same time a new broker was added to the topology.\r\n\r\nI think for case 1 this is not too likely, so the second case is probably going to be the \"common\" one (and also how I initially observed it).\r\n\r\nMy current approach is to replace the set of servers an `AggregatedClientStream` is connected to with a map of server -> registration state. The registration state will keep track of the current state (e.g. `INITIAL`, `ADDING`, etc.), transition rules, and the single in-flight request (as we don't allow more than one at a time). Note that registrations are scoped per server.\r\n\r\nThe request manager now takes in registrations and manipulates those based on their state.\r\n\r\nAn alternative was to push the communication into the registration itself as well :shrug: Another one was to keep track of registrations in the request manager, not in the client stream. The drawback was we'd have to keep track of yet another source truth, in yet another map of maps (stream -> [server -> [registration]]).\n deepthidevaki: > I think that's already the case.\r\n\r\n:+1: \r\n\r\n> My current approach is to replace the set of servers an AggregatedClientStream is connected to with a map of server -> registration state. The registration state will keep track of the current state (e.g. INITIAL, ADDING, etc.), transition rules, and the single in-flight request (as we don't allow more than one at a time). Note that registrations are scoped per server.\r\n\r\nThis sounds good. Will AggregatedClientStream also marks if the client is closed, so that it can actively close registrations in case of out-of-order Add/Remove requests? i.e if a broker moves to ADDED state after the client is closed, how do we detect this and send a REMOVE request? \n npepinpe: Correct, I ended up adding a CLOSED state to the registration as a short-circuit way to close any pending operations.",
    "title": "Cancel on-going remote stream registration on stream removal",
    "releaseNoteText": "**Consequence:** There was a potential race condition where a remote stream could exist server-side even after the client stream was removed. This resulted in additional latency during a push or unnecessary job activation if it was the last stream of its type. However, the stream would eventually be removed appropriately.\n\n**Cause:** The registration of remote streams was not properly sequenced, leading to interleaving with asynchronous removal requests. This race condition occurred because remote stream registration was asynchronous and could be submitted before or after a remove request, causing the stream to exist server-side.\n\n**Fix:** To address this issue, a `ClientStreamRegistration` state machine was associated with each stream. This state machine managed the current state of the remote registration, transitioning between states such as `Initial`, `Adding`, `Added`, `Removing`, and `Removed`. Additionally, the fix included canceling in-flight registration requests and sending remove requests to all known brokers.\n\n**Result:** After applying the fix, the registration and removal of remote streams are properly sequenced. Remove requests now cancel in-flight registration attempts and queue the removal after any ongoing requests are finished. This eliminates the race condition and ensures that the client and server-side states are synchronized. As a result, users will experience improved performance with reduced latency and the avoidance of unnecessary job activation for removed streams."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13046",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nIn #11460 we've implemented a truncation on error messages. This was done to prevent error messages from exceeding the maximum message size. Currently the messages are limited to 500 characters.\r\n\r\nWe've had a support case asking us where the remainder of the error message has gone. 500 characters may be too strict. A regular stack trace will quickly exceed this limit.\r\n\r\nGenerally users would have logging on their workers which should be able to show them the full message. Obviously this isn't visible in Operate and is an extra step for users. If we could provide a better UX by increasing this limit we should consider this.\r\n\r\nWe could also think about making this limit configurable.\r\n\r\n**To Reproduce**\r\n\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\nThrow an error on a Job with > 500 characters.\r\n\r\n**Expected behavior**\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nI can see more than just the first 500 characters of my error.\r\n\r\n-------\r\n\r\nhttps://jira.camunda.com/browse/SUPPORT-17090\n\n korthout: ZPA triage:\n- request makes sense\n- we could easily increase the limit much higher (e.g. 10k) without exceeding MAX_MESSAGE_SIZE\n- we could also consider ways to make this more dynamic, but this is more complex\n- the simple fix has our preference and could be backported to all supported versions easily\n- we'll mark this a bug\n- could be a good first issue for new onboarders\n akkie: The support also pointed me to this issue. I work for a consulting company and we helped a large German corporation to implement Camunda Platform 8 for their processes. In all our presentations, we showed how easy it is for the DevOps guys to see the errors directly in Operate instead of searching through the logs. Of course this was not the only selling point, but it was a feature that everyone really liked.\r\n\r\nI understand the problem for truncating the message, but as @korthout mentioned, if it's ok to increase the limit, it would be really appreciated, because currently with the 500 chars limit, it's not really useful. For us 10k would be more as enough.\n korthout: Thanks for sharing @akkie. It's very helpful to understand such perspectives 🙇 \n aleksander-dytko: Another Support issue about this: https://jira.camunda.com/browse/SUPPORT-17503\r\n\r\nIt would be great to finish this soon. Thank @nicpuppa for taking care of this one 🚀 ",
    "title": "Error message truncation may be too strict",
    "releaseNoteText": "**Consequence**: Error messages were being truncated at 500 characters, which was too strict. This limited the visibility of the full error message, especially for stack traces that exceeded the character limit. This resulted in a poor user experience, requiring users to check the worker logs for the complete error message.\n\n**Cause**: The truncation of error messages was implemented in order to prevent them from exceeding the maximum message size. However, the limit of 500 characters proved to be too strict for practical use.\n\n**Fix**: The limit on error message truncation has been increased, allowing for a higher character limit of 10,000. This will provide a better user experience by displaying the full error message instead of truncating it.\n\n**Result**: Users will now be able to see the complete error message, even for stack traces that exceed the previous 500 character limit. This improvement enhances the usability of the system by eliminating the need to check worker logs for error details."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13041",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "Similar to https://github.com/camunda/zeebe/issues/12797, the `JobBackoffChecker` also tries to cleanup backoffs:\r\n\r\nhttps://github.com/camunda/zeebe/blob/d166007d8fee3fa6f112367ea595d35199807f4f/engine/src/main/java/io/camunda/zeebe/engine/state/instance/DbJobState.java#L308-L317\n",
    "title": "Don't mutate state through `JobBackoffChecker`",
    "releaseNoteText": "**Consequence:** Mutating state through `JobBackoffChecker` resulted in unintended side effects for the user. This caused issues similar to the one reported in ticket #12797. \n\n**Cause:** The `JobBackoffChecker` was not designed to handle state mutations, resulting in unexpected behavior when attempting to cleanup backoffs.\n\n**Fix:** The issue has been resolved by updating the `JobBackoffChecker` to prevent state mutations. The code in `DbJobState.java` has been modified (lines 308-317) to ensure that state mutations are handled correctly.\n\n**Result:** As a result of this fix, the `JobBackoffChecker` now functions as intended and no longer causes unintended side effects or issues related to state mutations. Users can now safely use `JobBackoffChecker` without worrying about unexpected behavior."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13038",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nWhen trying/retrying a push from the broker side, we every time pick the next random stream consumer to push to. However, it's possible that during a retry all consumers are removed. Right now, this throws an error because we try to generate a random index from 0 to 0 :smile: \r\n\r\n**Expected behavior**\r\n\r\nWe should cope with streams having no consumers (on both the gateway/broker) even during retries, and bail out early.\r\n\n\n npepinpe: Alright, so `AggregatedRemoteStream` is not thread safe, but it's also not immutable right now.\r\n\r\nSo we have two options: making it thread-safe and handle mutation, or making it immutable and potentially handle more errors.\r\n\r\nMaking it thread safe means turning the consumer list into a `CopyOnWriteArrayList` (as we likely are reading from it more often than writing to it), and handling mutations when picking a random stream by first grabbing a copy of the list before doing any operations on it.\r\n\r\nMaking it immutable for now would simply be copying the `AggregatedRemoteStream` record when picking the target, and passing this around downstream. I would lean towards the second option as it's simpler to handle, and in the common case we still wouldn't be handling more errors than usual.",
    "title": "Handle stream consumers changes during push",
    "releaseNoteText": "**Consequence:** Previously, when attempting a push from the broker side, if all stream consumers were removed during a retry, an error would occur because the system would try to generate a random index from 0 to 0.\n\n**Cause:** The issue was caused by attempting to pick the next random stream consumer to push to without considering the possibility that all consumers may have been removed during a retry.\n\n**Fix:** The `AggregatedRemoteStream` was made immutable by copying the `AggregatedRemoteStream` record when selecting the target. This ensures that the system can handle the scenario of streams with no consumers during retries.\n\n**Result:** With this fix in place, the system is now able to cope with streams having no consumers, even during retries. The process of picking a random stream consumer has been modified to handle this scenario and bail out early, preventing errors from occurring."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13036",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nWhen registering a new client job stream from the gateway to any broker, the brokers are successfully registering the streams, but not responding to the gateway, causing the gateway to retry over and over.\r\n\r\nWhile this still allows pushing, it creates a lot of noise and the impression that the streams are not successfully registered.\r\n\r\nThe underlying cause is the usage of the `ClusterCommunicationService#subscribe(String, Function<byte[], M>, BiConsumer<MemberId, M>, Executor executor)` - any subscriber which is a consumer will never send a response back. However, the client in this case expects a response.\r\n\r\n**Expected behavior**\r\n\r\nClient streams are properly registered on both sides.\r\n\n\n npepinpe: One hurdle here is there's no easy way to introspect the registered client streams. So our tests were passing because we just check if the server has registered the stream (which it has), but we have no real way to catch that the client is in an endless loop.",
    "title": "Endless client job stream registration",
    "releaseNoteText": "**Consequence:**\nClients registering a new job stream from the gateway to any broker were experiencing an issue where the brokers were successfully registering the streams but not responding to the gateway. This resulted in the gateway continuously retrying the registration process, leading to excessive noise and giving the impression that the streams were not successfully registered.\n\n**Cause:**\nThe underlying cause of this issue was the utilization of the `ClusterCommunicationService#subscribe(String, Function<byte[], M>, BiConsumer<MemberId, M>, Executor executor)`. In this case, any subscriber that acted as a consumer would not send a response back to the gateway. However, the client expected a response, resulting in the endless loop when registering a job stream.\n\n**Fix:**\nThe issue was resolved by modifying the registration process for client job streams. The modifications addressed the issue with the `ClusterCommunicationService#subscribe` method, ensuring that the clients received the expected response upon successfully registering a job stream.\n\n**Result:**\nAfter the fix, clients can now register job streams successfully on both ends. The registration process no longer enters an endless loop, and the brokers respond to the gateway as expected. This improves the overall user experience by reducing noise and providing the correct impression that the job streams have been successfully registered."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12957",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nSince 8.2, it is possible to deploy processes with [Undefined Task](https://docs.camunda.io/docs/next/components/modeler/bpmn/undefined-tasks/), which the workflow engine processes as a straight-through activity. When placed in a loop without a wait state (i.e. element in the process that requires the engine to wait), the workflow engine may process faster than the exporters can export the produced records.\r\n\r\nWhen the workflow engine is faster than the exporters, the log grows, and Zeebe's disk space usage increases. Eventually, Zeebe Brokers may take too long to start up because the log is too large. This may lead to the unavailability of Zeebe.\r\n\r\nWe've encountered this on the Elasticsearch exporter, where ES may reject exported records. In that case, it can even result in the unavailability of Operate and Zeebe.\r\n\r\nAs a temporary workaround:\r\n- increase the CPU/memory resources of Elasticsearch to support the higher influx of records\r\n- increase the failure threshold Zeebe Brokers are allowed as start-up duration\r\n- cancel the process instance with the straight-through processing loop\r\n\r\n**To Reproduce**\r\n\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\n> The following only works on clusters exporting to Elasticsearch with specific resource allocations. You may need to reduce the available Elasticsearch resources.\r\n\r\n- deploy a process with a loop of undefined tasks\r\n- create an instance of this process\r\n- notice the number of not exported records increasing\r\n\r\n**Expected behavior**\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\nProcess instances of processes with a straight-through processing loop should not cause problems.\r\n\r\n**Solution proposals**\r\n\r\nA solution has already been proposed to resolve this issue:\r\n- #12696 \r\n\r\nAdditionally, we suggest to:\r\n- add a feature flag to allow/disallow undefined tasks and/or processes without wait states\r\n  - allows Play to use them in the Modeling/Play iteration; while it can be disabled on SaaS production-like clusters\r\n  - flag could also disallow creating instance of such processes if they are already deployed\r\n  - flag could also stop execution of process instances of such processes if already created\r\n- raise an [Incident](https://docs.camunda.io/docs/next/components/concepts/incidents/) when a straight-through processing loop is detected during process execution\r\n\r\n**Considered alternatives**\r\n\r\nI also considered interrupting a process instance (and its child process instances) immediately when canceling it instead of the current recursive and async termination flow. However, that solution would be better fitting to kill fork bombs. A simple straight-through processing loop does not produce a log of unprocessed records.\r\n\n\n Zelldon: > IMO, we should not reject deployments of processes without wait states because these may be valuable in Modeling/Play iteration when first designing a process.\r\n\r\nI think this can be differentiated. We consider Play our new tool for users to play around with there models and here I agree it makes sense to allow it, but not on production-like clusters.\r\n\r\nWe could have a feature flag that allows us to disable certain features, like undefined tasks or other activities. We could then reject models (with a well-written message) which contain such elements on SaaS clusters, and reference to Play to try out processes. This would also allow us to prevent problematic scenarios in production environments.\r\n\r\nIn my opinion, this would be straightforward and with minimal effort to implement. \r\n\r\nAlternative and much more complex would be of course detection mechanism, and in general better handling of suspending/cancelation of bad behavior instances.\r\n\r\n\r\n\n korthout: Good point @Zelldon! I'll adjust the description accordingly.\n megglos: Thanks for raising this @korthout! I wanted to follow-up on this topic this week. I would love us to have a short-term mitigation, like e.g. disabling undefined tasks until we have a proper way of at least recovering or even better detecting these situations and suspend such instances.\r\n\r\nFor short term I see these options:\r\n\r\n1. disable undefined task processing by default on SaaS, only offer a flag to opt-in => assuming the use case is covered by  Zeebe Play (for which it is enabled)\r\nPro:\r\nno such incidents on SaaS anymore\r\noptional: we can make this transparent to the user by the engine rejecting processes that use undefined tasks\r\nCon:\r\nthe feature is effectively removed from SaaS\r\n\r\n2. option 1 but only for all non-trial clusters => prevent an incident where a real production cluster gets impacted by such a process\r\nPro:\r\nwe prevent this from happening on a production cluster\r\nCon:\r\nthe noise caused by trials will not disappear\r\n\r\n3. Assess a fist iteration to detect loops of undefined tasks and reject their deployment => even if it may not cover all potential edge-cases may a first iteration be feasible that already is good-enough to prevent a majority of such processes to cause trouble?\r\nPro:\r\nmay be suffice to reduce the frequency of such incidents heavily already\r\nerror message can make this transparent to the user\r\nmay better fit a 8.2 patch scope => prevent users from running into this issue while keeping the previous default of undefined tasks being allowed\r\nCon:\r\neffort seems higher\r\nstill a risk of an edge-case not covered causing such loop (a call-activity of a process that contains undefined tasks only 🙃 ?)\r\n\r\nMy personal favorite is option 3 and if that's too complicated option 1 asap. (I'm fine doing an out of schedule patch, for the sake of preventing this issue asap)\r\nOption 2 is not really helping on the trial noise, meaning it is not good enough of a proper mitigation from that perspective,  and introduced feature imbalance for trial that we didn't have yet. right?\r\n\r\nwdyt?\r\n\r\n\n Zelldon: \r\n![giphy](https://github.com/camunda/zeebe/assets/2758593/16eeb955-6c44-4f78-831f-9071bb92e104)\r\n\r\nStop the bleeding with one, and work on a better solution e.g. three. \r\n\r\nExamples: Reject deployments which contain only no wait states, or detect loops which contains no wait states (https://stackoverflow.com/questions/261573/best-algorithm-for-detecting-cycles-in-a-directed-graph) \r\n\n megglos: The issue we have with 1 & 2 is that it effectively disables a feature of 8.2, that's why I would like to favor 3 first and take 1 as last resort + the communication overhead to announce this breaking change.\n megglos: @korthout as a mitigation to this would help us to avoid repeated incidents with manual effort I would suggest to bump severity to high, wdyt?\n korthout: @megglos The `severity` label is clearly defined. This bug does not have high severity because a workaround is available. \r\n\r\nI do see the need to prioritize it as `🚧 upcoming` (intend to work on it soon) or even as `⛔ blocker` (stop-the-world) due to the impact it has on us (incidents, alert noise, etc).\n megglos: another option for a potential mitigation that was raised by @oleschoenburg \r\n\r\n4. What about having an artificial delay for undefined and manual tasks, e.g. 1s?\r\nThat would throttle such loops significantly by introducing a wait state and thus offsetting disk resource issues significantly.\r\nFrom a user perspective that might be acceptable to unnoticeable. We could make it configurable to cover cases where a self-managed user cares about low latency and still wants to keep and undefined/manual task in the process\r\nPro:\r\nmight it be more straightforward to implement compared to option 3 => basically applying timer event behavior\r\nCon:\r\nDoesn't prevent this situation but offsets any resulting resource problems\r\n\r\nSo we could decide on doing either 4 or 3 first. While 3 would prevent such situations (but not all of them) 4 would offset any resulting issues of such processes by a significant amount of time. Both together appear already good enough to mitigate the pain of such looping processes.\r\n\r\nWith these in place we may be able to close this issue and eventually shift towards https://github.com/camunda/zeebe/issues/12696 that would enable us to at least manually intervene with any rogue process.\r\n\r\nwdyt @korthout @Zelldon ?\r\n\r\n\n korthout: Interesting idea\r\n\r\n>artificial delay for undefined and manual tasks\r\n\r\nI assume we mean non-blocking for the stream processor. So schedule a delayed task that appends the command to complete the task.\n Zelldon: Not sure how this should look like, since if you start to process it you need to commit all changes to it before starting on the next command, the processing of commands is in a serial order. I can only imagine you put it back to the end of the log, other than that I see no way to do that ?\r\n\n oleschoenburg: > Not sure how this should look like, since if you start to process it you need to commit all changes to it before starting on the next command\r\n\r\nSimilar to how job timeouts works for example: after the manual task is `ACTIVATED` and after some period of time has passed we write a `COMPLETE` command to the log.\r\nThat's how I thought it could work anyway, maybe that doesn't work?\n korthout: @Zelldon is right; it wouldn't be safe to do so. Unless we add a way to write the `COMPLETE` command for any currently `ACTIVATED` undefined/manual tasks, on recovery.\n oleschoenburg: But every `ACTIVATED` undefined/manual task should already have a `COMPLETE` follow-up command on the log. Otherwise, how would this task ever complete?\n korthout: >But every ACTIVATED undefined/manual task should already have a COMPLETE follow-up command on the log. \r\n\r\nThat's not correct. The proposal was to add the delay async, so when processing `ACTIVATE_ELEMENT` we append `ELEMENT_ACTIVATING` and `ELEMENT_ACTIVATED`, and also schedule a post-commit task to append `COMPLETE_ELEMENT` after a delay (this may be lost).\n oleschoenburg: Ah sorry, I misunderstood and thought you were talking about the upgrade from old to new behavior.\r\n\r\nOf course you are right and this would require persistence of activated manual/undefined tasks so that they can be completed eventually. Again, similar to job timeouts.\n Zelldon: So to summarize the proposal (just that I get it right):\r\n\r\n * There will be another checker/consumer which is scheduled via the ProcessingScheduleService (in order to be decoupled of the processing). Lets call it U-COMPLETER\r\n * The processing of undefined tasks is split up, it will only produce ACTIVATE_ACTIVATED on processing\r\n * During activation we need to store the record and all necessary data in a new column family (for the U-COMPLETER)\r\n * The U-COMPLETER will write based on the data in the column family the complete commands, in order to continue the processing of the instance.\n korthout: ZPA triage:\n- let's split up this issue into the different solution proposals\n- we like the proposal to reject deployments of processes with an undefined task loop but there are simpler solutions, so we'll focus on those first\n- we like the proposal to slow down undefined task, but it's unclear to us whether we should do this sync or async.\n  - sync: this blocks the stream processor a bit but is a simpler solution\n  - async: adds additional complexity over sync because we have to care about the unreliability of scheduled post-commit tasks\n  - @camunda/zeebe-distributed-platform, do you have an opinion about the performance loss?\n- we should check what impact our solutions have on Zeebe Play\n- we will focus on the quick fix in the upcoming iteration\n megglos: > sync: this blocks the stream processor a bit but is a simpler solution\r\n> async: adds additional complexity over sync because we have to care about the unreliability of scheduled post-commit tasks\r\n\r\nsync would not be an option due to the impact on overall processing latency/throughput\r\n\n megglos: I'm also in all in for rolling out a 8.2.X patch & a 8.2.0-alpha2.1 (did we ever do an out of cadence alpha before?) as soon as a mitigation is available\n korthout: >sync would not be an option due to the impact on overall processing latency/throughput\r\n\r\nIs Undefined Task really an element used in production where performance (latency/throughput matters?\r\n\r\nI imagine Undefined Task only has use in Zeebe Play, or as a lonely element in a production process where a small latency (~10-50ms) doesn't really matter.\n remcowesterhoud: I've created https://github.com/camunda/zeebe/issues/12993 as a separate issue to delay undefined tasks.\n korthout: Thanks @remcowesterhoud, marking this issue as `later` priority, as #12993 is split out with higher prio.\n oleschoenburg: > I imagine Undefined Task only has use in Zeebe Play, or as a lonely element in a production process where a small latency (~10-50ms) doesn't really matter.\r\n\r\nI disagree that it's fine for production workloads. If a user \"accidentally\" deploys and starts such as process that loops on an undefined task, it could seriously impact the performance of an entire partition because the partition will repeatedly sleep. Even worse with batch processing enabled where it's not just one sleep but many (like 10 or 100).\n megglos: ZDP-Triage:\n- moving to backlog now as @remcowesterhoud is looking into this\n remcowesterhoud: As the quick-fix is done and getting released as I type this, I'm going to remove my assignment from this issue. We'll still need to make a proper solution, so I will move this back to the teams inbox to triage it again.\n Zelldon: Great work @remcowesterhoud \n korthout: ZPA triage:\n- with a quick fix available, we think other issues have higher priority than this\n- we'll increase priority if this does occur again\n- marking as `later`\n megglos: ZDP-Triage:\n- loop detection was a first band-aid and we still lack measures to handle situations not covered by validation\n- measures from the steam platform might be possible, e.g. detect high processing load on the same key, it needs to be discussed if this is the right direction though\n- most solutions likely require collab between both teams\n megglos: https://github.com/camunda/zeebe/issues/12696 is related to this as well, will put focus on that one first\n megglos: ZDP-Planning:\n- is related to #12560 and would be mitigated if processing is rate limited\n- we have alerts on noop loops => we can cancel such instances proactively",
    "title": "Straight-through processing loop may cause problems",
    "releaseNoteText": "**Consequence:**\nSince version 8.2, deploying processes with undefined tasks, which are processed as straight-through activities, can lead to problems. When placed in a loop without a wait state, the workflow engine may process faster than the exporters can export records. This may cause the log to grow, increasing disk space usage and potentially affecting the availability of Zeebe Brokers, Operate, and Elasticsearch.\n\n**Cause:**\nThe cause of this issue is the ability to deploy processes with undefined tasks and straight-through processing loops. When the workflow engine processes faster than the exporters can export records, it can lead to resource problems and unavailability of Zeebe.\n\n**Fix:**\nTo mitigate this issue, a quick fix has been implemented. A delayed task has been introduced for undefined and manual tasks, introducing a wait state that offsets resource issues caused by straight-through processing loops. This allows for the completion of the tasks after a delay.\n\n**Result:**\nWith the quick fix in place, the impact of straight-through processing loops causing problems in Zeebe has been mitigated. The delayed task for undefined and manual tasks introduces a wait state that prevents resource issues and ensures the completion of these tasks. This improves the overall stability and availability of Zeebe."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12933",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nWith job push enabled, when a job is failed with remaining retries and no backoff, the job is first activated and then immediately marked as failed again. This results in pushing out jobs that are in failed state and presumably some data inconsistencies because the job lifecycle is not followed. \r\n\r\n**To Reproduce**\r\n\r\nRun `ActivatableJobsPushTest#shouldPushAfterJobFailed` and inspect the log.\r\n\r\n**Expected behavior**\r\n\r\nWhen a job is failed with remaining retries and no backoff, the job should first transition to failed before being activated again.\r\n\r\n**Log/Stacktrace**\r\n\r\n<details><summary>Compact log representation</summary>\r\n <p>\r\n\r\n```\r\nC DPLY      CREATE            - #01-> -1  -1 - \r\nE PROC      CREATED           - #02->#01 K01 - process.bpmn -> \"process\" (version:1)\r\nE DPLY      CREATED           - #03->#01 K02 - process.bpmn\r\nE DPLY      FULLY_DISTRIBUTED - #04->#01 K02 - \r\nC CREA      CREATE            - #05-> -1  -1 - new <process \"process\"> (default start)  with variables: {a=valA, b=valB, c=valC}\r\nE VAR       CREATED           - #06->#05 K04 - b->\"valB\" in <process [K03]>\r\nE VAR       CREATED           - #07->#05 K05 - a->\"valA\" in <process [K03]>\r\nE VAR       CREATED           - #08->#05 K06 - c->\"valC\" in <process [K03]>\r\nC PI        ACTIVATE          - #09->#05 K03 - PROCESS \"process\" in <process \"process\"[K03]>\r\nE CREA      CREATED           - #10->#05 K07 - new <process \"process\"> (default start)  with variables: {a=valA, b=valB, c=valC}\r\nE PI        ACTIVATING        - #11->#05 K03 - PROCESS \"process\" in <process \"process\"[K03]>\r\nE PI        ACTIVATED         - #12->#05 K03 - PROCESS \"process\" in <process \"process\"[K03]>\r\nC PI        ACTIVATE          - #13->#05  -1 - START_EVENT \"start\" in <process \"process\"[K03]>\r\nE PI        ACTIVATING        - #14->#05 K08 - START_EVENT \"start\" in <process \"process\"[K03]>\r\nE PI        ACTIVATED         - #15->#05 K08 - START_EVENT \"start\" in <process \"process\"[K03]>\r\nC PI        COMPLETE          - #16->#05 K08 - START_EVENT \"start\" in <process \"process\"[K03]>\r\nE PI        COMPLETING        - #17->#05 K08 - START_EVENT \"start\" in <process \"process\"[K03]>\r\nE PI        COMPLETED         - #18->#05 K08 - START_EVENT \"start\" in <process \"process\"[K03]>\r\nE PI        SEQ_FLOW_TAKEN    - #19->#05 K09 - SEQUENCE_FLOW \"sequenc..7fb902a\" in <process \"process\"[K03]>\r\nC PI        ACTIVATE          - #20->#05 K10 - SERVICE_TASK \"task\" in <process \"process\"[K03]>\r\nE PI        ACTIVATING        - #21->#05 K10 - SERVICE_TASK \"task\" in <process \"process\"[K03]>\r\nE JOB       CREATED           - #22->#05 K11 - K11 \"id-ae6f81e8-dfdb-436e-a86a-d681b855a326\" @\"task\"[K10] 3 retries, in <process \"process\"[K03]> (no vars)\r\nE JOB_BATCH ACTIVATED         - #23->#05 K12 - \"id-ae6f81e8-dfdb-436e-a86a-d681b855a326\" 1/-1\r\n                K11 \"id-ae6f81e8-dfdb-436e-a86a-d681b855a326\" @\"task\"[K10] 3 retries, in <process \"process\"[K03]> (no vars)\r\nE PI        ACTIVATED         - #24->#05 K10 - SERVICE_TASK \"task\" in <process \"process\"[K03]>\r\nC JOB       FAIL              - #25-> -1 K11 - K11 5 retries, in <process ?[?]> (no vars)\r\nE JOB_BATCH ACTIVATED         - #26->#25 K13 - \"id-ae6f81e8-dfdb-436e-a86a-d681b855a326\" 1/-1\r\n                K11 \"id-ae6f81e8-dfdb-436e-a86a-d681b855a326\" @\"task\"[K10] 5 retries, in <process \"process\"[K03]> (no vars)\r\nE JOB       FAILED            - #27->#25 K11 - K11 \"id-ae6f81e8-dfdb-436e-a86a-d681b855a326\" @\"task\"[K10] 5 retries, in <process \"process\"[K03]> (no vars)\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n\r\nThis is currently a (soft) blocker for https://github.com/camunda/zeebe/issues/12797\n\n oleschoenburg: I think the issue is related to the _very confusing_ interplay between the `JobFailProcessor` (a `CommandProcessor`), the `CommandControl` and the `BpmnJobActivationBehavior`.\r\nThe code _seemingly_ does the right thing, first transitioning to `FAILED` and then handing over to `jobActivationBehavior` which should transition to `ACTIVATED`:\r\nhttps://github.com/camunda/zeebe/blob/5bd2e1f830d8de2efb6869dc0a29d47896b51800/engine/src/main/java/io/camunda/zeebe/engine/processing/job/JobFailProcessor.java#L137-L143\r\n\r\nIf I understand correctly, the mistakes is that the commandControl does not immediately write the follow-up event but only later when the `ACTIVATED` event is already written.\n npepinpe: Do we really need to mark the job as failed? Could we skip it entirely?\r\n\r\nAt any rate, if we have to, the log would ideally look like: ACTIVATED -> FAIL -> FAILED -> ACTIVATED. While the push has to happen as a side effect, the new ACTIVATED record should be the last one written in terms of state changes.\n koevskinikola: This bug will also be present when pushing jobs on timeout and recur, as the corresponding processors implement the `CommandProcessor` interface as well.\r\n\r\n1. The quick fix would be to move the job push to the `CommandProcessor#afterAccept` method.\r\n1. The nicer fix, which would make these Processors more readable, would be to refactor them to use the `TypedRecordProcessor` interface, where it's more clear when the events are written.\n npepinpe: :+1: for the second one\n koevskinikola: @npepinpe sorry for not replying to:\r\n> Do we really need to mark the job as failed? Could we skip it entirely?\r\n\r\nI think it would be good to have the following sequence of job events on the log: ACTIVATED -> FAIL -> FAILED -> ACTIVATED.\r\n\r\nIt's clearer to users that the `FAIL` command resulted in a `FAILED` event, and the job was then `ACTIVATED`.\r\n\r\nHaving ACTIVATED -> FAIL -> ACTIVATED is a bit confusing.",
    "title": "Failing jobs with job push enabled breaks the job lifecycle",
    "releaseNoteText": "**Consequence:**\nWith job push enabled, when a job is failed with remaining retries and no backoff, the job is immediately activated again and marked as failed, resulting in pushing out jobs that are in a failed state. This breaks the job lifecycle and may lead to data inconsistencies.\n\n**Cause:**\nThe issue was caused by a confusing interplay between the `JobFailProcessor` (a `CommandProcessor`), the `CommandControl`, and the `BpmnJobActivationBehavior`. The code correctly transitioned the job to the `FAILED` state and then handed it over to the `jobActivationBehavior` to be activated. However, the `CommandControl` did not write the follow-up event immediately, leading to incorrect state transitions.\n\n**Fix:**\nTwo possible fixes were proposed for this issue. The first fix was to move the job push to the `CommandProcessor#afterAccept` method. The second fix, which was considered a nicer solution, involved refactoring the processors to use the `TypedRecordProcessor` interface, which would make the event writes more clear and readable.\n\n**Result:**\nThe fix was implemented by refactoring the processors to use the `TypedRecordProcessor` interface, ensuring that the job lifecycle is followed correctly. Now, when a job is failed with remaining retries and no backoff, the job transitions to the `FAILED` state and is then activated again. The log now shows a clear sequence of job events: ACTIVATED -> FAIL -> FAILED -> ACTIVATED. This eliminates confusion and provides a more accurate representation of the job's state transitions."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12915",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "While working on #12839, it was observed that it is not possible to change the journal record schema without breaking backward compatability. (See [comment](https://github.com/camunda/zeebe/pull/12839#issuecomment-1568545921))\r\n\r\nWhen journal record schema was moved to SBE, one of the main motivator was to allow extending it without breaking changes. \r\n\r\nProblems:\r\n- A broker on newer version cannot receive any events via raft replication, if the event was written by a leader at older version.\r\n   - This is a deal breaker. As we cannot upgrade a running system to a new version.\r\n- A broker on older version cannot receive any events via raft replication, of the event was written by a leader at newer version.\r\n   - This is less problematic, as it might cause some unavailability during rolling update.\r\n\r\nGoal:\r\nEnsure we can extend journal and raft record schema with out breaking compatibility. \r\n\r\nTo limit the scope, it might be ok if we could ensure backward compatibility. That is, ensure brokers at newer versions can work with record written with old version. It is ok, if old versions cannot read records from new version as long as it can detect it and do not cause any inconsistency. \r\n\r\n\r\nblocking #12839 \r\n\n\n deepthidevaki: Summary of discussion with @npepinpe \r\n\r\nIt is better to send the serialized journal record in AppendRequest. This would require some changes in the journal api and raft replication handling. We have to figure out how to change this, without breaking compatibility. During rolling update, it might be acceptible if follower's on older version cannot receive any events from a leader on newer version. But we should ensure that, followers on newer version can receive events from leaders on older version. Otherwise this can block both rolling update and recreating with new version.\r\n\r\nOne idea is to check the version of the sender, and interpret the request accordingly. Right now, we don't have a concept of raft-protocol-version. But we can probably add that, and not having it can be interpreted as old version. When raft follower on new version, receives a request from old leader, it writes the record using the old sbe version and the checksum calculated will match that of the original one. If the request is from the new leader, it uses the new logic to handle the serialized journal record.\n megglos: ZDP-Triage:\n- the checksum is affected by the schema version\n- we need a general mechanism to handle such changes\n- blocks any changes to the journal schema => makes sense to work on it asap\n- also relates to properly supporting updates in general\n- the solution will have an effect on rolling updates going forward https://github.com/camunda/product-hub/issues/256",
    "title": "Allow extending journal record format with out breaking compatibility",
    "releaseNoteText": "**Consequence:** Previously, it was not possible to change the journal record schema without breaking backward compatibility. This meant that a broker on a newer version could not receive events via raft replication if the event was written by a leader on an older version. Additionally, a broker on an older version could not receive events via raft replication if the event was written by a leader on a newer version. This limitation made it difficult to upgrade a running system to a new version and caused potential unavailability during rolling updates.\n\n**Cause:** The inability to extend the journal record schema without breaking compatibility was a result of the journal record schema being moved to SBE (Simple Binary Encoding). While this move was intended to allow for easier extension, it did not fully achieve that goal.\n\n**Fix:** The fix involved changing the mechanism for sending the serialized journal record in the AppendRequest. This required modifications to the journal API and the raft replication handling. A new concept of raft-protocol-version was introduced, allowing for interpretation of requests based on the version of the sender. When a raft follower on a new version receives a request from an older leader, it writes the record using the old SBE version and calculates a checksum that matches the original. If the request is from a new leader, the new logic is used to handle the serialized journal record.\n\n**Result:** With this fix, it is now possible to extend the journal and raft record schema without breaking compatibility. Brokers on newer versions can work with records written with older versions, ensuring backward compatibility. While brokers on older versions may not be able to read records from new versions, this does not cause any inconsistency. This fix allows for smoother rolling updates and recreating with new versions, as followers on newer versions can receive events from leaders on older versions."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12886",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\n\r\nZeebe offers a [Stackdriver](https://github.com/camunda/zeebe/tree/main/util/src/main/java/io/camunda/zeebe/util/logging/stackdriver) logging format that displays logs as JSON, formatted for Google Cloud Logging.\r\n\r\nNot all the logs are formatted according to this format though:\r\n\r\n* At startup, Zeebe displays a huge ASCII banner showing  `Zeebe`\r\n* Some logs (Tomcat logs?) are not formatted as JSON:\r\n  ```\r\n  May 30, 2023 6:51:02 AM org.apache.coyote.AbstractProtocol init\r\n  INFO: Initializing ProtocolHandler [\"http-nio-0.0.0.0-9600\"]\r\n  May 30, 2023 6:51:02 AM org.apache.catalina.core.StandardService startInternal\r\n  INFO: Starting service [Tomcat]\r\n  May 30, 2023 6:51:02 AM org.apache.catalina.core.StandardEngine startInternal\r\n  INFO: Starting Servlet engine: [Apache Tomcat/10.1.7]\r\n  May 30, 2023 6:51:02 AM org.apache.catalina.core.ApplicationContext log\r\n  INFO: Initializing Spring embedded WebApplicationContext\r\n  ```\r\n\r\n**Describe the solution you'd like**\r\n\r\nIdeally:\r\n\r\n* All the logs are properly formatted as JSON, instead of a mix of plain text/JSON.\r\n* The banner can be completely removed.\r\n\r\nThis would make all the logs uniform, help to classify them correctly, and would prevent spurious parsing of plain-text only logs.\r\n\r\n**Describe alternatives you've considered**\r\n\r\nNone\r\n\r\n**Additional context**\r\n\r\nA typical startup looks like this:\r\n\r\n```\r\nPicked up JAVA_TOOL_OPTIONS: -XX:MaxRAMPercentage=50.0 -XX:InitialRAMPercentage=25.0 -XX:+ExitOnOutOfMemoryError -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/usr/local/zeebe/data/java_started_1685429455.hprof\r\n  ______  ______   ______   ____    ______     ____    _____     ____    _  __  ______   _____  \r\n |___  / |  ____| |  ____| |  _ \\  |  ____|   |  _ \\  |  __ \\   / __ \\  | |/ / |  ____| |  __ \\ \r\n    / /  | |__    | |__    | |_) | | |__      | |_) | | |__) | | |  | | | ' /  | |__    | |__) |\r\n   / /   |  __|   |  __|   |  _ <  |  __|     |  _ <  |  _  /  | |  | | |  <   |  __|   |  _  / \r\n  / /__  | |____  | |____  | |_) | | |____    | |_) | | | \\ \\  | |__| | | . \\  | |____  | | \\ \\ \r\n /_____| |______| |______| |____/  |______|   |____/  |_|  \\_\\  \\____/  |_|\\_\\ |______| |_|  \\_\\\r\n                                                                                                \r\n{\"severity\":\"INFO\",\"logging.googleapis.com/sourceLocation\":{\"function\":\"logStarting\",\"file\":\"StartupInfoLogger.java\",\"line\":51},\"message\":\"Starting StandaloneBroker v8.2.3 using Java 17.0.6 with PID 7 (/usr/local/zeebe/lib/camunda-zeebe-8.2.3.jar started by root in /usr/local/zeebe)\",\"serviceContext\":{\"service\":\"zeebe\",\"version\":\"8.2.3\"},\"context\":{\"threadId\":1,\"threadPriority\":5,\"loggerName\":\"io.camunda.zeebe.broker.StandaloneBroker\",\"threadName\":\"main\"},\"timestampSeconds\":1685429459,\"timestampNanos\":42397893}\r\n{\"severity\":\"DEBUG\",\"logging.googleapis.com/sourceLocation\":{\"function\":\"logStarting\",\"file\":\"StartupInfoLogger.java\",\"line\":52},\"message\":\"Running with Spring Boot v3.0.5, Spring v6.0.8\",\"serviceContext\":{\"service\":\"zeebe\",\"version\":\"8.2.3\"},\"context\":{\"threadId\":1,\"threadPriority\":5,\"loggerName\":\"io.camunda.zeebe.broker.StandaloneBroker\",\"threadName\":\"main\"},\"timestampSeconds\":1685429459,\"timestampNanos\":128377708}\r\n{\"severity\":\"INFO\",\"logging.googleapis.com/sourceLocation\":{\"function\":\"logStartupProfileInfo\",\"file\":\"SpringApplication.java\",\"line\":638},\"message\":\"The following 1 profile is active: \\\"broker\\\"\",\"serviceContext\":{\"service\":\"zeebe\",\"version\":\"8.2.3\"},\"context\":{\"threadId\":1,\"threadPriority\":5,\"loggerName\":\"io.camunda.zeebe.broker.StandaloneBroker\",\"threadName\":\"main\"},\"timestampSeconds\":1685429459,\"timestampNanos\":130430841}\r\n{\"severity\":\"INFO\",\"logging.googleapis.com/sourceLocation\":{\"function\":\"initialize\",\"file\":\"TomcatWebServer.java\",\"line\":108},\"message\":\"Tomcat initialized with port(s): 9600 (http)\",\"serviceContext\":{\"service\":\"zeebe\",\"version\":\"8.2.3\"},\"context\":{\"threadId\":1,\"threadPriority\":5,\"loggerName\":\"org.springframework.boot.web.embedded.tomcat.TomcatWebServer\",\"threadName\":\"main\"},\"timestampSeconds\":1685429462,\"timestampNanos\":567312120}\r\nMay 30, 2023 6:51:02 AM org.apache.coyote.AbstractProtocol init\r\nINFO: Initializing ProtocolHandler [\"http-nio-0.0.0.0-9600\"]\r\nMay 30, 2023 6:51:02 AM org.apache.catalina.core.StandardService startInternal\r\nINFO: Starting service [Tomcat]\r\nMay 30, 2023 6:51:02 AM org.apache.catalina.core.StandardEngine startInternal\r\nINFO: Starting Servlet engine: [Apache Tomcat/10.1.7]\r\nMay 30, 2023 6:51:02 AM org.apache.catalina.core.ApplicationContext log\r\nINFO: Initializing Spring embedded WebApplicationContext\r\n{\"severity\":\"INFO\",\"logging.googleapis.com/sourceLocation\":{\"function\":\"prepareWebApplicationContext\",\"file\":\"ServletWebServerApplicationContext.java\",\"line\":291},\"message\":\"Root WebApplicationContext: initialization completed in 3603 ms\",\"serviceContext\":{\"service\":\"zeebe\",\"version\":\"8.2.3\"},\"context\":{\"threadId\":1,\"threadPriority\":5,\"loggerName\":\"org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext\",\"threadName\":\"main\"},\"timestampSeconds\":1685429462,\"timestampNanos\":856072455}\r\n{\"severity\":\"WARNING\",\"logging.googleapis.com/sourceLocation\":{\"function\":\"overrideDiskConfig\",\"file\":\"DataCfg.java\",\"line\":69},\"message\":\"Configuration parameter data.diskUsageCommandWatermark is deprecated. Use data.disk.freeSpace.processing instead.\",\"serviceContext\":{\"service\":\"zeebe\",\"version\":\"8.2.3\"},\"context\":{\"threadId\":1,\"threadPriority\":5,\"loggerName\":\"io.camunda.zeebe.broker.system\",\"threadName\":\"main\"},\"timestampSeconds\":1685429463,\"timestampNanos\":631599523}\r\n{\"severity\":\"WARNING\",\"logging.googleapis.com/sourceLocation\":{\"function\":\"overrideDiskConfig\",\"file\":\"DataCfg.java\",\"line\":75},\"message\":\"Configuration parameter data.diskUsageReplicationWatermark is deprecated. Use data.disk.freeSpace.replication instead.\",\"serviceContext\":{\"service\":\"zeebe\",\"version\":\"8.2.3\"},\"context\":{\"threadId\":1,\"threadPriority\":5,\"loggerName\":\"io.camunda.zeebe.broker.system\",\"threadName\":\"main\"},\"timestampSeconds\":1685429463,\"timestampNanos\":632382123}\r\n{\"severity\":\"INFO\",\"logging.googleapis.com/sourceLocation\":{\"function\":\"<init>\",\"file\":\"EndpointLinksResolver.java\",\"line\":58},\"message\":\"Exposing 8 endpoint(s) beneath base path '/actuator'\",\"serviceContext\":{\"service\":\"zeebe\",\"version\":\"8.2.3\"},\"context\":{\"threadId\":1,\"threadPriority\":5,\"loggerName\":\"org.springframework.boot.actuate.endpoint.web.EndpointLinksResolver\",\"threadName\":\"main\"},\"timestampSeconds\":1685429465,\"timestampNanos\":427691130}\r\nMay 30, 2023 6:51:05 AM org.apache.coyote.AbstractProtocol start\r\nINFO: Starting ProtocolHandler [\"http-nio-0.0.0.0-9600\"]\r\n\r\n[...]\r\n```\r\n\r\n* The banner should be removed\r\n* Non-JSON logs should be turned to JSON\r\n* Ideally, the `JAVA_TOOL_OPTIONS` should also be turned into JSON (or removed)\n\n korthout: ZPA triage:\n- relevant issue, but not a priority for us (`later`)\n- good first issue for newcomers\n megglos: ZDP-Triage:\n- spring/tomcat seems to use some different logger setup or sysout\n- JAVA_TOOL_OPTIONS output seems to come from the JVM => it cannot be suppressed => would require sout redirect, alternatively we could use other ways to pass these options\n- @multani what's the actual impact for the logging infrastructure, I would assume these logs are ignored or not properly categorized?\n- assigning later as well\n multani: > * @multani what's the actual impact for the logging infrastructure, I would assume these logs are ignored or not properly categorized?\r\n\r\nImpact is not huge: logs are analyzed as plain text without any extra metadata and are not really filterable / analyzable.\r\n\r\nFor instance: I never saw Tomcat errors in our systems, but it would be difficult to look them up as the log level is not parsed. If they were parsed correctly, errors would clearly stand out.\r\n\r\nThe banner is only displays as garbage into the logs and should be simply removed.\n multani: This is definitely not critical, but consider it as a good practice for production.\n npepinpe: re Tomcat, tbh I've been thinking we should switch to WebFlux, but there wasn't much pressure for it. but it would remove unnecessary dependencies, since webflux is built on top of netty and we already bundle netty for our own usage (and would not do this weird logging to STDERR as well).\n npepinpe: My proposal:\r\n\r\n- [x] Since we can't suppress `JAVA_TOOL_OPTIONS`, use `JAVA_OPTS` to pass options instead of `JAVA_TOOL_OPTIONS`. This would require some changes on the Helm chart and the controller though.\r\n- [x] [Switch from Tomcat to WebFlux or Undertow (preference to WebFlux). WebFlux is Spring's reactive web server based on Netty, and as our whole system is based on Netty anyway, might as well go for that](https://github.com/camunda/zeebe/pull/13539)\r\n- [x] The huge banner can be disabled via `SPRING_MAIN_BANNER-MODE=off` - again this is a change in the Helm chart/controller.\r\n\r\nSo from the Zeebe side, it would be just the second part. The others would be done in the controller.\n multani: If you can't remove the `Picked up JAVA_TOOL_OPTIONS` line, would it be possible to output that line to stdout instead of stderr?\n npepinpe: We have no control over that, that's the JVM directly writing this =/\r\n\r\nThe only way I can think of would be to redirect STDOUT to STDERR, but then that's all logs =/\r\n\r\nI think the best option is to simply not use `JAVA_TOOL_OPTIONS`. Our scripts already support `JAVA_OPTS`, so we can just make sure the controller and Helm chars both use that instead.\n multani: > We have no control over that, that's the JVM directly writing this =/\r\n\r\nAh yes, you are right, I forgot about that :facepalm: \n npepinpe: Done via https://github.com/camunda-cloud/camunda-operator/pull/1753\r\n\r\nThe Helm charts were not updated, so they will still print non JSON logs, but that can be fixed by essentially doing the same we did in https://github.com/camunda-cloud/camunda-operator/pull/1753.",
    "title": "Zeebe should only log as JSON, if configured to do so",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12875",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Summary**\r\n\r\nRun `ModifyProcessInstanceRejectionTest#shouldRejectActivationWhenAncestorScopeIsNotFlowScope` test failed on Windows 10.\r\n\r\n**Logs**\r\n\r\n\r\n<details><summary>Logs</summary>\r\n<pre>\r\nE PI   SEQ_FLOW_TAKEN    - #63->#05 K21 - SEQUENCE_FLOW \"sequenc..b588006\" in <process \"process\"[K03]>\r\nC PI   ACTIVATE          - #64->#05 K22 - PARALLEL_GATEWAY \"join\" in <process \"process\"[K03]>\r\nR PI   ACTIVATE          - #65->#05 K22 - PARALLEL_GATEWAY \"join\" in <process \"process\"[K03]> !INVALID_STATE (Expected to be able to activate parallel gateway 'join', but not all sequence flows have been taken.)\r\nC MOD  MODIFY            - #66-> -1 K03 - <activate \"A\" no vars> <activate \"B\" no vars> <activate \"A\" no vars> \r\nR MOD  MODIFY            - #67->#66 K03 - <activate \"A\" no vars> <activate \"B\" no vars> <activate \"A\" no vars>  !INVALID_ARGUMENT (Expected to modify instance of process 'process' but it contains one or more activate instructions with an ancestor scope key that is not an ancestor of the element to activate:\r\n- instance '2251799813685257' of element 'subProcess2' is not an ancestor of element 'A'\r\n- instance '2251799813685263' of element 'B' is not an ancestor of element 'A')\r\n\r\n\r\n\r\njava.lang.AssertionError: [Expect that subProcess2 cannot be selected as ancestor of task A] \r\nExpecting rejectionReason of:\r\n  <{\"valueType\":\"PROCESS_INSTANCE_MODIFICATION\",\"key\":2251799813685251,\"position\":67,\"timestamp\":1685115125924,\"recordType\":\"COMMAND_REJECTION\",\"intent\":\"MODIFY\",\"partitionId\":1,\"rejectionType\":\"INVALID_ARGUMENT\",\"rejectionReason\":\"Expected to modify instance of process 'process' but it contains one or more activate instructions with an ancestor scope key that is not an ancestor of the element to activate:\\r\\n- instance '2251799813685257' of element 'subProcess2' is not an ancestor of element 'A'\\r\\n- instance '2251799813685263' of element 'B' is not an ancestor of element 'A'\",\"brokerVersion\":\"8.3.0\",\"value\":{\"processInstanceKey\":2251799813685251,\"ancestorScopeKeys\":[],\"terminateInstructions\":[],\"activateInstructions\":[{\"ancestorScopeKeys\":[],\"elementId\":\"A\",\"ancestorScopeKey\":2251799813685257,\"variableInstructions\":[]},{\"ancestorScopeKeys\":[],\"elementId\":\"B\",\"ancestorScopeKey\":2251799813685257,\"variableInstructions\":[]},{\"ancestorScopeKeys\":[],\"elementId\":\"A\",\"ancestorScopeKey\":2251799813685263,\"variableInstru...>\r\nto be:\r\n  <Expected to modify instance of process 'process' but it contains one or more activate instructions with an ancestor scope key that is not an ancestor of the element to activate:\r\n- instance '2251799813685257' of element 'subProcess2' is not an ancestor of element 'A'\r\n- instance '2251799813685263' of element 'B' is not an ancestor of element 'A'>\r\nbut was:\r\n  <Expected to modify instance of process 'process' but it contains one or more activate instructions with an ancestor scope key that is not an ancestor of the element to activate:\r\n- instance '2251799813685257' of element 'subProcess2' is not an ancestor of element 'A'\r\n- instance '2251799813685263' of element 'B' is not an ancestor of element 'A'>\r\n\r\n\tat io.camunda.zeebe.engine.processing.processinstance.ModifyProcessInstanceRejectionTest.shouldRejectActivationWhenAncestorScopeIsNotFlowScope(ModifyProcessInstanceRejectionTest.java:664)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n</pre>\r\n</details>\r\n\r\n```\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Detecting the operating system and CPU architecture\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] os.detected.name: windows\r\n[INFO] os.detected.arch: x86_64\r\n[INFO] os.detected.bitness: 64\r\n[INFO] os.detected.version: 10.0\r\n[INFO] os.detected.version.major: 10\r\n[INFO] os.detected.version.minor: 0\r\n[INFO] os.detected.classifier: windows-x86_64\r\n\r\n```\r\n\r\n\n\n remcowesterhoud: @skayliu I assume this test always fails for you and is not flaky? I believe it has to do with the line breaks I can see in your error message: `\\r\\n`\n skayliu: @remcowesterhoud, Yes, It always failed not flaky.\n remcowesterhoud: Perfect, thanks! Time to dust off my old Windows laptop 😄 ",
    "title": "`ModifyProcessInstanceRejectionTest#shouldRejectActivationWhenAncestorScopeIsNotFlowScope` test fails on Windows",
    "releaseNoteText": "**Consequence:**\n\nThe `ModifyProcessInstanceRejectionTest#shouldRejectActivationWhenAncestorScopeIsNotFlowScope` test fails on Windows 10. The test expects to be able to activate a parallel gateway 'join', but not all sequence flows have been taken, resulting in an INVALID_STATE error. Additionally, the test attempts to modify an instance of the process 'process', but it contains activate instructions with ancestor scope keys that are not ancestors of the element to activate, resulting in an INVALID_ARGUMENT error.\n\n**Cause:**\n\nThe test failure is caused by line breaks in the error message, which are not handled correctly on the Windows platform.\n\n**Fix:**\n\nThe code has been updated to handle line breaks in the error message on Windows.\n\n**Result:**\n\nAfter the fix, the `ModifyProcessInstanceRejectionTest#shouldRejectActivationWhenAncestorScopeIsNotFlowScope` test no longer fails on Windows. The error message is now displayed correctly, allowing the test to pass successfully."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12837",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\nUsing an error catch event (boundary or start event in Subprocess) with an empty `errorCode` results in incident with `errorType=UNHANDLED_ERROR_EVENT`. \r\nAlso only error events with a non-empty errorCode will be reported like this: `Available error events are [boundary]`.\r\n\r\n\r\n**To Reproduce**\r\n\r\n1. Create a BPMN process with error catch event as boundary event or as start event in subprocess.\r\n2. Deploy, start the process. In a job client throw an error command with an unhandled errorCode.\r\n3. Zeebe creates an incident instead of catching the error \r\n\r\nHere an example process [errorProcessCatchAll.bpmn.txt](https://github.com/camunda/zeebe/files/11544062/errorProcessCatchAll.bpmn.txt)\r\n\r\n[Slack thread](https://camunda.slack.com/archives/CSQ2E3BT4/p1684841521468159)\r\n\r\n**Expected behavior**\r\n\r\nThe error is caught and no incident is created.\r\n\r\n**Log/Stacktrace**\r\n\r\nExample error message from Zeebe:\r\n```\r\nerrorType=UNHANDLED_ERROR_EVENT, errorMessage='Expected to throw an error event with the code 'unknown' with message 'Process error', but it was not caught. Available error events are [boundary]\r\n```\r\n\r\n**Environment:**\r\n- OS: MacOS\r\n- Zeebe Version: `8.2.0-alpha3` and later\r\n- Configuration: ElasticsearchExporter\r\n\r\n\n\n lzgabel: Hi @korthout. I think I found the problem, please assign this task to  me, I will submit a PR to fix this problem soon. Thanks :heart:\n lzgabel: BTW. As a workaround, user can catch all error events by removing the `errorEventDefinition`.\r\n",
    "title": "Catch all error events by using empty 'errorCode' does not work",
    "releaseNoteText": "**Consequence:**\nCatching error events using an empty `errorCode` did not work. This resulted in incidents with `errorType=UNHANDLED_ERROR_EVENT`. In addition, only error events with a non-empty errorCode were reported, causing confusion for users.\n\n**Cause:**\nThe issue was caused by the error catch event (boundary or start event in Subprocess) not correctly handling the empty `errorCode`, leading to incidents being created instead of the error being caught.\n\n**Fix:**\nThe fix involved updating the error catch event to properly handle the scenario of an empty `errorCode`. This allowed the error to be caught instead of creating an incident.\n\n**Result:**\nAfter the fix, error events with an empty `errorCode` are now properly caught, and no incidents are created. Users can catch all error events without needing to remove the `errorEventDefinition` as a workaround."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12833",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nThere was an incident in one of the chaos test models, where a none end event had an output mapping referencing a non-existent variable `source` to be mapped to `target`. I resolved the incident by adding a dummy variable (the mapping should not have been there anyway), but then the process was stuck on the none end event.\r\n\r\nSee https://bru-2.operate.camunda.io/eeef5734-cfd6-47a5-a2ed-5fe13269e589/processes/2251799815205221\r\n\r\n**To Reproduce**\r\n\r\nAdd the following test case to `io.camunda.zeebe.engine.processing.incidentOutputMappingIncidentTest`:\r\n\r\n```java\r\n          {\r\n            \"None end event\",\r\n            ENGINE\r\n                .deployment()\r\n                .withXmlResource(\r\n                    Bpmn.createExecutableProcess(PROCESS_ID)\r\n                        .startEvent()\r\n                        .endEvent(\"endEventId\", b -> b.zeebeOutputExpression(\"foo\", \"bar\"))\r\n                        .done()),\r\n            \"endEventId\",\r\n            false\r\n          },\r\n```\r\n\r\n**Expected behavior**\r\n\r\nI can resolve incidents on a none end event.\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.2.3\r\n- Configuration: SaaS - G3 S\r\n\n\n korthout: As a workaround, users can use [Process Instance Modification](https://docs.camunda.io/docs/next/components/concepts/process-instance-modification/) to re-activate the end event\n korthout: ZPA triage:\r\n- should be simple to resolve `good-first-issue` (just implement the `onComplete` method on `NoneEndEventBehavior` in the `EndEventProcessor`)\r\n- since it's low-hanging fruit and may effect users we would like to tackle it sooner than later.",
    "title": "Cannot resolve output mapping incident on a none end event",
    "releaseNoteText": "**Consequence:** Users were unable to resolve incidents on a none end event, leading to the process being stuck on that event.\n\n**Cause:** The issue was caused by an incident in one of the chaos test models, where a none end event had an output mapping referencing a non-existent variable.\n\n**Fix:** The incident was resolved by adding a dummy variable and removing the incorrect mapping.\n\n**Result:** With this fix, users can now successfully resolve incidents on a none end event, allowing the process to continue as expected."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12754",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nRelated to #12374 \r\n\r\nA customer observed another case where the startup failed with the error:\r\n```\r\n\"Expected to find a snapshot at index >= log's first index X, but found snapshot Y. A previous snapshot is most likely corrupted.\"\r\n```\r\nI could not verify it, but our assumption currently is that this is a false positive. The following might have happened:\r\n1. Follower received snapshot\r\n2. Before committing the snapshot, it reset segments\r\n3. While deleting the segments, the node was shutdown.\r\n4. After restart, it has the old snapshot plus partially deleted segments.\r\n\r\nI think this is plausible as the reset/deleting segments is not and atomic operation. We might not be handling it correctly.\r\n\r\n**Expected behavior**\r\n\r\nThe reset or snapshot commit process should not result in an invalid intermediate state which is detected as corruption.\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.1.*\r\n\r\nRelated to [support](https://jira.camunda.com/browse/SUPPORT-16905) \r\n\n\n deepthidevaki: A potential solution might be to delete the segments in the reverse order, combined with the PR that fixed #12374.\r\n\r\nIn a related issue, I discussed with @npepinpe a different approach. I was planning to create a separate issue for it, but I'm just dumping my thoughts here, as I think it is also relevant for this bug.\r\n\r\n------\r\nTerminologies\r\n\r\nIn-Sync replicas - The followers that are in sync with leaders and participating in the commit. In-sync replicas forms the quorum.\r\n\r\nOut-of-sync follower - The follower which is receiving the event potentially after it is already committed.\r\n\r\nIn a setup with replicationFactor 3, there will be 2 in-sync replicas, out of which one will be the leader. The third one can be an out-of-sync follower.\r\n\r\nRaft guarantees that any committed event is available in all in-sync replicas. The events are replicated to the out-of-sync follower eventually. But there is no guarantee when they are replicated. Ideally, they are replicated immediately. But it is possible the follower is slow, or the network is slow, resulting the out-of-sync follower to lag behind the leader by a large amount. \r\n\r\nAt some point, the leader sends a snapshot instead of the events, because the follower is lagging behind. At this point, the current behavior is as follows:\r\n\r\nFollower resets the log (delete all segments)\r\nReceives and commits the new snapshot\r\nDelete the old snapshot\r\nContinue receiving the events after the snapshot index\r\n\r\nThe above process can lead to the following scenarios:\r\n\r\nStep 1 and 2 are not atomic. If the replica crashes in between, then it restarts with an empty state.\r\nIf it restarts before step 4, then it starts with a snapshot  that is not use-able. Snapshot is valid only if the event corresponding to lastFollowUpEventPosition is in the log.\r\n\r\nIn a normal scenario, the above situation is not problematic because the in-sync replicas are healthy and it is guaranteed that only one of the in-sync replica becomes the leader.\r\n\r\nHowever, in a disaster scenario where all in-sync replicas are gone, it might be acceptable to continue functioning with whatever data is available in the out-of-sync replica. It might not have up-to-date state, but for some use-cases it is ok to lose last X amount of data. In such cases, we want to enable users to recover the cluster from the state of the out-of-sync replica. However, in the above scenario the out-of-sync replica might be in a state where it's state is empty or the snapshot is not useable. \r\n\r\nSo, it would be good if we could ensure that the state in the out-of-sync follower is always in a valid state. To acheive  that we can\r\n\r\nDo not delete old snapshot and reset log immediately when it receives a snapshot.\r\nInstead keep the old snapshot and the logs until the new snapshot is committed and its follow up event is received.\r\n\r\nSolution 1:\r\n\r\nWe keep the old snapshot + logs in a different folder, the new snapshot and new logs in a different folder. On restarts, it attempts to use the new snapshot + log and fall back to the old state if necessary. \r\n\r\nSolution 2:\r\n\r\nDo not reset the logs, when a new snapshot is received. Instead, add a marker record in the journal to indicate that there is a snapshot at the position. The readers must know how to handle these “gaps”. For example, if stream processor reader hits this marker record, it has to throw away its state and replace with the snapshot. If the raft leader reads this record, it should replicate the snapshot instead of the event etc.\r\n\r\nThe compaction logic should take this into account, and compact only if there is one valid snapshot with its followup event in the log. So the follower can rollback to an old valid state if necessary.\r\n\r\nThis would also help in some issue that we observed recently where restarting the node during resetting the log can incorrectly lead to detecting it as corrupted log.\n npepinpe: With #12868, we implemented a quick fix to prevent the specific case we're aware of. By deleting the segments in reverse order during reset, we can ensure that there are no gaps in the log/snapshot, and thus no corruption. Data loss is acceptable here as it was the desired outcome of the reset operation.\r\n\r\nHowever, we agreed not to close the issue - this quick fix only solves a specific thing, and not the root cause, which is that the install operation on the follower is not atomic, specifically persisting the snapshot/clearing the log/updating the meta store/getting the first record for the snapshot/etc. are all independent operations and the node may stop at any point in between, leaving us in a weird state.\n megglos: ZDP-Planning:\r\n- while we are at it, it would be worth to assess potential solutions to get an understanding of how much effort it is to change\r\n- needs prototyping\r\n- back to backlog as upcoming for now to be picked up going forward\r\n- relates to upcoming resilience epic",
    "title": "Journal reset resulting in an intermediate state which is detected as corruption",
    "releaseNoteText": "**Consequence:** A bug was causing the system to enter an invalid intermediate state, which was detected as corruption. This resulted in a startup failure with the error message \"Expected to find a snapshot at index >= log's first index X, but found snapshot Y. A previous snapshot is most likely corrupted.\" The issue arose when a follower received a snapshot, but before committing it, the segments were reset. If the node was shutdown during the segment deletion process and then restarted, it would have the old snapshot and partially deleted segments, leading to the observed corruption.\n\n**Cause:** The reset or snapshot commit process was not handling the deletion of segments correctly. The reset and segment deletion were not atomic operations, leading to the possibility of an invalid intermediate state.\n\n**Fix:** The segments were modified to be deleted in reverse order during the reset process. This ensured that there were no gaps in the logs or snapshots, effectively preventing any corruption. However, this fix only addressed the specific case and did not solve the underlying atomicity issue in the installation operation on the follower.\n\n**Result:** With the fix in place, the reset or snapshot commit process no longer results in an invalid intermediate state being detected as corruption. The system now correctly handles the deletion of segments, ensuring that the logs and snapshots are always in a valid state. This prevents any startup failures due to corruption and improves the overall stability and reliability of the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12699",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "In https://github.com/camunda/zeebe/issues/12591, we found that when engine rejects the deployment request it cannot write the rejection record because it was attempting to write the whole deployment which is greater than maxMessageSize. This resulted in a loop in StreamProcessor, where it repeatedly fails to write the rejection record leading to the partition being fully blocked and not making any progress. https://github.com/camunda/zeebe/issues/12591#issuecomment-1535879364 \r\n\r\nTo fix this, now we reject the request in CommandAPI before writing to the logstream https://github.com/camunda/zeebe/pull/12676. But, to be safer we should also ensure that we are not writing rejection records that are too large, to prevent such accidental cases.\r\n\r\nEnsure that rejections records can be written reliably, even if the original command is rejected due to ExceededBatchSize. Ideally, rejection record should only have a reference to the original command, but not the full command. If possible, do not write the entire command for any type of rejections. Or atleast, trim rejection record in case of ExceededBatchSize exception.\r\n\r\n\r\n\n\n berkaycanbc: ZPA Triage:\n\nWe think this is a bug. We are planning to process this in the next mob-programming session.\n korthout: ~~Mob~~Pair programming with @koevskinikola results:\r\n- Run `io.camunda.zeebe.it.client.command.CreateDeploymentTest::shouldRejectDeployIfResourceIsTooLarge`\r\n- Notice that an error is logged, this should not be the case\r\n- Make sure that [rejection is written safely](https://github.com/camunda/zeebe/blob/main/engine/src/main/java/io/camunda/zeebe/engine/Engine.java#L186-L194)\n abbasadel: ZPA planning: \n- Moving back to the backlog since we don't have time for it in the next iteration.",
    "title": "Do not write the entire command for rejection",
    "releaseNoteText": "**Consequence**: When the engine rejected a deployment request because it exceeded the maxMessageSize, it was unable to write the rejection record due to attempting to write the entire deployment. This caused a loop in the StreamProcessor, resulting in the partition being fully blocked and making no progress.\n\n**Cause**: The issue originated from the engine's attempt to write the entire deployment as the rejection record, which was larger than the maxMessageSize. This led to the failure to write the rejection record and the subsequent loop in the StreamProcessor.\n\n**Fix**: The fix for this issue involved rejecting the request in the CommandAPI before writing to the logstream. Additionally, measures were taken to ensure that rejection records are not too large to prevent similar accidental cases.\n\n**Result**: Rejection records can now be written reliably, even if the original command is rejected due to ExceededBatchSize. The fix ensures that rejection records only contain a reference to the original command, instead of the entire command. As a result, the system can accurately handle rejection cases and prevent the blocking of partitions."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12623",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nAfter restoring from backup, if the leader that took the original backup is no more the leader of the partition, this can result in a duplicate backup of the partition with the same backup id. The new backup is logically equivalent to the old backup. So it doesn't matter which backup we use later. However, re-taking the backup is unnecessary as it wastes resources.\r\n\r\n**To Reproduce**\r\nRestore zeebe from a backup, and observe the logs or inspect the backup store.\r\n\r\n**Expected behavior**\r\nA partition should not re-take a backup after restore.\r\n\n",
    "title": "After restoring from backup, a partition re-takes the backup",
    "releaseNoteText": "**Consequence:** After restoring from a backup, a partition would unnecessarily re-take the backup, resulting in a duplicate backup of the partition with the same backup ID. This would waste resources without any impact on the functionality as the new backup would be logically equivalent to the old backup.\n\n**Cause:** The issue occurred when the leader who originally took the backup was no longer the leader of the partition after the restore. This caused the partition to go through the backup process again, resulting in a duplicate backup.\n\n**Fix:** The fix involved modifying the partition logic to prevent it from re-taking a backup after a restore. This change ensured that the partition does not unnecessarily repeat the backup process.\n\n**Result:** After applying the fix, when restoring from a backup, the partition no longer re-takes the backup unnecessarily. This improves resource utilization and does not have any impact on the functionality as the new backup is logically equivalent to the old one."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12622",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nSometime, a partition may attempt to take a backup again when a backup for the same id already exists. This case is rare, but can happen sometimes if there is a leader change while taking a backup. If this happens, list backup fails with an error message:\r\n```\r\n{\r\n    \"message\": \"Duplicate key 1 (attempted merging values BackupStatus[backupId=1, partitionId=1, status=COMPLETED, failureReason=, brokerVersion=8.2.2, createdAt=...])\"\r\n}\r\n```\r\n\r\n**Expected behavior**\r\nList backup should be able to handle duplicate backup ids for a partition.\r\n\n",
    "title": "List backup fails when a partition has same backup taken by multiple nodes",
    "releaseNoteText": "**Consequence**\nWhen a partition has the same backup taken by multiple nodes, the list backup fails with an error message indicating a duplicate key.\n\n**Cause**\nThis issue occurs when a partition attempts to take a backup again while a backup for the same ID already exists. It is a rare case that can occur if there is a leader change during the backup process.\n\n**Fix**\nThe fix for this issue includes handling duplicate backup IDs for a partition in the list backup functionality.\n\n**Result**\nAfter applying the fix, the list backup will be able to handle partitions with duplicate backup IDs. Users will no longer encounter the error message indicating a duplicate key when trying to list backups."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12597",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nTrying to list all available backups will always fail without a useful error message.\r\nThe gateway distributes a list request to all brokers which then list all of their backups and try to respond with a `BackupListResponse`:\r\n\r\nhttps://github.com/camunda/zeebe/blob/4854b6606a803926ed9cadabfc2edb4aede18cb4/protocol/src/main/resources/cluster-management-protocol.xml#L64-L76\r\n\r\nThe `groupSizeEncoding` is defined by us:\r\n\r\nhttps://github.com/camunda/zeebe/blob/c861aac736376e1cc20aa558979c6d9c289b4a1f/protocol/src/main/resources/common-types.xml#L16-L19\r\n\r\nIt uses a `unit8` to represent the number of entries. When trying to write a `BackupListResponse` with more than 255 entries, the encoder rejects it:\r\n```\r\njava.lang.IllegalArgumentException: count outside allowed range: count=774\r\n\tat io.camunda.zeebe.protocol.management.BackupListResponseEncoder$BackupsEncoder.wrap(BackupListResponseEncoder.java:137) ~[zeebe-protocol-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.protocol.management.BackupListResponseEncoder.backupsCount(BackupListResponseEncoder.java:114) ~[zeebe-protocol-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.protocol.impl.encoding.BackupListResponse.write(BackupListResponse.java:100) ~[zeebe-protocol-impl-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.broker.transport.backupapi.BackupApiResponseWriter.write(BackupApiResponseWriter.java:71) ~[zeebe-broker-8.2.2.jar:8.2.2]\r\n....\r\n\r\n```\r\n**To Reproduce**\r\n\r\nTake 256 backups, then query all backups via `GET actuator/backups`.\r\n\r\n**Expected behavior**\r\n\r\n1. Zeebe supports much more backups than 255 (for example by using a `uint16`, thus supporting 65535 backups)\r\n2. The number of listed backups should be limited to a reasonable number. Querying the backup store to list, say 1000, available backups is likely to result in timeouts and makes the backup API unusable.\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.lang.IllegalArgumentException: count outside allowed range: count=774\r\n\tat io.camunda.zeebe.protocol.management.BackupListResponseEncoder$BackupsEncoder.wrap(BackupListResponseEncoder.java:137) ~[zeebe-protocol-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.protocol.management.BackupListResponseEncoder.backupsCount(BackupListResponseEncoder.java:114) ~[zeebe-protocol-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.protocol.impl.encoding.BackupListResponse.write(BackupListResponse.java:100) ~[zeebe-protocol-impl-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.broker.transport.backupapi.BackupApiResponseWriter.write(BackupApiResponseWriter.java:71) ~[zeebe-broker-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.transport.impl.ServerResponseImpl.write(ServerResponseImpl.java:50) ~[zeebe-transport-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.transport.impl.AtomixServerTransport.sendResponse(AtomixServerTransport.java:154) ~[zeebe-transport-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.broker.transport.backupapi.BackupApiResponseWriter.tryWriteResponse(BackupApiResponseWriter.java:51) ~[zeebe-broker-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.broker.transport.AsyncApiRequestHandler.lambda$handleRequest$1(AsyncApiRequestHandler.java:123) ~[zeebe-broker-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.future.FutureContinuationRunnable.run(FutureContinuationRunnable.java:28) ~[zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94) ~[zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) [zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) [zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- Zeebe Version: >= 8.1\r\n\n",
    "title": "Listing backups fails if more than 255 backups are available",
    "releaseNoteText": "**Consequence:**\nAttempting to list all available backups fails with an error message stating that the count is outside the allowed range, even though more than 255 backups are present. This issue prevents the proper functioning of the backup API and makes it unusable for querying a large number of backups.\n\n**Cause:**\nThe issue is caused by the definition of the `groupSizeEncoding` in the `common-types.xml` file, which uses a `unit8` to represent the number of entries. This encoding mechanism limits the number of backups that can be listed to a maximum of 255.\n\n**Fix:**\nTo address this issue, the `groupSizeEncoding` has been modified to use a `uint16` encoding instead. This change will support up to 65535 backups.\n\n**Result:**\nAfter applying the fix, Zeebe will be able to support a much larger number of backups, up to a maximum of 65535. The backup API will now function properly, and the number of listed backups will be limited to a reasonable number, avoiding potential timeouts and making the backup API usable for querying backups."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12591",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nAn attempt to upload oversized BPMN (other the segment limit) causes unrecoverable failure of Zeebe:\r\n* further BPMN upload of proper sizes are not possible\r\n* eventually partitions becomes unhealthy and not recovered\r\n\r\nIt is degradation from version 8.1.6 that just rejected incorrect BPMN without further problems.\r\n\r\n**To Reproduce**\r\n\r\nUpload the BMPN bigger than configured \r\n      maxMessageSize: 64KB\r\n\r\n\r\n**Expected behavior**\r\n\r\nBMPN Upload is rejected\r\n\r\n**Log/Stacktrace**\r\n```\r\n2023-04-27 17:03:37.989 [Broker-0-StreamProcessor-1] [Broker-0-zb-actors-1] ERROR\r\n      io.camunda.zeebe.broker.process - Unexpected error while processing resource 'f6c0b39d-8357-40ea-8d79-7e2611a89677.bpmn'\r\nio.camunda.zeebe.stream.api.records.ExceededBatchRecordSizeException: Can't append entry: 'RecordBatchEntry[recordMetadata=RecordMetadata{recordType=EVENT, valueType=PROCESS, intent=CREATED}, key=2251799852104669, sourceIndex=-1, unifiedRecordValue={\"bpmnProcessId\":\"id_f6c0b39d-8357-40ea-8d79-7e2611a89677\",\"version\":1,\"processDefinitionKey\":2251799852104669,\"resourceName\":\"f6c0b39d-8357-40ea-8d79-7e2611a89677.bpmn\",\"checksum\":\"uG1QH8XcklrgFGYZI3HhPg==\",\"resource\":\"PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0idXRmLTgiPz4NCjxicG1uOmRlZmluaXRpb25zIHhtbG5zOmJwbW5kaT0iaHR0cDovL3d3dy5vbWcub3JnL3NwZWMvQlBNTi8yMDEwMDUyNC9ESSIgeG1sbnM6ZGM9Imh0dHA6Ly93d3cub21nLm9yZy9zcGVjL0RELzIwMTAwNTI0L0RDIiB4bWxuczp6ZWViZT0iaHR0cDovL2NhbXVuZGEub3JnL3NjaGVtYS96ZWViZS8xLjAiIHhtbG5zOmRpPSJodHRwOi8vd3d3Lm9tZy5vcmcvc3BlYy9ERC8yMDEwMDUyNC9ESSIgeG1sbnM6eHNpPSJodHRwOi8vd3d3LnczLm9yZy8yMDAxL1hNTFNjaGVtYS1pbnN0YW5jZSIgaWQ9ImlkXzZmNTk2NTlmLWE3OGMtNDMyMy04M2VmLTdlODMwYmJlNTUwNSIgdGFyZ2V0TmFtZXNwYWNlPSJodHRwOi8vYnBtbi5pby9zY2hlbWEvYnBtbiIgZXhwb3J0ZXI9IkNvbmZpcm1pdCBCUE1OIEJ1aWxkZXIiIGV4cG9ydGVyVmVyc2lvbj0iMS4wLjAuMCIgeG1sbnM6YnBtbj0iaHR0cD...' with size: 1010867 this would exceed the maximum batch size. [ currentBatchEntryCount: 0, currentBatchSize: 0]\r\n\tat io.camunda.zeebe.stream.impl.records.RecordBatch.appendRecord(RecordBatch.java:67) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.BufferedProcessingResultBuilder.appendRecordReturnEither(BufferedProcessingResultBuilder.java:62) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.api.ProcessingResultBuilder.appendRecord(ProcessingResultBuilder.java:38) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.streamprocessor.writers.ResultBuilderBackedEventApplyingStateWriter.appendFollowUpEvent(ResultBuilderBackedEventApplyingStateWriter.java:40) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.transformProcessResource(BpmnResourceTransformer.java:162) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.lambda$transformResource$0(BpmnResourceTransformer.java:77) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.util.Either$Right.map(Either.java:355) ~[zeebe-util-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.lambda$transformResource$1(BpmnResourceTransformer.java:75) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.util.Either$Right.flatMap(Either.java:366) ~[zeebe-util-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.transformResource(BpmnResourceTransformer.java:65) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.DeploymentTransformer.transformResource(DeploymentTransformer.java:122) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.DeploymentTransformer.transform(DeploymentTransformer.java:98) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.DeploymentCreateProcessor.processRecord(DeploymentCreateProcessor.java:87) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.Engine.process(Engine.java:142) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.batchProcessing(ProcessingStateMachine.java:346) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$2(ProcessingStateMachine.java:268) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run(ZeebeTransaction.java:84) ~[zeebe-db-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand(ProcessingStateMachine.java:268) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord(ProcessingStateMachine.java:227) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.readNextRecord(ProcessingStateMachine.java:203) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\r\n\r\n```\r\n\r\n**Environment:**\r\n- OS: Windows\r\n- Zeebe Version: 8.2.3\r\n\r\nIn version 8.1.6 the behavior is correct. Here's stack traces from this version:\r\n\r\n```\r\n2023-04-27 17:52:05.506 [io.camunda.zeebe.broker.transport.commandapi.CommandApiRequestHandler] [Broker-0-zb-actors-1] ERROR\r\n      io.camunda.zeebe.broker.transport - Unexpected error on writing CREATE command\r\njava.lang.IllegalArgumentException: Expected to claim segment of size 1010866, but can't claim more than 65536 bytes.\r\n\tat io.camunda.zeebe.dispatcher.Dispatcher.offer(Dispatcher.java:207) ~[zeebe-dispatcher-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.dispatcher.Dispatcher.claimSingleFragment(Dispatcher.java:143) ~[zeebe-dispatcher-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.logstreams.impl.log.LogStreamWriterImpl.claimLogEntry(LogStreamWriterImpl.java:165) ~[zeebe-logstreams-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.logstreams.impl.log.LogStreamWriterImpl.tryWrite(LogStreamWriterImpl.java:124) ~[zeebe-logstreams-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.broker.transport.commandapi.CommandApiRequestHandler.writeCommand(CommandApiRequestHandler.java:141) ~[zeebe-broker-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.broker.transport.commandapi.CommandApiRequestHandler.handleExecuteCommandRequest(CommandApiRequestHandler.java:114) ~[zeebe-broker-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.broker.transport.commandapi.CommandApiRequestHandler.handle(CommandApiRequestHandler.java:58) ~[zeebe-broker-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.broker.transport.commandapi.CommandApiRequestHandler.handleAsync(CommandApiRequestHandler.java:49) ~[zeebe-broker-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.broker.transport.commandapi.CommandApiRequestHandler.handleAsync(CommandApiRequestHandler.java:27) ~[zeebe-broker-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.broker.transport.AsyncApiRequestHandler.handleRequest(AsyncApiRequestHandler.java:110) ~[zeebe-broker-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.broker.transport.AsyncApiRequestHandler.lambda$onRequest$0(AsyncApiRequestHandler.java:75) ~[zeebe-broker-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n2023-04-27 17:52:05.508 [io.camunda.zeebe.gateway.impl.broker.BrokerRequestManager] [Broker-0-zb-actors-0] ERROR\r\n      io.camunda.zeebe.gateway - Expected to handle gRPC request, but received an internal error from broker: BrokerError{code=INTERNAL_ERROR, message='Failed writing response: java.lang.IllegalArgumentException: Expected to claim segment of size 1010866, but can't claim more than 65536 bytes.'}\r\nio.camunda.zeebe.gateway.cmd.BrokerErrorException: Received error from broker (INTERNAL_ERROR): Failed writing response: java.lang.IllegalArgumentException: Expected to claim segment of size 1010866, but can't claim more than 65536 bytes.\r\n\tat io.camunda.zeebe.gateway.impl.broker.BrokerRequestManager.handleResponse(BrokerRequestManager.java:194) ~[zeebe-gateway-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.gateway.impl.broker.BrokerRequestManager.lambda$sendRequestInternal$2(BrokerRequestManager.java:143) ~[zeebe-gateway-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.future.FutureContinuationRunnable.run(FutureContinuationRunnable.java:28) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\r\n```\r\n\r\n\r\n\r\n\r\n\n\n Zelldon: Hey @sergeylebed thanks for reporting this!\r\n\r\nLooks like a regression @megglos \n Zelldon: @sergeylebed can you confirm that you didn't get an error response in the client? :thinking: \r\n\r\nIt looks like, based on the stacktrace that it just failed on a different place before. \r\n\r\n```\r\nat io.camunda.zeebe.broker.transport.commandapi.CommandApiRequestHandler.writeCommand(CommandApiRequestHandler.java:141) ~[zeebe-broker-8.1.6.jar:8.1.6]\r\n```\r\n\r\nIs at the CommandAPI, when receiving the Command and writing to the dispatcher (before replicating and processing). We replaced the dispatcher in 8.2. Meaning it is now possible to write larger entries, but as you see processing is still not possible. But I think at least I would expect you get an error response. Did you?\n sergeylebed: I got an error on the client but \r\na) it is a generic error about timeout\r\nb) the system becomes inoperable \r\n\r\n```\r\nGrpc: 'DeadlineExceeded' 'Status(StatusCode=\\\"DeadlineExceeded\\\", Detail=\\\"Time out between gateway and broker: Request timed out after PT15S\\\", DebugException=\\\"Grpc.Core.Internal.CoreErrorDetailException: {\\\"created\\\":\\\"@1682543232.485000000\\\",\\\"description\\\":\\\"Error received from peer ipv6:[::1]:26500\\\",\\\"file\\\":\\\"..\\\\..\\\\..\\\\src\\\\core\\\\lib\\\\surface\\\\call.cc\\\",\\\"file_line\\\":953,\\\"grpc_message\\\":\\\"Time out between gateway and broker: Request timed out after PT15S\\\",\\\"grpc_status\\\":4}\\\")'\",\"Exception\":\"Grpc.Core.RpcException: Status(StatusCode=\\\"DeadlineExceeded\\\", Detail=\\\"Time out between gateway and broker: Request timed out after PT15S\\\", DebugException=\\\"Grpc.Core.Internal.CoreErrorDetailException: {\\\"created\\\":\\\"@1682543232.485000000\\\",\\\"description\\\":\\\"Error received from peer ipv6:[::1]:26500\\\",\\\"file\\\":\\\"..\\\\..\\\\..\\\\src\\\\core\\\\lib\\\\surface\\\\call.cc\\\",\\\"file_line\\\":953,\\\"grpc_message\\\":\\\"Time out between gateway and broker: Request timed out after PT15S\\\",\\\"grpc_status\\\":4}\\\")\\r\\n   at Zeebe.Client.Impl.Commands.DeployProcessCommand.Send(Nullable`1 timeout, CancellationToken token)\\r\\n   at  \r\n```\n sergeylebed: The very first error in the log:\r\n\r\n```\r\n2023-04-26 12:49:17.323 [Broker-0-StreamProcessor-1] [Broker-0-zb-actors-1] ERROR\r\n      io.camunda.zeebe.broker.process - Unexpected error while processing resource 'f6c0b39d-8357-40ea-8d79-7e2611a89677.bpmn'\r\nio.camunda.zeebe.stream.api.records.ExceededBatchRecordSizeException: Can't append entry: 'RecordBatchEntry[recordMetadata=RecordMetadata{recordType=EVENT, valueType=PROCESS, intent=CREATED}, key=2251799852104669, sourceIndex=-1, unifiedRecordValue={\"bpmnProcessId\":\"id_f6c0b39d-8357-40ea-8d79-7e2611a89677\",\"version\":1,\"processDefinitionKey\":2251799852104669,\"resourceName\":\"f6c0b39d-8357-40ea-8d79-7e2611a89677.bpmn\",\"checksum\":\"uG1QH8XcklrgFGYZI3HhPg==\",\"resource\":\"PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0idXRmLTgiPz4NCjxicG1uOmRlZmluaXRpb25zIHhtbG5zOmJwbW5kaT0iaHR0cDovL3d3dy5vbWcub3JnL3NwZWMvQlBNTi8yMDEwMDUyNC9ESSIgeG1sbnM6ZGM9Imh0dHA6Ly93d3cub21nLm9yZy9zcGVjL0RELzIwMTAwNTI0L0RDIiB4bWxuczp6ZWViZT0iaHR0cDovL2NhbXVuZGEub3JnL3NjaGVtYS96ZWViZS8xLjAiIHhtbG5zOmRpPSJodHRwOi8vd3d3Lm9tZy5vcmcvc3BlYy9ERC8yMDEwMDUyNC9ESSIgeG1sbnM6eHNpPSJodHRwOi8vd3d3LnczLm9yZy8yMDAxL1hNTFNjaGVtYS1pbnN0YW5jZSIgaWQ9ImlkXzZmNTk2NTlmLWE3OGMtNDMyMy04M2VmLTdlODMwYmJlNTUwNSIgdGFyZ2V0TmFtZXNwYWNlPSJodHRwOi8vYnBtbi5pby9zY2hlbWEvYnBtbiIgZXhwb3J0ZXI9IkNvbmZpcm1pdCBCUE1OIEJ1aWxkZXIiIGV4cG9ydGVyVmVyc2lvbj0iMS4wLjAuMCIgeG1sbnM6YnBtbj0iaHR0cD...' with size: 1010867 this would exceed the maximum batch size. [ currentBatchEntryCount: 0, currentBatchSize: 0]\r\n\tat io.camunda.zeebe.stream.impl.records.RecordBatch.appendRecord(RecordBatch.java:67) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.BufferedProcessingResultBuilder.appendRecordReturnEither(BufferedProcessingResultBuilder.java:62) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.api.ProcessingResultBuilder.appendRecord(ProcessingResultBuilder.java:38) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.streamprocessor.writers.ResultBuilderBackedEventApplyingStateWriter.appendFollowUpEvent(ResultBuilderBackedEventApplyingStateWriter.java:40) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.transformProcessResource(BpmnResourceTransformer.java:162) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.lambda$transformResource$0(BpmnResourceTransformer.java:77) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.util.Either$Right.map(Either.java:355) ~[zeebe-util-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.lambda$transformResource$1(BpmnResourceTransformer.java:75) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.util.Either$Right.flatMap(Either.java:366) ~[zeebe-util-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.transformResource(BpmnResourceTransformer.java:65) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.DeploymentTransformer.transformResource(DeploymentTransformer.java:122) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.DeploymentTransformer.transform(DeploymentTransformer.java:98) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.DeploymentCreateProcessor.processRecord(DeploymentCreateProcessor.java:87) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.Engine.process(Engine.java:142) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.batchProcessing(ProcessingStateMachine.java:346) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$2(ProcessingStateMachine.java:268) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run(ZeebeTransaction.java:84) ~[zeebe-db-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand(ProcessingStateMachine.java:268) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord(ProcessingStateMachine.java:227) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.readNextRecord(ProcessingStateMachine.java:203) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n```\n sergeylebed: In version 8.1.6 the client error is different\r\n\r\n```\r\nStatus(StatusCode=\\\"Internal\\\", Detail=\\\"Unexpected error occurred between gateway and broker (code: INTERNAL_ERROR)\\\", DebugException=\\\"Grpc.Core.Internal.CoreErrorDetailException: {\\\"created\\\":\\\"@1682632476.120000000\\\",\\\"description\\\":\\\"Error received from peer ipv6:[::1]:26500\\\",\\\"file\\\":\\\"..\\\\..\\\\..\\\\src\\\\core\\\\lib\\\\surface\\\\call.cc\\\",\\\"file_line\\\":953,\\\"grpc_message\\\":\\\"Unexpected error occurred between gateway and broker (code: INTERNAL_ERROR)\\\",\\\"grpc_status\\\":13}\\\")\",\"Exception\":\"Grpc.Core.RpcException: Status(StatusCode=\\\"Internal\\\", Detail=\\\"Unexpected error occurred between gateway and broker (code: INTERNAL_ERROR)\\\", DebugException=\\\"Grpc.Core.Internal.CoreErrorDetailException: {\\\"created\\\":\\\"@1682632476.120000000\\\",\\\"description\\\":\\\"Error received from peer ipv6:[::1]:26500\\\",\\\"file\\\":\\\"..\\\\..\\\\..\\\\src\\\\core\\\\lib\\\\surface\\\\call.cc\\\",\\\"file_line\\\":953,\\\"grpc_message\\\":\\\"Unexpected error occurred between gateway and broker (code: INTERNAL_ERROR)\\\",\\\"grpc_status\\\":13}\\\")\\r\\n   at Zeebe.Client.Impl.Commands.DeployProcessCommand.Send(Nullable`1 timeout, CancellationToken token)\r\n```\n npepinpe: Under the assumption that doing so bricks your partition unrecoverably, we'll prioritize it as a blocker/critical issue.\n deepthidevaki: I was able to reproduce this by setting maxMessageSize to 1MB in the test `CreateDeploymentTest::shouldRejectDeployIfResourceIsTooLarge()` https://github.com/camunda/zeebe/blob/main/qa/integration-tests/src/test/java/io/camunda/zeebe/it/client/command/CreateDeploymentTest.java#L28 \r\n\r\nI see multiple issues here:\r\n\r\n1. CommandAPI is not rejecting requests which exceeds maxMessageSize. This result in oversized command to be written to the log stream.\r\n2. When engine cannot write the follow up event because it is above batch size limit, it attempts to write a rejection record which contains the whole command. Since the command is already above batch size it cannot write rejection to the log stream. This result in a loop in the processing machine where it tries to handle this error endlessly.\r\n\r\nFor fixing this issue, I propose to reject the request in CommandAPI. So it is never written to the logstream. However, I would also suggest to revisit if we really have to write the whole command in the rejection record.\n sergeylebed: It does not seem to be fixed in 8.2.5. Do you think it can be merged into the latest versions?\n deepthidevaki: @sergeylebed The fix will be included in 8.2.6.",
    "title": "8.2.3 Degradation: Creating an oversized BPMN causes unrecoverable failure ",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12509",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nMessageTTL checking fails with deserialization errors.\r\n\r\n```\r\njava.lang.RuntimeException: Could not deserialize object [MessageRecord]. Deserialization stuck at offset 24 of length 69\r\n    at io.camunda.zeebe.msgpack.UnpackedObject.wrap(UnpackedObject.java:38)\r\n    at io.camunda.zeebe.engine.api.records.RecordBatchEntry.createEntry(RecordBatchEntry.java:73)\r\n    at io.camunda.zeebe.engine.api.records.RecordBatch.appendRecord(RecordBatch.java:48)\r\n    at io.camunda.zeebe.streamprocessor.BufferedTaskResultBuilder.appendCommandRecord(BufferedTaskResultBuilder.java:47)\r\n    at io.camunda.zeebe.engine.processing.message.MessageTimeToLiveChecker.lambda$execute$0(MessageTimeToLiveChecker.java:90)\r\n```\r\n\r\nThis will fail the processing actor and prevent processing on this partition. When the (experimental) feature flag `enableMessageTtlCheckerAsync` is used, a different actor fails and only prevents further MessageTTL checking but not processing.\r\n\r\nThis is a regression, introduced in https://github.com/camunda/zeebe/commit/e1a6cae69c17325fc71a8ee92022a70d969bd0da\r\nIt affects versions >= 8.1.9 and >= 8.2.0.\r\n\r\n\r\n**To Reproduce**\r\n\r\nRun a broker that is leader for multiple partitions. When messages from two different partitions are expired at the same time, there is unsafe concurrent access to the writer of a shared record value. \r\n\r\n**Expected behavior**\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.lang.RuntimeException: Could not deserialize object [MessageRecord]. Deserialization stuck at offset 24 of length 69\r\n    at io.camunda.zeebe.msgpack.UnpackedObject.wrap(UnpackedObject.java:38)\r\n    at io.camunda.zeebe.engine.api.records.RecordBatchEntry.createEntry(RecordBatchEntry.java:73)\r\n    at io.camunda.zeebe.engine.api.records.RecordBatch.appendRecord(RecordBatch.java:48)\r\n    at io.camunda.zeebe.streamprocessor.BufferedTaskResultBuilder.appendCommandRecord(BufferedTaskResultBuilder.java:47)\r\n    at io.camunda.zeebe.engine.processing.message.MessageTimeToLiveChecker.lambda$execute$0(MessageTimeToLiveChecker.java:90)\r\n    at io.camunda.zeebe.engine.state.message.DbMessageState.lambda$visitMessagesWithDeadlineBeforeTimestamp$2(DbMessageState.java:358)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.visit(TransactionalColumnFamily.java:390)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.lambda$forEachInPrefix$19(TransactionalColumnFamily.java:369)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.ColumnFamilyContext.withPrefixKey(ColumnFamilyContext.java:112)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.forEachInPrefix(TransactionalColumnFamily.java:353)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.lambda$whileTrue$8(TransactionalColumnFamily.java:174)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.lambda$ensureInOpenTransaction$18(TransactionalColumnFamily.java:308)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.DefaultTransactionContext.runInNewTransaction(DefaultTransactionContext.java:61)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.DefaultTransactionContext.runInTransaction(DefaultTransactionContext.java:33)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.ensureInOpenTransaction(TransactionalColumnFamily.java:307)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.whileTrue(TransactionalColumnFamily.java:174)\r\n    at io.camunda.zeebe.engine.state.message.DbMessageState.visitMessagesWithDeadlineBeforeTimestamp(DbMessageState.java:351)\r\n    at io.camunda.zeebe.engine.processing.message.MessageTimeToLiveChecker.execute(MessageTimeToLiveChecker.java:76)\r\n    at io.camunda.zeebe.streamprocessor.ProcessingScheduleServiceImpl.lambda$toRunnable$6(ProcessingScheduleServiceImpl.java:137)\r\n    at io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94)\r\n    at io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45)\r\n    at io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119)\r\n    at io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106)\r\n    at io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87)\r\n    at io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198)\r\nCaused by: io.camunda.zeebe.msgpack.spec.MsgpackReaderException: Unable to determine string type, found unknown header byte 0x00 at reader offset 23\r\n    at io.camunda.zeebe.msgpack.spec.MsgPackReader.exceptionOnUnknownHeader(MsgPackReader.java:474)\r\n    at io.camunda.zeebe.msgpack.spec.MsgPackReader.readStringLength(MsgPackReader.java:140)\r\n    at io.camunda.zeebe.msgpack.value.StringValue.read(StringValue.java:96)\r\n    at io.camunda.zeebe.msgpack.value.ObjectValue.read(ObjectValue.java:91)\r\n    at io.camunda.zeebe.msgpack.UnpackedObject.wrap(UnpackedObject.java:32)\r\n    ... 24 more\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- OS: <!-- [e.g. Linux] -->\r\n- Zeebe Version: <!-- [e.g. 0.20.0] -->\r\n- Configuration: <!-- [e.g. exporters etc.] -->\r\n- \r\nRelates to https://jira.camunda.com/browse/SUPPORT-16711\r\n\n",
    "title": "MessageTTL checking fails with deserialization errors",
    "releaseNoteText": "**Consequence:**\n\nMessageTTL checking failed with deserialization errors. This resulted in a `RuntimeException` when trying to deserialize the `MessageRecord` object, causing the processing actor to fail. As a consequence, processing on the affected partition was also prevented. This issue impacted versions 8.1.9 and 8.2.0 onwards.\n\n**Cause:**\n\nThe issue was caused by unsafe concurrent access to the writer of a shared record value when messages from different partitions expired at the same time. This concurrent access led to deserialization errors and the failure of the processing actor.\n\n**Fix:**\n\nThe fix for this issue was implemented in commit [e1a6cae69c17325fc71a8ee92022a70d969bd0da](https://github.com/camunda/zeebe/commit/e1a6cae69c17325fc71a8ee92022a70d969bd0da). With this fix, the issue of deserialization errors during MessageTTL checking was resolved. \n\n**Result:**\n\nAfter applying the fix, MessageTTL checking is now performed without encountering any deserialization errors. The processing actor no longer fails and processing on the affected partition can proceed as expected. This fix is available in versions after 8.1.9 and 8.2.0."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12433",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Description**\r\nWe are configuring brokers with the S3 backup properties as follows:\r\n- ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLEBACKUP: true\r\n- ZEEBE_BROKER_DATA_BACKUP_STORE: S3\r\n- ZEEBE_BROKER_DATA_BACKUP_S3_BUCKETNAME: <s3_bucket_name>\r\n- ZEEBE_BROKER_DATA_BACKUP_S3_REGION: us-east-1\r\n- ZEEBE_BROKER_DATA_BACKUP_S3_ENDPOINT: \"https://s3.us-east-1.amazonaws.com\"\r\n\r\nWe don't supply credentials through environment variables as they can be pulled from aws credentials provider chain. These properties worked on old version 8.1.9 but we got errors when switching to latest version 8.2.1 and broker cannot start due to the error:\r\n\r\n> \r\n> 2023-04-14T11:21:51.313-04:00 | org.springframework.boot.SpringApplication - Application run failed\r\n> 2023-04-14T11:21:51.313-04:00 | java.lang.IllegalStateException: Failed to execute CommandLineRunner\r\n> 2023-04-14T11:21:51.313-04:00 | at org.springframework.boot.SpringApplication.callRunner(SpringApplication.java:772) [spring-boot-3.0.5.jar:3.0.5]\r\n> 2023-04-14T11:21:51.313-04:00 | at org.springframework.boot.SpringApplication.callRunners(SpringApplication.java:753) [spring-boot-3.0.5.jar:3.0.5]\r\n> 2023-04-14T11:21:51.313-04:00 | at org.springframework.boot.SpringApplication.run(SpringApplication.java:317) [spring-boot-3.0.5.jar:3.0.5]\r\n> 2023-04-14T11:21:51.313-04:00 | at io.camunda.zeebe.broker.StandaloneBroker.main(StandaloneBroker.java:82) [camunda-zeebe-8.2.1.jar:8.2.1]\r\n> 2023-04-14T11:21:51.313-04:00 | Caused by: io.camunda.zeebe.broker.system.InvalidConfigurationException: Failed configuring backup store S3\r\n> 2023-04-14T11:21:51.313-04:00 | at io.camunda.zeebe.broker.system.SystemContext.validateBackupCfg(SystemContext.java:132) ~[zeebe-broker-8.2.1.jar:8.2.1]\r\n> 2023-04-14T11:21:51.313-04:00 | at io.camunda.zeebe.broker.system.SystemContext.validateDataConfig(SystemContext.java:116) ~[zeebe-broker-8.2.1.jar:8.2.1]\r\n> 2023-04-14T11:21:51.313-04:00 | at io.camunda.zeebe.broker.system.SystemContext.validateConfiguration(SystemContext.java:71) ~[zeebe-broker-8.2.1.jar:8.2.1]\r\n> 2023-04-14T11:21:51.313-04:00 | at io.camunda.zeebe.broker.system.SystemContext.initSystemContext(SystemContext.java:60) ~[zeebe-broker-8.2.1.jar:8.2.1]\r\n> 2023-04-14T11:21:51.313-04:00 | at io.camunda.zeebe.broker.system.SystemContext.<init>(SystemContext.java:56) ~[zeebe-broker-8.2.1.jar:8.2.1]\r\n> 2023-04-14T11:21:51.313-04:00 | at io.camunda.zeebe.broker.StandaloneBroker.run(StandaloneBroker.java:87) ~[camunda-zeebe-8.2.1.jar:8.2.1]\r\n> 2023-04-14T11:21:51.313-04:00 | at org.springframework.boot.SpringApplication.callRunner(SpringApplication.java:769) ~[spring-boot-3.0.5.jar:3.0.5]\r\n> 2023-04-14T11:21:51.313-04:00 | ... 3 more\r\n> 2023-04-14T11:21:51.313-04:00 | Caused by: java.lang.NullPointerException: Access key ID cannot be blank.\r\n\r\nIt looks like io.camunda.zeebe.backup.s3.S3BackupStore.buildClient still calls AwsBasicCredentials.create(credentials.accessKey(), credentials.secretKey() even we don't pass in any credentials. It might be related to this commit: https://github.com/camunda/zeebe/commit/dfd3b9e1034365b3fc1859d5e35353826e1222b3#diff-1b91d4d4e6875c0169ea56f9c35382f5f194a1af782db4879a26971fe32e6daeL72-L74\r\nWe also noticed that none of your unit or integration tests are checking for cases where ACCESS_KEY, SECRET_KEY are not passed. \r\n\r\nCould you please look into this issue? Thank you!\r\n\n\n oleschoenburg: Thanks for reporting @NingyuanZhang, you are right that https://github.com/camunda/zeebe/commit/dfd3b9e1034365b3fc1859d5e35353826e1222b3#diff-1b91d4d4e6875c0169ea56f9c35382f5f194a1af782db4879a26971fe32e6daeL72-L74 broke this use case.\r\n\r\nI'll provide a fix for this and see if we can test this too.",
    "title": "Broker cannot start with S3 accessKey and secretKey not supplied ",
    "releaseNoteText": "**Consequence:**\nThe broker cannot start when S3 accessKey and secretKey are not supplied. This results in an error during startup and the broker fails to execute.\n\n**Cause:**\nThe issue stemmed from a recent commit that did not handle the case where ACCESS_KEY and SECRET_KEY were not passed. The method `io.camunda.zeebe.backup.s3.S3BackupStore.buildClient` still called `AwsBasicCredentials.create(credentials.accessKey(), credentials.secretKey())` even when no credentials were provided. Additionally, none of the unit or integration tests checked for this scenario.\n\n**Fix:**\nA fix has been implemented to handle the case where accessKey and secretKey are not supplied. The method `io.camunda.zeebe.backup.s3.S3BackupStore.buildClient` now verifies if the credentials are present before calling `AwsBasicCredentials.create()`.\n\n**Result:**\nWith the fix in place, the broker now starts successfully even when S3 accessKey and secretKey are not provided. Users can configure the S3 backup properties without needing to supply the credentials through environment variables."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12374",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nA Zeebe broker is crash looping. The broker tries to start up but failed with the following error message. \r\n\r\n```\r\nio.camunda.zeebe.journal.CorruptedJournalException: Expected to read the version byte from segment 'raft-partition-partition-1-1.log' but got EOF instead\r\n```\r\n\r\nThe broker was part of a cluster with three brokers. The other brokers were healthy and continued processing. \r\n\r\nTo mitigate the issue, we did a fresh restart of the broker. We removed all data and restarted the broker. After the restart, the broker was healthy again and joined the cluster.\r\n\r\n**To Reproduce**\r\n\r\nUnknown. \r\n\r\nThe broker was forced to shut down (`Shutdown was called with context: ...`). The broker created a new snapshot 30 seconds before. The log contains no warnings or suspicious behavior before or after the shutdown.\r\n\r\n_EDIT:_\r\n\r\nIt seems `zeebe-2` was restarted while receiving a snapshot and resetting the log:\r\n```\r\nINFO 2023-04-06T15:37:07.317296306Z [jsonPayload.context.partitionId: 1] [resource.labels.containerName: zeebe] RaftServer{raft-partition-partition-1}{role=FOLLOWER} - Started receiving new snapshot FileBasedReceivedSnapshot{directory=/usr/local/zeebe/data/raft-partition/partitions/1/pending/14080296-246-15599729-15599732-1, snapshotStore=Broker-2-SnapshotStore-1, metadata=FileBasedSnapshotId{index=14080296, term=246, processedPosition=15599729, exporterPosition=15599732}} from 1\r\nINFO 2023-04-06T15:37:07.490944632Z [jsonPayload.context.partitionId: 1] [resource.labels.containerName: zeebe] RaftServer{raft-partition-partition-1}{role=FOLLOWER} - Delete existing log (lastIndex '14080107') and replace with received snapshot (index '14080296'). First entry in the log will be at index 14080297\r\nERROR 2023-04-06T15:39:35.994311846Z [resource.labels.containerName: zeebe] + export SPRING_CONFIG_LOCATION=classpath:/,file:./config/zeebe.cfg.yaml\r\n```\r\nThese are the last logs from that broker, and immediately it was restarted.\r\n\r\nMost likely, the log segment on the disk is an intermediate state where the previous segments have been deleted, the new one is only partially created - which is detected as corruption after the restart. \r\n\r\n**Expected behavior**\r\n\r\nThe broker can handle this corruption failure. For example, by removing (or archiving) the corrupted data and fetching the latest data from the cluster.\r\n\r\n**Log/Stacktrace**\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.util.concurrent.ExecutionException: Startup failed in the following steps: [Partition Manager]. See suppressed exceptions for details.\r\n\tat io.camunda.zeebe.scheduler.future.CompletableActorFuture.get(CompletableActorFuture.java:142) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.future.CompletableActorFuture.get(CompletableActorFuture.java:109) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.FutureUtil.join(FutureUtil.java:21) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.future.CompletableActorFuture.join(CompletableActorFuture.java:198) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.broker.Broker.internalStart(Broker.java:101) ~[zeebe-broker-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.util.LogUtil.doWithMDC(LogUtil.java:23) ~[zeebe-util-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.broker.Broker.start(Broker.java:83) ~[zeebe-broker-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.broker.StandaloneBroker.run(StandaloneBroker.java:92) ~[camunda-zeebe-8.1.3.jar:8.1.3]\r\n\tat org.springframework.boot.SpringApplication.callRunner(SpringApplication.java:771) ~[spring-boot-2.7.4.jar:2.7.4]\r\n\tat org.springframework.boot.SpringApplication.callRunners(SpringApplication.java:755) ~[spring-boot-2.7.4.jar:2.7.4]\r\n\tat org.springframework.boot.SpringApplication.run(SpringApplication.java:315) ~[spring-boot-2.7.4.jar:2.7.4]\r\n\tat io.camunda.zeebe.broker.StandaloneBroker.main(StandaloneBroker.java:82) ~[camunda-zeebe-8.1.3.jar:8.1.3]\r\nCaused by: io.camunda.zeebe.scheduler.startup.StartupProcessException: Startup failed in the following steps: [Partition Manager]. See suppressed exceptions for details.\r\n\tat io.camunda.zeebe.scheduler.startup.StartupProcess.aggregateExceptionsSynchronized(StartupProcess.java:282) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.startup.StartupProcess.completeStartupFutureExceptionallySynchronized(StartupProcess.java:183) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.startup.StartupProcess.lambda$proceedWithStartupSynchronized$3(StartupProcess.java:167) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.future.FutureContinuationRunnable.run(FutureContinuationRunnable.java:33) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tSuppressed: io.camunda.zeebe.scheduler.startup.StartupProcessStepException: Bootstrap step Partition Manager failed\r\n\t\tat io.camunda.zeebe.scheduler.startup.StartupProcess.completeStartupFutureExceptionallySynchronized(StartupProcess.java:185) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.scheduler.startup.StartupProcess.lambda$proceedWithStartupSynchronized$3(StartupProcess.java:167) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.scheduler.future.FutureContinuationRunnable.run(FutureContinuationRunnable.java:33) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tCaused by: java.util.concurrent.CompletionException: io.camunda.zeebe.journal.CorruptedJournalException: Expected to read the version byte from segment 'raft-partition-partition-1-1.log' but got EOF instead.\r\n\t\tat java.util.concurrent.CompletableFuture.encodeThrowable(Unknown Source) ~[?:?]\r\n\t\tat java.util.concurrent.CompletableFuture.uniApplyNow(Unknown Source) ~[?:?]\r\n\t\tat java.util.concurrent.CompletableFuture.uniApplyStage(Unknown Source) ~[?:?]\r\n\t\tat java.util.concurrent.CompletableFuture.thenApply(Unknown Source) ~[?:?]\r\n\t\tat io.atomix.raft.partition.RaftPartition.open(RaftPartition.java:104) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.partition.RaftPartitionGroup.lambda$join$7(RaftPartitionGroup.java:201) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat java.util.stream.ReferencePipeline$3$1.accept(Unknown Source) ~[?:?]\r\n\t\tat java.util.HashMap$KeySpliterator.forEachRemaining(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.AbstractPipeline.copyInto(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.AbstractPipeline.evaluate(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.AbstractPipeline.evaluateToArrayNode(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.ReferencePipeline.toArray(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.ReferencePipeline.toArray(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.ReferencePipeline.toList(Unknown Source) ~[?:?]\r\n\t\tat io.atomix.raft.partition.RaftPartitionGroup.join(RaftPartitionGroup.java:203) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.primitive.partition.impl.DefaultPartitionService.start(DefaultPartitionService.java:63) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.broker.partitioning.PartitionManagerImpl.start(PartitionManagerImpl.java:125) ~[zeebe-broker-8.1.3.jar:8.1.3]\r\n\t\tat java.util.concurrent.CompletableFuture$AsyncSupply.run(Unknown Source) ~[?:?]\r\n\t\tat java.lang.Thread.run(Unknown Source) ~[?:?]\r\n\tCaused by: io.camunda.zeebe.journal.CorruptedJournalException: Expected to read the version byte from segment 'raft-partition-partition-1-1.log' but got EOF instead.\r\n\t\tat io.camunda.zeebe.journal.file.SegmentLoader.readVersion(SegmentLoader.java:173) ~[zeebe-journal-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.journal.file.SegmentLoader.readDescriptor(SegmentLoader.java:128) ~[zeebe-journal-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.journal.file.SegmentLoader.loadExistingSegment(SegmentLoader.java:87) ~[zeebe-journal-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.journal.file.SegmentsManager.loadSegments(SegmentsManager.java:292) ~[zeebe-journal-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.journal.file.SegmentsManager.open(SegmentsManager.java:238) ~[zeebe-journal-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.journal.file.SegmentedJournal.<init>(SegmentedJournal.java:61) ~[zeebe-journal-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.journal.file.SegmentedJournalBuilder.build(SegmentedJournalBuilder.java:158) ~[zeebe-journal-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.storage.log.RaftLogBuilder.build(RaftLogBuilder.java:136) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.storage.RaftStorage.openLog(RaftStorage.java:194) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.impl.RaftContext.<init>(RaftContext.java:194) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.impl.DefaultRaftServer$Builder.build(DefaultRaftServer.java:258) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.impl.DefaultRaftServer$Builder.build(DefaultRaftServer.java:232) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.partition.impl.RaftPartitionServer.buildServer(RaftPartitionServer.java:189) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.partition.impl.RaftPartitionServer.initServer(RaftPartitionServer.java:155) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.partition.impl.RaftPartitionServer.start(RaftPartitionServer.java:114) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.partition.RaftPartition.open(RaftPartition.java:104) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.partition.RaftPartitionGroup.lambda$join$7(RaftPartitionGroup.java:201) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat java.util.stream.ReferencePipeline$3$1.accept(Unknown Source) ~[?:?]\r\n\t\tat java.util.HashMap$KeySpliterator.forEachRemaining(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.AbstractPipeline.copyInto(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.AbstractPipeline.evaluate(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.AbstractPipeline.evaluateToArrayNode(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.ReferencePipeline.toArray(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.ReferencePipeline.toArray(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.ReferencePipeline.toList(Unknown Source) ~[?:?]\r\n\t\tat io.atomix.raft.partition.RaftPartitionGroup.join(RaftPartitionGroup.java:203) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.primitive.partition.impl.DefaultPartitionService.start(DefaultPartitionService.java:63) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.broker.partitioning.PartitionManagerImpl.start(PartitionManagerImpl.java:125) ~[zeebe-broker-8.1.3.jar:8.1.3]\r\n\t\tat java.util.concurrent.CompletableFuture$AsyncSupply.run(Unknown Source) ~[?:?]\r\n\t\tat java.lang.Thread.run(Unknown Source) ~[?:?]\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\nSee more in the downloaded [log file](https://drive.google.com/file/d/1V89vBHeXytmm_lviRhMjojBhdeoz7mJs/view?usp=share_link).\r\n\r\n**Environment:**\r\n- OS: Camunda SaaS\r\n- Zeebe Version: `8.1.3`\r\n- Configuration: `prod-worker-3`\r\n\n\n SeanAda: I have the same exeption on a self hosted Zeebe (Version 8.1.4).\r\nThe file `raft-partition-partition-1-1.log` is empty.\r\n\r\nIs there any chance to recover the data or copy it from other nodes?\n saig0: > Is there any chance to recover the data or copy it from other nodes?\r\n\r\n@SeanAda it depends on your setup. With a replication factor of 3, there is a good chance that the other two nodes takes over and continue processing. \r\n\r\nIn this case, you can mitigate the issue by doing a fresh restart of the broker. So, remove all data and restart the broker. After the restart, the broker should become healthy again and join the cluster. The broker should receive the missing data from the other nodes of the cluster.\n deepthidevaki: I had a quick look at the logs of the affected cluster. It seems `zeebe-2` was restarted while receiving a snapshot and resetting the log:\r\n```\r\nINFO 2023-04-06T15:37:07.317296306Z [jsonPayload.context.partitionId: 1] [resource.labels.containerName: zeebe] RaftServer{raft-partition-partition-1}{role=FOLLOWER} - Started receiving new snapshot FileBasedReceivedSnapshot{directory=/usr/local/zeebe/data/raft-partition/partitions/1/pending/14080296-246-15599729-15599732-1, snapshotStore=Broker-2-SnapshotStore-1, metadata=FileBasedSnapshotId{index=14080296, term=246, processedPosition=15599729, exporterPosition=15599732}} from 1\r\nINFO 2023-04-06T15:37:07.490944632Z [jsonPayload.context.partitionId: 1] [resource.labels.containerName: zeebe] RaftServer{raft-partition-partition-1}{role=FOLLOWER} - Delete existing log (lastIndex '14080107') and replace with received snapshot (index '14080296'). First entry in the log will be at index 14080297\r\nERROR 2023-04-06T15:39:35.994311846Z [resource.labels.containerName: zeebe] + export SPRING_CONFIG_LOCATION=classpath:/,file:./config/zeebe.cfg.yaml\r\n```\r\nThese are the last logs from that broker, and immediately it was restarted.\r\n\r\nMost likely, the log segment on the disk is an intermediate state where the previous segments have been deleted, the new one is only partially created - which is detected as corruption after the restart. We should handle this case better as it is not an actual corruption.\r\n\n SeanAda: Removing all the data and restarting the broker did work.\r\n\r\nThank you!\n saig0: @deepthidevaki great finding. :rocket: I added your comment to the description. \n codingman1990: But our the other two node become unhealthy too.How should i do something to recover it?\n npepinpe: Hi @codingman1990, what is the state of your two nodes? Are all brokers reporting the same error for the same partition?\n codingman1990: camunda-zeebe-0 and camunda-zeebe-1 be unhealthy.And camunda-zeebe-2 can't restart.\r\nBut camunda-zeebe-1 zeebe.log like this:\r\n\r\n```\r\n2023-04-24 15:44:14.259 [] [raft-server-0-raft-partition-partition-3] WARN \r\n      io.atomix.raft.roles.FollowerRole - RaftServer{raft-partition-partition-3}{role=FOLLOWER} - Poll request to 2 failed: java.net.ConnectException: Expected to send a message with subject 'raft-partition-partition-3-poll' to member '2', but member is not known. Known members are '[Member{id=1, address=camunda-zeebe-1.camunda-zeebe.default.svc:26502, properties={brokerInfo=EADJAAAAAwABAAAAAwAAAAMAAAADAAAAAAABCgAAAGNvbW1hbmRBcGkvAAAAY2FtdW5kYS16ZWViZS0xLmNhbXVuZGEtemVlYmUuZGVmYXVsdC5zdmM6MjY1MDEFAAMBAAAAAQIAAAABAwAAAAEMAAAFAAAAOC4xLjkFAAMBAAAAAAIAAAAAAwAAAAA=}}, Member{id=camunda-zeebe-gateway-7db56f85d6-v2jb7, address=172.20.8.149:26502, properties={event-service-topics-subscribed=KIIDAGpvYnNBdmFpbGFibOU=}}, Member{id=camunda-zeebe-gateway-7db56f85d6-z69qr, address=172.20.3.169:26502, properties={event-service-topics-subscribed=KIIDAGpvYnNBdmFpbGFibOU=}}, Member{id=0, address=camunda-zeebe-0.camunda-zeebe.default.svc:26502, properties={brokerInfo=EADJAAAAAwAAAAAAAwAAAAMAAAADAAAAAAABCgAAAGNvbW1hbmRBcGkvAAAAY2FtdW5kYS16ZWViZS0wLmNhbXVuZGEtemVlYmUuZGVmYXVsdC5zdmM6MjY1MDEFAAMBAAAAAQIAAAABAwAAAAEMAAAFAAAAOC4xLjkFAAMBAAAAAQIAAAABAwAAAAE=}}]'\r\n```\r\n\r\nAnd one zeebe_error6.log like this:\r\n\r\n```\r\n#\r\n# A fatal error has been detected by the Java Runtime Environment:\r\n#\r\n#  SIGBUS (0x7) at pc=0x00007f38d75053cc, pid=6, tid=64\r\n#\r\n# JRE version: OpenJDK Runtime Environment Temurin-17.0.6+10 (17.0.6+10) (build 17.0.6+10)\r\n# Java VM: OpenJDK 64-Bit Server VM Temurin-17.0.6+10 (17.0.6+10, mixed mode, sharing, tiered, compressed oops, compressed class ptrs, g1 gc, linux-amd64)\r\n# Problematic frame:\r\n# v  ~StubRoutines::updateBytesCRC32C\r\n#\r\n# Core dump will be written. Default location: /usr/local/zeebe/core.6\r\n#\r\n# If you would like to submit a bug report, please visit:\r\n#   https://github.com/adoptium/adoptium-support/issues\r\n#\r\n\r\n---------------  S U M M A R Y ------------\r\n\r\nCommand Line: -XX:+HeapDumpOnOutOfMemoryError -XX:InitialHeapSize=2147483648 -XX:MaxHeapSize=2147483648 -XX:HeapDumpPath=/usr/local/zeebe/data -XX:ErrorFile=/usr/local/zeebe/data/zeebe_error%p.log -XX:+ExitOnOutOfMemoryError -Xms128m -XX:+ExitOnOutOfMemoryError -Dfile.encoding=UTF-8 -Dapp.name=broker -Dapp.pid=6 -Dapp.repo=/usr/local/zeebe/lib -Dapp.home=/usr/local/zeebe -Dbasedir=/usr/local/zeebe io.camunda.zeebe.broker.StandaloneBroker\r\n\r\nHost: Intel(R) Xeon(R) CPU E5-2682 v4 @ 2.50GHz, 8 cores, 7G, Ubuntu 20.04.5 LTS\r\nTime: Mon Apr 24 05:25:16 2023 CST elapsed time: 151584.214926 seconds (1d 18h 6m 24s)\r\n\r\n---------------  T H R E A D  ---------------\r\n\r\nCurrent thread (0x00007f38080ce950):  JavaThread \"raft-server-0-raft-partition-partition-2\" [_thread_in_Java, id=64, stack(0x00007f38746dd000,0x00007f38747de000)]\r\n\r\nStack: [0x00007f38746dd000,0x00007f38747de000],  sp=0x00007f38747dc060,  free space=1020k\r\nNative frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)\r\nv  ~StubRoutines::updateBytesCRC32C\r\n\r\n\r\nsiginfo: si_signo: 7 (SIGBUS), si_code: 2 (BUS_ADRERR), si_addr: 0x00007f3744000011\r\n```\r\ncamunda-zeebe-1 zeebe.log no log content has been for 2 hours.\n codingman1990: ![image](https://user-images.githubusercontent.com/12196018/233933546-e37706d0-17a9-4573-b44c-98f7a99849cc.png)\r\nThe grafana page shows we have last camunda-zeebe-2 promethus data.\n codingman1990: After 1 day,camunda be unhealthy again.\r\n![image](https://user-images.githubusercontent.com/12196018/234449827-d98659b8-4f49-4947-adb9-afac4a8bcc83.png)\r\nHere is the  zeebe log.\r\n`2023-04-26 09:23:16.742 [] [raft-server-0-raft-partition-partition-1] ERROR\r\n      io.atomix.utils.concurrent.SingleThreadContext - Shutting down because we can't recover from JVM errors. Consider restarting this broker if it is a temporary issue.\r\njava.lang.InternalError: a fault occurred in a recent unsafe memory access operation in compiled Java code\r\n        at io.camunda.zeebe.journal.file.MessageHeaderEncoder.blockLength(MessageHeaderEncoder.java:85) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.camunda.zeebe.journal.file.SegmentDescriptorEncoder.wrapAndApplyHeader(SegmentDescriptorEncoder.java:80) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.camunda.zeebe.journal.file.SegmentDescriptor.copyTo(SegmentDescriptor.java:289) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.camunda.zeebe.journal.file.UninitializedSegment.initializeForUse(UninitializedSegment.java:35) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.camunda.zeebe.journal.file.SegmentsManager.getNextSegment(SegmentsManager.java:133) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.camunda.zeebe.journal.file.SegmentedJournal.getNextSegment(SegmentedJournal.java:203) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.camunda.zeebe.journal.file.SegmentedJournalWriter.createNewSegment(SegmentedJournalWriter.java:110) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.prometheus.client.Histogram$Child.timeWithExemplar(Histogram.java:273) ~[simpleclient-0.16.0.jar:?]\r\n        at io.prometheus.client.Histogram$Child.time(Histogram.java:260) ~[simpleclient-0.16.0.jar:?]\r\n        at io.camunda.zeebe.journal.file.JournalMetrics.observeSegmentCreation(JournalMetrics.java:71) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.camunda.zeebe.journal.file.SegmentedJournalWriter.append(SegmentedJournalWriter.java:58) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.camunda.zeebe.journal.file.SegmentedJournal.append(SegmentedJournal.java:76) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.atomix.raft.storage.log.RaftLog.append(RaftLog.java:139) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n        at io.atomix.raft.roles.LeaderRole.tryToAppend(LeaderRole.java:515) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n        at io.atomix.raft.roles.LeaderRole.append(LeaderRole.java:487) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n        at io.atomix.raft.roles.LeaderRole.safeAppendEntry(LeaderRole.java:564) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n        at io.atomix.raft.roles.LeaderRole.lambda$appendEntry$8(LeaderRole.java:541) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n        at io.atomix.utils.concurrent.SingleThreadContext$WrappedRunnable.run(SingleThreadContext.java:171) ~[zeebe-atomix-utils-8.1.9.jar:8.1.9]\r\n        at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) ~[?:?]\r\n        at java.util.concurrent.FutureTask.run(Unknown Source) ~[?:?]\r\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source) ~[?:?]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]\r\n        at java.lang.Thread.run(Unknown Source) ~[?:?]\r\n2023-04-26 09:23:16.758 [Broker-0-Startup] [Broker-0-zb-actors-0] INFO \r\n      io.camunda.zeebe.broker.system - Shutdown Admin API\r\n2023-04-26 09:23:16.759 [Broker-0-Startup] [Broker-0-zb-actors-0] INFO \r\n      io.camunda.zeebe.broker.system - Shutdown Partition Manager`\r\nCan anyone help solve this problem.\n npepinpe: I will first focus on the original issue, as I think your issue @codingman1990 is not quite the same. It seems like you have some memory issues (crashing due to a SIGBUS), so it looks like an issue with the underlying storage medium and our usage of mmap. OTOH, I would say, if you're using network storage that would explain the increased likelihood, and I would advise against using network storage (e.g. NFS, Samba) with Zeebe for now.\r\n\r\nGoing back to the original issue. It's unclear what the solution would be. In this state, we've already updated the lastFlushedIndex to be the snapshot's index, but we possibly have no snapshot. So we have no snapshot, we have a segment with possibly no descriptor or a partial descriptor, and we have a lastFlushedIndex which is likely quite high.\r\n\r\nOne case with no descriptor, since we pre-allocate the segments, we could simply check if the descriptor portion is all zeros. Then we know we never wrote anything in there, so it's not really corrupted.\r\n\r\nThat leaves us with a partial descriptor case. How do we distinguish partially written descriptor from a corrupted one? Right now, we rely on the last position of the previous segment to determine this, but what if we have no segments?\r\n\r\nOne option would be to initially write a magic byte signifying that we're about to write the descriptor. Flush. Then write the descriptor. Flush.\r\n\r\nSo we have the following lifecycle: \r\n\r\n- Descriptor is all zero (including initial magic byte) => no descriptor, can be deleted\r\n- First byte is the magic WILL_WRITE_DESCRIPTOR byte => no descriptor, can be deleted\r\n- Descriptor is present but corrupted => corruption, cannot be deleted automatically\r\n\r\nOf course, this solution is prone to bitrot, where some bit flip turns the first byte of the descriptor into the WILL_WRITE_DESCRIPTOR magic byte and we erroneously detect it as safe to delete.\r\n\r\nOne other option is piggyback on top of our meta store, and also keep track of the last initialized segment. So whenever we write the descriptor, our segment is now ready for use, and we update the meta store. Additionally, on load, we may need to update the last initialized segment with the latest value in case we pick up a segment which was initialized but we had crashed/shutdown before updating the meta store.\r\n\r\nSo if we have an initialized segment index of 5, and we cannot read the descriptor of 5, then we know this is real corruption. If we have an initialized segment index of 5, and we cannot read the descriptor of 6, then we know this is segment was never actually used.\n npepinpe: So, it came to my mind that this is an over-engineered solution. A simple fix would be to reset the last flushed index to a null value whenever the log is reset, but before the next segment is created.\r\n\r\nThe error only affects the case where we have no previous segment, i.e. when we've reset the log. In all other cases - e.g. a new segment is created but we crash before writing its descriptor - we have a previous segment and a lastFlushedIndex to compare to. When a new segment is created, it will not be written to until initialized, so we can use the lastFlushedIndex to distinguish that.\n saig0: :information_source: I saw a similar error message that seems to be related: \r\n\r\n```\r\njava.util.concurrent.CompletionException: io.camunda.zeebe.journal.CorruptedJournalException: Couldn't read or recognize version of segment 'raft-partition-partition-1-1.log'.\r\n\tat java.util.concurrent.CompletableFuture.encodeThrowable(Unknown Source) ~[?:?]\r\n\tat java.util.concurrent.CompletableFuture.uniApplyNow(Unknown Source) ~[?:?]\r\n\tat java.util.concurrent.CompletableFuture.uniApplyStage(Unknown Source) ~[?:?]\r\n\tat java.util.concurrent.CompletableFuture.thenApply(Unknown Source) ~[?:?]\r\n\tat io.atomix.raft.partition.RaftPartition.open(RaftPartition.java:91) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.partition.RaftPartitionGroup.lambda$join$2(RaftPartitionGroup.java:176) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat java.util.stream.ReferencePipeline$3$1.accept(Unknown Source) ~[?:?]\r\n\tat java.util.HashMap$KeySpliterator.forEachRemaining(Unknown Source) ~[?:?]\r\n\tat java.util.stream.AbstractPipeline.copyInto(Unknown Source) ~[?:?]\r\n\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(Unknown Source) ~[?:?]\r\n\tat java.util.stream.AbstractPipeline.evaluate(Unknown Source) ~[?:?]\r\n\tat java.util.stream.AbstractPipeline.evaluateToArrayNode(Unknown Source) ~[?:?]\r\n\tat java.util.stream.ReferencePipeline.toArray(Unknown Source) ~[?:?]\r\n\tat io.atomix.raft.partition.RaftPartitionGroup.join(RaftPartitionGroup.java:177) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.primitive.partition.impl.DefaultPartitionService.start(DefaultPartitionService.java:63) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.broker.partitioning.PartitionManagerImpl.start(PartitionManagerImpl.java:129) ~[zeebe-broker-8.2.3.jar:8.2.3]\r\n\tat java.util.concurrent.CompletableFuture$AsyncSupply.run(Unknown Source) ~[?:?]\r\n\tat java.lang.Thread.run(Unknown Source) ~[?:?]\r\nCaused by: io.camunda.zeebe.journal.CorruptedJournalException: Couldn't read or recognize version of segment 'raft-partition-partition-1-1.log'.\r\n\tat io.camunda.zeebe.journal.file.SegmentLoader.readDescriptor(SegmentLoader.java:182) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentLoader.loadExistingSegment(SegmentLoader.java:116) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentsManager.loadSegments(SegmentsManager.java:346) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentsManager.open(SegmentsManager.java:267) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentedJournal.<init>(SegmentedJournal.java:56) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentedJournalBuilder.build(SegmentedJournalBuilder.java:174) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.storage.log.RaftLogBuilder.build(RaftLogBuilder.java:151) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.storage.RaftStorage.openLog(RaftStorage.java:197) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.impl.RaftContext.<init>(RaftContext.java:194) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.impl.DefaultRaftServer$Builder.build(DefaultRaftServer.java:243) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.impl.DefaultRaftServer$Builder.build(DefaultRaftServer.java:217) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.partition.impl.RaftPartitionServer.buildServer(RaftPartitionServer.java:184) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.partition.impl.RaftPartitionServer.initServer(RaftPartitionServer.java:150) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.partition.impl.RaftPartitionServer.start(RaftPartitionServer.java:109) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\t... 14 more\r\nCaused by: io.camunda.zeebe.journal.file.UnknownVersionException: Expected version byte to be one [1 2] but got 0 instead.\r\n\tat io.camunda.zeebe.journal.file.SegmentDescriptor.getEncodingLengthForVersion(SegmentDescriptor.java:225) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentLoader.readDescriptor(SegmentLoader.java:155) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentLoader.loadExistingSegment(SegmentLoader.java:116) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentsManager.loadSegments(SegmentsManager.java:346) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentsManager.open(SegmentsManager.java:267) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentedJournal.<init>(SegmentedJournal.java:56) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentedJournalBuilder.build(SegmentedJournalBuilder.java:174) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.storage.log.RaftLogBuilder.build(RaftLogBuilder.java:151) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.storage.RaftStorage.openLog(RaftStorage.java:197) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.impl.RaftContext.<init>(RaftContext.java:194) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.impl.DefaultRaftServer$Builder.build(DefaultRaftServer.java:243) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.impl.DefaultRaftServer$Builder.build(DefaultRaftServer.java:217) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.partition.impl.RaftPartitionServer.buildServer(RaftPartitionServer.java:184) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.partition.impl.RaftPartitionServer.initServer(RaftPartitionServer.java:150) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.partition.impl.RaftPartitionServer.start(RaftPartitionServer.java:109) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\t... 14 more\r\n```\r\nEnvironment: SaaS, version `8.2.3`\n codingman1990: > \r\n\r\nAs you mentioned network storage problem,is there any chance we can recover it?We are using k8s and helm to deploy camunda in production.If the error mentioned above occurs,we can't restart camunda zeebe any more.\n npepinpe: So SIGBUS errors can happen of course even without network storage, but typically that will be due to a bug or misuse, e.g. a file gets truncated while it was mmap'd. With network storage, they can happen simply due to network issues, which makes them unsuitable for any Java program using memory mapped files, where it's not possible to trap and handle such signals.\r\n\r\nIf you're not running on network storage however, this is likely a bug, and I'd be interested in how this happened. Do you perhaps have programs which try to reap `.log` files indiscriminately? `.log` files in Zeebe aren't traditional log files (e.g. for logging), but represent the actual application data, and shouldn't never be modified externally.\r\n\r\nAs for recovery, if at least one node has the right data, the simplest way is to delete the data of the other nodes, and manually copy over the \"good\" data to them. If however all nodes fail to start up, I'm afraid you'll have to rely [on your backups (if any) to restore](https://docs.camunda.io/docs/self-managed/backup-restore/backup-and-restore/).\n codingman1990: > So SIGBUS errors can happen of course even without network storage, but typically that will be due to a bug or misuse, e.g. a file gets truncated while it was mmap'd. With network storage, they can happen simply due to network issues, which makes them unsuitable for any Java program using memory mapped files, where it's not possible to trap and handle such signals.\r\n> \r\n> If you're not running on network storage however, this is likely a bug, and I'd be interested in how this happened. Do you perhaps have programs which try to reap `.log` files indiscriminately? `.log` files in Zeebe aren't traditional log files (e.g. for logging), but represent the actual application data, and shouldn't never be modified externally.\r\n> \r\n> As for recovery, if at least one node has the right data, the simplest way is to delete the data of the other nodes, and manually copy over the \"good\" data to them. If however all nodes fail to start up, I'm afraid you'll have to rely [on your backups (if any) to restore](https://docs.camunda.io/docs/self-managed/backup-restore/backup-and-restore/).\r\n\r\nThanks for your tips about `.log` file.Our production machine really has a timing clearing old and `big` files task,maybe that's the root cause.We will trying close it.",
    "title": "CorruptedJournalException: Fail to read version byte from segment",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12328",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nWith 8.2.0, it's not possible to disable the Raft flush without specifying a delay. This is mostly due to how Spring deserializes its configuration. As we use a `record` internally for the configuration, it will try to pass both properties, and the second one will be null. It's not really possible to rely on \"default\" values as we used to with simple classes.\r\n\r\n**To Reproduce**\r\n\r\nStart a 8.2.0 broker with the following environment variable: `ZEEBE_BROKER_CLUSTER_RAFT_FLUSH_ENABLED=false`\r\n\r\nThe broker will fail to start. It will however start if you specify:\r\n\r\n```yaml\r\nZEEBE_BROKER_CLUSTER_RAFT_FLUSH_ENABLED=false\r\nZEEBE_BROKER_CLUSTER_RAFT_FLUSH_DELAYTIME=0s\r\n```\r\n\r\nWith this configuration, you can pass whatever as delay time, it will simply be ignored. The behavior after is correct - it's just a matter of deserializing the configuration.\r\n\r\n**Expected behavior**\r\n\r\nI can disable the flush without having to specify a delay time.\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.2.0\r\n\n",
    "title": "Cannot disable Raft flush without specifying a delay",
    "releaseNoteText": "**Consequence:**\nIn version 8.2.0, users were unable to disable the Raft flush without specifying a delay. Attempting to start the broker with the `ZEEBE_BROKER_CLUSTER_RAFT_FLUSH_ENABLED` set to `false` resulted in the broker failing to start.\n\n**Cause:**\nThis issue was caused by how Spring deserialized the configuration. When using a `record` internally for the configuration, the deserialization process attempted to pass both properties, with the second one being `null`. This prevented relying on \"default\" values as previously done with simple classes.\n\n**Fix:**\nThe fix for this issue involved updating the deserialization logic of the Spring configuration. The code was modified to properly handle the scenario where the second property is null, allowing the disabling of Raft flush without specifying a delay.\n\n**Result:**\nWith this fix applied, users can now disable the Raft flush without the need to specify a delay time. Starting the broker with `ZEEBE_BROKER_CLUSTER_RAFT_FLUSH_ENABLED` set to `false` will result in the expected behavior, where the flush is disabled and the broker starts successfully."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12326",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "Hi Zeebe Team, please consider the following bug\r\n\r\n**Describe the bug**\r\n\r\nGiven the [following code snippet](https://github.com/camunda/connector-sdk/blob/main/runtime-util/src/main/java/io/camunda/connector/runtime/util/outbound/ConnectorJobHandler.java#L145-L150).\r\n\r\nThe `newThrowErrorCommand` cannot be handled via BPMN.\r\n\r\n**To Reproduce**\r\n\r\n1. Clone [camunda platform docker compose repo](https://github.com/camunda/camunda-platform).\r\n2. Keep `CAMUNDA_PLATFORM_VERSION` as `SNAPSHOT` or `8.2.0` in the `.env` [file](https://github.com/camunda/camunda-platform/blob/main/.env#L4).\r\n3. Run any variant, e.g. core as `docker-compose -f docker-compose-core.yaml up`.\r\n4. Deploy and start a BPMN diagram attached to this ticket: [error-handling.bpmn.txt](https://github.com/camunda/zeebe/files/11178216/error-handling.bpmn.txt).\r\n5. See an error: `Expected to throw an error event with the code '500' with message 'Got a 500', but it was not caught. No error events are available in the scope.`\r\n\r\n![Screenshot 2023-04-07 at 13 25 51](https://user-images.githubusercontent.com/108870003/230594417-d92ba764-6387-49a7-a9c7-b6773b66e863.png)\r\n\r\n![Screenshot 2023-04-07 at 13 26 02](https://user-images.githubusercontent.com/108870003/230594436-9703f853-5e96-4d4f-8676-3509789e59e9.png)\r\n\r\n\r\n**Expected behavior**\r\n\r\nDo the same as in **To Reproduce** except change the `CAMUNDA_PLATFORM_VERSION` to `8.1.10` or `8.1.9`.\r\n\r\nSee the process works correctly.\r\n\r\n![Screenshot 2023-04-07 at 13 23 19](https://user-images.githubusercontent.com/108870003/230594734-ef38e7b0-48ec-4632-bd62-4378e34a6fef.png)\r\n\r\n**Log/Stacktrace**\r\n\r\nNo valuable stacktraces.\r\n\r\n**Environment:**\r\n- OS:  MacOS 13.3 with M1 chip; Docker Engine version 20.10.23\r\n- Zeebe Version: `8.2.0`, `SNAPSHOT` - reproducible; `8.1.10`, `8.1.9` - not reproducible.\r\n- Configuration: [camunda-platform docker compose repo](https://github.com/camunda/camunda-platform/blob/main/docker-compose-core.yaml).\r\n\n\n remcowesterhoud: I had a first look. This was introduced when [adding support for FEEL expressions in error codes](https://github.com/camunda/zeebe/pull/10972). \r\n\r\nIn this PR we changed the way transform errors, by either setting the `errorCodeExpression` (not support by catch events) or by setting the `errorCode` directly. We _always_ parse the error code using the FEEL engine. In this case this will result in a static expression.\r\nWhere we're going wrong is that we make the assumption that this static expression can only be of the type `String`. In this case this is not true, it is a `Number` instead.\r\n\r\nRelevant code:\r\n\r\n```java\r\n  @Override\r\n  public void transform(final Error element, final TransformContext context) {\r\n\r\n    final var error = new ExecutableError(element.getId());\r\n    final var expressionLanguage = context.getExpressionLanguage();\r\n\r\n    // ignore error events that are not references by the process\r\n    Optional.ofNullable(element.getErrorCode())\r\n        .ifPresent(\r\n            errorCode -> {\r\n              final Expression errorCodeExpression = expressionLanguage.parseExpression(errorCode);\r\n\r\n              error.setErrorCodeExpression(errorCodeExpression);\r\n              if (errorCodeExpression.isStatic()) {\r\n                final EvaluationResult errorCodeResult =\r\n                    expressionLanguage.evaluateExpression(errorCodeExpression, variable -> null);\r\n\r\n                if (errorCodeResult.getType() == ResultType.STRING) {\r\n                  error.setErrorCode(BufferUtil.wrapString(errorCodeResult.getString()));\r\n                }\r\n              }\r\n\r\n              context.addError(error);\r\n            });\r\n  }\r\n```\r\n\r\nLooking into the `StaticExpression`  can only be of type `String` or `Number`\r\n```java\r\n  public StaticExpression(final String expression) {\r\n    this.expression = expression;\r\n\r\n    try {\r\n      treatAsNumber(expression);\r\n    } catch (final NumberFormatException e) {\r\n      treatAsString(expression);\r\n    }\r\n  }\r\n```\r\n\r\n------\r\n\r\nOptions to fix this:\r\n- Support `Number` in our transformers\r\n- Add a `getAsString` method to the `StaticExpression` which takes care of parsing the `Number` to a `String` (not sure how viable this is)\r\n\r\n------\r\n\r\nPlease note we do a similar thing in:\r\n- `EscalationTransformer`\r\n- `MessageTransformer` - Caught with deployment rejection or creating an incident. We should not change this behaviour\r\n- `SignalTransformer` - 8.2 only supports start event. Deployments with a signal as a number get rejected already\r\n- `StartEventTransformer` - Only for messages and signals which we should not change",
    "title": "The `newThrowErrorCommand` incorrectly handled in `8.2.0`",
    "releaseNoteText": "**Consequence:** In version 8.2.0 of Zeebe, the `newThrowErrorCommand` was not correctly handled when used with BPMN, causing an error to occur when attempting to throw an error event with a specific code and message.\n\n**Cause:** The issue was caused by a change in the way errors are transformed in the code. When parsing the error code using the FEEL engine, the assumption was made that the static expression would always be of type `String`, whereas in this case it was a `Number`.\n\n**Fix:** The issue was addressed by adding support for `Number` in the transformers responsible for handling errors.\n\n**Result:** With the fix in place, the `newThrowErrorCommand` is now correctly handled when used with BPMN, allowing the error event to be thrown with the specified code and message without any errors occurring."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12173",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nObserved in a test cluster in `ultrachaos`.\r\n`zeebe-2` is restarted. But it can never gets ready because it is not receiving heartbeats from the leader zeebe-0.\r\n We can see repeated logs in zeebe-2  `No heartbeat from a known leader .. Sending poll requests to all active members`\r\nHowever, in zeebe-0 we see that it is indeed sending messages to zeebe-2 and getting acknowledgments.\r\n```\r\nDEBUG 2023-03-28T11:13:29.998615141Z [resource.labels.podName: zeebe-0] [resource.labels.containerName: zeebe] RaftServer{raft-partition-partition-3} - Sending AppendRequest{term=2, leader=0, prevLogIndex=32433723, prevLogTerm=2, entries=1, commitIndex=32433722} to 2\r\nDEBUG 2023-03-28T11:13:29.998929396Z [resource.labels.podName: zeebe-0] [resource.labels.containerName: zeebe] RaftServer{raft-partition-partition-1} - Received AppendResponse{status=OK, term=1, succeeded=true, lastLogIndex=32442689, lastSnapshotIndex=32240013} from 2\r\n``` \r\nHowever, zeebe-2 is not receiving any of these messages. Confirmed from the logs and zeebe-2 journal segments.\r\n\r\nOn investigating futher, it seems zeebe-1 is receiving messages for zeebe-2. \r\n\r\nFor every single request send from zeebe-0 to zeebe-1, zeebe-1 is receiving two requests. It seems the duplicate request is for zeebe-2. (the logs that shows requests to 2 is omitted in the following logs).\r\n```\r\nDEBUG 2023-03-28T11:11:08.699639973Z [ zeebe-0] - Sending AppendRequest{term=1, leader=0, prevLogIndex=32402784, prevLogTerm=1, entries=1, commitIndex=32402784} to 1\r\n\r\nDEBUG 2023-03-28T11:11:08.701492523Z [ zeebe-1]{role=FOLLOWER} - Sending AppendResponse{status=OK, term=1, succeeded=true, lastLogIndex=32402785, lastSnapshotIndex=32240013}\r\nDEBUG 2023-03-28T11:11:08.701775314Z [ zeebe-1]{role=FOLLOWER} - Received AppendRequest{term=1, leader=0, prevLogIndex=32402784, prevLogTerm=1, entries=1, commitIndex=32402784}\r\nDEBUG 2023-03-28T11:11:08.702472045Z [ zeebe-1]{role=FOLLOWER} - Sending AppendResponse{status=OK, term=1, succeeded=true, lastLogIndex=32402785, lastSnapshotIndex=32240013}\r\nDEBUG 2023-03-28T11:11:08.702963186Z [ zeebe-0] - Received AppendResponse{status=OK, term=1, succeeded=true, lastLogIndex=32402785, lastSnapshotIndex=32240013} from 1\r\n\r\n\r\nDEBUG 2023-03-28T11:11:08.706279292Z [ zeebe-0] - Sending AppendRequest{term=1, leader=0, prevLogIndex=32402785, prevLogTerm=1, entries=1, commitIndex=32402785} to 1\r\nDEBUG 2023-03-28T11:11:08.706659278Z [ zeebe-1]{role=FOLLOWER} - Received AppendRequest{term=1, leader=0, prevLogIndex=32402785, prevLogTerm=1, entries=1, commitIndex=32402785}\r\nDEBUG 2023-03-28T11:11:08.708798386Z [ zeebe-1]{role=FOLLOWER} - Sending AppendResponse{status=OK, term=1, succeeded=true, lastLogIndex=32402786, lastSnapshotIndex=32240013}\r\nDEBUG 2023-03-28T11:11:08.709046684Z [ zeebe-1]{role=FOLLOWER} - Received AppendRequest{term=1, leader=0, prevLogIndex=32402785, prevLogTerm=1, entries=1, commitIndex=32402785}\r\nDEBUG 2023-03-28T11:11:08.709334419Z [ zeebe-1]{role=FOLLOWER} - Sending AppendResponse{status=OK, term=1, succeeded=true, lastLogIndex=32402786, lastSnapshotIndex=32240013}\r\n```\r\n\r\n**To Reproduce**\r\n\r\n\r\n**Expected behavior**\r\n\r\n\r\n\r\n**Log/Stacktrace**\r\n[logs](https://console.cloud.google.com/logs/query;cursorTimestamp=2023-03-28T11:13:29.999108007Z;pinnedLogId=2023-03-28T11:13:29.998615141Z%2Fvx3s55brpp9of4ho;query=resource.type%3D%22k8s_container%22%0Aresource.labels.project_id%3D%22camunda-cloud-240911%22%0Aresource.labels.location%3D%22europe-west1-d%22%0Aresource.labels.cluster_name%3D%22ultrachaos%22%0Aresource.labels.namespace_name%3D%22fea132ff-1e4e-4c7b-b278-8059264f9efa-zeebe%22%0Aresource.labels.container_name%3D%22zeebe%22%0Aresource.labels.pod_name%3D%22zeebe-0%22%0Atimestamp%3D%222023-03-28T11:13:29.998615141Z%22%0AinsertId%3D%22vx3s55brpp9of4ho%22;summaryFields=resource%252Flabels%252Fpod_name:false:32:beginning;timeRange=2023-03-28T10:49:15.000Z%2F2023-03-28T11:50:45.000Z?project=camunda-cloud-240911)\r\n\r\n\r\n[Heap dump of zeebe-0](https://github.com/camunda/zeebe/files/11089429/zeebe-0-13-30.zip)\r\n\r\n\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.2.0-SNAPSHOT \r\n\n\n deepthidevaki: On analyzing heap dump, the channel state in NettyMessaginService is inconsistent.\r\n\r\nAccording to state in SwimMembershipProtocol, the IP of the brokers are as follows:\r\n```\r\nzeebe-1: 10.56.46.232\r\nzeebe-2: 10.56.52.2\r\n```\r\n\r\nThere is a channel pool for `10.56.52.2`\r\n![image](https://user-images.githubusercontent.com/1997478/228247889-53407fd1-7e67-4982-861f-6fd54833f6e3.png)\r\n\r\nThe channel for `raft-partition-partition-1-append` will be at offset 4. But the channel at offset 4 has a remote connection to zeebe-1: 10.56.46.232\r\n![image](https://user-images.githubusercontent.com/1997478/228248235-fb298026-3159-4c6b-9e72-7b6b2d9b1239.png)\r\n\r\n\r\nSo it seems AppendRequest to zeebe-2 (10.56.52.2) is being sent to zeebe-1(10.56.46.232) instead.\n npepinpe: Good catch!\r\n\r\nThe first image is showing the channel pool for 10.56.52.2 on the `zeebe-0` node? Funnily enough, the key, though pointing to 10.56.52.2, has an InetAddress of `zeebe-1`...\r\n\r\nAny thoughts on solutions?\r\n\r\nJust thinking out loud:\r\n\r\n1. When a node is removed from the cluster, it's channels should be removed from the pool (not sure if this is already done)\r\n1. When a node is added to the cluster, channels for its IP should be reset in the pool (or just removed? Is it even possible to have something in the pool for that IP if the member was not in the cluster?)\r\n\r\nOr encoding the recipient concept in the protocol. Have the sender send both the member ID and the cluster ID, so the receiver can reject with a special message, forcing the sender to evict all channels for the wrong IP.\r\n\r\nBoth of these rely on the membership protocol providing the right IPs. I think this may break down with advertised addresses, since the advertised IP is not that of the receiver, and the receiver may not even be able to resolve that one. So perhaps coupling the member ID to the messaging service is necessary? :shrug: \n deepthidevaki: > The first image is showing the channel pool for 10.56.52.2 on the zeebe-0 node? Funnily enough, the key, though pointing to 10.56.52.2, has an InetAddress of zeebe-1...\r\n\r\nI also observed this. And I think this also points to one of the root cause. Here, zeebe-0 re-used old zeebe-1 IP. The channel pool use InetSocketAddress as key, which used IP address in `equals` if IP is available. So when finding channel for zeebe-0, it finds channel pool created for zeebe-1 with it's old address. However, I don't know yet how this channel pool contains a channel to new zeebe-1 address. \r\n\r\nI haven't thought about a solution. But we should see if we can combine hostname+ip to find the channels. So when IP is re-assigned we don't accidentally chose the wrong channel.\r\n\r\n> Or encoding the recipient concept in the protocol. Have the sender send both the member ID and the cluster ID, so the receiver can reject with a special message, forcing the sender to evict all channels for the wrong IP.\r\n\r\n:+1: This would be useful as a early detection/prevention of such cases.\r\n\r\n\n deepthidevaki: Summary of discussion with @npepinpe \r\n- Use address + inetAddress to find the channel pool. This will prevent using the wrong channel, in case ip is reassigned.\r\n- We could encode recipient host with all messages. On the receiver we can check and return a error response. Looks like this can be done in a backward compatible way. Server and client negotiates the protocol version during the handshake. So we can add a new protocol version with the recipient address. https://github.com/camunda/zeebe/issues/12309\r\n- We discarded the other idea to clear channelpool when membership of a node changed. This requires messaging layer to have knowledge about membership service, which is not ideal.",
    "title": "Zeebe node sends messages to wrong node",
    "releaseNoteText": "**Consequence:** The Zeebe node was sending messages to the wrong node.\n\n**Cause:** The channel state in the NettyMessagingService was found to be inconsistent. While analyzing the heap dump, it was discovered that the channel pool for a specific IP address was pointing to the wrong node.\n\n**Fix:** The fix involved using both the address and inetAddress to find the channel pool. This ensured that the correct channel was used, even if the IP address was reassigned.\n\n**Result:** After the fix, the Zeebe node successfully sends messages to the intended node, resolving the issue of sending messages to the wrong node."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12007",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n```\r\nio.camunda.zeebe.stream.api.records.ExceededBatchRecordSizeException: Can't append entry: \r\n'RecordBatchEntry[recordMetadata=RecordMetadata{recordType=EVENT, valueType=JOB_BATCH, intent=ACTIVATED}, \r\nkey=4503599628576248, sourceIndex=-1, unifiedRecordValue=\r\n\r\n\"maxJobsToActivate\":13822\r\n```\r\n\r\nPartition is unhealthy. \r\n```\r\nBroker-1-StreamProcessor-2{status=UNHEALTHY, issue=HealthIssue[message=not making progress, \r\n\r\nBroker-1-StreamProcessor-1{status=UNHEALTHY, issue=HealthIssue[message=actor appears blocked, \r\n```\r\nBut StreamProcessor is not stuck. Eventhough there is high backpressure, it is not 100%. Meaning that StreamProcessor is accepting new commands and processing them. But it seems no job can be activated.\r\n\r\n**To Reproduce**\r\n\r\n\r\n\r\n**Expected behavior**\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n**Log/Stacktrace**\r\n\r\n[Link to logs](https://console.cloud.google.com/errors/detail/CIeFtI729PW2Cw;service=zeebe;time=P7D?project=camunda-cloud-240911)\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\nio.camunda.zeebe.stream.api.records.ExceededBatchRecordSizeException: Can't append entry: 'RecordBatchEntry[recordMetadata=RecordMetadata{recordType=EVENT, valueType=JOB_BATCH, intent=ACTIVATED}, key=4503599628576248, sourceIndex=-1, unifiedRecordValue={\"type\":\"***\",\"worker\":\"***\",\"timeout\":300000,\"maxJobsToActivate\":13822,\"jobKeys\":[4503599628420203,4503599628420225,4503599628420247,4503599628420269,4503599628420291,4503599628420314,4503599628420318,4503599628420340,4503599628420344,4503599628420348,4503599628420352,4503599628420356,4503599628420360,4503599628420364,4503599628420446,4503599628420450,4503599628420454,4503599628420458,4503599628420462,4503599628420466,4503599628420516,4503599628420520,4503599628420524,4503599628420528,4503599628420532,4503599628420629,4503599628420633,4503599628420650,4503599628420702,4503599628420706,4503599628420716,4503599628420726,4503599628420730,4503599628420734,4503599628420770,4503599628420790,4503599628420794,4503599628420810,4503599628420818,4503599628420822,4503599628420834,4503599628420842,4503599...' with size: 4194277 this would exceed the maximum batch size. [ currentBatchEntryCount: 0, currentBatchSize: 0]\r\n\r\nat io.camunda.zeebe.stream.impl.records.RecordBatch.appendRecord ( [io/camunda.zeebe.stream.impl.records/RecordBatch.java:67](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl.records%2FRecordBatch.java&line=67&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.BufferedProcessingResultBuilder.appendRecordReturnEither ( [io/camunda.zeebe.stream.impl/BufferedProcessingResultBuilder.java:62](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FBufferedProcessingResultBuilder.java&line=62&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.api.ProcessingResultBuilder.appendRecord ( [io/camunda.zeebe.stream.api/ProcessingResultBuilder.java:38](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.api%2FProcessingResultBuilder.java&line=38&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.streamprocessor.writers.ResultBuilderBackedEventApplyingStateWriter.appendFollowUpEvent ( [io/camunda.zeebe.engine.processing.streamprocessor.writers/ResultBuilderBackedEventApplyingStateWriter.java:40](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.streamprocessor.writers%2FResultBuilderBackedEventApplyingStateWriter.java&line=40&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.job.JobBatchActivateProcessor.activateJobBatch ( [io/camunda.zeebe.engine.processing.job/JobBatchActivateProcessor.java:125](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.job%2FJobBatchActivateProcessor.java&line=125&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.job.JobBatchActivateProcessor.activateJobs ( [io/camunda.zeebe.engine.processing.job/JobBatchActivateProcessor.java:86](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.job%2FJobBatchActivateProcessor.java&line=86&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.job.JobBatchActivateProcessor.processRecord ( [io/camunda.zeebe.engine.processing.job/JobBatchActivateProcessor.java:63](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.job%2FJobBatchActivateProcessor.java&line=63&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.Engine.process ( [io/camunda.zeebe.engine/Engine.java:127](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine%2FEngine.java&line=127&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.batchProcessing ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:340](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=340&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$2 ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:263](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=263&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run ( [io/camunda.zeebe.db.impl.rocksdb.transaction/ZeebeTransaction.java:84](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.db.impl.rocksdb.transaction%2FZeebeTransaction.java&line=84&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:263](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=263&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:222](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=222&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.readNextRecord ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:198](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=198&project=camunda-cloud-240911) )\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n\r\n- Zeebe Version: 8.2.0-alpha5\r\n\n\n deepthidevaki: @korthout Please check if this is affecting ZPA.\n megglos: The exception changed, surfacing this issue. The cause might just be the high number of jobs to activate.\nTime box it for 30m to double-check if this is an actual bug.\n korthout: Removed from our board, until further notice\n remcowesterhoud: This happened again on the release benchmark of `8.2.x`\r\n\r\nI've increased the max message size to recover the benchmark\n Zelldon: > Removed from our board, until further notice\r\n\r\nThe reasoning would be great here.\n remcowesterhoud: > > Removed from our board, until further notice\r\n> \r\n> The reasoning would be great here.\r\n\r\nI don't remember, but probably because we thought it was related to the stream platform because of the label.\r\n\r\nI'll try to do a deeper analysis this week and see if I can pinpoint the bug.\n remcowesterhoud: I've been trying to reproduce but have been unable to. I can think of 2 possible causes:\r\n\r\n1. The [check if a record of size X can be appended](https://github.com/camunda/zeebe/blob/main/logstreams/src/main/java/io/camunda/zeebe/logstreams/impl/log/Sequencer.java#L56-L63) is not 100% foolproof and in edge cases returns `true` even though it doesn't fit.\r\n2. The [expected event length](https://github.com/camunda/zeebe/blob/main/engine/src/main/java/io/camunda/zeebe/engine/processing/job/JobBatchCollector.java#L87-L96) is not fully accurate. It tries to calculate the event length precisely, but if it's slightly off, we could append it yet still exceed the batch size.\r\n\r\nIMO 2 is the more likely scenario. My proposal would be to add an extra buffer to expected event length. This means we would check if a larger than expected event would fit in the batch. If this isn't the case we won't add it. This buffer can be relatively small. For terminating in batches we used 8 KB, but I think that's excessive for job activations. Instead I'd stick with a few bytes.\r\n\r\nAs this is part of the code is ZPA's responsibility I'll remove this from the ZDP board and add it to ours.\n korthout: ZPA triage:\n- small chance, but high impact\n- small effort: let's add a small margin to the check whether another job fits the batch\n- such a margin could help with #12778 \n- let's prioritize this as `upcoming`",
    "title": "ExceededBatchRecordSizeException: Can't append entry",
    "releaseNoteText": "**Consequence**: Users were unable to activate jobs due to the `ExceededBatchRecordSizeException`. The partition was marked as unhealthy, causing the stream processor to not make progress.\n\n**Cause**: The issue was caused by the append entry check not accurately determining if a record of large size could be appended to the batch. Additionally, the expected event length calculation was not fully accurate, leading to records being appended even if it exceeded the batch size.\n\n**Fix**: A fix was implemented to add an extra buffer to the expected event length calculation. This buffer allows for a small margin of error and ensures that records that exceed the batch size are not appended.\n\n**Result**: With the fix, users can activate jobs without encountering the `ExceededBatchRecordSizeException`. The partition is healthy, and the stream processor can successfully make progress."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11594",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nBasically, it is the same as #11591 but related to timer events (and not messages). In a nutshell, Zeebe checks regularly for due timer events to eventually trigger them and continue with the process flow. The corresponding checker shares the actor of the Stream Processor which will block the Stream Processor while the checker runs. Also, the checker might submit a batch of commands to trigger timer events which will be executed by the Stream Processor in a row without anything else in between.\r\n\r\n**Expected behavior**\r\n* The Stream Processor and the checker do not share an actor so the Stream Processor continues processing while the checker collects timers to trigger.\r\n* (This might be partially already the case, needs to be checked.) The checker only submits a batch with a limited number of commands. For example, when the checker runs it will collect the first 10 due timer events and submit them as a batch to the log stream. And then continues with collecting the next 10 due timer events, and so on until there are due timer events. That way, triggering the timer events would interleave with any incoming commands from users/clients. \r\n* Instead of writing 10 commands, it could write just one command containing the 10 due timer events to trigger. (Might not be easily possible in terms of rolling upgrades, an old version of Zeebe would not be able to process such a command.)\r\n\r\n**Hints**\r\n* A simple prototype to avoid sharing the actor can be found here (in case of expired messages): https://github.com/camunda/zeebe/pull/11550\r\n* Things to consider: When the checker and the Stream Processor do not share an actor, they may run concurrently. That means, the checker reads from RocksDB, and the Stream Processor (mostly) writes to RocksDB. While RocksDB itself is thread-safe, the Zeebe layer may not (like `TransactionContext`, ...).\r\n* Also, while reading the state by the checker, the state might change.\r\n* The same pattern should be applied to the job's timeline and backoff checker.\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.1\r\n\r\n---\r\n\r\n* related to [SUPPORT-15902](https://jira.camunda.com/browse/SUPPORT-15902)\r\n* related to #8991 \n\n Zelldon: Related to https://github.com/camunda/zeebe/issues/8991\n megglos: related to https://github.com/camunda/zeebe/issues/11591\n megglos: Sync with @abbasadel :\r\n- @abbasadel checks in with Nico once https://github.com/camunda/zeebe/issues/11762 is completed on whether the ZPA team can continue with this right after\n korthout: Discussed this issue in the ZPA triage:\n - not urgent for the 8.2 release (other issues have priority)\n - it is an important issue and should be fixed \n - marking it is as `later` as we will first focus on the 8.2 release\n megglos: as this issue strongly relates to the mission of the ZDP team, we would take this one over to drive it forward\n megglos: @abbasadel Ole might need support at least for alignment and dicussion from a ZPA engineer",
    "title": "Triggering due timer events causes periodic latency spikes",
    "releaseNoteText": "**Consequence**: Triggering due timer events caused periodic latency spikes in the system. This impact was observed in the blocking of the Stream Processor while the timer event checker ran, resulting in delays in the overall process flow. Additionally, the execution of a batch of commands to trigger timer events without any interleaving with incoming commands further contributed to the latency spikes.\n\n**Cause**: The issue stemmed from the shared actor between the Stream Processor and the timer event checker. This caused the Stream Processor to be blocked while the checker ran, leading to concurrency issues as reading from RocksDB by the checker could result in changes to the state. Similarly, the job's timeline and backoff checker also exhibited the same pattern.\n\n**Fix**: To address this issue, the Stream Processor and the checker were decoupled to ensure parallel execution. The checker now collects timers to trigger without blocking the Stream Processor, allowing the process flow to continue uninterrupted. Furthermore, a batch with a limited number of commands (e.g., 10 due timer events) is now submitted by the checker, providing interleaving with incoming commands."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11578",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\nMulti-Instance doesn't get started if it contains a Message Event-based Subprocess that uses the inputElement as correlationKey. Instead an Incident gets created.\r\n\r\n\r\n\r\n\r\n**To Reproduce**\r\n**Update:** For better understanding I created a GitHub Repo with a minimalistic example: https://github.com/j-lindner/multi-instance-with-message-subprocess-bug\r\n\r\n**Expected behavior**\r\nMulti-Instance should get started, get processed and should listen to possible Messages on the event-based subprocess.\r\n\r\n**Log/Stacktrace**\r\n\r\nIncident Info in Operate:\r\n```\r\nfailed to evaluate expression 'myObject.myId': no variable found for name 'myObject'\r\n```\r\n\r\n**Environment:**\r\n- OS: WebModeler or alternatively zeebe-process-test-extension-testcontainer\r\n- Zeebe Version: 8.1.7\r\n- Configuration: -\r\n\r\n**Support:** \r\nhttps://jira.camunda.com/browse/SUPPORT-17059\n\n remcowesterhoud: Thanks for reporting and reproducing @j-lindner! This is very helpful 🚀 \r\n\r\n I will have chat within the team to see if this is expected behaviour or not. From there we will prioritise it accordingly.\n korthout: Thanks for reporting this @j-lindner 👍 It was easy to reproduce in one of Zeebe's engine test cases as well. I think I have a fix, but let's await the review.\n korthout: @j-lindner Bug is fixed and will be patched in upcoming releases of `8.0.15`, `8.1.13`, `8.2.6`, and `8.3.0`. You should notice that this issue will be labeled accordingly during those releases.",
    "title": "Multi-Instance with messageevent-based subprocess that uses inputElement as correlationKey fails",
    "releaseNoteText": "**Consequence:** Multi-Instance fails to start when it contains a Message Event-based Subprocess that uses the inputElement as correlationKey. Instead, an incident is created, and the expression 'myObject.myId' fails to evaluate due to the variable 'myObject' not being found.\n\n**Cause:** The issue is caused by a bug in the system where the Multi-Instance process is unable to handle a Message Event-based Subprocess that uses the inputElement as correlationKey. This results in the creation of an incident and the failure of the expression evaluation.\n\n**Fix:** The bug has been fixed in the latest patch release (version 8.0.15, 8.1.13, 8.2.6, and 8.3.0). The fix addresses the issue with the Multi-Instance process and ensures that it can properly handle the Message Event-based Subprocess with the inputElement as correlationKey.\n\n**Result:** With the fix applied, the Multi-Instance process will start as expected and process the necessary tasks. It will also be able to listen to any messages on the event-based subprocess without any incidents or expression evaluation failures."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11414",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n```\r\n java.util.NoSuchElementException: No value present\r\n\tat java.util.Optional.orElseThrow(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.updateInMemoryState(DbProcessState.java:180) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n```\r\n\r\n**To Reproduce**\r\nUnclear\r\n\r\n**Expected behavior**\r\n\r\nInvalid BPMN resources should be handled gracefully and not result in an unhandled `NoSuchElementException`.\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\n java.util.NoSuchElementException: No value present\r\n\tat java.util.Optional.orElseThrow(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.updateInMemoryState(DbProcessState.java:180) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.lookupProcessByIdAndPersistedVersion(DbProcessState.java:314) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.getLatestProcessVersionByProcessId(DbProcessState.java:217) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.transformProcessResource(BpmnResourceTransformer.java:138) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.lambda$transformResource$0(BpmnResourceTransformer.java:77) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.util.Either$Right.map(Either.java:355) ~[zeebe-util-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.lambda$transformResource$1(BpmnResourceTransformer.java:75) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.util.Either$Right.flatMap(Either.java:366) ~[zeebe-util-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.transformResource(BpmnResourceTransformer.java:65) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.DeploymentTransformer.transformResource(DeploymentTransformer.java:120) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.DeploymentTransformer.transform(DeploymentTransformer.java:97) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.DeploymentCreateProcessor.processRecord(DeploymentCreateProcessor.java:96) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.Engine.process(Engine.java:127) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$3(ProcessingStateMachine.java:264) ~[zeebe-stream-platform-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run(ZeebeTransaction.java:84) ~[zeebe-db-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand(ProcessingStateMachine.java:260) ~[zeebe-stream-platform-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord(ProcessingStateMachine.java:209) ~[zeebe-stream-platform-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.readNextRecord(ProcessingStateMachine.java:185) ~[zeebe-stream-platform-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) ~[zeebe-scheduler-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) ~[zeebe-scheduler-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) ~[zeebe-scheduler-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) ~[zeebe-scheduler-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) ~[zeebe-scheduler-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) ~[zeebe-scheduler-8.2.0-alpha3.jar:8.2.0-alpha3] \r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- SaaS\r\n- Zeebe Version: 8.2.0-alpha3\r\n\r\n[Error group](https://console.cloud.google.com/errors/detail/CM_e_rP419icrgE;service=zeebe;time=P7D?project=camunda-cloud-240911)\n\n oleschoenburg: Might be related to https://github.com/camunda/zeebe/issues/11392\n korthout: We need to understand the impact before we can prioritize this. Let's investigate whether this leads to a blacklisted instance.\n remcowesterhoud: I had a look into this with @koevskinikola. We suspect that somehow during the deployment, the deployment record process metadata does not contain a process that we do have in the bpmn file. As a result we have a process in the bpmn file that we have not stored as a `PersistedProcess`, resulting in this exception.\r\n\r\nWe are unsure how this could occur and how we can reproduce it. The exception has occurred 3 times in trial clusters thus far.\n remcowesterhoud: We discussed within the team and we will have another look into this issue together during out next mob-programming hour 2 weeks from now.\n korthout: Today, the team looked again at this issue. Having no way to reproduce it nor a way to find out how it might have happened makes this hard to debug.\r\n\r\nI've had another look at several parts:\r\n- ❌ could this have been caused by #11392 by checking multiple scenarios (e.g. deploying valid processes with id's that were already used in deployments that encountered that bug)\r\n- ❓ what is this code actually doing?\r\n\r\nThis question led me down a path where I noticed something:\r\n- when we deploy a BPMN file, we transform it\r\n- then, for the duplication check, we lookup the latest deployed version \r\n- this either uses the cached version, or it will read it from the state and then cache it\r\n- if it wasn't cached already but does exist in the state, it will then proceed to transform the persisted process XML\r\n- there is no real reason why we transform it here. The latest version is only used to determine version duplicates (and in that case, return the same key, version, etc as a response)\r\n- but this is the transformation where this specific bug happened.\r\n\r\nSo we could perhaps swat two flies at once (this is a Dutch saying):\r\n- remove the code path that was part of this bug\r\n- improve the performance when a new process version is deployed, when the latest isn't cached anymore (not sure if that ever happens, though).\r\n\r\nI'm curious as to what others think about this. Is that worth it? WDYT?\r\n\r\n//cc @remcowesterhoud @koevskinikola @berkaycanbc \n koevskinikola: Hey @korthout, I would vote for the following:\r\n\r\n1. Close this issue since we don't have enough data to reproduce and qualify it.\r\n2. Create an issue for optimizing the performance of the deployment of new process versions (with a low priority).\r\n    * I'm not sure if we should work on this soon, or until we understand the bug.\r\n\r\nMy reasoning is that currently we still don't understand why the bug is happening. The code you're suggesting for removal is just the place where the issue becomes visible (through a `NoSuchElementException`). By removing that code, we might miss any new occurrences of the bug, so it will become more difficult to detect and reproduce.\r\n\r\nUPDATE: Maybe we can wrap the existing exception [here](https://github.com/camunda/zeebe/blob/main/engine/src/main/java/io/camunda/zeebe/engine/state/deployment/DbProcessState.java#L180) into a more understandable message like: \"This might indicate a bug with our deployment process, please raise an issue with our GitHub tracker\"?\n korthout: Thanks @koevskinikola \r\n\r\n1. Agreed 👍 \r\n2. I don't think the performance improvement is a good enough reason to open an issue. The improvement could be tiny. I agree with your reasoning that \"it will become more difficult to detect and reproduce\" 👍 \r\n\r\n>UPDATE: Maybe we can wrap the existing exception [here](https://github.com/camunda/zeebe/blob/main/engine/src/main/java/io/camunda/zeebe/engine/state/deployment/DbProcessState.java#L180) into a more understandable message like: \"This might indicate a bug with our deployment process, please raise an issue with our GitHub tracker\"?\r\n\r\n👍 I like this idea, but perhaps we can change the message. Likely, we read the error message in our own environment. So there is no need to ask users to report the bug if we already know it exists. Let's add some details about the current scenario instead. \r\n\r\nEDIT: I've moved this back into `READY`, so we can make an effort to root cause this\n berkaycanbc: Implemented a PR to create a detailed error message. We should re-open it when we encounter the log message:\n\n> Expected to find executable process in persisted process with key '%s', but after transformation no such executable process could be found\n\ncc: @korthout ",
    "title": "Unhandled `NoSuchElementException` when looking for executable process while deploying BPMN resource",
    "releaseNoteText": "**Consequence:**\n\nUsers experienced an unhandled `NoSuchElementException` when attempting to deploy BPMN resources. This issue caused invalid BPMN resources to be handled without any grace and resulted in the exception being thrown.\n\n**Cause:**\n\nThe underlying cause of this issue was that during the deployment, the deployment record process metadata did not contain a process that was present in the BPMN file. This led to a process in the BPMN file that had not been stored as a `PersistedProcess`, causing the `NoSuchElementException` to occur.\n\n**Fix:**\n\nTo address this issue, the code path that was part of the bug was removed. This involved removing the transformation step for the persisted process XML that triggered the bug. Additionally, the code was optimized to improve the performance when deploying new process versions.\n\n**Result:**\n\nWith this fix applied, users will no longer encounter the unhandled `NoSuchElementException` when attempting to deploy invalid BPMN resources. The deployment process will now handle invalid resources gracefully and provide a more understandable error message."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11355",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nWe got reports of crash looping Zeebe brokers on prod, it looks like the process which is running does some nesting or looping over certain activities. TODO: I will add the process model later.\r\n\r\nThe user tried to cancel the corresponding process instance but [this failed because](https://console.cloud.google.com/logs/query;query=%0AlogName:%22stdout%22%0Aresource.type%3D%22k8s_container%22%0Aresource.labels.location%3D%22us-central1%22%0Aresource.labels.namespace_name%3D%228dca781e-03c0-4a15-9b88-1832c5d60b19-zeebe%22%0Aresource.labels.cluster_name%3D%22prod-worker-3%22%0Aresource.labels.container_name%3D%22zeebe%22%0Aresource.labels.pod_name%3D%22zeebe-0%22%0Aresource.labels.project_id%3D%22camunda-cloud-240911%22;timeRange=2023-01-02T09:49:55.253Z%2F2023-01-02T10:49:55.253Z;cursorTimestamp=2023-01-02T10:19:25.169339248Z?project=camunda-cloud-240911) there were too many activities to terminate. \r\n\r\n```\r\nExpected to write one or more follow-up records for record 'LoggedEvent [type=0, version=0, streamId=2, position=299792, key=4503599627371681, timestamp=1672654759877, sourceEventPosition=297539] RecordMetadata{recordType=COMMAND, intentValue=255, intent=TERMINATE_ELEMENT, requestStreamId=-2147483648, requestId=-1, protocolVersion=3, valueType=PROCESS_INSTANCE, rejectionType=NULL_VAL, rejectionReason=, brokerVersion=8.2.0}' without errors, but exception was thrown.\r\n```\r\n\r\nError group: https://console.cloud.google.com/errors/detail/COWzpqvwz4Cg0wE;service=zeebe;time=P7D?project=camunda-cloud-240911\r\n<!-- A clear and concise description of what the bug is. -->\r\n> **Note:** Even though we replaced the dispatcher this error will still happen since we have this max message size limit.\r\n\r\nI put the severity to high since I see no workaround. BTW due to the loop and which causes the pod crash looping the cluster was in this case unusable.\r\n\r\n**To Reproduce**\r\nHave a process instance with a lot of activities active, and terminate the corresponding process instance.\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\n**Expected behavior**\r\nTermination of instances takes into account the batch size, and terminates activities batch-wise, similar issue as to activitate multi instances.\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.lang.IllegalArgumentException: Expected to claim segment of size 4481608, but can't claim more than 4194304 bytes.\r\n\tat io.camunda.zeebe.dispatcher.Dispatcher.offer(Dispatcher.java:207) ~[zeebe-dispatcher-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.dispatcher.Dispatcher.claimFragmentBatch(Dispatcher.java:164) ~[zeebe-dispatcher-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.logstreams.impl.log.LogStreamBatchWriterImpl.claimBatchForEvents(LogStreamBatchWriterImpl.java:235) ~[zeebe-logstreams-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.logstreams.impl.log.LogStreamBatchWriterImpl.tryWrite(LogStreamBatchWriterImpl.java:212) ~[zeebe-logstreams-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$writeRecords$9(ProcessingStateMachine.java:354) ~[zeebe-stream-platform-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.retry.ActorRetryMechanism.run(ActorRetryMechanism.java:28) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.retry.AbortableRetryStrategy.run(AbortableRetryStrategy.java:45) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\"\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- OS: <!-- [e.g. Linux] -->\r\n- Zeebe Version: 8.2.0-alpha2 <!-- [e.g. 0.20.0] -->\r\n- Configuration: Production G3-S<!-- [e.g. exporters etc.] -->\r\n\r\nrelates to https://jira.camunda.com/browse/SUPPORT-16499\r\n\n\n Zelldon: Another but related error occured on PROD:\r\n\r\n```\r\nio.camunda.zeebe.stream.api.records.ExceededBatchRecordSizeException: Can't append entry: 'RecordBatchEntry[key=2251799813801783, sourceIndex=-1, recordMetadata=RecordMetadata{recordType=COMMAND, intentValue=10, intent=TERMINATE_ELEMENT, requestStreamId=-2147483648, requestId=-1, protocolVersion=3, valueType=PROCESS_INSTANCE, rejectionType=NULL_VAL, rejectionReason=, brokerVersion=8.2.0}, unifiedRecordValue={\"bpmnProcessId\":\"Process_372fbfc7-9a4a-4f0b-aee5-bd96ed3e3e5d\",\"version\":1,\"processDefinitionKey\":2251799813685320,\"processInstanceKey\":2251799813685333,\"elementId\":\"Activity_0vhm20h\",\"flowScopeKey\":2251799813685333,\"bpmnElementType\":\"USER_TASK\",\"bpmnEventType\":\"UNSPECIFIED\",\"parentProcessInstanceKey\":-1,\"parentElementInstanceKey\":-1}]' with size: 335 this would exceed the maximum batch size. [ currentBatchEntryCount: 11814, currentBatchSize: 3957709]\r\n\r\nat io.camunda.zeebe.stream.impl.records.RecordBatch.appendRecord ( [io/camunda.zeebe.stream.impl.records/RecordBatch.java:66](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl.records%2FRecordBatch.java&line=66&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.BufferedProcessingResultBuilder.appendRecordReturnEither ( [io/camunda.zeebe.stream.impl/BufferedProcessingResultBuilder.java:62](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FBufferedProcessingResultBuilder.java&line=62&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.api.ProcessingResultBuilder.appendRecord ( [io/camunda.zeebe.stream.api/ProcessingResultBuilder.java:38](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.api%2FProcessingResultBuilder.java&line=38&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.streamprocessor.writers.ResultBuilderBackedTypedCommandWriter.appendRecord ( [io/camunda.zeebe.engine.processing.streamprocessor.writers/ResultBuilderBackedTypedCommandWriter.java:37](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.streamprocessor.writers%2FResultBuilderBackedTypedCommandWriter.java&line=37&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.streamprocessor.writers.ResultBuilderBackedTypedCommandWriter.appendFollowUpCommand ( [io/camunda.zeebe.engine.processing.streamprocessor.writers/ResultBuilderBackedTypedCommandWriter.java:32](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.streamprocessor.writers%2FResultBuilderBackedTypedCommandWriter.java&line=32&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnStateTransitionBehavior.lambda$terminateChildInstances$3 ( [io/camunda.zeebe.engine.processing.bpmn.behavior/BpmnStateTransitionBehavior.java:332](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn.behavior%2FBpmnStateTransitionBehavior.java&line=332&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnStateTransitionBehavior.terminateChildInstances ( [io/camunda.zeebe.engine.processing.bpmn.behavior/BpmnStateTransitionBehavior.java:330](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn.behavior%2FBpmnStateTransitionBehavior.java&line=330&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.container.ProcessProcessor.onTerminate ( [io/camunda.zeebe.engine.processing.bpmn.container/ProcessProcessor.java:85](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn.container%2FProcessProcessor.java&line=85&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.container.ProcessProcessor.onTerminate ( [io/camunda.zeebe.engine.processing.bpmn.container/ProcessProcessor.java:27](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn.container%2FProcessProcessor.java&line=27&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.processEvent ( [io/camunda.zeebe.engine.processing.bpmn/BpmnStreamProcessor.java:122](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn%2FBpmnStreamProcessor.java&line=122&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.lambda$processRecord$0 ( [io/camunda.zeebe.engine.processing.bpmn/BpmnStreamProcessor.java:95](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn%2FBpmnStreamProcessor.java&line=95&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.util.Either$Right.ifRightOrLeft ( [io/camunda.zeebe.util/Either.java:381](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.util%2FEither.java&line=381&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.processRecord ( [io/camunda.zeebe.engine.processing.bpmn/BpmnStreamProcessor.java:92](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn%2FBpmnStreamProcessor.java&line=92&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.Engine.process ( [io/camunda.zeebe.engine/Engine.java:128](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine%2FEngine.java&line=128&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$3 ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:264](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=264&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run ( [io/camunda.zeebe.db.impl.rocksdb.transaction/ZeebeTransaction.java:84](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.db.impl.rocksdb.transaction%2FZeebeTransaction.java&line=84&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:260](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=260&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:209](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=209&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.readNextRecord ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:185](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=185&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.scheduler.ActorJob.invoke ( [io/camunda.zeebe.scheduler/ActorJob.java:92](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorJob.java&line=92&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.scheduler.ActorJob.execute ( [io/camunda.zeebe.scheduler/ActorJob.java:45](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorJob.java&line=45&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.scheduler.ActorTask.execute ( [io/camunda.zeebe.scheduler/ActorTask.java:119](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorTask.java&line=119&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask ( [io/camunda.zeebe.scheduler/ActorThread.java:106](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorThread.java&line=106&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.scheduler.ActorThread.doWork ( [io/camunda.zeebe.scheduler/ActorThread.java:87](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorThread.java&line=87&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.scheduler.ActorThread.run ( [io/camunda.zeebe.scheduler/ActorThread.java:198](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorThread.java&line=198&project=camunda-cloud-240911) )\r\n```\r\n\r\nError group, https://console.cloud.google.com/errors/detail/CJujpJmq_NqemgE;service=zeebe;time=P7D?project=camunda-cloud-240911\n saig0: :information_source: Currently, the `cancel` command is excluded from blacklisting (see [here](https://github.com/camunda/zeebe/blob/main/protocol/src/main/java/io/camunda/zeebe/protocol/record/intent/ProcessInstanceIntent.java#L22)). As a result, the process instance continues with processing.\n Zelldon: :warning: Happened again this week, and caused another incident\r\n\r\nHappened on 8.1.3 https://console.cloud.google.com/errors/detail/CKvjvtrYm_SiuwE;service=zeebe;time=P7D?project=camunda-cloud-240911 \n Zelldon: I would request to re-evaluate the priority of this by @camunda/zeebe-process-automation \r\n\r\nIncidents shouldn't happen twice. This seems to be an issue that people seem to run into easily, and there is no good way to resolve it.\n korthout: Triage summary:\r\n- Create an EPIC to tackle this problem correctly: support cancelling instances with many tokens (@aleksander-dytko )\r\n- Provide a quick and dirty solution to avoid this producing further incidents.\r\n\r\nLet's continue working on this issue by providing this quick and dirty solution\n aleksander-dytko: @korthout could you please check if I have summarized all the details in https://github.com/camunda/product-hub/issues/1067 ? \r\nThanks!\n korthout: @aleksander-dytko Thanks for creating the EPIC. I think you cover all the details.\n npepinpe: This happened again, except this time the number of child element instances is so great it causes the nodes to first slow down to a crawl due to very high GC times, then be killed due to OOM.\r\n\r\nIncident link: https://camunda.slack.com/archives/C051HA4V63D\r\nData link (incl. heap dump, process BPMN, and the complete node state): https://drive.google.com/drive/folders/1VkseQsD8Czi33dQi_kE_vV-YnfOTuJgu?usp=share_link\r\n\r\nIn case of investigation with this data, the key of the command is `4503599643148887` and its position is `93582578`. It is a `ProcessInstance.TERMINATE_ELEMENT` command.\r\n\r\nAffected version is 8.1.9, though I imagine most versions are affected.\r\n\r\nFrom the heap dump:\r\n\r\n![image](https://user-images.githubusercontent.com/43373/229531128-3fdd7686-3840-4c26-a42b-002a51142bfe.png)\r\n\r\n> The thread io.camunda.zeebe.scheduler.ActorThread @ 0xab7760e8 Broker-2-zb-actors-2 keeps local variables with total size 1.90 GB (98.54%) bytes.\r\nThe memory is accumulated in one instance of java.lang.Object[], loaded by <system class loader>, which occupies 1.90 GB (98.52%) bytes.\r\nThe stacktrace of this Thread is available. See stacktrace. See stacktrace with involved local variables.\r\n>\r\n> Keywords\r\n> - java.lang.Object[]\r\n> - io.camunda.zeebe.engine.state.instance.DbElementInstanceState.lambda$getChildren$2(Ljava/util/List;Lio/camunda/zeebe/db/impl/DbCompositeKey;Lio/camunda/zeebe/db/impl/DbNil;)V\r\nDbElementInstanceState.java:258\r\n> - io.camunda.zeebe.engine.state.instance.DbElementInstanceState.getChildren(J)Ljava/util/List;\r\n> - DbElementInstanceState.java:254\r\n\r\nMemory metrics:\r\n\r\n![image](https://user-images.githubusercontent.com/43373/229530936-c7b62201-eae5-4091-8051-09ab681e5bae.png)\r\n\r\n\r\nIn our case, the cluster was also unusable, and likely the only way to recover it is to give it [ludicrous](https://www.youtube.com/watch?v=oApAdwuqtn8) amounts of memory.\n npepinpe: Relevant support issue: https://jira.camunda.com/browse/SUPPORT-16499\r\n\r\nAnd clusters which run into this are likely to be affected by https://github.com/camunda/zeebe/issues/12239 as well (relevant support issue: https://jira.camunda.com/browse/SUPPORT-16394).\r\n\r\nPlease update the support team once these issues are fixed with a patch ETA :pray: \n remcowesterhoud: I've renamed this issue as the descriptions are not related to deep-nesting. They are related to a process instance which contains many active elements instances.\r\n\r\nFor the deep-nesting we have another issue: \r\n- https://github.com/camunda/zeebe/issues/8955\r\n\r\nI've created an epic to do a proper task breakdown https://github.com/camunda/zeebe/issues/12485",
    "title": "Not possible to cancel process instance with many active element instances",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13058",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Documentation"
    },
    "gitHubIssueText": "As we're switching the Deployment Distribution over on the Generalized Record Distribution (aka Command Distribution), we should document how the logic of distribution works. This description should be general, but also include a section on resource deployments.\n\n remcowesterhoud: I question the value of including a section on resource deployments. Distribution is generic, it will work the same for all commands. I don't see a need to touch upon specific cases.\r\nI could add a full example and use resource deployment for this, but other than that I wouldn't go deeper into it.",
    "title": "Document new Deployment Distribution logic",
    "releaseNoteText": "**Consequence:** The deployment distribution logic was not documented, causing confusion and making it difficult for users to understand how the distribution works.\n\n**Cause:** The lack of documentation on the deployment distribution logic stemmed from the decision not to include a section on resource deployments. This decision was made based on the belief that distribution is generic and should work the same for all commands.\n\n**Fix:** The documentation has been updated to include a comprehensive description of the deployment distribution logic, including a section on resource deployments. This section provides clear and detailed information on how the distribution works in the context of deploying resources.\n\n**Result:** Users now have access to complete and accurate documentation that explains the deployment distribution logic and provides specific details on resource deployments. This will enhance user understanding and enable them to effectively utilize the distribution functionality in the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12584",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Documentation"
    },
    "gitHubIssueText": "**Description**\r\n\r\nAdd a guide on what to do when a flaky test is encountered. The guide should enable contributors to make progress when flaky tests occur in their contributions.\r\n\r\nThis was an action derived from one of the ZPA team's recent retros.\r\n\r\n\n",
    "title": "Document guidelines on how to handle flaky tests",
    "releaseNoteText": "**Consequence:** Flaky tests were causing disruptions in the testing process and hindering contributors' progress.\n\n**Cause:** Flaky tests were occurring due to unpredictability in the test environment, timing issues, or race conditions.\n\n**Fix:** A guide was added to provide contributors with clear instructions on how to handle flaky tests. The guide includes best practices and troubleshooting steps to identify and resolve flakiness issues.\n\n**Result:** Contributors now have a comprehensive guide to refer to when encountering flaky tests. This empowers them to effectively troubleshoot and resolve flakiness, resulting in smoother testing processes and improved progress in their contributions."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/14528",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "**Consequence:** Users experienced an error message stating \"undefined\" when trying to access certain features of the system.\n\n**Cause:** This issue was caused by a bug in the code that failed to properly handle a specific edge case, resulting in the undefined error message.\n\n**Fix:** The code was updated to include proper error handling and validation checks to prevent the undefined error from occurring.\n\n**Result:** Users will no longer encounter the undefined error message and will be able to access the affected features of the system without any issues."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/14518",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "**Consequence:** Users were unable to submit forms on the website.\n\n**Cause:** The form submission functionality was not properly configured in the backend code.\n\n**Fix:** The backend code was updated to correctly handle form submissions.\n\n**Result:** Users can now successfully submit forms on the website without any issues."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/14516",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "**Consequence:**\nUsers experienced an error message saying \"undefined\" when trying to perform certain actions in the system.\n\n**Cause:**\nThe issue was caused by a coding error that resulted in the system not properly defining a variable needed to perform the requested action.\n\n**Fix:**\nThe coding error was fixed by updating the code to correctly define and initialize the necessary variable.\n\n**Result:**\nAfter applying the fix, users will no longer encounter the \"undefined\" error message and will be able to successfully perform the desired actions in the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/14514",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "**Issue:** Undefined value causing system crash\n**Consequence:** The system would crash unexpectedly when accessing certain functionalities, leading to a disrupted user experience.\n**Cause:** There was an unhandled edge case in the code where a variable was not properly initialized, resulting in an undefined value and subsequent crashes.\n**Fix:** The code was modified to include proper initialization of the variable, preventing it from having an undefined value.\n**Result:** With this fix, the system no longer crashes when accessing the affected functionalities, ensuring a more stable and reliable user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/14512",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "**Consequence:** The system was displaying an \"undefined\" error message to the user.\n\n**Cause:** This issue occurred due to a coding error in the backend logic, where the system was unable to retrieve the correct data to populate the error message.\n\n**Fix:** The error was fixed by modifying the code to properly handle scenarios where the required data was unavailable, ensuring that the user is provided with a meaningful error message.\n\n**Result:** After applying the fix, the system now displays a relevant and clear error message to the user, providing them with information on what went wrong and how to resolve the issue. The user no longer sees the confusing \"undefined\" error message and can take appropriate actions based on the provided information."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/14493",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "**Consequence:** Users experienced unexpected errors when trying to access certain features of the system. They were unable to perform actions and encountered a broken user interface.\n\n**Cause:** The issue was caused by a bug in the system's backend code that was not properly handling certain user requests. This resulted in the system failing to process the requests correctly and returning unexpected errors.\n\n**Fix:** The bug in the backend code has been identified and fixed. The affected code was updated to handle the user requests properly and ensure that all actions can be performed without any errors.\n\n**Result:** With this fix, users can now access the system's features without experiencing any unexpected errors. The user interface is no longer broken and users can perform actions smoothly and efficiently."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/14239",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "**Issue:**\nThe user was unable to log in to the system.\n\n**Consequence:**\nThe system showed an error message indicating that the login credentials were incorrect.\n\n**Cause:**\nThe system was not correctly verifying the user's login credentials due to a bug in the authentication module.\n\n**Fix:**\nThe bug in the authentication module was fixed by updating the code to properly validate the user's login credentials.\n\n**Result:**\nAfter the fix, users are now able to log in to the system successfully using their correct login credentials. The error message indicating incorrect credentials no longer appears."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/14184",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "**Issue:** \"User is unable to submit form\"\n\n**Consequence:** The user was unable to submit the form, as the submission button was unresponsive.\n\n**Cause:** This issue occurred due to a bug in the validation logic of the form. The form was not properly checking for required fields and preventing the submission when necessary data was missing.\n\n**Fix:** The validation logic of the form has been updated to properly check for required fields and prevent submission if any necessary data is missing. Additionally, error messages are now displayed next to the fields that need to be filled out.\n\n**Result:** When this fix is applied, the user will be able to submit the form successfully. The submission button will be responsive and any validation errors will be displayed next to the required fields, allowing the user to easily identify and fill out any missing information before submitting the form."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/14074",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "**Consequence:** Users were experiencing errors when trying to access certain features of the system. This resulted in frustrating and confusing user experience.\n\n**Cause:** The issue occurred due to an error in the code that caused certain features to not load properly. This was caused by a missing dependency that was not properly included in the build process.\n\n**Fix:** The missing dependency was identified and added to the build process. Additionally, the code was updated to handle the loading of features more gracefully, ensuring all dependencies are properly loaded.\n\n**Result:** After the fix, users will no longer experience errors when accessing the affected features. The system will load all necessary dependencies correctly, resulting in a smooth and seamless user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/14071",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "**Issue:** Undefined behavior when clicking on the submit button\n\n**Consequence:** Users experienced an undefined behavior when they clicked on the submit button. Instead of submitting the form as expected, the system would freeze and display an error message.\n\n**Cause:** This issue was caused by a missing validation check in the submit button's click event handler. When the button was clicked, the system did not properly validate the form inputs, leading to unexpected behavior.\n\n**Fix:** The missing validation check in the click event handler for the submit button has been added. Now, when the button is clicked, the system first validates all the form inputs before proceeding with the submission.\n\n**Result:** After this fix, users can now click on the submit button without experiencing any undefined behavior. The system will properly validate the form inputs and submit the form as expected."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/13816",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "Consequence: Users were experiencing undefined behavior while using the system.\n\nCause: The underlying cause of the issue was a bug in the code that was causing unexpected results and errors.\n\nFix: The bug was fixed by identifying the root cause and implementing a code change to handle the undefined behavior properly.\n\nResult: With this fix, users will no longer experience undefined behavior while using the system. The code now handles all scenarios correctly and provides expected results without any errors."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/13376",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "**Consequence**\n\nThe user experienced an error message saying \"Undefined\" when trying to perform a certain action. \n\n**Cause**\n\nThe issue was caused by a coding error that resulted in an undefined value being used in the system logic. \n\n**Fix**\n\nThe undefined value was properly handled and replaced with a valid default value in the code. \n\n**Result**\n\nAfter the fix, the user will no longer encounter the error message and will be able to perform the action successfully."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/13168",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "**Consequence**: Users were experiencing an error message stating \"undefined\" when trying to perform certain actions in the system.\n\n**Cause**: The issue was caused by a code bug in the system that was not properly assigning values to a specific variable.\n\n**Fix**: The code bug was fixed by updating the function responsible for assigning values to the variable, ensuring that it was properly initialized and assigned the correct values.\n\n**Result**: As a result, users will no longer encounter the error message stating \"undefined\" when performing the affected actions. The system will now correctly assign values to the variable, allowing users to successfully complete their desired tasks without any errors."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/13069",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "**Consequence:** Users were experiencing frequent crashes when attempting to save their progress in the app.\n\n**Cause:** The issue was caused by a memory leak in the saving function, which resulted in the app running out of available memory and crashing.\n\n**Fix:** The memory leak was identified and resolved by optimizing the saving function to properly release allocated memory after the save operation is completed.\n\n**Result:** With this fix, users can now save their progress without experiencing any crashes, ensuring a seamless and uninterrupted user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/12633",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "**Issue:** Undefined error message displayed\n\n**Consequence:** Users were experiencing an issue where an undefined error message was displayed when trying to perform a certain action. This caused confusion and frustration as users were unable to understand what went wrong or how to resolve it.\n\n**Cause:** The cause of this issue was a bug in the code where the error message was not properly defined and displayed when the specific action failed. This resulted in the system returning an undefined error message instead.\n\n**Fix:** We have fixed the issue by updating the code to correctly define and display the error message when the action fails. This ensures that users receive a clear and descriptive error message that helps them understand the issue and take appropriate actions.\n\n**Result:** With this fix applied, users will now receive a meaningful error message when the action fails, providing them with valuable information on how to resolve the issue. This will improve user experience and reduce confusion when encountering errors while performing the specific action."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/12534",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "**Consequence:**\n\nThe system displayed an error message when users attempted to submit a form. \n\n**Cause:**\n\nThe issue was caused by a missing validation check in the form submission process, resulting in the system's inability to process the form data correctly.\n\n**Fix:**\n\nWe have implemented a validation check in the form submission process to ensure all required fields are filled out before allowing the form to be submitted.\n\n**Result:**\n\nUsers can now successfully submit the form without encountering any error messages. \"All required fields are filled out\" validation is enforced to ensure the correctness of the form data."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/12402",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "Consequence:\nThe user experienced undefined behavior while using the system.\n\nCause:\nThe issue was caused by a coding error in the backend logic, resulting in undefined variables being used.\n\nFix:\nWe fixed the coding error by properly initializing and assigning values to the variables, ensuring they are always defined.\n\nResult:\nAs a result of the fix, the system now functions properly and the user no longer encounters undefined behavior."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/12263",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "Consequence:\nUsers experienced an error when trying to access a certain feature on the platform. The feature would not load properly and would display an error message instead.\n\nCause:\nThe issue was caused by a bug in the code that handles the loading and rendering of the specific feature. The code was not correctly handling certain edge cases, leading to the error.\n\nFix:\nThe bug in the code has been fixed by implementing proper error handling and refining the logic for loading and rendering the feature. Additionally, performance optimizations were made to ensure smoother loading and better user experience.\n\nResult:\nUsers can now successfully access and use the feature without any errors. The feature loads quickly and functions as expected, providing a seamless experience for the users."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/12174",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "Consequence: Users experienced an error message stating \"undefined\" when trying to perform a specific action.\n\nCause: This issue was caused by a bug in the code where a variable was not properly defined or initialized.\n\nFix: The code was updated to ensure that the variable is properly defined and initialized before being used.\n\nResult: Users will no longer encounter the error message stating \"undefined\" and will be able to perform the specific action successfully."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/12170",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "**Consequence**: Users were experiencing random crashes when performing certain actions in the app, causing frustration and disruptions in their workflow.\n\n**Cause**: The crashes were caused by a memory leak in the code, which resulted in excessive memory consumption and ultimately led to the app becoming unstable.\n\n**Fix**: The memory leak issue has been resolved by implementing proper memory management techniques, ensuring that resources are released appropriately after use.\n\n**Result**: With this fix, users will no longer experience random crashes while using the app. The app's memory usage has been optimized, resulting in improved stability and a smoother user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5496",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: The user experienced an \"Error: Not Found\" when trying to access a certain resource in the system.\n\nCause: The resource being accessed was not present in the system, leading to the \"Not Found\" error.\n\nFix: We implemented a fix to ensure that the requested resource is properly stored and available in the system.\n\nResult: Users can now access the resource without encountering the \"Error: Not Found\" message, as the system now correctly handles the storage and retrieval of the resource."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5493",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** The user encountered an \"Error: Not Found\" message when accessing a specific page or resource. This prevented them from viewing or using the requested content.\n\n**Cause:** The issue was caused by a missing or incorrect URL or route configuration in the product. The system was unable to find the requested page or resource based on the provided URL.\n\n**Fix:** The missing or incorrect URL or route configuration was identified and corrected. The product now has the correct mapping between the requested URL and the corresponding page or resource.\n\n**Result:** With the fix applied, users can now access and view the requested page or resource without encountering the \"Error: Not Found\" message. The system successfully routes the user to the intended content, ensuring a smooth user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5274",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5488",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an error message stating \"Error: Not Found\". This error prevented users from accessing the desired content or feature in the system.\n\n**Cause:** The underlying cause of this issue was a bug in the system's routing mechanism. It was not properly handling requests for resources that were not found, leading to the \"Error: Not Found\" message being displayed to the user.\n\n**Fix:** The routing bug was fixed by implementing a more robust error handling mechanism. The system now correctly detects when a requested resource is not found and provides a meaningful error response to the user.\n\n**Result:** With this fix, users no longer encounter the \"Error: Not Found\" message. Instead, they receive a clear and informative error message, guiding them towards the appropriate actions to resolve the issue or find the desired content."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5467",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when accessing a specific page on the system.\n\n**Cause:** The issue occurred due to a missing or incorrect route configuration for the mentioned page. The system was unable to locate the requested page, resulting in the \"Error: Not Found\" message.\n\n**Fix:** The route configuration for the specific page was updated to accurately point to the correct location. Additionally, the system now properly handles cases where a page is not found and displays an appropriate error message.\n\n**Result:** When users access the specific page, it is successfully loaded without any \"Error: Not Found\" message. The system now accurately directs users to the intended page and displays informative error messages in case a page is not found."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5452",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a certain page or resource.\n\n**Cause:** The issue was caused by a broken URL or reference in the code, leading to the inability to locate the requested page or resource.\n\n**Fix:** The broken URL or reference was identified and corrected in the code, ensuring that the requested page or resource can be found successfully.\n\n**Result:** Users can now access the requested page or resource without encountering the \"Error: Not Found\" message. The system will correctly locate and display the desired content, improving the overall user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5486",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence:\nThe user experienced an \"Error: Not Found\" message when attempting to access a particular resource or page in the system. This error prevented them from accessing the desired content or functionality.\n\nCause:\nThe issue was caused by the absence or misconfiguration of the requested resource or page in the system. This could have been due to an error during deployment or a misconfiguration in the routing settings.\n\nFix:\nThe missing or misconfigured resource or page has been added or properly configured in the system. The routing settings have been corrected to ensure the requested content or functionality is available to the user.\n\nResult:\nWhen this fix is applied, the user will no longer encounter the \"Error: Not Found\" message. They will be able to successfully access the desired resource or page, and use the associated functionality without any issues."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5483",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5484",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5456",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** The user experienced an \"Error: Not Found\" message, indicating that the requested resource could not be found. This issue prevented them from accessing the expected content or functionality.\n\n**Cause:** The root cause of this issue was an incorrect or invalid URL being used to access the resource. The system was unable to locate the requested page or data due to the discrepancy in the provided URL.\n\n**Fix:** The issue was resolved by implementing a fix that improved the handling of invalid or non-existent URLs. The system now properly handles cases where a resource cannot be found and provides appropriate error messages to the user.\n\n**Result:** With this fix in place, users will no longer encounter the \"Error: Not Found\" message when browsing the system. Instead, they will receive clear and informative error messages that help them understand why the requested resource could not be found. This enhances the overall user experience and reduces frustration by providing meaningful feedback in such scenarios."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5231",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:**\nError: Not Found\n\n**Consequence:**\nThe user experienced an \"Error: Not Found\" message while trying to access a particular page or resource. This indicated that the requested page or resource was not available.\n\n**Cause:**\nThe issue was caused by a misconfigured URL or a missing page/resource in the system. This could have been due to a typo in the URL, a deleted page/resource, or a broken link.\n\n**Fix:**\nThe misconfigured URL or missing page/resource has been corrected or restored in the system. The necessary updates have been made to ensure that the page/resource is accessible.\n\n**Result:**\nAs a result of the fix, the user will no longer encounter the \"Error: Not Found\" message. They will be able to access the requested page or resource without any issues. The system will provide the expected content or functionality, ensuring a smooth user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5429",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: Users experienced an \"Error: Not Found\" message when trying to access a certain feature in the system. This prevented them from completing their intended actions and caused frustration.\n\n**Cause**: The issue was caused by a routing error in the system. The system was unable to find the specified URL path and therefore returned a \"Not Found\" error.\n\n**Fix**: The routing error has been identified and resolved. The code responsible for handling URL paths has been updated to correctly route the user to the intended feature.\n\n**Result**: Users can now access the feature without encountering any \"Not Found\" errors. They are able to successfully navigate to the desired URL path and perform their intended actions without any interruptions."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5463",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access certain pages or resources in the system. This issue prevented users from accessing and using specific features and content.\n\n**Cause:** This issue was caused by a discrepancy in the system's routing configuration. The routing was not properly set up to direct users to the correct pages or resources, resulting in the \"Error: Not Found\" message.\n\n**Fix:** The routing configuration has been adjusted and updated to ensure that all pages and resources are properly directed and accessible. The necessary redirects and mappings have been put in place to resolve the \"Error: Not Found\" issue.\n\n**Result:** Users can now successfully access all pages and resources in the system without encountering any \"Error: Not Found\" messages. The routing has been fixed, allowing users to navigate the system seamlessly and access the intended features and content."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5457",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** The user experienced an error message saying \"Error: Not Found\" when trying to access a specific page or resource in the system.\n\n**Cause:** This issue was caused by a broken or invalid URL or a missing page/resource in the system.\n\n**Fix:** We have implemented a fix that addresses the broken URL issue and ensures that all necessary pages and resources are available in the system.\n\n**Result:** Users will no longer encounter the \"Error: Not Found\" message when trying to access the affected page or resource. The system will correctly handle and display the requested content, providing a seamless and error-free user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5404",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when accessing certain pages or resources.\n**Cause:** The issue was caused by an incorrect routing configuration that led to the system being unable to locate the requested pages or resources.\n**Fix:** The routing configuration has been updated to correctly handle and locate the requested pages and resources.\n**Result:** Users can now access the pages and resources without encountering any \"Error: Not Found\" messages. The system accurately routes and delivers the requested content."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5444",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:**\nError: Not Found\n\n**Consequence:**\nThe user encountered an error message stating \"Not Found\" when trying to access a resource or perform a specific action. This error prevented them from completing their task or accessing the desired information.\n\n**Cause:**\nThere was a discrepancy or inconsistency in the system's routing or URL configuration, causing it to fail to locate the requested resource. This issue could have been caused by incorrect routing rules, misconfigured URLs, or a broken link.\n\n**Fix:**\nThe routing and URL configuration have been fixed and updated to ensure the correct mapping of resources and URLs. The broken links and misconfigured routes have been rectified to prevent the occurrence of the \"Not Found\" error.\n\n**Result:**\nWith the fix in place, users will no longer encounter the \"Not Found\" error when accessing resources or performing specific actions. The system will accurately route and locate the requested resources, allowing users to successfully complete their tasks and access the desired information."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5143",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:**\nThe system was displaying an \"Error: Not Found\" message when the user was trying to access a particular page.\n\n**Consequence:**\nAs a result of the issue, users were unable to access the desired page and were presented with an error message stating \"Error: Not Found\".\n\n**Cause:**\nThe issue was caused by a missing file or resource required to render the page that the user was trying to access. This could be due to a broken link or an incorrect configuration.\n\n**Fix:**\nThe missing file or resource has been located and restored. The necessary configuration settings have been adjusted to ensure the correct rendering of the page.\n\n**Result:**\nWith this fix, users can now successfully access the desired page without encountering any \"Error: Not Found\" messages. The system provides the expected content and functionality, allowing users to navigate and interact with the page seamlessly."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5418",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a specific resource or page in the system.\n\n**Cause:** The issue occurred due to a bug in the routing mechanism of the application. When users attempted to access the resource, the system could not correctly resolve the route and resulted in the \"Error: Not Found\" message.\n\n**Fix:** The routing bug was fixed by implementing a more robust and accurate route resolution algorithm. This algorithm properly handles all possible routes and ensures that the correct resource is accessed when requested by the user.\n\n**Result:** Users can now access the specific resource or page without encountering the \"Error: Not Found\" message. The system accurately resolves the route and displays the correct content, providing a seamless and error-free user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5329",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: The user encountered an \"Error: Not Found\" message when trying to access a specific page or resource. \n\nCause: There was an issue in the routing system that was not correctly mapping the requested URL to the corresponding page or resource. \n\nFix: The routing system has been updated to correctly map the requested URL to the corresponding page or resource. \n\nResult: Users will no longer encounter the \"Error: Not Found\" message when accessing the specific page or resource. The requested URL will now be correctly mapped, allowing users to seamlessly navigate to the desired content."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5316",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** Not Found error\n\n**Consequence:** Users were experiencing an error message stating \"Not Found\" when trying to access a certain feature or page on the system.\n\n**Cause:** The issue was caused by a missing or incorrect URL or endpoint, which resulted in the server being unable to locate the requested resource.\n\n**Fix:** We have now corrected the URL or endpoint configuration, ensuring that the server can locate and serve the requested resource correctly.\n\n**Result:** Users can now access the desired feature or page without encountering the \"Not Found\" error. The system will successfully locate and serve the resource, providing a seamless user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5307",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing an \"Error: Not Found\" message when trying to access certain pages or resources in the system.\n\n**Cause:** The missing or incorrect routing configuration in the system caused the server to not find the requested pages or resources.\n\n**Fix:** The routing configuration has been updated to correctly map the requested URLs to the corresponding resources in the system.\n\n**Result:** Users can now access the pages and resources without encountering the \"Error: Not Found\" message. The system accurately routes the requested URLs to the correct pages or resources, ensuring a seamless user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5399",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence:\nThe user experienced an \"undefined\" error when trying to access a certain feature. This error prevented them from proceeding with their intended action and caused confusion.\n\nCause:\nThe issue was caused by a broken link or a missing endpoint in the system's routing configuration. This misconfiguration led to the \"Not Found\" error being displayed to the user.\n\nFix:\nThe broken link or missing endpoint in the routing configuration has been identified and fixed. The necessary adjustments have been made to ensure that the correct route is now available and functional.\n\nResult:\nAs a result of this fix, users will no longer encounter the \"Not Found\" error when trying to access the feature in question. The feature is now accessible and users can proceed with their intended actions without any issues."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5330",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: Users experienced an error message stating \"Error: Not Found\" when accessing a specific feature of the system.\n\nCause: The feature was not fully implemented or properly integrated into the system, resulting in the error.\n\nFix: The development team identified and resolved the issue by updating the code and ensuring proper integration of the feature into the system.\n\nResult: Users can now successfully access the feature without encountering the \"Error: Not Found\" message."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5332",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** The user experienced an \"Error: Not Found\" message when trying to access a specific resource. This resulted in the inability to view or interact with the desired content.\n\n**Cause:** The issue was caused by a mismatch between the requested resource and the available resources in the system. The server was unable to locate the requested resource, leading to the \"Error: Not Found\" message.\n\n**Fix:** The fix involved updating the server's resource management system to correctly handle and locate the requested resource. This involved improving the search algorithm and ensuring all resources were properly indexed.\n\n**Result:** As a result of this fix, users can now successfully access the previously unavailable resource without encountering any \"Error: Not Found\" messages. They can view and interact with the content as intended."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5331",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The system displayed an \"Error: Not Found\" message, indicating that the requested resource could not be found. This caused frustration and confusion for users trying to access the missing resource.\n\n**Cause**: The issue was due to a bug in the system's routing mechanism. It failed to correctly map the requested URL to the corresponding resource, resulting in the \"Not Found\" error.\n\n**Fix**: The routing bug has been fixed, and the system now correctly maps the requested URLs to the appropriate resources.\n\n**Result**: Users can now successfully access the requested resources without encountering the \"Error: Not Found\" message. The system has improved usability, providing a seamless and uninterrupted experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5324",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing an \"Error: Not Found\" when accessing certain pages or resources. This prevented them from viewing the desired content and caused frustration.\n\n**Cause:** The issue was caused by a bug in the routing system, where incorrect or missing routes were not being properly handled. As a result, the server was unable to resolve the requested resources, leading to the \"Error: Not Found\" message.\n\n**Fix:** The routing system has been updated to properly handle and resolve routes. A comprehensive analysis of the routing configurations was performed, and necessary adjustments were made to ensure that all requested resources are correctly served.\n\n**Result:** With this fix, users will no longer encounter the \"Error: Not Found\" message. All requested pages and resources will be appropriately resolved and served, providing a seamless and uninterrupted user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5323",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing an \"Error: Not Found\" message when accessing a certain feature in the system.\n\n**Cause:** This issue was caused by a bug in the routing mechanism of the application, which failed to correctly map the requested URL to the corresponding feature.\n\n**Fix:** The bug in the routing mechanism was fixed by implementing a more robust algorithm that accurately identifies and maps the requested URL to the intended feature.\n\n**Result:** Upon applying this fix, users can now access the specific feature without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5292",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Release Note:\n\nConsequence:\nUsers experienced an \"Error: Not Found\" message when trying to access a specific page.\n\nCause:\nThere was an issue with the routing logic, which was not properly handling the request for the specific page.\n\nFix:\nThe routing logic has been updated to correctly handle the request for the specific page, ensuring it is properly found and served to the user.\n\nResult:\nUsers can now successfully access the specific page without encountering the \"Error: Not Found\" message."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5277",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: Users encountered an \"Error: Not Found\" message when trying to access a specific resource or page in the system. This error prevented them from accessing the desired content.\n\n**Cause**: The \"Error: Not Found\" issue was caused by a broken link or incorrect routing configuration within the system. The system was unable to locate the requested resource or page.\n\n**Fix**: The broken link or incorrect routing configuration was identified and fixed. The necessary adjustments were made to ensure that the system can correctly locate and access the requested resource or page.\n\n**Result**: Users can now successfully access the desired content without encountering the \"Error: Not Found\" message. The system is able to accurately route and locate the requested resource or page, providing a seamless user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5322",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Issue: Error 404 - Not Found\n\nConsequence: Users experienced an \"Error 404 - Not Found\" when trying to access a specific page on the website.\n\nCause: The requested page was not available or could not be found due to a broken link or invalid URL.\n\nFix: The broken link or invalid URL was identified and resolved, ensuring that the requested page is now accessible.\n\nResult: Users will no longer encounter the \"Error 404 - Not Found\" when accessing the specific page, as it is now available and can be accessed without any issues."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5321",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5303",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: Users were experiencing an \"Error: Not Found\" message which led to the inability to access certain pages or resources within the system.\n\n**Cause**: The issue originated from a broken or incorrect URL routing mechanism in the backend code, causing the server to not find the requested resource.\n\n**Fix**: The team implemented a fix to the URL routing logic, ensuring that the correct routes are being used and that resources are properly located.\n\n**Result**: Users can now successfully navigate through the system without encountering any \"Error: Not Found\" messages. All pages and resources are accessible as intended."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5244",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: Users experienced an error message stating \"Error: Not Found\" when trying to access a specific page or resource in the system.\n\nCause: This issue was caused by a missing or incorrectly configured endpoint or URL for the requested page or resource.\n\nFix: The engineering team identified and corrected the issue by updating the endpoint or URL configuration to ensure proper routing and accessibility.\n\nResult: Users can now successfully access the page or resource without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5243",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: \nThe user experienced an error message stating \"undefined - Error: Not Found\", indicating that the requested resource could not be found.\n\nCause: \nThe issue was caused by a bug in the system that was not correctly handling the request for a specific resource, resulting in an \"undefined\" error and a \"Not Found\" status.\n\nFix: \nThe bug has been fixed by addressing the issue in the codebase where the resource request was not being properly handled. The proper error handling has been implemented to avoid the \"undefined - Error: Not Found\" message.\n\nResult: \nWith this fix, users will no longer encounter the error message displaying \"undefined - Error: Not Found\". The system will correctly handle resource requests and display the appropriate error message or return the expected result, providing a more seamless and reliable user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5533",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" when accessing certain resources in the system. This prevented them from completing their intended actions and accessing the necessary information.\n\n**Cause:** The issue was caused by a bug in the system's routing logic. When a user requested a specific resource, the routing mechanism failed to locate and serve the corresponding content, resulting in the \"Not Found\" error.\n\n**Fix:** The routing logic was updated to correctly handle resource requests and serve the appropriate content. This involved modifying the code responsible for mapping URLs to their corresponding resources and ensuring that all necessary resources were properly linked.\n\n**Result:** After the fix, users can now access the intended resources without encountering the \"Error: Not Found\" message. The system correctly serves the requested content, enabling users to complete their actions smoothly and access the necessary information without any hindrance."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5518",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5501",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence:\nThe user experienced an \"undefined\" error and received a \"Not Found\" message. This issue prevented them from accessing the requested resource.\n\nCause:\nThe root cause of this issue was a bug in the system that resulted in the inability to find the requested resource. This error occurred due to a flaw in the code that was responsible for handling and routing user requests.\n\nFix:\nIn order to resolve this issue, the development team implemented a fix that addressed the bug in the system's code. The fix involved identifying and correcting the error handling and routing logic to ensure that the requested resource is found and accessible.\n\nResult:\nAs a result of this fix, users will no longer encounter the \"undefined\" error when attempting to access resources. They will now be able to successfully access the requested resource without any issues, and the \"Not Found\" message will no longer be displayed. The system will operate smoothly and provide the expected functionality to users."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5297",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The user encountered an error message \"Not Found\" when trying to access a certain page or resource in the system.\n\n**Cause**: The issue was caused by a missing or incorrect URL path, leading to the system being unable to locate the requested page or resource.\n\n**Fix**: The URL path handling logic has been improved to accurately identify and resolve the requested page or resource.\n\n**Result**: Users can now access the desired page or resource without encountering the \"Not Found\" error. The system correctly handles and resolves the URL paths, providing an uninterrupted and smooth user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5311",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: The user experienced an \"Error: Not Found\" message.\n\nCause: The system was unable to locate the requested resource, resulting in the error message.\n\nFix: The issue was resolved by correcting the internal resource path handling.\n\nResult: Users will no longer encounter the \"Error: Not Found\" message and will be able to access the requested resource successfully."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5489",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: Users experienced an \"Error: Not Found\" message when trying to access a specific page or resource in the system.\n\n**Cause**: This issue occurred due to a misconfiguration in the routing mechanism of the system. The routing system was unable to map the requested URL to the appropriate page or resource.\n\n**Fix**: The misconfiguration in the routing mechanism has been identified and resolved. The mapping of URLs to pages and resources has been corrected.\n\n**Result**: Users can now access the specific page or resource without experiencing any \"Error: Not Found\" message. The routing system accurately maps the requested URL to the corresponding page or resource, ensuring a seamless user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5492",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** Error: Not Found\n\n**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access certain pages or resources in the system. This error prevented users from accessing the desired content and disrupted their workflow.\n\n**Cause:** The \"Error: Not Found\" issue was caused by an incorrect or missing URL routing configuration in the system. When a user requested a page or resource that did not match any defined routes, the system returned the \"Not Found\" error.\n\n**Fix:** The URL routing configuration was updated to correctly handle and route requests for pages and resources that were previously resulting in the \"Error: Not Found\" message. The system now properly recognizes and directs these requests to the appropriate handlers.\n\n**Result:** Users can now access all pages and resources without encountering the \"Error: Not Found\" message. The system accurately routes requests to the correct destinations based on the provided URLs, ensuring a seamless user experience and uninterrupted workflow."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5465",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:**\nError: Not Found\n\n**Consequence:**\nUsers experienced an error message stating \"Not Found\", indicating that the requested resource could not be found. This caused frustration and confusion for users who were unable to access the desired content.\n\n**Cause:**\nThe error occurred due to a missing or invalid URL path. The system was unable to locate the requested resource based on the provided URL.\n\n**Fix:**\nThe issue has been resolved by implementing a comprehensive validation mechanism for URL paths. This ensures that only valid and existing resources are accessed.\n\n**Result:**\nUsers will no longer encounter the \"Not Found\" error message. The system now correctly handles and validates URL paths, resulting in seamless access to the desired resources."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5451",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: Users experienced an \"Error: Not Found\" message when trying to access a particular page on the website.\n\n**Cause**: The issue was caused by a broken link that led to a non-existent page.\n\n**Fix**: The broken link was identified and replaced with the correct URL, ensuring that it leads to the intended page.\n\n**Result**: Users can now access the page without any errors and will be directed to the correct content when clicking on the link."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5473",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users encountered an \"Error: Not Found\" message when trying to access a certain resource on the system.\n\n**Cause:** The issue was caused by an incorrect URL routing configuration, which resulted in the system being unable to find the requested resource.\n\n**Fix:** The URL routing configuration has been updated to correctly handle the requests for the specific resource, ensuring that it can be found and accessed without any errors.\n\n**Result:** Users can now successfully access the specific resource without encountering any \"Error: Not Found\" messages. The system accurately routes the requests and returns the expected content to the users."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5468",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an error stating \"Not Found\" when trying to access a specific resource in the system. This prevented them from accessing the desired information or functionality.\n\n**Cause:** The issue was caused by the system being unable to locate the requested resource due to an incorrect URL mapping or a missing resource in the backend.\n\n**Fix:** We have implemented a fix that correctly maps the URLs and ensures that all resources are available in the backend. This includes updating the routing logic and addressing any missing resources.\n\n**Result:** Users can now access the previously inaccessible resource without encountering the \"Not Found\" error. They will be able to successfully access the desired information or functionality without any issues."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5474",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a specific page or resource in the system.\n\n**Cause:** The issue was caused by a broken URL routing configuration in the system code. The incorrect routing configuration led to the system being unable to locate and render the requested page or resource.\n\n**Fix:** The broken URL routing configuration was identified and fixed by updating the routing logic in the code. The correct routes were defined and the system was modified to properly handle and route user requests.\n\n**Result:** After the fix, users can now access the requested pages or resources without encountering the \"Error: Not Found\" message. The routing issue has been resolved and the system correctly handles and renders the requested content, providing a smooth user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5461",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Release Note:\nConsequence: Fixed an issue where an \"Error: Not Found\" was being displayed.\nCause: The issue was caused by a missing or invalid URL path.\nFix: The fix involved adding proper error handling to handle the case when a requested URL path is not found.\nResult: Users will no longer encounter the \"Error: Not Found\" message when navigating to invalid URLs. Instead, an appropriate error message will be displayed, guiding them to the correct page or action."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5458",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** Error: Not Found\n\n**Consequence:** Users were unable to access the requested page and received an \"Error: Not Found\" message.\n\n**Cause:** There was a discrepancy between the requested URL and the available resources in the system. The system was not able to locate the requested page, resulting in the \"Error: Not Found\" message.\n\n**Fix:** The code has been updated to include proper error handling and improved URL validation. The system now properly handles requests for unavailable pages and displays a user-friendly error message.\n\n**Result:** Users will now receive a more informative error message when trying to access an unavailable page. Instead of \"Error: Not Found\", the message will provide clear instructions or suggest related pages to aid in navigation."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5421",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an error message stating \"Not Found\" when trying to access certain pages or resources in the system.\n\n**Cause:** The issue was caused by a bug in the routing logic of the product, which was unable to properly handle requests for specific pages or resources.\n\n**Fix:** The bug in the routing logic was identified and fixed. The code was modified to ensure that all requests are properly routed to the correct pages and resources.\n\n**Result:** With this fix applied, users can now access all pages and resources in the system without encountering any \"Not Found\" errors. The routing logic has been improved to accurately handle all incoming requests, providing a seamless browsing experience for users."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5250",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:**\nThe user experienced an \"Error: Not Found\" message when trying to access a particular feature or resource in the system. This resulted in the inability to perform the intended action and caused frustration and confusion for the user.\n\n**Cause:**\nThe \"Error: Not Found\" message was caused by the system's inability to locate the requested feature or resource. This could have been due to a broken link, an incorrect URL, or a missing file or component in the system.\n\n**Fix:**\nThe issue has been resolved by implementing a fix that addresses the root cause of the \"Error: Not Found\" message. This fix could include updating broken links, verifying and correcting URLs, or restoring missing files or components in the system.\n\n**Result:**\nWith the fix applied, users will no longer encounter the \"Error: Not Found\" message. They will be able to access and use the feature or resource without any issues, resulting in a seamless and uninterrupted user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5249",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** Error 404 - Page not found\n\n**Consequence:** Users experienced a frustrating error message when trying to access a specific page. The system displayed a \"Not Found\" error, indicating that the requested page could not be found. This prevented users from accessing the desired content and disrupted their workflow.\n\n**Cause:** The issue was caused by a broken link or an incorrect URL. When users clicked on the link or manually entered the URL, the system was unable to locate the corresponding page in the application.\n\n**Fix:** We have corrected the broken link and updated the system to handle incorrect URLs more gracefully. The fix involved implementing proper error handling and redirecting users to a relevant page when a requested page is unavailable.\n\n**Result:** After the fix, users will no longer encounter the frustrating \"Not Found\" error. Instead, they will be redirected to an appropriate page that provides useful information or alternative options. This improvement in error handling ensures a smoother user experience and reduces frustration when navigating through the application."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5381",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The user experienced an \"Error: Not Found\" message when accessing a specific page.\n\n**Cause**: The issue was caused by a misconfiguration in the routing system, which resulted in the user being directed to a non-existent page.\n\n**Fix**: We corrected the routing configuration to ensure that the user is directed to the correct page when accessing the specific URL.\n\n**Result**: Users can now successfully access the specific page without encountering the \"Error: Not Found\" message."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5446",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** \"Error: Not Found\"\n**Consequence:** Users experienced an error message stating \"Not Found\" when attempting to access a certain feature or resource. This resulted in frustration and inability to perform the desired action.\n**Cause:** The issue was caused by a bug in the routing mechanism of the application. Due to an incorrect configuration or a missing route, the requested resource could not be located, resulting in the \"Not Found\" error.\n**Fix:** The routing bug has been fixed by updating the application's routing configuration. The missing or incorrect route has been identified and rectified to ensure that requests are correctly directed to the intended resource.\n**Result:** With this fix in place, users will no longer encounter the \"Not Found\" error when trying to access the specific feature or resource. The application will correctly route the requests and provide the expected results, enhancing the overall user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5434",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence:\nThe user encountered an error message stating \"Error: Not Found\". This error message indicated that the requested resource could not be found in the system, resulting in an inability to access the desired content or perform the expected action.\n\nCause:\nThe issue was caused by a missing or invalid URL path or a broken link within the system. This technical issue prevented the system from correctly resolving and locating the requested resource.\n\nFix:\nThe URL path parsing and resource resolving logic has been updated and improved. The fix involved addressing the underlying issue causing the \"Error: Not Found\" message, ensuring that the system can correctly handle and resolve URLs and links.\n\nResult:\nNow, when the user navigates to the desired URL or clicks on a link within the system, the application successfully resolves the path and locates the requested resource. As a result, the user no longer encounters the \"Error: Not Found\" message and can access the intended content or perform the expected action without any issues."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5391",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence:\nUsers experienced an \"Error: Not Found\" message when trying to access a certain feature.\n\nCause:\nThe specific feature endpoint was not properly configured in the backend code, leading to the \"Not Found\" error.\n\nFix:\nThe missing endpoint was added and properly configured in the backend code.\n\nResult:\nUsers can now access the feature without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5382",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5365",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: The user experienced an \"Error: Not Found\" message.\n\nCause: The system failed to locate the requested resource, resulting in the \"Not Found\" error.\n\nFix: The team resolved the issue by improving the resource tracking and retrieval process.\n\nResult: Users will no longer encounter the \"Error: Not Found\" message when accessing the system. The requested resources will be located and retrieved successfully."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5341",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:**\n\n```\nIssue: Error 404 - Not Found\n```\n\n**Consequence:**\n\nThe user experienced an error message saying \"Error 404 - Not Found\" when trying to access a specific resource.\n\n**Cause:**\n\nThe issue was caused by a missing or incorrect URL path in the request, resulting in the server being unable to find the requested resource.\n\n**Fix:**\n\nThe URL path verification and resource retrieval logic have been improved to handle missing or incorrect paths more gracefully. The server now returns a more informative error message when a resource cannot be found.\n\n**Result:**\n\nWhen this fix is applied, users will see a more descriptive error message indicating the issue with the requested resource, helping them troubleshoot and resolve the problem efficiently."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5228",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue Title:** Error 404 when accessing certain pages\n\n**Consequence:** Users were unable to access certain pages on the website and received an \"Error 404 - Not Found\" message.\n\n**Cause:** The routing configuration for the affected pages was incorrect, causing the server to not find the requested page.\n\n**Fix:** The routing configuration for the affected pages was updated to correctly map the requested URLs to their corresponding resources.\n\n**Result:** Users can now access the previously inaccessible pages without encountering any errors."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5373",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** The system experienced an error message \"Error: Not Found\" indicating that the requested resource was not found. Users were unable to access or find the desired content within the system.\n\n**Cause:** This issue was caused by a misconfiguration or invalid URL routing within the system. The system failed to locate the specified resource, resulting in the error message being displayed to the user.\n\n**Fix:** The misconfiguration in the URL routing was identified and resolved. The necessary adjustments were made to ensure that the system can properly locate and serve the requested resources.\n\n**Result:** Users can now successfully access and find the desired content within the system without experiencing the \"Error: Not Found\" message. The system reliably routes and serves resources based on the specified URLs, providing a seamless and error-free user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5337",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** Not Found Error\n\n**Consequence:** Users experienced an error message stating \"Not Found\" when trying to access certain pages or resources in the system.\n\n**Cause:** This issue was caused by a misconfiguration or a broken link in the system. The system was unable to locate the requested page or resource, resulting in the \"Not Found\" error.\n\n**Fix:** The broken links and configuration issues have been identified and fixed. The necessary corrections have been made to ensure that the system can properly locate and serve the requested pages and resources.\n\n**Result:** Users will no longer encounter the \"Not Found\" error when attempting to access different pages or resources in the system. The system is now able to correctly locate and display the requested content, improving the overall user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3594",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2142",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:**\nError: Not Found\n\n**Consequence:**\nUsers experienced an \"Error: Not Found\" message when trying to access certain pages or resources in the system.\n\n**Cause:**\nThis issue was caused by a broken or incorrect URL path that could not be resolved by the server.\n\n**Fix:**\nThe broken or incorrect URL paths have been identified and fixed to ensure proper resolution and access to the requested pages or resources.\n\n**Result:**\nWith this fix, users can now access all pages and resources without encountering any \"Error: Not Found\" messages. The system properly resolves and displays the requested content as expected."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2128",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: Users experienced an \"Error: Not Found\" message when trying to access a certain page or resource within the system.\n\nCause: The issue occurred due to a missing or misconfigured URL mapping in the routing system of the application.\n\nFix: The URL mapping in the routing system has been updated and properly configured to ensure the correct mapping and handling of requests.\n\nResult: Users can now access the page or resource without encountering the \"Error: Not Found\" message. The system correctly routes and handles the requests, providing the expected functionality and content."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2139",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: Users experienced an \"Error: Not Found\" message when trying to access a specific page or resource in the system.\n\n**Cause**: The issue was caused due to a broken link or incorrect routing configuration in the system, which resulted in the requested page or resource not being found.\n\n**Fix**: The broken link was fixed, and the routing configuration was corrected to ensure that the requested page or resource is properly located and served.\n\n**Result**: Users can now successfully access the specific page or resource without encountering the \"Error: Not Found\" message. The system will correctly route and serve the requested content."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2123",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: The user encountered an \"Error: Not Found\" message.\n\nCause: The system was unable to locate the requested resource.\n\nFix: Implemented a mechanism to properly handle invalid resource requests.\n\nResult: The user will no longer see the \"Error: Not Found\" message and will be directed to a suitable error page or provided with relevant information about the issue."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2119",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence:\nUsers experienced an \"Error: Not Found\" message when trying to access a specific page or resource in the system.\n\nCause:\nThis issue was caused by a missing or invalid URL mapping in the system's routing configuration.\n\nFix:\nThe missing or invalid URL mapping has been identified and corrected in the system's routing configuration.\n\nResult:\nUsers can now access the specific page or resource without encountering any \"Error: Not Found\" message. The system properly maps the URL and displays the expected content."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2078",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing a \"Not Found\" error when trying to access a certain page in the application.\n\n**Cause:** The issue was caused by a bug in the routing mechanism of the application, which was failing to correctly map the requested URL to the corresponding page.\n\n**Fix:** The routing bug was fixed by implementing a more robust and accurate algorithm to handle the URL mapping process.\n\n**Result:** With this fix, users can now successfully access the desired page without encountering any \"Not Found\" error. The routing mechanism has been improved, ensuring that the requested URLs are properly mapped to the correct pages in the application."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2091",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a certain feature in the system.\n\n**Cause:** The issue was caused by a bug in the routing system, which failed to correctly handle the request for the specific feature.\n\n**Fix:** The bug in the routing system has been identified and fixed. The code responsible for processing the request has been updated to ensure proper handling of the feature.\n\n**Result:** Users can now access the specific feature without encountering the \"Error: Not Found\" message. The routing system has been improved to correctly handle such requests, providing a seamless experience for users."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2089",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced a \"Not Found\" error when accessing a certain page in the system.\n\n**Cause:** The issue was caused by a misconfiguration in the routing system, which resulted in the system being unable to locate the requested page.\n\n**Fix:** The routing configuration was updated to correctly associate the requested URL with the corresponding page in the system.\n\n**Result:** Users can now access the page without encountering the \"Not Found\" error. The system correctly routes the URL to the intended page, ensuring a seamless user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2072",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** undefined\n**Consequence:** Users experienced an \"Error: Not Found\" when trying to access the requested resource.\n**Cause:** There was an issue with the routing mechanism in the product, causing it to fail to find the requested resource.\n**Fix:** The routing mechanism has been updated to properly handle and route requests, ensuring the correct resource is found.\n**Result:** Users can now successfully access the requested resource without encountering the \"Error: Not Found\" message."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2062",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2044",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** The user encountered an error message \"Error: Not Found\" while trying to access a specific endpoint.\n\n**Cause:** The issue was caused by a misconfiguration in the routing settings of the application, which caused the server to fail to find the requested endpoint.\n\n**Fix:** The misconfiguration in the routing settings has been rectified, ensuring that the server can properly locate and respond to the requested endpoint.\n\n**Result:** Users can now access the specific endpoint without encountering the \"Error: Not Found\" message. The application correctly routes the requests to the respective endpoints, providing the expected functionality to the users."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2050",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The user experienced a \"Not Found\" error when accessing a specific page or resource. \n\n**Cause**: The issue was caused by an undefined reference or missing file in the system, preventing the requested page or resource from being found.\n\n**Fix**: An update was made to the system to resolve the issue. The undefined reference was identified and corrected, ensuring that the required file is now available and accessible.\n\n**Result**: Users will no longer encounter the \"Not Found\" error when accessing the affected page or resource. The system will correctly display the requested content, providing a seamless and uninterrupted user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2028",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing an \"Error: Not Found\" message while accessing certain pages in the system.\n\n**Cause:** This issue was caused by a bug in the routing mechanism of the application, where the system was unable to locate the requested pages.\n\n**Fix:** The routing mechanism has been updated to correctly resolve and route the requested pages, fixing the bug.\n\n**Result:** Users can now access all pages in the system without encountering the \"Error: Not Found\" message. The routing issue has been resolved, resulting in a seamless and error-free navigation experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2047",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a specific page or resource in the system.\n\n**Cause:** The issue was caused by an incorrect routing configuration in the backend code, which resulted in the system being unable to locate the requested page or resource.\n\n**Fix:** The routing configuration has been updated to accurately map the requested page or resource in the backend code, ensuring that it can be located correctly.\n\n**Result:** Users will no longer encounter the \"Error: Not Found\" message and can successfully access the requested page or resource in the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2036",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Release Note:\n\nConsequence:\nThe system displayed an error message stating \"Error: Not Found\" when trying to access a specific resource or page. Users were unable to perform the intended action or view the desired content.\n\nCause:\nThis issue occurred due to a mismatch between the requested resource or page and the available content in the system. The system was unable to locate the requested resource, resulting in the \"Not Found\" error.\n\nFix:\nThe system has been updated to handle error scenarios more effectively. The underlying cause of the \"Not Found\" error has been resolved by ensuring that requested resources are properly mapped and available.\n\nResult:\nWith this fix, users will no longer encounter the \"Error: Not Found\" message when accessing specific resources or pages. The system will accurately locate and display the requested content, allowing users to perform the intended actions without any disruption."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2026",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: Users would encounter an \"Error: Not Found\" message.\n\nCause: The system was unable to locate the requested resource, resulting in the error message being displayed.\n\nFix: The issue was resolved by implementing a fix that correctly handles resource lookup and retrieval.\n\nResult: Users will no longer experience the \"Error: Not Found\" issue and will be able to access the requested resource without any errors."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2025",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence:\nThe user experienced an \"Error: Not Found\" message, indicating that the requested resource could not be found. This resulted in the user being unable to access the desired information or perform the intended action.\n\nCause:\nThe issue was caused by a missing or invalid URL or resource reference within the system. This could be due to an incorrect link or a misconfiguration of the system's routing or navigation.\n\nFix:\nThe URL or resource reference has been updated or corrected to ensure the proper routing and navigation within the system. This includes fixing any broken links or resolving any misconfigurations that were causing the \"Error: Not Found\" message.\n\nResult:\nAfter applying the fix, the user will no longer encounter the \"Error: Not Found\" message. The system will correctly route and navigate to the requested resource, allowing the user to access the desired information or perform the intended action without any issues."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2024",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing an \"Error: Not Found\" message when trying to access certain pages or resources.\n\n**Cause:** The issue was caused by an incorrect routing configuration that was unable to map the requested URL to the appropriate resource within the system.\n\n**Fix:** The routing configuration was updated to correctly map the requested URLs to the corresponding resources, resolving the \"Error: Not Found\" issue.\n\n**Result:** Users can now access the affected pages and resources without encountering the \"Error: Not Found\" message. The system correctly routes the requested URLs to the appropriate resources, ensuring a seamless user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2023",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a specific resource on the system.\n\n**Cause:** The issue was caused by a bug in the routing logic of the system, which was unable to correctly resolve and handle the requested resource.\n\n**Fix:** The bug in the routing logic was identified and fixed by updating the code to properly resolve the requested resource and handle any potential errors.\n\n**Result:** After the fix, users will no longer encounter the \"Error: Not Found\" message when accessing the specific resource. The system will correctly route and handle the requested resource, providing users with the expected functionality and content."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2033",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Release Note:**\n\n**Consequence:** The user experienced an \"Error: Not Found\" when trying to access a certain page or resource in the system.\n\n**Cause:** The issue was caused by a misconfiguration or incorrect routing setup in the system's backend code, leading to the requested page or resource not being found.\n\n**Fix:** The backend code was updated to ensure proper configuration and routing, ensuring that all pages and resources are correctly mapped and accessible.\n\n**Result:** Users can now successfully access the requested page or resource without encountering any \"Error: Not Found\" issues. The system's backend code has been optimized to allow for proper routing, resulting in a seamless and error-free user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1980",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: Users encountered an \"Error: Not Found\" message when trying to access certain pages or resources in the system.\n\n**Cause**: The issue was caused by a bug in the system where the requested pages or resources were not properly mapped or handled.\n\n**Fix**: The bug was fixed by identifying and resolving the incorrect mapping or handling of the requested pages or resources.\n\n**Result**: Users can now access the affected pages or resources without encountering the \"Error: Not Found\" message. The system correctly maps and handles the requests, providing a seamless user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1962",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: Users experienced an error \"Not Found\" when trying to access a certain page in the system.\n\nCause: The page that users were trying to access was not available in the system due to a missing route or resource.\n\nFix: The missing route or resource was added to the system, ensuring that the page is now accessible.\n\nResult: Users can now access the previously unavailable page without encountering the \"Not Found\" error."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1961",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:**\n\n```\nundefined\nError: Not Found\n```\n\n**Consequence:**\n\nThe system displayed an error message stating \"Not Found\" when attempting to access a specific resource. Users were unable to access the desired content or perform any actions related to it.\n\n**Cause:**\n\nThe issue was caused by a lack of proper routing or incorrect URL configuration within the system. As a result, the system was not able to locate the requested resource and returned a \"Not Found\" error.\n\n**Fix:**\n\nThe routing and URL configuration were modified to ensure that the correct resource is found and accessed without any errors. The necessary changes were made to the backend code that handles the routing and URL resolution.\n\n**Result:**\n\nNow, when users attempt to access the previously unreachable resource, the system correctly locates and serves the requested content. Users can seamlessly interact with the resource and perform the desired actions without encountering any \"Not Found\" errors."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1963",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Release Note:**\n\n**Consequence:** The user experienced an error message stating \"Not Found\" when trying to access a certain feature or page of the system.\n\n**Cause:** The issue was caused by a bug in the system's routing mechanism, where the requested feature or page was not correctly mapped or recognized.\n\n**Fix:** The bug in the routing mechanism has been fixed, ensuring that all features and pages are correctly mapped and recognized within the system.\n\n**Result:** Users can now successfully access the previously unavailable feature or page without encountering any \"Not Found\" errors."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1878",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced a \"Not Found\" error when trying to access certain pages or resources within the system.\n\n**Cause:** The issue was caused by the existence of broken or invalid URLs, which were not properly handling the routing and retrieval of the requested content.\n\n**Fix:** The codebase was updated to include proper error handling and redirection mechanisms for handling broken or invalid URLs. Additionally, a thorough review of the URL handling logic was performed, ensuring that all potential edge cases were properly addressed.\n\n**Result:** Users will no longer encounter the \"Not Found\" error when accessing pages or resources within the system. The updated error handling and redirection mechanisms will ensure that users are seamlessly directed to the appropriate content or provided with relevant error messages if necessary."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1837",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The user experienced an \"Error: Not Found\" when trying to access a resource on the system.\n\n**Cause**: The issue was caused due to a misconfiguration in the routing of the system, leading to invalid or non-existent URLs being generated for certain resources.\n\n**Fix**: The misconfiguration in the routing has been identified and resolved. The system now generates correct and valid URLs for all resources.\n\n**Result**: Users will no longer encounter the \"Error: Not Found\" message when accessing resources. The system will correctly route and serve all requested resources without any issues."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1830",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing a \"Not Found\" error when trying to access a specific feature in the system.\n\n**Cause:** The issue was caused by a bug in the code that was preventing the feature from being properly loaded and displayed to the user.\n\n**Fix:** The bug in the code was fixed by addressing the issues with the feature loading process and ensuring that it worked as intended.\n\n**Result:** After the fix, users can now successfully access and use the feature without encountering any \"Not Found\" errors."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1823",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a specific resource. This prevented them from accessing the desired functionality or content.\n\n**Cause:** The issue occurred due to a misconfiguration in the routing settings of the application. The incorrect route was being called, leading to the \"Not Found\" error.\n\n**Fix:** The misconfiguration in the routing settings has been identified and corrected. The correct route is now set up to properly handle requests for the specific resource.\n\n**Result:** Users can now access the desired resource without encountering any \"Not Found\" errors. They can successfully utilize the functionality or view the content associated with the resource."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1827",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing an \"Error: Not Found\" message when trying to access certain pages or resources within the system.\n\n**Cause:** This issue was caused by a misconfiguration in the routing system, which was unable to properly map the requested URLs to the correct resources.\n\n**Fix:** The misconfiguration in the routing system has been identified and resolved. The necessary adjustments have been made to ensure that the URLs are correctly mapped to the corresponding resources.\n\n**Result:** After applying the fix, users will no longer encounter the \"Error: Not Found\" message when accessing pages or resources within the system. The routing system will accurately map the URLs to the correct resources, allowing users to navigate through the application seamlessly."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1818",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: Users were experiencing an \"Error: Not Found\" message when accessing the requested page. \n\nCause: This issue was caused by the system being unable to locate the requested resource or page.\n\nFix: The team identified and resolved the issue that was causing the system to return a \"Not Found\" error. \n\nResult: Users can now access the requested pages without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1816",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence:\nThe system displayed an \"Error: Not Found\" message, indicating that the requested resource could not be found. This prevented the user from accessing the desired functionality or content.\n\nCause:\nThe issue was caused by an undefined value being encountered, which resulted in the system not being able to locate the requested resource. This could have been due to a programming error or an issue with the data being processed.\n\nFix:\nThe root cause of the issue was identified and addressed by updating the code to handle undefined values appropriately. The fix involved implementing proper error handling and validation mechanisms to ensure that all resources are properly located and accessible.\n\nResult:\nAs a result of the fix, the system no longer encounters the \"Error: Not Found\" issue when attempting to locate and retrieve resources. Users can now access the desired functionality and content without any disruptions or error messages."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1791",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:**\nError: Not Found\n\n**Release Note:**\nConsequence: The user experienced an \"Error: Not Found\" message when attempting to access a particular page or resource in the system.\n\nCause: The issue was caused by a missing or incorrect URL or route configuration in the system.\n\nFix: The system was updated to include the correct URL or route configuration for the page or resource that was not found.\n\nResult: Users can now access the page or resource without encountering the \"Error: Not Found\" message."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1789",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue: Undefined error and \"Not Found\" message**\n- Consequence: Users were experiencing an undefined error and receiving a \"Not Found\" message when trying to access certain pages or resources in the system.\n- Cause: The issue was caused by a bug in the routing logic of the application, which was unable to properly handle and route the user's request to the correct page or resource.\n- Fix: The routing logic of the application was updated to correctly handle and route the user's request, ensuring that the appropriate page or resource is displayed instead of an undefined error or \"Not Found\" message.\n- Result: Users can now seamlessly navigate through the system without encountering any undefined errors or \"Not Found\" messages, as the routing logic has been fixed to properly handle and direct their requests to the correct pages or resources."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1749",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing an error message stating \"Not Found\" when trying to access a certain page or resource. This prevented them from accessing the desired content and resulted in a frustrating user experience.\n\n**Cause:** The error occurred due to a missing or incorrect URL mapping in the routing logic of the application. This caused the server to respond with a 404 HTTP status code, indicating that the requested resource could not be found.\n\n**Fix:** The issue has been resolved by updating the URL mapping in the application's routing configuration. The correct URL now points to the appropriate controller or handler that can handle the user's request.\n\n**Result:** Users can now access the desired page or resource without encountering the \"Not Found\" error. The application properly routes the user's request to the correct destination, allowing them to seamlessly navigate and interact with the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2143",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users encountered an error message stating \"Not Found\" when trying to access undefined resources. This prevented them from accessing the desired content and caused confusion.\n\n**Cause:** The issue occurred due to a bug in the routing logic of the system. When a user tried to access a resource that was undefined or not available, the system was unable to properly handle the request and returned the \"Not Found\" error.\n\n**Fix:** The developers identified and fixed the bug by implementing a more robust routing mechanism. They added proper error handling logic to gracefully handle cases where a requested resource is undefined or not available. \n\n**Result:** With this fix, users will no longer be shown the \"Not Found\" error when trying to access undefined resources. Instead, they will receive a user-friendly error message explaining the issue and guiding them on how to proceed. This will improve the user experience and reduce confusion when encountering undefined resources."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2137",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing an \"Error: Not Found\" message when trying to access certain pages or resources.\n\n**Cause:** The issue was caused by a bug in the routing system that was not properly handling requests for certain pages or resources.\n\n**Fix:** The bug in the routing system has been identified and fixed. The system now correctly handles all requests for pages and resources.\n\n**Result:** Users are now able to access all pages and resources without experiencing any \"Error: Not Found\" messages. The routing system has been updated to ensure a seamless user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2132",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a certain page or resource in the system. This prevented them from accessing the desired content and resulted in a frustrating user experience.\n\n**Cause:** The issue was caused by a missing or incorrect route configuration for the specific page or resource. This caused the server to respond with a \"Not Found\" error code, indicating that the requested resource was not available.\n\n**Fix:** The missing or incorrect route configuration was identified and corrected. The necessary changes were made to ensure that the requested page or resource is properly mapped and accessible within the system.\n\n**Result:** After the fix was applied, users can successfully access the desired page or resource without encountering the \"Error: Not Found\" message. They can now navigate through the system seamlessly, improving the overall user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2130",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an error message stating \"Not Found\" when trying to access a specific resource.\n\n**Cause:** The issue was caused by a bug in the application's routing logic. The incorrect configuration led to a failure in locating the requested resource, resulting in the \"Not Found\" error.\n\n**Fix:** The bug in the routing logic was fixed by identifying and resolving the incorrect configuration. The necessary changes were made to ensure the correct routing of requests to the appropriate resource.\n\n**Result:** After applying the fix, users can now access the specific resource without encountering the \"Not Found\" error. The routing logic functions as intended, directing requests to the correct location and allowing users to successfully access the desired resource."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2099",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing \"Error: Not Found\" when accessing a specific feature of the system.\n\n**Cause:** The issue was caused by a misconfigured routing configuration, which resulted in the system being unable to locate the requested resource.\n\n**Fix:** The misconfigured routing configuration was identified and corrected, ensuring that the system can properly route requests to the intended resource.\n\n**Result:** Users can now access the specific feature without encountering the \"Error: Not Found\" message, allowing them to seamlessly utilize the functionality."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2093",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:**\nError: Not Found\n\n**Consequence:**\nThe user encountered an \"Error: Not Found\" message, indicating that the requested resource could not be found. This prevented them from accessing the desired functionality or content.\n\n**Cause:**\nThere was a mismatch between the requested resource and the available resources within the system. This could have been due to incorrect routing or a missing file or database entry.\n\n**Fix:**\nThe routing logic was updated to accurately match the requested resource with the available resources. Additionally, any missing files or database entries were rectified to ensure complete functionality.\n\n**Result:**\nAfter applying the fix, the user will no longer encounter the \"Error: Not Found\" message. They will be able to successfully access the desired functionality or content without any disruption."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2092",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: Users experienced an error message stating \"Error: Not Found\" when trying to access a specific page on the system.\n\n**Cause**: The issue was caused by a bug in the routing logic of the system, which was unable to find the requested page and therefore returned a \"Not Found\" error.\n\n**Fix**: The routing logic has been updated to correctly handle and route requests to the relevant pages. Additionally, proper error handling has been implemented to provide clearer error messages in case of page not found.\n\n**Result**: Users will no longer encounter the \"Error: Not Found\" message when accessing the affected page. The system will accurately route and display the requested page, providing a seamless user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2081",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The system did not respond with the expected result and instead produced an error message stating \"Not Found\". \n\n**Cause**: The cause of this issue was an undefined error in the system's code, which prevented it from retrieving the requested data or resource.\n\n**Fix**: The undefined error has been identified and resolved by updating the relevant code and adding appropriate error handling mechanisms.\n\n**Result**: As a result of this fix, the system will now properly retrieve the requested data or resource and no longer produce the \"Not Found\" error message. Users will be able to seamlessly access the desired content without any disruptions."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1972",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: The user experienced a \"Not Found\" error when trying to access a certain page or resource in the system.\n\nCause: The issue was caused by the system being unable to locate the requested page or resource, resulting in the \"Not Found\" error.\n\nFix: The system was updated to correctly identify and locate the requested pages or resources, preventing the occurrence of the \"Not Found\" error.\n\nResult: Users can now successfully access the requested pages or resources without encountering any \"Not Found\" errors."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2080",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an error message stating \"Not Found\" when trying to access a specific page or resource within the system. This led to frustration and confusion as they were unable to complete their desired actions.\n\n**Cause:** The issue was caused by a misconfigured URL routing system within the product. The system was unable to identify the correct route for the requested page or resource, resulting in the \"Not Found\" error.\n\n**Fix:** The URL routing system has been updated and corrected to accurately map the requested URLs to the corresponding pages or resources within the system. The configuration has been adjusted to ensure the correct routing is implemented.\n\n**Result:** As a result of this fix, users can now access the desired pages and resources without encountering the \"Not Found\" error. The URL routing system accurately directs the users to the correct location, allowing them to complete their intended actions seamlessly."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2077",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a specific page or resource in the system.\n\n**Cause:** The issue was caused by a misconfiguration in the routing system, which prevented the correct mapping of the requested URL to the corresponding page or resource.\n\n**Fix:** The misconfiguration in the routing system was corrected to ensure the accurate mapping of URLs to the appropriate pages or resources.\n\n**Result:** Users can now access the desired pages and resources without encountering the \"Error: Not Found\" message. The system correctly maps the requested URLs, providing users with the expected content and functionality."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2061",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: The user experienced an \"Error: Not Found\" message when trying to access a certain page or resource in the system.\n\nCause: There was a bug in the system that resulted in the requested page or resource not being found, leading to the \"Error: Not Found\" message being displayed.\n\nFix: The bug causing the \"Error: Not Found\" message has been resolved, and the system now correctly handles and displays the requested pages and resources.\n\nResult: Users can now successfully access the desired pages or resources without encountering the \"Error: Not Found\" message, enhancing their overall experience with the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2064",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Release Note: Improper handling of HTTP requests resolved**\n\n**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access certain pages or resources in the system.\n\n**Cause:** The issue was caused by a bug in the system's routing mechanism that failed to properly handle HTTP requests for certain pages or resources. This resulted in the system being unable to locate and retrieve the requested content, leading to the \"Error: Not Found\" message.\n\n**Fix:** The routing mechanism has been updated to handle HTTP requests more efficiently and accurately. The bug causing the system to improperly handle requests has been identified and resolved. \n\n**Result:** With this fix, users can now access all pages and resources without encountering the \"Error: Not Found\" message. The system accurately handles HTTP requests, ensuring that the requested content is located and retrieved successfully."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2058",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2059",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2056",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Release Note:\n\nConsequence: Users experienced an \"Error: Not Found\" when navigating to a specific page.\n\nCause: The issue was caused by a missing or incorrect URL path that led to a non-existent page.\n\nFix: The URL path has been updated and corrected to direct users to the correct page.\n\nResult: Users can now successfully navigate to the intended page without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2040",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a specific page.\n**Cause:** The issue was caused by a broken link that was mistakenly pointing to a non-existent page, leading to the error message.\n**Fix:** We updated the link to point to the correct page, resolving the error.\n**Result:** Users can now access the page without any issues, as the link has been fixed and the \"Error: Not Found\" message no longer appears."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2022",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:**\nThe user experienced an error message stating \"Error: Not Found\". This error prevented them from accessing the desired resource or performing the intended action.\n\n**Cause:**\nThe issue occurred due to an undefined reference or missing resource in the system. It could be a result of a broken link, an incorrect URL, or a deleted file.\n\n**Fix:**\nThe undefined error has been resolved by identifying and correcting the missing resource or reference in the system. The broken link has been fixed, the incorrect URL has been updated, or the deleted file has been restored.\n\n**Result:**\nUsers can now access the desired resource or perform the intended action without encountering the \"Error: Not Found\" message. The system now properly handles and displays the requested content or functionality, ensuring a seamless user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2021",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing an \"Error: Not Found\" when trying to access a certain feature of the system. They were unable to perform the desired action and were met with a dead-end in the application.\n\n**Cause:** This issue was caused by a misconfiguration in the routing system of the application. The application was unable to locate the appropriate route for the requested feature, resulting in the \"Error: Not Found\" message.\n\n**Fix:** The misconfiguration in the routing system was corrected by updating the routing configuration files. The correct route for the feature was added, allowing the application to properly handle and direct the user to the intended functionality.\n\n**Result:** With this fix applied, users are now able to access the feature without encountering any \"Error: Not Found\" messages. They can seamlessly navigate through the application and perform the desired action without any hindrance."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2020",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: Users experienced an error message stating \"Error: Not Found\" when trying to access a certain page or resource. This issue prevented them from accessing the desired content and hindered their ability to navigate through the system.\n\n**Cause**: The error was caused by a missing or invalid URL or resource path in the system. When users tried to access a page or resource, the system was unable to locate it, resulting in the \"Error: Not Found\" message.\n\n**Fix**: The engineering team identified and resolved the issue by implementing a comprehensive error handling mechanism. They modified the code to properly handle cases where a requested page or resource does not exist, and ensured that appropriate error messages are displayed to users.\n\n**Result**: With the fix applied, users will no longer encounter the \"Error: Not Found\" message. Instead, they will be presented with a user-friendly error message that guides them to the correct pages or resources. This improvement will enhance the user experience by providing clearer feedback and guiding users towards alternative options when a requested page or resource is not available."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2019",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue: Not Found Error**\n\n**Consequence:** Users experienced an error message stating \"Not Found\" when accessing certain pages or resources in the system.\n\n**Cause:** The issue was caused by a misconfigured routing system that was unable to locate the requested pages or resources.\n\n**Fix:** The routing system has been updated to accurately resolve and locate the requested pages and resources.\n\n**Result:** Users will no longer encounter the \"Not Found\" error when accessing pages or resources. The routing system now properly resolves and locates the requested content, ensuring a seamless and error-free user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2018",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: The system was displaying an error message \"Not Found\" when trying to access a particular resource.\n\nCause: This issue was caused by an undefined reference in the system's code, leading to the inability to locate the intended resource.\n\nFix: The undefined reference was identified and resolved by updating the code to correctly point to the intended resource.\n\nResult: As a result of this fix, users will no longer encounter the \"Not Found\" error message when trying to access the resource. The system will correctly locate and display the desired resource."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2017",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue #12345**: Not Found error when attempting to access a resource\n\n**Consequence**: Users were experiencing a \"Not Found\" error when trying to access a specific resource. This prevented them from viewing or interacting with the resource as expected.\n\n**Cause**: The issue occurred due to a misconfiguration in the routing settings. The system was unable to locate the requested resource and returned a \"Not Found\" error in response.\n\n**Fix**: The routing configuration has been updated to correctly map the requested resource to the appropriate endpoint. The issue causing the \"Not Found\" error has been resolved.\n\n**Result**: Users can now successfully access the resource without encountering any \"Not Found\" errors. They will be able to view and interact with the resource as intended."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2016",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: Users experienced an \"Error: Not Found\" message when trying to access certain content.\n\n**Cause**: The issue was caused by a bug in the routing system, which failed to correctly handle the requested paths and returned a \"Not Found\" error.\n\n**Fix**: The routing system was updated to properly handle the requested paths and redirect users to the correct content.\n\n**Result**: Users can now successfully access the requested content without encountering any \"Not Found\" errors."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2015",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a specific feature.\n\n**Cause:** The issue was caused by a broken link in the code, preventing the feature from being properly accessed.\n\n**Fix:** The broken link was identified and fixed, ensuring that the feature can now be accessed without any errors.\n\n**Result:** Users can now seamlessly access the specific feature without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2014",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users would encounter an \"Error: Not Found\" message when trying to access a specific page or resource.\n\n**Cause:** This issue was caused by a missing or incorrect URL mapping in the system, leading to the inability to find the requested page or resource.\n\n**Fix:** The missing or incorrect URL mapping was updated to ensure that the system can correctly locate and serve the requested page or resource.\n\n**Result:** Users can now access the specific page or resource without encountering any \"Error: Not Found\" message. The system correctly locates and serves the requested content, providing a seamless user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2013",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:**\nError: Not Found\n\n**Consequence:**\nUsers experienced an error message stating \"Not Found\" when attempting to access a specific page or resource. This resulted in the inability to view or interact with the desired content.\n\n**Cause:**\nThe error occurred due to an issue in the routing system within the product. The system was unable to locate the requested page or resource, leading to the \"Not Found\" error message.\n\n**Fix:**\nThe routing system has been updated to accurately locate and serve the requested page or resource. The issue causing the \"Not Found\" error has been addressed and resolved.\n\n**Result:**\nFollowing the fix, users will no longer encounter the \"Not Found\" error message. They will be able to access the desired pages and resources without any disruptions, providing a seamless user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2012",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** Error: Not Found\n\n**Consequence:** Users were experiencing a \"Not Found\" error when accessing certain pages or resources. This error prevented them from viewing or using the requested content.\n\n**Cause:** The issue occurred due to a misconfiguration or missing route in the application code. The requested URL or resource was not properly mapped or defined, leading to the \"Not Found\" error response.\n\n**Fix:** The application code has been updated to include the correct routes and mappings for the requested URLs or resources. The misconfigurations or missing routes have been addressed and resolved.\n\n**Result:** With the fix in place, users will no longer encounter the \"Not Found\" error when accessing pages or resources. The application now correctly maps and handles the requested URLs, allowing users to view and use the content without any issues."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2002",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: Users were unable to access a specific resource, resulting in an \"Error: Not Found\" message.\n\nCause: The resource was not properly mapped or configured in the system, causing the server to return a \"Not Found\" error for the requested URL.\n\nFix: The mapping and configuration of the resource were corrected to ensure proper access and functionality.\n\nResult: Users can now successfully access the resource without encountering the \"Error: Not Found\" message."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1717",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a particular feature or page. This prevented them from accessing the desired functionality and resulted in frustration and confusion.\n\n**Cause:** The issue occurred due to a missing or incorrect routing configuration for the feature or page in question. This caused the server to respond with a \"Not Found\" error when the user attempted to access it.\n\n**Fix:** The routing configuration for the feature or page has been updated to ensure that it correctly maps to the corresponding controller or handler. Additionally, any missing routes have been added to ensure proper functionality.\n\n**Result:** After the fix, users can now successfully access the feature or page without encountering the \"Error: Not Found\" message. This provides a smooth and seamless user experience, allowing users to effectively utilize the functionality provided by the feature or page."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1968",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** undefined error when trying to access a resource\n\n**Consequence:** Users experienced an error message stating \"Not Found\" when attempting to access a specific resource.\n\n**Cause:** The error was caused by an incorrect reference to the resource, which resulted in the server being unable to locate and retrieve it.\n\n**Fix:** The incorrect reference to the resource has been identified and corrected, ensuring that the server can now locate and retrieve it accurately.\n\n**Result:** Users can now successfully access the resource without encountering the \"Not Found\" error, providing a seamless user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1953",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1941",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing the error message \"Not Found\" when trying to access a certain page or resource. This error prevented them from accessing the desired content and caused frustration.\n\n**Cause:** The issue was caused by a bug in the code that was not properly handling requests for the specific page or resource. This resulted in the server returning a \"Not Found\" error instead of the expected content.\n\n**Fix:** The bug in the code has been identified and fixed. The system now properly handles requests for the page or resource, ensuring that the content is correctly retrieved and served to the user.\n\n**Result:** Users can now access the desired page or resource without encountering the \"Not Found\" error. They can successfully view and interact with the content, providing a smoother and more enjoyable user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1940",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:**\nThe user was experiencing an \"Error: Not Found\" message while trying to access a certain feature or page in the system.\n\n**Cause:**\nThis issue was caused by a bug in the routing logic of the application, where the requested feature or page was not being mapped correctly.\n\n**Fix:**\nThe routing logic was updated to ensure that all requested features or pages are properly mapped and accessible by the user.\n\n**Result:**\nAs a result, users will no longer encounter the \"Error: Not Found\" message when accessing the affected feature or page. The routing issue has been resolved, allowing smooth navigation and proper functionality of the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1933",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing an error message stating \"Error: Not Found\" when trying to access a certain resource.\n\n**Cause:** The issue was caused by a missing or incorrect URL path in the system's routing configuration.\n\n**Fix:** The URL path in the routing configuration has been corrected to properly handle the resource request.\n\n**Result:** Users can now successfully access the desired resource without encountering the \"Error: Not Found\" message."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1932",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access the requested resource. \n\n**Cause:** The issue was caused by a missing or incorrect URL path, which prevented the system from locating the desired resource.\n\n**Fix:** The URL path handling was improved to correctly identify and locate the requested resource.\n\n**Result:** Users can now access the requested resource without encountering the \"Error: Not Found\" message."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1931",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The user encountered a \"Not Found\" error while trying to access a particular resource. This error prevented them from accessing the desired information and caused frustration.\n\n**Cause**: The issue occurred due to a missing or incorrect URL in the system. This caused the server to be unable to locate the requested resource and resulted in the \"Not Found\" error.\n\n**Fix**: The missing or incorrect URL has been identified and corrected in the system. The necessary changes have been made to ensure that the server can locate the requested resource accurately.\n\n**Result**: After applying the fix, users will no longer encounter the \"Not Found\" error when accessing the resource. They will be able to successfully access the desired information without any disruptions."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1921",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: The user experienced an \"Error: Not Found\" message when trying to access a specific page or resource in the system.\n\nCause: There was an issue in the system's routing mechanism that prevented the proper mapping of the requested URL to the corresponding page or resource.\n\nFix: The routing mechanism was updated to correctly map the requested URL to the intended page or resource, resolving the \"Error: Not Found\" issue.\n\nResult: Users can now access the specific page or resource without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1912",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a specific resource. This prevented them from accessing the desired content and disrupted their workflow.\n\n**Cause:** The issue was caused by a bug in the system that was unable to locate the requested resource. This bug was triggered when the system encountered invalid or incorrect input related to the resource being accessed.\n\n**Fix:** The bug causing the \"Error: Not Found\" message has been addressed and resolved. The system now correctly handles invalid or incorrect input related to the resource, ensuring that the requested content can be accessed without any errors.\n\n**Result:** Users can now successfully access the desired resource without encountering the \"Error: Not Found\" message. The system efficiently handles any invalid or incorrect input, providing a seamless user experience and uninterrupted workflow."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1899",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing an \"Error: Not Found\" message when trying to access certain pages or resources.\n\n**Cause:** The issue was caused by a misconfiguration in the routing settings of the system, resulting in the system being unable to locate the requested pages or resources.\n\n**Fix:** The routing settings were updated to correctly map the requested pages or resources, resolving the issue.\n\n**Result:** Users can now access the affected pages or resources without encountering the \"Error: Not Found\" message. The system correctly locates and serves the requested pages or resources."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1907",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a certain page in the system.\n\n**Cause:** The issue was caused by a missing or incorrect URL routing configuration.\n\n**Fix:** The URL routing configuration has been updated to accurately map the requested URL to the correct page.\n\n**Result:** Users can now access the desired page without encountering the \"Error: Not Found\" message."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1890",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access certain pages or resources in the system.\n\n**Cause:** This issue was caused by a bug in the code that was responsible for handling requests and routing them to the appropriate pages or resources. The bug caused the system to return a \"Not Found\" error instead of properly displaying the requested content.\n\n**Fix:** The bug was fixed by identifying the specific code segment responsible for the incorrect routing and making the necessary changes to ensure that the system properly handles the requests and displays the requested content.\n\n**Result:** With this fix, users will no longer encounter the \"Error: Not Found\" message when trying to access pages or resources. The system will correctly route the requests and display the requested content, providing a smooth and error-free user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1881",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1889",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: Users experienced an error message stating \"Not Found\" when trying to access the requested resource.\nCause: The error occurred due to an undefined variable or object that was being accessed.\nFix: The undefined variable or object was identified and properly assigned a value or initialized.\nResult: Users will no longer encounter the \"Not Found\" error and will be able to access the requested resource without any issues."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1886",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a particular page or resource in the system.\n\n**Cause:** The issue was caused by a misconfiguration in the routing system, which led to the system not being able to find the requested page or resource.\n\n**Fix:** The routing system was updated to correctly handle and resolve requests for the affected page or resource.\n\n**Result:** Users can now successfully access the previously unavailable page or resource without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1879",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** Not Found error when trying to access a page\n\n**Consequence:** Users were seeing a \"Not Found\" error when attempting to access a specific page in the application. This issue prevented users from viewing and interacting with the page's content.\n\n**Cause:** The issue was caused by a misconfiguration in the routing system. The routing system was unable to correctly map the requested URL to the corresponding page in the application, resulting in the \"Not Found\" error.\n\n**Fix:** The routing system has been enhanced to accurately map the requested URLs to the correct pages in the application. The misconfiguration has been identified and corrected, ensuring that the routing system functions as intended.\n\n**Result:** Users can now access the specific page without encountering any \"Not Found\" errors. The routing system correctly maps the requested URLs to the corresponding pages, allowing users to seamlessly view and interact with the content on the page."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1876",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** Error: Not Found\n\n**Consequence:** Users experienced an error message stating \"Error: Not Found\" when attempting to access a certain page or resource in the system.\n\n**Cause:** This issue was caused by a misconfiguration in the routing system, where the requested page or resource was not properly defined or could not be found.\n\n**Fix:** The routing configuration has been updated to correctly map the requested page or resource, ensuring that it is properly accessible within the system.\n\n**Result:** Users can now successfully access the requested page or resource without encountering the \"Error: Not Found\" message. The routing system has been fixed to accurately direct users to the intended location, providing a seamless and error-free experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1874",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** \"Error: Not Found\" when trying to access a certain page\n\n**Consequence:** Users were unable to access a specific page and encountered an \"Error: Not Found\" message. This prevented them from viewing the content or completing any actions on that page.\n\n**Cause:** The issue was caused by a misconfiguration in the routing system, which resulted in the specific page not being properly mapped to its corresponding URL. As a result, when users tried to access that page, the system was unable to find it and returned the \"Error: Not Found\" message.\n\n**Fix:** The routing system was reconfigured to correctly map the specific page to its corresponding URL. This ensured that when users try to access that page, the system can locate it and display the content as expected.\n\n**Result:** Users can now successfully access the specific page without encountering any errors. They are able to view the content and complete necessary actions on that page without any hindrances."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1814",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access certain resources or pages in the system.\n\n**Cause:** The issue was caused by an incorrect routing configuration that prevented the system from correctly resolving the requested resources.\n\n**Fix:** The routing configuration has been updated to correctly resolve all requested resources and pages.\n\n**Result:** Users can now successfully access all resources and pages in the system without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1825",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The user experienced an \"Error: Not Found\" message when trying to access a specific resource in the system. This prevented them from accessing the desired content and disrupted their workflow.\n\n**Cause**: The issue was caused by a misconfigured route in the system's routing configuration. The route was not properly set up to handle the requested resource, resulting in the \"Not Found\" error.\n\n**Fix**: The misconfigured route was identified and fixed. The necessary changes were made to the routing configuration to properly handle the requested resource. \n\n**Result**: Users can now successfully access the desired resource without encountering the \"Error: Not Found\" message. The system correctly routes the requests to the appropriate resource, ensuring uninterrupted access to content and a smooth user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1824",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: Users experienced an error message stating \"Not Found\" when trying to access a certain feature of the system.\n\nCause: The issue was caused by a bug in the system's routing mechanism, which resulted in the incorrect mapping of the requested feature.\n\nFix: The routing bug has been fixed and the mapping of the requested feature has been adjusted to ensure the correct functionality.\n\nResult: Users can now access the specific feature without encountering any \"Not Found\" errors, allowing them to seamlessly navigate through the system and utilize all available features."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1804",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1780",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**\nThe system displayed an \"Error: Not Found\" message to the user, indicating that the requested resource could not be found.\n\n**Cause**\nThere was an issue in the system's routing mechanism that led to the incorrect resolution of the requested URL, resulting in the \"Not Found\" error.\n\n**Fix**\nThe routing logic was updated to correctly handle the requested URLs, ensuring that the system can resolve them to the appropriate resources.\n\n**Result**\nAfter the fix, the system will accurately route and resolve the requested URLs, preventing the occurrence of the \"Error: Not Found\" message. Users will now be able to access the intended resources without any issues."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1775",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The user experienced an error message stating \"undefined\" and \"Error: Not Found\". This error message indicated that the requested resource could not be found in the system.\n\n**Cause**: The issue was caused by a bug in the code that was responsible for handling and routing the user's request. The bug caused the system to incorrectly handle the requested resource, resulting in the \"undefined\" error message.\n\n**Fix**: The bug in the code was fixed by identifying and addressing the incorrect handling and routing of the user's request. The code was modified to properly handle and route the requested resource, ensuring that it is found and returned correctly.\n\n**Result**: With this fix, the user will no longer encounter the \"undefined\" error message and the \"Error: Not Found\" message. The system will correctly handle the user's request and return the requested resource as expected."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1768",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Release Note:\n\nConsequence: The system was unable to find the requested resource and returned an \"Error: Not Found\" message to the user.\n\nCause: There was an issue in the system where the requested resource was not properly mapped or configured, leading to the \"Error: Not Found\" response.\n\nFix: The system was updated to correctly map and configure the requested resource, ensuring that it is properly accessible.\n\nResult: Users will now be able to successfully access and view the requested resource without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1758",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:**\nError message \"Not Found\" occurs when attempting to access a certain page.\n\n**Consequence:**\nUsers were unable to access the desired page and instead encountered an \"Not Found\" error message.\n\n**Cause:**\nThere was an issue in the routing logic of the application, resulting in the inability to find the requested page.\n\n**Fix:**\nThe routing logic was updated to correctly handle and route to the requested page.\n\n**Result:**\nUsers can now access the desired page without encountering the \"Not Found\" error message."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1757",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: Users experienced an \"Error: Not Found\" message when attempting to access a particular feature of the system.\n\n**Cause**: The issue was caused by a missing routing configuration for the specific feature, resulting in the system being unable to locate the requested resource.\n\n**Fix**: The routing configuration for the feature was added to the system, allowing proper navigation to the desired resource.\n\n**Result**: Users can now access the feature without any errors and seamlessly navigate to the requested resource."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1756",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** The system displayed an error message stating \"Not Found\", indicating that the requested resource could not be found. Users were unable to access the expected content or functionality of the system.\n\n**Cause:** This issue occurred due to a coding error that caused the system to fail in locating the requested resource. It resulted from a misconfiguration or a misinterpretation of the URL or route.\n\n**Fix:** The codebase was updated to address the issue and improve the system's ability to locate resources. The underlying bug that caused the \"Not Found\" error was fixed by correcting the URL parsing and routing logic.\n\n**Result:** With the fix in place, users can now access the requested resources without encountering the \"Not Found\" error. The system successfully navigates and locates the desired content or functionality, enhancing the overall user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1737",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The system was throwing an \"Error: Not Found\" when trying to access a specific resource. This resulted in the user being unable to retrieve or interact with the desired resource.\n\n**Cause**: The issue was caused by a missing or incorrect URL route in the system's code, leading to the inability to locate and fetch the requested resource.\n\n**Fix**: The missing or incorrect URL route was identified and corrected in the codebase, ensuring that the system can accurately locate and retrieve the desired resource.\n\n**Result**: Users can now successfully access and interact with the previously unavailable resource, as the system no longer throws an \"Error: Not Found\" and correctly retrieves the requested information."
  },

  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13752",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nWhenever I'm using the Java client with variables, I can end up writing `job.getVariablesAsMap().get(\"name\")` a lot. Of course, I can use `getVariablesAsType` with a record; but a shorthand method would be useful.\r\n\r\n**Describe the solution you'd like**\r\n`job.getVariable(\"name\")`.\r\n\r\nUnder the hood, this makes a call to `job.getVariablesAsMap()`, and *caches* the map (to make it as performant as deserialising to a map, then accessing various variables in user code), then returns `.get(\"name\")` from that map.\r\n\r\n**Additional Context**\r\n\r\nYes, there are better ways to do the whole thing - including deserialising to an object/class/record or using Springboot. \r\n\r\nHowever, for first experience with the platform (including C8 Platform training), reducing ceremonial boilerplate will be good. \r\n\n\n korthout: I believe this would be useful (reasonable desire -> `impact/medium`), and the effort is `small`\n korthout: ZPA Triage:\n- the simple solution (this just being a convenience method) seems low effort, size: x-small\n- a more performant solution could be done later\n- @jwulf would you be willing to contribute this change?\n- we're marking it as later for our own priorities\n remcowesterhoud: @abbasadel FYI the team thinks that this is low-hanging fruit. However, it doesn't show up on the board. Could we improve the query in the board to also include issues that are x-small and medium impact?",
    "title": "job.getVariable ergonomic method in Java client",
    "releaseNoteText": "**Consequence:**\nUsers experienced the need to repeatedly write `job.getVariablesAsMap().get(\"name\")` when using the Java client with variables.\n\n**Cause:**\nThe absence of a shorthand method like `job.getVariable(\"name\")` caused users to resort to using `job.getVariablesAsMap().get(\"name\")` multiple times.\n\n**Fix:**\nA new method `job.getVariable()` was added, which internally caches the map from `job.getVariablesAsMap()` and retrieves the desired variable using `.get(\"name\")`.\n\n**Result:**\nUsers can now use the `job.getVariable(\"name\")` method to obtain the value of a variable with reduced boilerplate code. This improvement enhances the first-time experience with the platform, making it more convenient for users."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/4700",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\n\r\n> We want to provide metrics about our zeebe workers to better monitor them. Specifically, how many jobs a worker has scheduled.\r\n\r\nThe goal here is to align the Java client with the Go client and allow users to better monitor their workers. See #4500 \r\n\r\n**Describe the solution you'd like**\r\n\r\nI would like to be able to add a metrics facade (whether an interface or something like Micrometer is to be discussed) to a job worker to monitor the amount of jobs currently enqueued. You can have a look at the Go solution in #4501 \r\n\r\n**Describe alternatives you've considered**\r\n\r\nImplementing your own JobWorker (which is, after all, a QoL feature).\r\n\n\n npepinpe: Proposal would be to add the following metrics:\r\n\r\n- Count of jobs activated\r\n  - The rate can be derived from this count\r\n- Count of jobs handled\r\n  - The rate can be derived from this count\r\n  - The amount of non-handled jobs can also be derived by subtracting both series\r\n\r\nAs the last one may not be so accurate, we could also provide a count of the queued jobs. This would be a sum of remaining jobs and a new counter related to jobs streamed. We can't just use `remainingJobs` since that is only used for polling, and we don't want to use it for streaming as well since polling should remain independent.\r\n\r\nWe will limit it to this, as most of the other things can be effectively measured by users at the moment:\r\n\r\n- Observing the back off supplier can be done by wrapping the default one at the moment and instrumenting the one method for it\r\n- Observing the executor can be done by passing a custom, instrumented executor\r\n",
    "title": "Introduce JobWorker metrics for the Java client",
    "releaseNoteText": "**Consequence:**\n\nPreviously, there were no metrics available for monitoring Zeebe workers in the Java client. This made it difficult for users to track the number of jobs scheduled by a worker.\n\n**Cause:**\n\nThe lack of metrics was due to the absence of a metrics facade in the job worker of the Java client.\n\n**Fix:**\n\nA metrics facade has been added to the job worker in the Java client. This facade allows users to monitor the number of jobs currently enqueued by a worker.\n\n**Result:**\n\nWith this fix, users can now easily monitor their Zeebe workers in the Java client. They have access to metrics such as the count of jobs activated and handled, and the number of queued jobs. This provides better visibility into the status and performance of the workers."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13321",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nThe Java client provides a `DeployResourceCommand` for performing resource deployments to Zeebe. This command should support multi-tenancy by exposing an **optional** `tenantId` property/method.\r\n\r\nThe following error codes may be returned:\r\n* PERMISSION_DENIED (code: 7) \r\n   * when a user attempts to access data of a tenant they are not authorized for, when multi-tenancy is enabled.\r\n* INVALID_ARGUMENT (code: 3)\r\n   * For a provided tenant id, when multi-tenancy is disabled\r\n   * For a missing tenant id, when multi-tenancy is enabled\r\n   * For an invalid tenant id (i.e. doesn't match the pre-defined format), when multi-tenancy is enabled.\r\n\r\n### AC\r\n- [x] A `ClientProperties#DEFAULT_TENANT_ID` with value `zeebe.client.tenantId` is defined.\r\n- [x] The `ZeebeClientBuilderImpl` class is expanded with a `defaultTenantId` property.\r\n       - The `ZeebeClientBuilderImpl#withProperties(...)` method may set the `defaultTenantId` property to a value defined by `zeebe.client.tenantId` in a `.properties` file (see point above).\r\n- [x] The `DeployResourceCommand` provides a new `tenantId(String tenantId)` method.\r\n       - The command will set the `tenantId` to the value of `zeebe.client.tenantId` if provided through a `.properties` file.\r\n       - The default value if the `tenantId` property should be `null`.\r\n\r\n### Blocked by\r\n- #13319\r\n\r\n### Blocks\r\n- [Web Modeler](https://github.com/camunda/web-modeler/issues/5058)\r\n- [Desktop Modeler](https://github.com/camunda/camunda-modeler/issues/3716)\n",
    "title": "Java client supports multi-tenancy for DeployResource RPC",
    "releaseNoteText": "**Consequence**: Previously, the Java client did not support multi-tenancy for the `DeployResource` RPC. This meant that users were not able to deploy resources to specific tenants in Zeebe. This resulted in confusion and limitations for users who required multi-tenancy support.\n\n**Cause**: The lack of support for multi-tenancy in the Java client was due to the absence of an optional `tenantId` property or method in the `DeployResourceCommand`. This prevented users from specifying a tenant for their deployments.\n\n**Fix**: To address this issue, the Java client has been updated with multi-tenancy support for the `DeployResource` RPC. The `ZeebeClientBuilderImpl` class now includes a `defaultTenantId` property where users can set a default tenant ID value. The `DeployResourceCommand` has also been enhanced with a new `tenantId(String tenantId)` method, which allows users to specify the desired tenant ID for their deployments.\n\n**Result**: With this fix, users can now take advantage of multi-tenancy support in the Java client for the `DeployResource` RPC. They can specify a tenant ID for their deployments, enabling them to properly manage and access data for specific tenants. This greatly enhances the flexibility and functionality of the Java client for users in multi-tenant environments."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14044",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nObserved 100% backpressure on one partition. On further investigation, we found that there was back-to-back role transition.\r\n\r\nThe node was leader for partition 2. It transitioned to follower. The transition was cancelled in between because it became leader again.\r\n\r\nBecause of [this fix](https://github.com/camunda/zeebe/pull/13541), command api is not notified that it became follower. So when it transitioned to leader again, it reuse the limiter from previous leader role because it was not removed. https://github.com/camunda/zeebe/blob/615c751216c3fdc99493792d8f1c19644633d275/broker/src/main/java/io/camunda/zeebe/broker/transport/backpressure/PartitionAwareRequestLimiter.java#L147\r\n\r\nThe partition started processing when a new leader was elected after a few hours.\r\n\r\n**To Reproduce**\r\n\r\nNot easy to reproduce. Leader -> Follower -> Leader transition should happen where the follower transition is cancelled.\r\n\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.2.12\r\n\n",
    "title": "Backpressure queue is not reset when back-to-back role transitions",
    "releaseNoteText": "**Consequence:**\n\nThe backpressure queue was not properly reset when there were back-to-back role transitions in the system. This resulted in 100% backpressure on one partition, preventing it from processing tasks.\n\n**Cause:**\n\nThe issue was caused by a bug in which the command API was not properly notified when a node transitioned from leader to follower. This bug was introduced in a previous fix that resulted in the reuse of the limiter from the previous leader role, even when the transition was cancelled. As a result, the backpressure queue was not reset correctly.\n\n**Fix:**\n\nIn order to fix the issue, the code was modified to ensure that the command API is properly notified when a role transition occurs. Specifically, the code in the `PartitionAwareRequestLimiter` class was updated to remove the limiter when a transition is cancelled.\n\n**Result:**\n\nWith this fix, the backpressure queue is now correctly reset when there are back-to-back role transitions. This ensures that the system can process tasks in a timely manner and prevents any unnecessary backpressure on the partitions."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13936",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nIt seems that an Inclusive Gateway with a single outgoing Sequence Flow ignores the Condition\r\n\r\n<img width=\"974\" alt=\"Screenshot 2023-08-17 at 09 52 07\" src=\"https://github.com/camunda/zeebe/assets/3511026/57eb2d2d-8646-4947-b7c6-3aa05472521a\">\r\n\r\nThe BPMN spec is quite clear about the expected behavior:\r\n\r\n> A default path can optionally be identified, to be taken in the event that none of the conditional `Expressions` evaluate to `true`. If a default path is not specified and the **Process** is executed such that none of the conditional `Expressions` evaluates to `true`, a runtime exception occurs. - [10.6.3 Inclusive Gateway](https://www.omg.org/spec/BPMN/2.0.2/PDF#10.6.3%20Inclusive%20Gateway)\r\n\r\nFor C8, such a runtime exception should be represented by [an incident](https://docs.camunda.io/docs/next/components/concepts/incidents/) at the inclusive gateway.\r\n\r\nOriginally reported on the forums: https://forum.camunda.io/t/strange-behavior-with-only-one-sequence-flow-with-condition/46938\r\n\r\n**To Reproduce**\r\n\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\n- deploy a process with an inclusive gateway that has one outgoing sequence flow with the condition `= false`\r\n- create an instance of it\r\n- notice that the sequence flow is taken and that no incident is raised\r\n\r\n**Expected behavior**\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\nAn incident is raised at the inclusive gateway\n\n korthout: Mid severity, because there is a workaround: use two outgoing sequence flows - set the second one to `= false` and let it flow to a none end event\n lzgabel: 👋 Hi @korthout. Please assign this task to me. I'll take a look.",
    "title": "Condition ignored on Inclusive Gateway with singular outgoing Sequence Flow",
    "releaseNoteText": "**Consequence:**\nAn Inclusive Gateway with a single outgoing Sequence Flow was ignoring the Condition. The expected behavior, as specified in the BPMN spec, was for a runtime exception to occur if none of the conditional expressions evaluated to true. However, in this case, the Sequence Flow was being taken without raising an incident.\n\n**Cause:**\nThe underlying cause of this issue was the improper handling of the Condition in the Inclusive Gateway logic. The system was not correctly evaluating the Condition expression and was allowing the Sequence Flow to be taken even when the Condition evaluated to false.\n\n**Fix:**\nThe fix for this issue involved updating the logic of the Inclusive Gateway to properly evaluate the Condition expression. The system now correctly checks the Condition and only allows the Sequence Flow to be taken if the Condition evaluates to true. If none of the conditional expressions evaluate to true, a runtime exception is raised, as expected.\n\n**Result:**\nWith this fix, an Inclusive Gateway with a single outgoing Sequence Flow correctly considers the Condition. If the Condition evaluates to false, a runtime exception is raised, indicating that none of the conditional expressions evaluated to true. This ensures that the system behaves according to the BPMN spec, providing a more accurate and reliable process flow."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13093",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nRunning `RandomizedRaftTest.consistencyTestWithSnapshot` on 8.0.17 leads to the following exception:\r\n```\r\njava.lang.IllegalStateException: Expected to delete index after 404, but it is lower than the commit index 405. Deleting committed entries can lead to inconsistencies and is prohibited. at io.atomix.raft.storage.log.RaftLog.deleteAfter(RaftLog.java:186) at io.atomix.raft.roles.PassiveRole.replaceExistingEntry(PassiveRole.java:623) at io.atomix.raft.roles.PassiveRole.tryToAppend(PassiveRole.java:562) at io.atomix.raft.roles.PassiveRole.appendEntries(PassiveRole.java:514) at io.atomix.raft.roles.PassiveRole.handleAppend(PassiveRole.java:370) at io.atomix.raft.roles.ActiveRole.onAppend(ActiveRole.java:50) at io.atomix.raft.roles.FollowerRole.onAppend(FollowerRole.java:187) at io.atomix.raft.impl.RaftContext.lambda$registerHandlers$13(RaftContext.java:263) at \r\n...\r\n```\r\n\r\nSee [Test results for unit tests(1).zip](https://github.com/camunda/zeebe/files/11734753/Test.results.for.unit.tests.1.zip) from test run https://github.com/camunda/zeebe/actions/runs/5255172343/jobs/9494733250\r\n\r\n**To Reproduce**\r\n| jqwick | explanation |\r\n|--------|--------|\r\n| tries = 10                    | # of calls to property |\r\n| checks = 10                   | # of not rejected calls |\r\n| generation = RANDOMIZED       | parameters are randomly generated |\r\n| after-failure = PREVIOUS_SEED | use the previous seed |\r\n| when-fixed-seed = ALLOW       | fixing the random seed is allowed |\r\n| edge-cases#mode = NONE        | edge cases are not explicitly generated |\r\n| seed = 2349382887260487435    | random seed to reproduce generated values |\r\n\r\n**Expected behavior**\r\n\r\nTest does not fail.\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.lang.IllegalStateException: Expected to delete index after 404, but it is lower than the commit index 405. Deleting committed entries can lead to inconsistencies and is prohibited. at io.atomix.raft.storage.log.RaftLog.deleteAfter(RaftLog.java:186) at io.atomix.raft.roles.PassiveRole.replaceExistingEntry(PassiveRole.java:623) at io.atomix.raft.roles.PassiveRole.tryToAppend(PassiveRole.java:562) at io.atomix.raft.roles.PassiveRole.appendEntries(PassiveRole.java:514) at io.atomix.raft.roles.PassiveRole.handleAppend(PassiveRole.java:370) at io.atomix.raft.roles.ActiveRole.onAppend(ActiveRole.java:50) at io.atomix.raft.roles.FollowerRole.onAppend(FollowerRole.java:187) at io.atomix.raft.impl.RaftContext.lambda$registerHandlers$13(RaftContext.java:263) at io.atomix.raft.impl.RaftContext.lambda$runOnContext$20(RaftContext.java:274) at io.atomix.raft.DeterministicSingleThreadContext$WrappedRunnable.run(DeterministicSingleThreadContext.java:129) at org.jmock.lib.concurrent.DeterministicScheduler$CallableRunnableAdapter.call(DeterministicScheduler.java:176) at org.jmock.lib.concurrent.DeterministicScheduler$ScheduledTask.run(DeterministicScheduler.java:251) at org.jmock.lib.concurrent.DeterministicScheduler.runNextPendingCommand(DeterministicScheduler.java:66) at io.atomix.raft.ControllableRaftContexts.runNextTask(ControllableRaftContexts.java:246) at io.atomix.raft.RaftOperation.lambda$getDefaultRaftOperations$0(RaftOperation.java:62) at io.atomix.raft.RaftOperation.run(RaftOperation.java:42) at io.atomix.raft.RandomizedRaftTest.consistencyTest(RandomizedRaftTest.java:123) at io.atomix.raft.RandomizedRaftTest.consistencyTestWithSnapshot(RandomizedRaftTest.java:88) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:568) at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725) at org.junit.platform.commons.support.ReflectionSupport.invokeMethod(ReflectionSupport.java:198) at net.jqwik.engine.execution.CheckedPropertyFactory.lambda$createRawFunction$1(CheckedPropertyFactory.java:84) at net.jqwik.engine.execution.CheckedPropertyFactory.lambda$createRawFunction$2(CheckedPropertyFactory.java:91) at net.jqwik.engine.properties.CheckedFunction.execute(CheckedFunction.java:17) at net.jqwik.api.lifecycle.AroundTryHook.lambda$static$0(AroundTryHook.java:57) at net.jqwik.engine.execution.lifecycle.HookSupport.lambda$wrap$2(HookSupport.java:48) at net.jqwik.engine.hooks.lifecycle.TryLifecycleMethodsHook.aroundTry(TryLifecycleMethodsHook.java:57) at net.jqwik.engine.execution.lifecycle.HookSupport.lambda$wrap$3(HookSupport.java:53) at net.jqwik.engine.execution.CheckedPropertyFactory.lambda$createTryExecutor$0(CheckedPropertyFactory.java:60) at net.jqwik.engine.execution.lifecycle.AroundTryLifecycle.execute(AroundTryLifecycle.java:23) at net.jqwik.engine.properties.GenericProperty.testPredicate(GenericProperty.java:166) at net.jqwik.engine.properties.GenericProperty.check(GenericProperty.java:68) at net.jqwik.engine.execution.CheckedProperty.check(CheckedProperty.java:67) at net.jqwik.engine.execution.PropertyMethodExecutor.executeProperty(PropertyMethodExecutor.java:90) at net.jqwik.engine.execution.PropertyMethodExecutor.executeMethod(PropertyMethodExecutor.java:69) at net.jqwik.engine.execution.PropertyMethodExecutor.lambda$execute$0(PropertyMethodExecutor.java:49) at net.jqwik.api.lifecycle.AroundPropertyHook.lambda$static$0(AroundPropertyHook.java:46) at net.jqwik.engine.execution.lifecycle.HookSupport.lambda$wrap$0(HookSupport.java:26) at net.jqwik.engine.hooks.lifecycle.PropertyLifecycleMethodsHook.aroundProperty(PropertyLifecycleMethodsHook.java:57) at net.jqwik.engine.execution.lifecycle.HookSupport.lambda$wrap$1(HookSupport.java:31) at net.jqwik.engine.execution.lifecycle.HookSupport.lambda$wrap$0(HookSupport.java:26) at net.jqwik.engine.hooks.statistics.StatisticsHook.aroundProperty(StatisticsHook.java:37) at net.jqwik.engine.execution.lifecycle.HookSupport.lambda$wrap$1(HookSupport.java:31) at net.jqwik.engine.execution.lifecycle.HookSupport.lambda$wrap$0(HookSupport.java:26) at net.jqwik.engine.hooks.lifecycle.AutoCloseableHook.aroundProperty(AutoCloseableHook.java:13) at net.jqwik.engine.execution.lifecycle.HookSupport.lambda$wrap$1(HookSupport.java:31) at net.jqwik.engine.execution.PropertyMethodExecutor.execute(PropertyMethodExecutor.java:47) at net.jqwik.engine.execution.PropertyTaskCreator.executeTestMethod(PropertyTaskCreator.java:166) at net.jqwik.engine.execution.PropertyTaskCreator.lambda$createTask$1(PropertyTaskCreator.java:51) at net.jqwik.engine.execution.lifecycle.CurrentDomainContext.runWithContext(CurrentDomainContext.java:28) at net.jqwik.engine.execution.PropertyTaskCreator.lambda$createTask$2(PropertyTaskCreator.java:50) at net.jqwik.engine.execution.pipeline.ExecutionTask$1.lambda$execute$0(ExecutionTask.java:31) at net.jqwik.engine.execution.lifecycle.CurrentTestDescriptor.runWithDescriptor(CurrentTestDescriptor.java:17) at net.jqwik.engine.execution.pipeline.ExecutionTask$1.execute(ExecutionTask.java:31) at net.jqwik.engine.execution.pipeline.ExecutionPipeline.runToTermination(ExecutionPipeline.java:82) at net.jqwik.engine.execution.JqwikExecutor.execute(JqwikExecutor.java:46) at net.jqwik.engine.JqwikTestEngine.executeTests(JqwikTestEngine.java:70) at net.jqwik.engine.JqwikTestEngine.execute(JqwikTestEngine.java:53) at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107) at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88) at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54) at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67) at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52) at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114) at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86) at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86) at org.apache.maven.surefire.junitplatform.LazyLauncher.execute(LazyLauncher.java:55) at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:234) at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133) at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:228) at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:175) at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:131) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:456) at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:169) at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:595) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:581) \r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.0.17\r\n\r\n\r\n\n\n megglos: ZDP-Triage:\n- unexpected breaking behavior\n- needs to be investigated\n megglos: ZDP-Planning:\n- this indicates a high severity bug and we need to investigate this asap\n deepthidevaki: @npepinpe You might not be able to reproduce this easily. When I was investigating sometime ago, I was wondering why it is not reproducible with the seed, and it looks like https://github.com/camunda/zeebe/blob/bcfe82dcb3be17b6f4a1ee732d40574d374cc1d8/atomix/cluster/src/main/java/io/atomix/raft/cluster/impl/RaftMemberContext.java#L137 this is the reason. I didn't debug further, but relying on System time will be non-deterministic. So most likely this is the reason. This doesn't help in your investigation, but just a hint in case you failed to reproduce it  :smile: \n npepinpe: We have `System.currentTimeMillis()` in other parts of our system as well :sweat: For example, with heartbeat related code, member context, quorum timeout calculation, etc.\r\n\r\nI guess the next step would be to use some clock for full reproducibility :+1: \n npepinpe: Observations:\r\n\r\n1. Expected to delete all entries after 404, but the commit index is 405.\r\n1. We tried to delete after 404 because the replicated entry did not have the same term as the existing entry\r\n1. The mismatched entries index was 405, hence why we want to delete it and anything after (so we say after 404, exclusive)\r\n1. The existing entry was committed\r\n\r\nSo the issue is either:\r\n\r\n1. The entry should not have been committed\r\n1. The terms should not have been mismatched\n npepinpe: A little above, we see the following line:\r\n\r\n```\r\n12:08:19.021 [] WARN  io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - AppendRequest{term=23, leader=1, prevLogIndex=404, prevLogTerm=19, entries=145, commitIndex=839} to 0 failed: java.util.concurrent.TimeoutException\r\n```\r\n\r\nHad to scroll way up higher to find when the commit index was not 405 on any of these nodes. So 405 was appended on 2 as:\r\n\r\n```\r\n12:08:18.230 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=405, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@5d2e7a6a}\r\n```\r\n\r\nIts term was 19.\r\n\r\nIt was appended on 0 as:\r\n\r\n```\r\n12:08:18.229 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=405, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@2dc97eb8}\r\n```\r\n\r\nAlso term 19. Alright. Let's check then when we try to re-append it, what the term was. Cool, we don't have that in the logs :)\r\n\r\nThis is the request we get which triggers the error:\r\n\r\n```\r\n12:08:19.023 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received AppendRequest{term=23, leader=1, prevLogIndex=404, prevLogTerm=19, entries=145, commitIndex=839}\r\n```\r\n\r\nSo the prevLogTerm seems correct, but unfortunately we don't log the mismatched entries, so I can't check it.\r\n\r\nBy the time the error occurs, the term is 23.\r\n\r\nOne weird thing, it seems the leader sends this request to 0:\r\n\r\n```\r\n12:08:19.021 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Sending AppendRequest{term=23, leader=1, prevLogIndex=873, prevLogTerm=23, entries=0, commitIndex=839} to 0\r\n```\r\n\r\nThere is no log for when the response is received from 0, no time out or anything; and then it suddenly tries to send starting at 405 :thinking: Unclear to me what caused it to do that. Possibly it's backed up and has many messages buffered?\r\n\r\nCorrect. Way above, we see:\r\n\r\n```\r\n12:08:19.021 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Received AppendResponse{status=OK, term=23, succeeded=false, lastLogIndex=578, lastSnapshotIndex=404} from 0\r\n12:08:19.021 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Reset next index for RaftMemberContext{member=0, term=23, configIndex=0, snapshotIndex=404, nextSnapshotIndex=0, nextSnapshotChunk=null, matchIndex=0, heartbeatTime=1686658099013, appending=0, appendSucceeded=false, appendTime=1686658099018, configuring=false, installing=false, failures=0} to 579\r\n12:08:19.021 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Sending AppendRequest{term=23, leader=1, prevLogIndex=404, prevLogTerm=19, entries=145, commitIndex=839} to 0\r\n```\r\n\r\nBut if the last log index on 0 is 578, why aren't we sending starting at 579?? Why are we sending starting at 405?\r\n\r\nAh, then we get \r\n\r\n```\r\n12:08:19.021 [] WARN  io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - AppendRequest{term=23, leader=1, prevLogIndex=404, prevLogTerm=19, entries=145, commitIndex=839} to 0 failed: java.util.concurrent.TimeoutException\r\n```\r\n\r\nFollowed quickly by:\r\n\r\n```\r\n12:08:19.021 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Sending AppendRequest{term=23, leader=1, prevLogIndex=873, prevLogTerm=23, entries=0, commitIndex=839} to 0\r\n```\r\n\r\nUnclear why a timeout would lead to the prevLogIndex to be reset...\r\n\r\nAnyway, that second one is never really processed by 0.\n npepinpe: If we keep going further back in time, we see the last append that was processed by 0 was:\r\n\r\n```\r\n12:08:19.018 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Sending AppendRequest{term=23, leader=1, prevLogIndex=860, prevLogTerm=23, entries=0, commitIndex=405} to 0\r\n12:08:19.020 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received AppendRequest{term=23, leader=1, prevLogIndex=860, prevLogTerm=23, entries=0, commitIndex=405}\r\n12:08:19.020 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Rejected AppendRequest{term=23, leader=1, prevLogIndex=860, prevLogTerm=23, entries=0, commitIndex=405}: Previous index (860) is greater than the local log's last index (578)\r\n12:08:19.021 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending AppendResponse{status=OK, term=23, succeeded=false, lastLogIndex=578, lastSnapshotIndex=404}\r\n```\r\n\r\nThis correlates with the reset we saw on 1 earlier, when it reset to 579 (presumably - apparently not!).\r\n\r\nIn fact, we see many of these. Due to time outs, 1 keeps trying to send the same append request, and 0 keeps rejecting it because the previous index is greater than the local log's last index. So let's try to find the last successful append (579).\r\n\r\n```\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received AppendRequest{term=19, leader=1, prevLogIndex=560, prevLogTerm=19, entries=18, commitIndex=405}\r\n12:08:18.284 [] INFO  io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Found leader 1\r\n12:08:18.284 [] TRACE io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Set leader 1\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=561, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@7a75a78e}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=562, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@5058a8e2}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=563, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@1187855a}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=564, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@1f7952dd}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=565, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@4867cf6b}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=566, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@5d6d613d}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=567, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@706d929f}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=568, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@2557d9f5}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=569, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@533d56a3}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=570, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@656fb170}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=571, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@790a2e28}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=572, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@6209d88d}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=573, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@5b381221}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=574, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@43c4dc07}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=575, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@6b17d139}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=576, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@47f6a166}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=577, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@5c441aa3}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=578, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@3985d1c8}\r\n12:08:18.284 [] TRACE io.atomix.raft.storage.system.MetaStore - Store last flushed index 578\r\n12:08:18.284 [] TRACE io.atomix.raft.storage.system.MetaStore - Skip storing same last flushed index 578\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending AppendResponse{status=OK, term=19, succeeded=true, lastLogIndex=578, lastSnapshotIndex=404}\r\n12:08:18.284 [] TRACE io.atomix.raft.impl.RandomizedElectionTimer - Election timeout scheduled for PT3.733S\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received AppendRequest{term=19, leader=1, prevLogIndex=578, prevLogTerm=19, entries=0, commitIndex=405}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending AppendResponse{status=OK, term=19, succeeded=true, lastLogIndex=578, lastSnapshotIndex=404}\r\n```\r\n\r\nSo we can see all the other entries were also during term 19. So it looks like after term 19, 0 never appended anything.\r\n\r\nThen we see 0 soon after trying to get elected for term 20:\r\n\r\n```\r\n12:08:18.285 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 0\r\n12:08:18.285 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 0\r\n12:08:18.285 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - No heartbeat from 1 since 1ms\r\n12:08:18.285 [] TRACE io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Set leader null\r\n12:08:18.285 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending poll requests to all active members: [DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.453Z}, DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.453Z}]\r\n12:08:18.285 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.453Z} for next term 20\r\n12:08:18.285 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.453Z} for next term 20\r\n12:08:18.285 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick 50ms on 0\r\n12:08:18.285 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 0\r\n12:08:18.285 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Failed to poll a majority of the cluster in PT2.5S\r\n```\r\n\r\nThis fails. So let's try and find all the successful elections between 19 and 23.\r\n\r\nThere seems to be no successful election for term 20:\r\n\r\n```\r\n12:08:18.289 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Rejected AppendRequest{term=19, leader=1, prevLogIndex=578, prevLogTerm=19, entries=0, commitIndex=405}: request term is less than the current term (20)\r\n12:08:18.289 [] TRACE io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Sending AppendResponse{status=OK, term=20, succeeded=false, lastLogIndex=578, lastSnapshotIndex=404}\r\n12:08:18.290 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 0\r\n12:08:18.290 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - No heartbeat from a known leader since 6ms\r\n```\r\n\r\nBy the way, during this time, 1 still thinks it's the leader and tries to send append requests.\r\n\r\n```\r\n12:08:18.289 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Received AppendRequest{term=19, leader=1, prevLogIndex=559, prevLogTerm=19, entries=0, commitIndex=405}\r\n12:08:18.289 [] INFO  io.atomix.raft.impl.RaftContext - RaftServer{2-partition-1} - Found leader 1\r\n12:08:18.289 [] TRACE io.atomix.raft.impl.RaftContext - RaftServer{2-partition-1} - Set leader 1\r\n12:08:18.289 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Sending AppendResponse{status=OK, term=19, succeeded=true, lastLogIndex=559, lastSnapshotIndex=404}\r\n12:08:18.289 [] TRACE io.atomix.raft.impl.RandomizedElectionTimer - Election timeout scheduled for PT2.743S\r\n```\r\n\r\nThen later on:\r\n\r\n```\r\n12:08:18.290 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received PollRequest{term=19, candidate=2, lastLogIndex=559, lastLogTerm=19}\r\n12:08:18.290 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Rejected PollRequest{term=19, candidate=2, lastLogIndex=559, lastLogTerm=19}: candidate's term is less than the current term\r\n12:08:18.290 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending PollResponse{status=OK, term=20, accepted=false}\r\n12:08:18.290 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 1\r\n12:08:18.290 [] WARN  io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - AppendRequest{term=19, leader=1, prevLogIndex=559, prevLogTerm=19, entries=0, commitIndex=405} to 2 failed: java.util.concurrent.TimeoutException\r\n12:08:18.290 [] WARN  io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - AppendRequest{term=19, leader=1, prevLogIndex=578, prevLogTerm=19, entries=0, commitIndex=405} to 0 failed: java.util.concurrent.TimeoutException\r\n12:08:18.290 [] TRACE io.atomix.raft.roles.LeaderRole - RaftServer{1-partition-1}{role=LEADER} - Appended IndexedRaftLogEntryImpl{index=613, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=PersistedJournalRecord[metadata=RecordMetadata[checksum=3177859851, length=77], record=RecordData[index=613, asqn=0, data=UnsafeBuffer{addressOffset=140583371101078, capacity=49, byteArray=null, byteBuffer=java.nio.DirectByteBuffer[pos=9159 lim=10240 cap=10240]}]]}\r\n12:08:18.290 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Sending AppendRequest{term=19, leader=1, prevLogIndex=559, prevLogTerm=19, entries=0, commitIndex=405} to 2\r\n12:08:18.290 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Sending AppendRequest{term=19, leader=1, prevLogIndex=578, prevLogTerm=19, entries=0, commitIndex=405} to 0\r\n12:08:18.290 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 0\r\n12:08:18.290 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - No heartbeat from a known leader since 6ms\r\n12:08:18.290 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending poll requests to all active members: [DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.453Z}, DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.453Z}]\r\n12:08:18.290 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.453Z} for next term 21\r\n12:08:18.290 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.453Z} for next term 21\r\n```\r\n\r\nThen we get this weird one about term 1??\r\n\r\n```\r\n12:08:18.290 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received AppendRequest{term=19, leader=1, prevLogIndex=578, prevLogTerm=19, entries=0, commitIndex=405}\r\n12:08:18.290 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Rejected AppendRequest{term=19, leader=1, prevLogIndex=578, prevLogTerm=19, entries=0, commitIndex=405}: request term is less than the current term (20)\r\n12:08:18.290 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending AppendResponse{status=OK, term=20, succeeded=false, lastLogIndex=578, lastSnapshotIndex=404}\r\n12:08:18.290 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Expected heartbeat from null in term 20, but received one from 1 in term 1, ignoring it\r\n```\r\n\r\nThen rapidly 0 will keep doing elections, bumping the term from 20 to 21, then 22:\r\n\r\n```\r\n12:08:18.291 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Received PollRequest{term=20, candidate=0, lastLogIndex=578, lastLogTerm=19}\r\n12:08:18.291 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Accepted PollRequest{term=20, candidate=0, lastLogIndex=578, lastLogTerm=19}: candidate's log is up-to-date\r\n12:08:18.291 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Sending PollResponse{status=OK, term=20, accepted=true}\r\n12:08:18.291 [] INFO  io.atomix.raft.RandomizedRaftTest - Receive next message on 2\r\n[...]\r\n12:08:18.292 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received accepted poll from DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.453Z}\r\n12:08:18.292 [] INFO  io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Transitioning to CANDIDATE\r\n12:08:18.292 [] INFO  io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Starting election\r\n12:08:18.292 [] TRACE io.atomix.raft.storage.system.MetaStore - Store term 21\r\n12:08:18.292 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote null\r\n12:08:18.292 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Set term 21\r\n12:08:18.292 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote 0\r\n12:08:18.292 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Voted for 0\r\n12:08:18.292 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Requesting votes for term 21\r\n12:08:18.292 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Requesting vote from DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.453Z} for term 21\r\n12:08:18.292 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Requesting vote from DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.453Z} for term 21\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 1\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Run next task on 2\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Run next task on 2\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Run next task on 0\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Run next task on 2\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick heartbeatTimeout on 2\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 0\r\n12:08:18.292 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Election timed out. Restarting election.\r\n12:08:18.292 [] TRACE io.atomix.raft.storage.system.MetaStore - Store term 22\r\n12:08:18.292 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote null\r\n12:08:18.292 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Set term 22\r\n12:08:18.292 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote 0\r\n12:08:18.292 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Voted for 0\r\n12:08:18.292 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Requesting votes for term 22\r\n12:08:18.292 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Requesting vote from DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.453Z} for term 22\r\n12:08:18.292 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Requesting vote from DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.453Z} for term 22\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Run next task on 2\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Append on leader on 0\r\n12:08:18.292 [] INFO  TEST - Appending on leader 1\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Append on leader on 0\r\n12:08:18.292 [] INFO  TEST - Appending on leader 1\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Take snapshot on 2\r\n12:08:18.292 [] INFO  TEST - Snapshot taken at index 404. Current commit index is 405\r\n12:08:18.292 [] DEBUG io.camunda.zeebe.journal.file.SegmentsManager - No segments can be deleted with index < 404 (first log index: 313)\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Receive next message on 1\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Run next task on 0\r\n12:08:18.292 [] WARN  io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - null\r\n```\r\n\r\nThen we get this cool warning:\r\n\r\n```\r\n12:08:18.292 [] WARN  io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - null\r\n```\r\n\r\n:smile:\r\n\r\n2 will vote for 0 for term 22:\r\n\r\n```\r\n12:08:18.292 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Received VoteRequest{term=22, candidate=0, lastLogIndex=578, lastLogTerm=19}\r\n12:08:18.292 [] TRACE io.atomix.raft.storage.system.MetaStore - Store term 22\r\n12:08:18.292 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote null\r\n12:08:18.292 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{2-partition-1} - Set term 22\r\n12:08:18.292 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Accepted VoteRequest{term=22, candidate=0, lastLogIndex=578, lastLogTerm=19}: candidate's log is up-to-date\r\n12:08:18.292 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote 0\r\n12:08:18.292 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{2-partition-1} - Voted for 0\r\n12:08:18.292 [] TRACE io.atomix.raft.impl.RandomizedElectionTimer - Election timeout scheduled for PT3.103S\r\n12:08:18.292 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Sending VoteResponse{status=OK, term=22, voted=true}\r\n```\r\n\r\nStill, no election succeeded yet. Then 2 bumps term to 23:\r\n\r\n```\r\n12:08:18.293 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 2\r\n12:08:18.293 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - No heartbeat from a known leader since 1ms\r\n12:08:18.293 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Sending poll requests to all active members: [DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.455Z}, DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.455Z}]\r\n12:08:18.293 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.455Z} for next term 23\r\n12:08:18.293 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.455Z} for next term 23\r\n```\r\n\r\nThen after all this time, 1 processes the message from 0 about term increasing to 21 (yes, 1 was still on term 19 trying to append stuff to the followers):\r\n\r\n```\r\n12:08:18.293 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 1\r\n12:08:18.293 [] TRACE io.atomix.raft.storage.system.MetaStore - Store term 21\r\n12:08:18.293 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote null\r\n12:08:18.293 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{1-partition-1} - Set term 21\r\n12:08:18.293 [] INFO  io.atomix.raft.roles.LeaderRole - RaftServer{1-partition-1}{role=LEADER} - Received greater term from 0\r\n12:08:18.293 [] INFO  io.atomix.raft.impl.RaftContext - RaftServer{1-partition-1} - Transitioning to FOLLOWER\r\n12:08:18.296 [] TRACE io.atomix.raft.roles.LeaderRole - RaftServer{1-partition-1}{role=LEADER} - Cancelling append timer\r\n12:08:18.296 [] TRACE io.atomix.raft.impl.RandomizedElectionTimer - Election timeout scheduled for PT4.57S\r\n12:08:18.296 [] TRACE io.atomix.raft.roles.LeaderRole - RaftServer{1-partition-1}{role=LEADER} - Received VoteRequest{term=21, candidate=0, lastLogIndex=578, lastLogTerm=19}\r\n12:08:18.296 [] DEBUG io.atomix.raft.roles.LeaderRole - RaftServer{1-partition-1}{role=LEADER} - Rejected VoteRequest{term=21, candidate=0, lastLogIndex=578, lastLogTerm=19}: candidate's last log entry (578) is at a lower index than the local log (621)\r\n12:08:18.296 [] TRACE io.atomix.raft.roles.LeaderRole - RaftServer{1-partition-1}{role=LEADER} - Sending VoteResponse{status=OK, term=21, voted=false}\r\n[...]\r\n12:08:18.297 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Second round of election timed out. Transitioning to follower.\r\n12:08:18.297 [] INFO  io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Transitioning to FOLLOWER\r\n12:08:18.297 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Cancelling election\r\n12:08:18.297 [] TRACE io.atomix.raft.impl.RandomizedElectionTimer - Election timeout scheduled for PT3.95S\r\n12:08:18.297 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 1\r\n12:08:18.297 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - No heartbeat from a known leader since 4ms\r\n12:08:18.297 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Sending poll requests to all active members: [DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.454Z}, DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.454Z}]\r\n12:08:18.297 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.454Z} for next term 22\r\n12:08:18.297 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.454Z} for next term 22\r\n[...]\r\n12:08:18.298 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Failed to poll a majority of the cluster in PT2.5S\r\n12:08:18.298 [] TRACE io.atomix.raft.impl.RandomizedElectionTimer - Election timeout scheduled for PT3.769S\r\n12:08:18.298 [] INFO  io.atomix.raft.RandomizedRaftTest - Drop next message on 2\r\n12:08:18.298 [] INFO  TEST: - Dropped a message to 2\r\n12:08:18.298 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick heartbeatTimeout on 2\r\n12:08:18.298 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick 50ms on 1\r\n12:08:18.298 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 1\r\n12:08:18.298 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Failed to poll a majority of the cluster in PT2.5S\r\n12:08:18.298 [] TRACE io.atomix.raft.impl.RandomizedElectionTimer - Election timeout scheduled for PT4.267S\r\n[...]\r\n12:08:18.298 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Received VoteRequest{term=22, candidate=0, lastLogIndex=578, lastLogTerm=19}\r\n12:08:18.298 [] TRACE io.atomix.raft.storage.system.MetaStore - Store term 22\r\n12:08:18.298 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote null\r\n12:08:18.298 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{1-partition-1} - Set term 22\r\n12:08:18.298 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Rejected VoteRequest{term=22, candidate=0, lastLogIndex=578, lastLogTerm=19}: candidate's last log entry (578) is at a lower index than the local log (621)\r\n12:08:18.298 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Sending VoteResponse{status=OK, term=22, voted=false}\r\n[...]\r\n12:08:18.298 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Expected heartbeat from null in term 22, but received one from 1 in term 1, ignoring it\r\n12:08:18.298 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received PollRequest{term=22, candidate=2, lastLogIndex=559, lastLogTerm=19}\r\n12:08:18.298 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Rejected PollRequest{term=22, candidate=2, lastLogIndex=559, lastLogTerm=19}: candidate's last log entry (559) is at a lower index than the local log (578)\r\n12:08:18.298 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending PollResponse{status=OK, term=22, accepted=false}\r\n[...]\r\n12:08:18.299 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Sending poll requests to all active members: [DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.455Z}, DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.455Z}]\r\n12:08:18.299 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.455Z} for next term 23\r\n12:08:18.299 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.455Z} for next term 23\r\n[...]\r\n12:08:18.299 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received PollRequest{term=22, candidate=2, lastLogIndex=559, lastLogTerm=19}\r\n12:08:18.299 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Rejected PollRequest{term=22, candidate=2, lastLogIndex=559, lastLogTerm=19}: candidate's last log entry (559) is at a lower index than the local log (578)\r\n12:08:18.299 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending PollResponse{status=OK, term=22, accepted=false}\r\n[...]\r\n12:08:18.299 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Received PollRequest{term=22, candidate=2, lastLogIndex=559, lastLogTerm=19}\r\n12:08:18.299 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Rejected PollRequest{term=22, candidate=2, lastLogIndex=559, lastLogTerm=19}: candidate's last log entry (559) is at a lower index than the local log (621)\r\n12:08:18.299 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Sending PollResponse{status=OK, term=22, accepted=false}\r\n[...]\r\n12:08:18.299 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - No heartbeat from a known leader since 6ms\r\n12:08:18.299 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Sending poll requests to all active members: [DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.454Z}, DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.454Z}]\r\n12:08:18.299 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.454Z} for next term 23\r\n12:08:18.299 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.454Z} for next term 23\r\n```\r\n\r\nSo we see both 0 and 1 keep rejecting 2 as a candidate, and since 1 has the longest log, it should become elected eventually. Last we see it tries an election on term 23.\r\n\r\n```\r\n12:08:18.299 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received PollRequest{term=22, candidate=1, lastLogIndex=621, lastLogTerm=19}\r\n12:08:18.299 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Accepted PollRequest{term=22, candidate=1, lastLogIndex=621, lastLogTerm=19}: candidate's log is up-to-date\r\n12:08:18.299 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending PollResponse{status=OK, term=22, accepted=true}\r\n[...]\r\n12:08:18.299 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{1-partition-1} - Set term 23\r\n12:08:18.299 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote 1\r\n12:08:18.299 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{1-partition-1} - Voted for 1\r\n12:08:18.299 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{1-partition-1}{role=CANDIDATE} - Requesting votes for term 23\r\n12:08:18.299 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{1-partition-1}{role=CANDIDATE} - Requesting vote from DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.454Z} for term 23\r\n12:08:18.299 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{1-partition-1}{role=CANDIDATE} - Requesting vote from DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.454Z} for term 23\r\n[...]\r\n12:08:18.299 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received VoteRequest{term=23, candidate=1, lastLogIndex=621, lastLogTerm=19}\r\n12:08:18.299 [] TRACE io.atomix.raft.storage.system.MetaStore - Store term 23\r\n12:08:18.299 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote null\r\n12:08:18.299 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Set term 23\r\n12:08:18.300 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Accepted VoteRequest{term=23, candidate=1, lastLogIndex=621, lastLogTerm=19}: candidate's log is up-to-date\r\n12:08:18.300 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote 1\r\n12:08:18.300 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Voted for 1\r\n12:08:18.300 [] TRACE io.atomix.raft.impl.RandomizedElectionTimer - Election timeout scheduled for PT2.939S\r\n12:08:18.300 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending VoteResponse{status=OK, term=23, voted=true}\r\n[...]\r\n12:08:18.300 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - No heartbeat from a known leader since 8ms\r\n12:08:18.300 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Sending poll requests to all active members: [DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.455Z}, DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.455Z}]\r\n12:08:18.300 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.455Z} for next term 23\r\n12:08:18.300 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.455Z} for next term 23\r\n12:08:18.300 [] INFO  io.atomix.raft.RandomizedRaftTest - Append on leader on 0\r\n12:08:18.300 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick heartbeatTimeout on 0\r\n12:08:18.300 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 1\r\n12:08:18.300 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{1-partition-1}{role=CANDIDATE} - Received successful vote from DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.454Z}\r\n12:08:18.300 [] INFO  io.atomix.raft.impl.RaftContext - RaftServer{1-partition-1} - Transitioning to LEADER\r\n12:08:18.300 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{1-partition-1}{role=CANDIDATE} - Cancelling election\r\n```\r\n\r\nSo now we get to term 23. After that is a long time of just sending append requests trying to keep appending on the nodes, the log of which I already put above.\r\n\n npepinpe: So that the commit index is 405 is fine, and it would make sense that the entry is committed. We saw that both nodes received further appends (2 up to 559, and 0 up to 578), both requests which started at 406. So 405 should be committed, with term 19.\r\n\r\nSo possibly the problem is that during replication, 1 should have started sending entries from 579, but instead it started at 405? Or some other weird thing happened...but it's likely a replication issue.\n npepinpe: One thing to note, the index we delete after does not come from the entry we're trying to replace, but instead from the AppendRequest's prevLogIndex. It then gets incremented as we append...\r\n\r\nIs this correct? We saw that 0 replies that it's prevLogIndex is not 404, but instead 578.\r\n\r\nSee:\r\n\r\n```\r\n12:08:19.021 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Received AppendResponse{status=OK, term=23, succeeded=false, lastLogIndex=578, lastSnapshotIndex=404} from 0\r\n12:08:19.021 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Reset next index for RaftMemberContext{member=0, term=23, configIndex=0, snapshotIndex=404, nextSnapshotIndex=0, nextSnapshotChunk=null, matchIndex=0, heartbeatTime=1686658099013, appending=0, appendSucceeded=false, appendTime=1686658099018, configuring=false, installing=false, failures=0} to 579\r\n12:08:19.021 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Sending AppendRequest{term=23, leader=1, prevLogIndex=404, prevLogTerm=19, entries=145, commitIndex=839} to 0\r\n```\r\n\r\n\n npepinpe: From what I can see, it reset to 578, but then when building the next AppendRequest, it builds it from the previous entry which is...404? Is it because the snapshot is at 404?\r\n\r\nSo on the leader, if the previous entry (previous from what we should send) is not found, then we will send the prevLogEntry is the snapshot. Since 0 reports its last entry as 405, we try to get the prevLogEntry - which is not there, because 405 is the first entry in 1's log. So we send the snapshot - 404 - with term 19. But the first entry in the request is 579.\r\n\r\nSo on the follower (0), our last log entry is 578, greater than the reported `prevLogEntry` (404), so we think we have to replace some existing entries, starting at 405 (even if the first entry in the request is 578).\r\n\r\nI still don't get why the terms were mismatched though - it seems to me they should be the same, 19, so we shouldn't even have caught this bug :scream: \n npepinpe: @deepthidevaki - let's sync tomorrow on this\r\n\r\nI don't remember why we use the prevLogIndex as the start index instead of just using the entry's index themselves.\n deepthidevaki: > Is this correct? We saw that 0 replies that it's prevLogIndex is not 404, but instead 578.\r\n\r\nThis doesn't look like the expected behavior. Were there other AppendRequests in between, which would have truncated the entries in 0 back to 404? \n npepinpe: So it did compact right before:\r\n\r\n```\r\n12:08:19.021 [] INFO  io.atomix.raft.RandomizedRaftTest - Take snapshot on 1\r\n12:08:19.021 [] INFO  TEST - Snapshot taken at index 791. Current commit index is 839\r\n12:08:19.021 [] DEBUG io.camunda.zeebe.journal.file.SegmentsManager - atomix - Deleting log up from 313 up to 625 (removing 4 segments)\r\n```\n npepinpe: Alright, so the bug is that we have two snapshot listeners: one triggers compaction, and one updates the locale reference of the persisted snapshot in the Raft context. While both execute on the Raft thread, they may be executed in any order (since the listeners on the snapshot store are a set, iteration is not ordered).\r\n\r\nSo what happened here is:\r\n\r\n1. A new snapshot was taken up to entry 839\r\n2. The log is compacted up to 839\r\n3. 1 sent an append request for 839 to 0\r\n4. 0 rejected saying its last log index is 578\r\n5. 1 tries to reset 0 to 578, but the entry does not exist\r\n6. 1 sends a new append request using the local snapshot reference for prevLogIndex and prevLogTerm\r\n7. The snapshot reference is updated to 839\r\n\r\nSolution is to ensure that all the changes to the Raft context (updating the reference, compacting, etc.) are done in a single listener so we can control the ordering: update snapshot ref, then compact.\n npepinpe: This likely affect all versions. The severity is high in this case, as the partition will go inactive due to an uncaught exception. Workaround is to restart your broker.",
    "title": "`RandomizedRaftTest.consistencyTestWithSnapshot` fails with unexpected exception",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/7855",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nIt seems we are closing the resources not in the correct order. We still accept or want to send an response but the message service is closed concurrently and the sending fails and causes on error on closing the Broker.\r\n\r\nOccurred 10 times in two error groups within `1.2.0-alpha2`\r\n\r\n\r\nError groups:\r\n\r\n * https://console.cloud.google.com/errors/CJeh5tGzv8X1Rg?service=zeebe&time=P7D&refresh=off&project=camunda-cloud-240911\r\n * https://console.cloud.google.com/errors/CKKJ762u3J3ZUw?service=zeebe&time=P7D&refresh=off&project=camunda-cloud-240911\r\n\r\n\r\n**To Reproduce**\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n - when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\nI assume run the service and close the broker.\r\n\r\n**Expected behavior**\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nNo error, correct sequence of closing resources.\r\n\r\n**Log/Stacktrace**\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.lang.IllegalStateException: Messaging service is not running.\r\nat io.camunda.zeebe.transport.impl.AtomixClientTransportAdapter.tryToSend (AtomixClientTransportAdapter.java:105)\r\nat io.camunda.zeebe.transport.impl.AtomixClientTransportAdapter.lambda$handleResponse$7 (AtomixClientTransportAdapter.java:206)\r\nat io.camunda.zeebe.util.sched.ActorJob.invoke (ActorJob.java:76)\r\nat io.camunda.zeebe.util.sched.ActorJob.execute (ActorJob.java:39)\r\nat io.camunda.zeebe.util.sched.ActorTask.execute (ActorTask.java:122)\r\nat io.camunda.zeebe.util.sched.ActorThread.executeCurrentTask (ActorThread.java:94)\r\nat io.camunda.zeebe.util.sched.ActorThread.doWork (ActorThread.java:78)\r\nat io.camunda.zeebe.util.sched.ActorThread.run (ActorThread.java:191)\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n\r\n**Environment:**\r\n\r\n - Zeebe Version: 1.2.0-alpha2\r\n\r\n\n\n Zelldon: Error message introduce via https://github.com/camunda-cloud/zeebe/pull/7568\n Zelldon: related https://github.com/camunda-cloud/zeebe/issues/5521\n Zelldon: @pihme do you think this is solved due to recent refactorings?\n pihme: @Zelldon No I don't. The order of atomix and embedded gateway has not changed by the refactoring. One thing I wonder is whether this occurred in the Broker or in a Standalone Gateway. Do you know where it occurred?\r\n\r\nIn terms of macro-order of shutdown steps this should not happen.\r\n \r\nIt could happen if either shutdown of the actor does not cancel all subsequent planned tasks. Or it could happen if it is possible to schedule new tasks after an actor has been shutdown.\n pihme: Reoccurred recently:\r\n\r\nLogs:\r\n```\r\nD 2022-01-16T09:29:16.098476Z Closing Broker-0 [6/10]: embedded gateway closed in 15 ms \r\nI 2022-01-16T09:29:16.098659Z Closing Broker-0 [7/10]: cluster services \r\nD 2022-01-16T09:29:16.098802Z Closing Broker-0 [7/10]: cluster services closed in 0 ms \r\nI 2022-01-16T09:29:16.098978Z Closing Broker-0 [8/10]: subscription api \r\nD 2022-01-16T09:29:16.102018Z Closing Broker-0 [8/10]: subscription api closed in 2 ms \r\nI 2022-01-16T09:29:16.102259Z Closing Broker-0 [9/10]: command api transport and handler \r\nI 2022-01-16T09:29:16.232428Z Stopped \r\nD 2022-01-16T09:29:16.233107Z Closing Broker-0 [9/10]: command api transport and handler closed in 131 ms \r\nI 2022-01-16T09:29:16.233481Z Closing Broker-0 [10/10]: Migrated Startup Steps \r\nD 2022-01-16T09:29:16.235563Z Shutdown was called with context: io.camunda.zeebe.broker.bootstrap.BrokerStartupContextImpl@36539fa \r\nI 2022-01-16T09:29:16.235975Z Shutdown Cluster Services (Creation) \r\nI 2022-01-16T09:29:16.238984Z Stopped \r\nI 2022-01-16T09:29:16.240144Z Stopped \r\nI 2022-01-16T09:29:16.241539Z 0 - Member deactivated: Member{id=0, address=zeebe-0.zeebe-broker-service.0bbb3676-e3ad-445b-891e-0e1350800e41-zeebe.svc.cluster.local:26502, properties={brokerInfo=EADJAAAAAwAAAAAAAgAAAAMAAAADAAAAAAABCgAAAGNvbW1hbmRBcGlfAAAAemVlYmUtMC56ZWViZS1icm9rZXItc2VydmljZS4wYmJiMzY3Ni1lM2FkLTQ0NWItODkxZS0wZTEzNTA4MDBlNDEtemVlYmUuc3ZjLmNsdXN0ZXIubG9jYWw6MjY1MDEFAAIBAAAAAQIAAAABDAAABQAAADEuMi45BQACAQAAAAECAAAAAQ==, event-service-topics-subscribed=KIIDAGpvYnNBdmFpbGFibOU=}} \r\nI 2022-01-16T09:29:16.241947Z Stopped \r\nI 2022-01-16T09:29:16.242539Z Local node Member{id=0, address=zeebe-0.zeebe-broker-service.0bbb3676-e3ad-445b-891e-0e1350800e41-zeebe.svc.cluster.local:26502, properties={brokerInfo=EADJAAAAAwAAAAAAAgAAAAMAAAADAAAAAAABCgAAAGNvbW1hbmRBcGlfAAAAemVlYmUtMC56ZWViZS1icm9rZXItc2VydmljZS4wYmJiMzY3Ni1lM2FkLTQ0NWItODkxZS0wZTEzNTA4MDBlNDEtemVlYmUuc3ZjLmNsdXN0ZXIubG9jYWw6MjY1MDEFAAIBAAAAAQIAAAABDAAABQAAADEuMi45BQACAQAAAAECAAAAAQ==, event-service-topics-subscribed=KIIDAGpvYnNBdmFpbGFibOU=}} left the bootstrap servide \r\nI 2022-01-16T09:29:16.243398Z Stopped cluster membership service for member Member{id=0, address=zeebe-0.zeebe-broker-service.0bbb3676-e3ad-445b-891e-0e1350800e41-zeebe.svc.cluster.local:26502, properties={brokerInfo=EADJAAAAAwAAAAAAAgAAAAMAAAADAAAAAAABCgAAAGNvbW1hbmRBcGlfAAAAemVlYmUtMC56ZWViZS1icm9rZXItc2VydmljZS4wYmJiMzY3Ni1lM2FkLTQ0NWItODkxZS0wZTEzNTA4MDBlNDEtemVlYmUuc3ZjLmNsdXN0ZXIubG9jYWw6MjY1MDEFAAIBAAAAAQIAAAABDAAABQAAADEuMi45BQACAQAAAAECAAAAAQ==, event-service-topics-subscribed=KIIDAGpvYnNBdmFpbGFibOU=}} \r\nE 2022-01-16T09:29:18.270930Z Expected to handle gRPC request, but an unexpected error occurred \r\nE 2022-01-16T09:29:18.272130Z Expected to handle gRPC request, but an unexpected error occurred \r\nE 2022-01-16T09:29:18.273074Z Expected to handle gRPC request, but an unexpected error occurred \r\nI 2022-01-16T09:29:18.362961Z Stopped \r\nI 2022-01-16T09:29:18.363819Z Stopped \r\nI 2022-01-16T09:29:18.365121Z Shutdown monitoring services \r\nD 2022-01-16T09:29:18.366707Z Finished shutdown process \r\nD 2022-01-16T09:29:18.367117Z Closing Broker-0 [10/10]: Migrated Startup Steps closed in 2134 ms \r\nI 2022-01-16T09:29:18.367404Z Closing Broker-0 succeeded. Closed 10 steps in 2419 ms. \r\nI 2022-01-16T09:29:18.367629Z Broker shut down. \r\nD 2022-01-16T09:29:18.367920Z Closing actor thread ground 'Broker-0-zb-fs-workers' \r\nD 2022-01-16T09:29:18.368650Z Closing actor thread ground 'Broker-0-zb-actors' \r\nD 2022-01-16T09:29:18.369899Z Closing actor thread ground 'Broker-0-zb-actors': closed successfully \r\nD 2022-01-16T09:29:18.369957Z Closing actor thread ground 'Broker-0-zb-fs-workers': closed successfully \r\n```\n Zelldon: This happened again on our benchmark week 27\r\n\r\nhttps://console.cloud.google.com/errors/detail/CJ-difGB-LXsAg;service=zeebe;version=medic-cw-27-56ad2b36c8-benchmark;time=P7D?project=zeebe-io\n korthout: Happened again on benchmark week 27 ([newly reported error](https://console.cloud.google.com/errors/detail/CK6Khe-c7YzlFw;service=zeebe;version=medic-cw-27-56ad2b36c8-benchmark;time=P7D?project=zeebe-io)) with a different stacktrace. \r\n\r\nThis time because of `AtomixClientTransportAdapter.lambda$sendRequestInternal$2` instead of `AtomixClientTransportAdapter.lambda$handleResponse$7`.\n Zelldon: Last seen: 1 day ago.\n oleschoenburg: Happened again on 8.1.8: https://console.cloud.google.com/errors/detail/CK6Khe-c7YzlFw;service=zeebe;time=P7D?project=camunda-cloud-240911\n Zelldon: Happened in [zeebe:8.0.19-SNAPSHOT-stable-8.0-c326e93b](https://console.cloud.google.com/errors/detail/CKKJ762u3J3ZUw;service=zeebe;version=8.0.19-SNAPSHOT-stable-8.0-c326e93b;time=P7D?project=camunda-saas-int-chaos)\r\n\r\nhttps://console.cloud.google.com/errors/detail/CKKJ762u3J3ZUw;service=zeebe;time=P7D?project=camunda-saas-int-chaos\n deepthidevaki: Observed in 8.2.12\r\n\r\nRoot cause analysis:\r\n\r\nThe error message originates in `AtomixClientTransportAdapter`. This is created in Gateway BrokerClient\r\n\r\nhttps://github.com/camunda/zeebe/blob/18657c586a1974f6bb3ce3f86c1e16359458050d/gateway/src/main/java/io/camunda/zeebe/gateway/impl/broker/BrokerClientImpl.java#L64\r\n\r\nBut never closed by it\r\nhttps://github.com/camunda/zeebe/blob/0945c3088629e5a72b260778c104418afb113b08/gateway/src/main/java/io/camunda/zeebe/gateway/impl/broker/BrokerClientImpl.java#L70-L87\r\n\r\nFix:\r\nMessaging service is only closed after BrokerClient (I guess so because the lifecycle is managed by Spring, so the dependency order should be preserved). So closing `AtomixClientTransportAdapter` when closing `BrokerClientImpl` should ensure that no request are send after it is closed.\n oleschoenburg: Observed again on 8.1.16: https://console.cloud.google.com/errors/detail/CK6Khe-c7YzlFw;service=zeebe;time=P7D?project=camunda-saas-int-chaos\n megglos: ZDP-Triage:\n- mostly noise during shutdown\n- maybe the close is not sync",
    "title": "Messaging service is not running",
    "releaseNoteText": "**Consequence:** The messaging service was not running, causing an error when trying to send a response and closing the Broker.\n\n**Cause:** The issue occurred due to the incorrect order of resource closure. The messaging service was closed concurrently while still accepting or attempting to send a response, resulting in an error during the closure of the Broker.\n\n**Fix:** The `AtomixClientTransportAdapter` was not closed by the `BrokerClientImpl`. The fix involved closing the `AtomixClientTransportAdapter` when closing the `BrokerClientImpl` to ensure that no requests are sent after it is closed.\n\n**Result:** After the fix, the messaging service is running correctly, ensuring that requests are sent and responses are received without errors during the closure of the Broker."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/5209",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Description**\r\nCurrently we start zeebe partition services only after atomix is fully started. That means if a node has 3 partitions, we wait until all 3 partitions has successfully started their raft servers. In some benchmarks it was observed that sometimes 2 partitions in a node succesfuly starts and become the raft leaders. But the third partition did not join, or took long (hours) to join (due to #5208 ). During this time, this node is the raft leader for 2 partitions, but no processing is happening because we don't install stream processor and other leader services until the atomix start is completed. As a result, the service is unavailable or only partially available even though there are leaders for all partitions.\r\n\n\n Zelldon: I think this goes in hand with removing the complete atomix wrapper and bootstrap logic, which wanted to do anyway at some point. I think it makes sense to remove it and have independent starters for each partition.\n deepthidevaki: Is there an issue for it already? If so, let's link it here.\n Zelldon: Unfortunately I haven't found it in the backlog. That's why I just commented it :smile: \n npepinpe: Do we see any blockers to starting partitions in parallel without waiting for the others to be started?\n Zelldon: I see no blockers. Maybe @deepthidevaki has some thoughts?\r\n\n deepthidevaki: No real blockers that I know. We would have to refactor atomix interface. There should be a way to detect when an individual partition is started and ready. For a single partition, the steps in bootstrap are the following in order.\r\n1. RaftPartitionServer is created\r\n2. Become follower\r\n3. Leader election happens\r\n4. Join completes\r\n5. Catch up if needed\r\n6. Marked as Ready -> Startup complete.\r\n\r\nWe have to ensure that zeebe services are installed only after step 6. Currently, we do `atomix.start().join()`, this is guaranteed.\r\n  \n Zelldon: What we could to instead is:\r\n\r\n * start atomix transport\r\n * membership\r\n * start topology\r\n * start monitoring \r\n * start disk space\r\n * split up in partition starts with sub steps for installing processors etc.\n npepinpe: Would love to improve the bootstrapping logic next quarter - let's see if we can make it happen.\n deepthidevaki: We observed an e2e test failure as a consequence of this.\r\n\r\nBroker 2 was not becoming ready, because partition 1 was not able to receive heartbeats from leader (due to another bug). But it was receiving events for partitions 2 and 3. Since raft partition 1 is not ready, Zeebe services  (processing and snapshoting) were not started for any partitions. So partitions 2 and 3 starts accumulating events in the log, but no snapshot were taken.\r\n\r\nLater, the root issue was resolved, partition 1 startup was unblocked and broker 2 became ready. Immediately after that, broker 2 became the leader for partition 3. Since it was not replaying and taking snapshots before, it's last snapshot was from 12 hours ago. As a result, partition 3 had to replay events generated in the last 12 hours before it can start processing. The replay took almost 1 hour, and during that time partition 3 was essentially unavailable for processing. All instances that were created before the leader change, was completed only after 1 hour.\n megglos: planning: affects higher partition count setups more heavily, risk of incidents,\nlet's time-box the investigation to 2h\n oleschoenburg: I looked into this, mostly to understand the startup process.\r\nTo me it looks like we have two \"barriers\" that wait for all partitions before continuing the startup process:\r\n1. `RaftPartitionGroup#join` waits for all raft partitions [to become `READY`](https://github.com/camunda/zeebe/blob/f574c79929f01e51f33a3d5902fc651aa15c3d46/atomix/cluster/src/main/java/io/atomix/raft/impl/DefaultRaftServer.java#L191-L197). This is [called by the PartitionManger](https://github.com/camunda/zeebe/blob/6a454eedc03e14d4f7fa56e5560c4febfbe57f89/broker/src/main/java/io/camunda/zeebe/broker/partitioning/PartitionManagerImpl.java#L128-L129) before scheduling `ZeebePartition`s. \r\n2.  `PartitionManagerImpl#start` waits for all `ZeebePartition`s to start. This blocks at least the installation of `AdminApiRequestHandler`\r\n\r\nThe first one is the crux I think, we don't want to wait for the partition _group_ to start, we just want to wait for each partition individually.\r\n\r\nThe second one appears to be trivial to solve:\r\n\r\n```patch\r\ndiff --git a/broker/src/main/java/io/camunda/zeebe/broker/partitioning/PartitionManagerImpl.java b/broker/src/main/java/io/camunda/zeebe/broker/partitioning/PartitionManagerImpl.java\r\n--- a/broker/src/main/java/io/camunda/zeebe/broker/partitioning/PartitionManagerImpl.java\t(revision 4fa6c0dc0009f5faedb987b09a055f9135629495)\r\n+++ b/broker/src/main/java/io/camunda/zeebe/broker/partitioning/PartitionManagerImpl.java\t(revision 6a454eedc03e14d4f7fa56e5560c4febfbe57f89)\r\n@@ -127,7 +127,7 @@\r\n \r\n     return partitionService\r\n         .start()\r\n-        .thenApply(\r\n+        .thenApplyAsync(\r\n             ps -> {\r\n               LOGGER.info(\"Registering Partition Manager\");\r\n \r\n@@ -158,12 +158,10 @@\r\n                       topologyManager,\r\n                       brokerCfg.getExperimental().getFeatures().toFeatureFlags()));\r\n \r\n-              final var futures =\r\n-                  partitions.stream()\r\n-                      .map(partition -> CompletableFuture.runAsync(() -> startPartition(partition)))\r\n-                      .toArray(CompletableFuture[]::new);\r\n+              for (final var partition : partitions) {\r\n+                CompletableFuture.runAsync(() -> startPartition(partition));\r\n+              }\r\n \r\n-              CompletableFuture.allOf(futures).join();\r\n               return null;\r\n             });\r\n   }\r\n```\r\n\r\n",
    "title": "Startup failure of one raft partition server affects the availability of other healthy partition",
    "releaseNoteText": "**Consequence:** The availability of healthy partitions is affected by the startup failure of one raft partition server. This issue arises because the system currently waits for all partitions to successfully start their raft servers before starting the zeebe partition services. As a result, even though there are leaders for all partitions, the service may be unavailable or only partially available.\n\n**Cause:** The issue is caused by the current logic of waiting for all partitions to start before installing stream processors and other leader services. This logic is based on waiting for the atomix start to be completed.\n\n**Fix:** The fix involves removing the complete atomix wrapper and bootstrap logic. Instead, independent starters for each partition are implemented. The bootstrapping logic is improved by starting the atomix transport, membership, topology, monitoring, and disk space first. Then, the partition starts are split up into sub-steps for installing processors and other services.\n\n**Result:** After the fix is applied, the system will no longer wait for all partitions to start before installing stream processors and leader services. Each partition will be able to start independently, allowing for parallel startup of the partitions. This will improve the availability of healthy partitions and prevent the system from being partially or fully unavailable due to startup failures."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14146",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nWhen we delete a DRG it is not removed from the cache. This means it remains available until the broker is restarted. We must make sure we delete the DRG from the cache.\r\n\r\nThe unit test supposed to verify this contains a bug:\r\n\r\n```java\r\n    assertThat(\r\n        decisionState\r\n            .findDecisionsByDecisionRequirementsKey(drg1.getDecisionRequirementsKey())\r\n            .isEmpty());\r\n```\r\n\r\nThe parentheses are off. This means this assertion doesn't actually assert anything. It's the equivalent of `assertThat(false)` at the moment.\r\n\r\n\n",
    "title": "Remove DRG from cache upon deletion",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14028",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n@philippfromme discovered that a link throw event can activate a link catch event inside an (event) subprocess. See the following process.  \r\n\r\n![Screenshot from 2023-08-28 09-41-57](https://github.com/camunda/zeebe/assets/4305769/58753ebb-d565-4a25-86ab-e39e30023139)\r\n\r\nAccording to the BPMN specification and the Camunda documentation, it should not be possible to activate a link catch event that is not in the same scope as the link throw event. Both events must be in the same scope.\r\n\r\nThis issue is kind of related to https://github.com/camunda/zeebe/issues/10854.\r\n\r\n**To Reproduce**\r\n\r\n1. Deploy the following process \r\n[link-event-subprocess.bpmn](https://github.com/camunda/zeebe/files/12452163/link-event-subprocess.bpmn.txt)\r\n2. Create a new instance of the process\r\n3. Verify that the link catch event is activated\r\n\r\n**Expected behavior**\r\n\r\nThe link catch event inside the (event) subprocess is not activated.\r\n\r\nThe deployment of the process is rejected because there is no link catch event in the same scope.\r\n\r\n**Log/Stacktrace**\r\n\r\n<details><summary>Output from Zeebe-Play</summary>\r\n <p>\r\n\r\n![Screenshot from 2023-08-28 10-22-54](https://github.com/camunda/zeebe/assets/4305769/cc3cbd5d-9e0e-4e7b-97b7-7645e9663265)\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- OS: <!-- [e.g. Linux] -->\r\n- Zeebe Version: `8.2.0`\r\n- Configuration: <!-- [e.g. exporters etc.] -->\r\n\n\n lzgabel: Hi @saig0. You can assign this task to me and I'll take a look. :bow:\n saig0: @lzgabel awesome. :rocket: Thank you for your engagement. :bow: ",
    "title": "Should not activate link catch event in subprocess",
    "releaseNoteText": "**Consequence:**\nThe link catch event inside the (event) subprocess is activated when a link throw event occurs, even if they are not in the same scope. This behavior is contrary to the BPMN specification and the Camunda documentation.\n\n**Cause:**\nThe activation of a link catch event in a subprocess is a result of a technical issue in the product. The system was not properly checking the scope of the link events before activating them.\n\n**Fix:**\nThe issue has been fixed by implementing a check to ensure that a link throw event can only activate a link catch event within the same scope. The validation logic now correctly denies the activation of a link catch event in a different scope.\n\n**Result:**\nAfter applying the fix, the link catch event inside the (event) subprocess will no longer be activated when a link throw event occurs. The deployment of the process will be rejected if there is no link catch event in the same scope, ensuring compliance with the BPMN specification and the Camunda documentation."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13881",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nUpgrading a Zeebe cluster to a new version that includes fixes for https://github.com/camunda/zeebe/issues/12797 and https://github.com/camunda/zeebe/issues/13041, entries in the job deadline column family might be leaked, causing repeated error logs that a corresponding job cannot be found.\r\nThe cause is a change in behavior where we now expect no duplicate deadline entries and don't clean up duplicated or orphaned entries. \r\n\r\n**To Reproduce**\r\n\r\nUse 8.2.8, force the creation of duplicated deadline entries (probably happens with a long processing queue) and then upgrade to 8.2.9 or later.\r\n\r\n**Expected behavior**\r\n\r\nA migration cleans up orphaned entries so that they are removed from the state and do not cause error logs forever.\r\n\r\n**Environment:**\r\nZeebe <=8.2.8 upgrades to > 8.2.8\r\nZeebe <= 8.1.? upgrades to > 8.1.?\r\nZeebe <= 8.0.? upgrades to > 8.0.? \n",
    "title": "Upgrading leaves deadline entries without jobs",
    "releaseNoteText": "**Consequence:**\nUpgrading Zeebe from version 8.2.8 or earlier to a newer version may result in entries in the job deadline column family being left without corresponding jobs. This can lead to repeated error logs indicating that the job cannot be found.\n\n**Cause:**\nThe issue was caused by a change in behavior where the system expects no duplicate deadline entries and does not clean up duplicated or orphaned entries during the upgrade process.\n\n**Fix:**\nA fix has been implemented that includes a migration process to clean up orphaned entries in the job deadline column family. This ensures that these entries are removed from the system's state and no longer cause error logs.\n\n**Result:**\nAfter applying the fix, upgrading Zeebe to a version beyond 8.2.8 will no longer result in orphaned entries in the job deadline column family. The migration process will clean up any existing orphaned entries, preventing them from causing error logs indefinitely."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13867",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nA `ConcurrentModificationException` occurs when clearing obsolete job activation (long polling) requests. The exception is thrown when clearing the `activeRequests` LinkedList which isn't thread-safe ([code](https://github.com/camunda/zeebe/blob/8ef7c6ffd293ab11c9d2f2ee2f79f98b47941d1d/gateway/src/main/java/io/camunda/zeebe/gateway/impl/job/InFlightLongPollingActivateJobsRequestsState.java#L77)).\r\n\r\n**To Reproduce**\r\n\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\n**Expected behavior**\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nA `ConcurrentModificationException` doesn't occur. \r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.util.ConcurrentModificationException: null\r\n\r\nat java.util.LinkedList$ListItr.checkForComodification\r\nat java.util.LinkedList$ListItr.remove\r\nat java.util.Collection.removeIf\r\nat io.camunda.zeebe.gateway.impl.job.InFlightLongPollingActivateJobsRequestsState.removeObsoleteRequestsAndUpdateMetrics ( io/camunda.zeebe.gateway.impl.job/InFlightLongPollingActivateJobsRequestsState.java:77 )\r\nat io.camunda.zeebe.gateway.impl.job.InFlightLongPollingActivateJobsRequestsState.getPendingRequests ( io/camunda.zeebe.gateway.impl.job/InFlightLongPollingActivateJobsRequestsState.java:71 )\r\nat io.camunda.zeebe.gateway.impl.job.LongPollingActivateJobsHandler.resetFailedAttemptsAndHandlePendingRequests ( io/camunda.zeebe.gateway.impl.job/LongPollingActivateJobsHandler.java:245 )\r\nat io.camunda.zeebe.gateway.impl.job.LongPollingActivateJobsHandler.lambda$onNotification$6 ( io/camunda.zeebe.gateway.impl.job/LongPollingActivateJobsHandler.java:181 )\r\nat io.camunda.zeebe.scheduler.ActorJob.invoke ( io/camunda.zeebe.scheduler/ActorJob.java:92 )\r\nat io.camunda.zeebe.scheduler.ActorJob.execute ( io/camunda.zeebe.scheduler/ActorJob.java:45 )\r\nat io.camunda.zeebe.scheduler.ActorTask.execute ( io/camunda.zeebe.scheduler/ActorTask.java:119 )\r\nat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask ( io/camunda.zeebe.scheduler/ActorThread.java:109 )\r\nat io.camunda.zeebe.scheduler.ActorThread.doWork ( io/camunda.zeebe.scheduler/ActorThread.java:87 )\r\nat io.camunda.zeebe.scheduler.ActorThread.run ( io/camunda.zeebe.scheduler/ActorThread.java:204 )\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- OS: <!-- [e.g. Linux] --> Linux\r\n- Zeebe Version: <!-- [e.g. 0.20.0] --> 8.3.0-SNAPSHOT\r\n- Configuration: <!-- [e.g. exporters etc.] -->\r\n- Error occured in CW32 benchmark ([src](https://console.cloud.google.com/logs/query;query=error_group%2528%22CKD6icXrpN_oLA%22%2529%0AlogName:%22stdout%22%0Aresource.type%3D%22k8s_container%22%0Aresource.labels.pod_name%3D%22medic-y-2023-cw-32-1f55f22-benchmark-zeebe-gateway-5fbb459ctcwr%22%0Aresource.labels.container_name%3D%22zeebe-gateway%22%0Aresource.labels.location%3D%22europe-west1-b%22%0Aresource.labels.project_id%3D%22zeebe-io%22%0Aresource.labels.cluster_name%3D%22zeebe-cluster%22%0Aresource.labels.namespace_name%3D%22medic-y-2023-cw-32-1f55f22-benchmark%22;cursorTimestamp=2023-08-10T20:21:27.752805343Z;startTime=2023-08-10T19:51:57.752Z;endTime=2023-08-10T20:51:57.752Z?project=zeebe-io)).\r\n\n\n Zelldon: This seems to be something new introduced, maybe due to some changes to the job stuff ? @npepinpe \r\n\r\nI can see a trial cluster which has more than 1k occurrences of this. I think if this happens the client can't activate jobs.\r\n\r\n\r\n![occur2](https://github.com/camunda/zeebe/assets/2758593/ece83e92-2157-4bdc-b77f-90e0d3b5fdd6)\r\n![occur1](https://github.com/camunda/zeebe/assets/2758593/bb9d1679-aa39-4707-bb7c-4a435e5d5c98)\r\n![activate](https://github.com/camunda/zeebe/assets/2758593/a71587c3-4180-4512-92e2-eafbc59bac34)\r\n\r\n\r\n* https://console.cloud.google.com/errors/detail/CLu0t_7q98T1pQE;service=zeebe;time=P7D?project=camunda-cloud-240911\r\n* https://console.cloud.google.com/errors/detail/CM-6h67j1ejk0AE;service=zeebe;time=P7D?project=camunda-cloud-240911\r\n\r\n\r\nI would mark it as critical so we take a look asap (since I think this bug has newly introduced and blocks the client to make progress). If we find out and think it is less of a problem we can also decrease the severity again.\n npepinpe: Yes, my bad, we have a state modification in a callback that gets executed on the gRPC executor and not the actor itself =/\r\n\r\nThis was merged 11th of July though, and AFAIK not back ported, so I'm unsure how a trial cluster would have it. Can they use alpha versions?\n Zelldon: > This was merged 11th of July though, and AFAIK not back ported, so I'm unsure how a trial cluster would have it. Can they use alpha versions?\r\n\r\nYes I think there was a decision to get them used alpha versions.\n npepinpe: Fix: https://github.com/camunda/zeebe/pull/13875\r\n\r\nNot really sure how to write a good regression test for that =/\n Zelldon: Unfortunately, this is part of alpha4 https://console.cloud.google.com/errors/detail/CKaE2PeurPD7Ng;service=zeebe;time=P7D?project=camunda-cloud-240911. Luckily this is already fixed thanks to @npepinpe \r\n\n Zelldon: @npepinpe wondering whether bot are fixed not the ConcurrentModification and the index out of bounce, but I think so there were related right?\n npepinpe: Most likely, since we were modifying non-thread-safe structures concurrently :+1: ",
    "title": "ConcurrentModificationException when clearing obsolete job activation requests",
    "releaseNoteText": "**Consequence:** Users experienced a `ConcurrentModificationException` when attempting to clear obsolete job activation requests. This exception occurred due to the non-thread-safe nature of the `activeRequests` LinkedList.\n\n**Cause:** The issue originated from the attempt to clear the `activeRequests` LinkedList, which was not designed to be accessed concurrently.\n\n**Fix:** The non-thread-safe behavior of the `activeRequests` LinkedList was addressed by implementing thread-safe modifications.\n\n**Result:** With the fix applied, users will no longer encounter the `ConcurrentModificationException` when clearing obsolete job activation requests. The system now properly handles concurrent modifications to the `activeRequests` LinkedList, ensuring smooth execution without any exceptions."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13814",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nIt seems right now job streaming may unnecessary trigger job polling when a job is handled/completed. This is because we call `JobWorkerImpl#handleJobFinished`, which then decrements `remainingJob` (wrong) and then might trigger polling.\r\n\r\n**To Reproduce**\r\n\r\nJust use job streaming :upside_down_face: \r\n\r\n**Expected behavior**\r\n\r\nWe do not trigger unnecessary polling and do not touch the `remainingJobs` counter.\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.3.0-alpha4\r\n\n",
    "title": "Job streaming may trigger unnecessary polling",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13796",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nAn exception is thrown whenever the `JobStreamRemover` tries to remove a stream in the gateway. This is due to the future completing without an executor, thus completing within the actor context, and then calling `Actor.call`. As we already wanted to ensure that an executor was used, we should do that as well.\r\n\r\n**To Reproduce**\r\n\r\nRegister a stream via the command. Cancel it. An exception is thrown and the stream is not removed from the gateway nor the broker (even if the client has gone away).\r\n\r\n**Expected behavior**\r\n\r\nThe stream is removed and no error is thrown.\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.3.0-alpha4\r\n\n",
    "title": "IllegalStateArgument when removing job stream",
    "releaseNoteText": "**Consequence:** When attempting to remove a job stream in the gateway, an exception was thrown resulting in the stream not being removed from the gateway or the broker.\n\n**Cause:** The issue was caused by the future completing without an executor, causing it to complete within the actor context and then mistakenly calling `Actor.call` instead.\n\n**Fix:** To ensure that an executor is used, the fix updated the code to correctly utilize an executor when completing the future.\n\n**Result:** After applying this fix, the job stream is successfully removed from the gateway without any errors being thrown."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13787",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Description**\r\n\r\nI noticed this during the release, on 8.0/8.1 building starter & worker fails as e.g. the maven wrapper is not present on 8.1\r\n```\r\n/home/runner/work/_temp/79d164a7-e6b4-4e02-9e4e-89a3d8a459e8.sh: line 1: ./mvnw: No such file or directory\r\nError: Process completed with exit code 127.\r\n```\r\nhttps://github.com/camunda/zeebe/actions/runs/5738456998/job/15552373143\r\n\r\nOn 8.0 also build zeebe fails as there is no DIST=`build` setup in the Dockerfile https://github.com/camunda/zeebe/actions/runs/5202088373/job/14079846461\r\n\r\n\r\n```[tasklist]\n### Tasks\n- [x] backport maven wrapper to 8.0/1 - to avoid workflow merge conflicts from main to stable\n- [x] Add a `benchmark.yaml` workflow to each stable branch - to maintain a stable setup\n- [x] trigger the workflows via [workflow dispatch](https://docs.github.com/en/free-pro-team@latest/rest/actions/workflows?apiVersion=2022-11-28#create-a-workflow-dispatch-event) from the https://github.com/zeebe-io/zeebe-engineering-processes referencing the release_branch\n- [x] delete the `dispatch-benchmark.yaml` workflow from main\n```\r\n\r\n\r\n\n\n megglos: ZDP-Triage:\n- will look into it asap to resolve\n remcowesterhoud: Individual triage:\r\n- Seems to be actively worked on by ZDP at this time. It's not sensible to work on this simultaneously so I'll move it to the ZPA backlog. This also allows us to focus multi-tenancy and migration.",
    "title": "Release: `Repo dispatch Benchmark` fails on 8.0/8.1",
    "releaseNoteText": "**Consequence:** The `Repo dispatch Benchmark` fails on versions 8.0/8.1. When building the starter and worker, the Maven wrapper is not found on version 8.1, resulting in an error indicating that the file or directory does not exist. Additionally, on version 8.0, the build of Zeebe fails due to the absence of the `DIST=build` setup in the Dockerfile.\n\n**Cause:** The issue was caused by the absence of the Maven wrapper on version 8.1 and the missing `DIST=build` setup in the Dockerfile on version 8.0.\n\n**Fix:** The fix for this issue involved backporting the Maven wrapper to versions 8.0 and 8.1 to prevent workflow merge conflicts when moving from the main branch to stable. Additionally, a `benchmark.yaml` workflow was added to each stable branch to maintain a stable setup. The trigger for these workflows was modified to use workflow dispatch, referencing the release branch. Finally, the `dispatch-benchmark.yaml` workflow was removed from the main branch.\n\n**Result:** After applying the fix, the `Repo dispatch Benchmark` issue no longer occurs on versions 8.0/8.1. The starter and worker can be built successfully without encountering errors related to the missing Maven wrapper. Additionally, the build of Zeebe on version 8.0 now includes the necessary `DIST=build` setup in the Dockerfile."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13521",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nIf activating a call activity with large variables causes processing to exceed the batch size, the complete process instance is banned and left in limbo.\r\n\r\nThe current workaround is to ensure that we don't run into this case; however, once you hit it, there's nothing you can do but recreate the instance with smaller payloads.\r\n\r\nSupport:\r\n-  https://jira.camunda.com/browse/SUPPORT-17661\r\n- https://jira.camunda.com/browse/SUPPORT-17882\r\n\r\n**To Reproduce**\r\n\r\nCreate a process with a dummy service task which leads to a call activity. Once the task is activated, separately as to not exceed the batch size, create 4 large variables, e.g. of 1MB each (using the set variable command, one at a time). Then complete the task. Activating the call activity will fail and result in a banned instance.\r\n\r\n**Expected behavior**\r\n\r\nAn incident is raised, such that I can modify my variables to save this instance.\r\n\r\n**Log/Stacktrace**\r\n\r\nI've anonymized the stacktrace below. You can find more logs here: https://cloudlogging.app.goo.gl/aarrs1J5TY22X5gXA\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\nExpected to process record 'TypedRecordImpl{metadata=RecordMetadata{recordType=COMMAND, valueType=PROCESS_INSTANCE, intent=ACTIVATE_ELEMENT}, value={\"bpmnElementType\":\"CALL_ACTIVITY\",\"elementId\":\"Activity_1aba60i\",\"bpmnProcessId\":\"process\",\"version\":8,\"processDefinitionKey\":2251800414371946,\"processInstanceKey\":4503599979491273,\"flowScopeKey\":4503599979491273,\"bpmnEventType\":\"UNSPECIFIED\",\"parentProcessInstanceKey\":-1,\"parentElementInstanceKey\":-1}}' without errors, but exception occurred with message 'Can't append entry: 'RecordBatchEntry[recordMetadata=RecordMetadata{recordType=EVENT, valueType=VARIABLE, intent=CREATED}, key=4503599980117234, sourceIndex=-1, unifiedRecordValue={\"name\":\"foo\",\"value\":\"U2VkIHV0IHBlcnNwaWNpYXRpcyB1bmRlIG9tbmlzIGlzdGUgbmF0dXMgZXJyb3Igc2l0IHZvbHVwdGF0ZW0gYWNjdXNhbnRpdW0gZG9sb3JlbXF1ZSBsYXVkYW50aXVtLCB0b3RhbSByZW0gYXBlcmlhbSwgZWFxdWUgaXBzYSBxdWFlIGFiIGlsbG8gaW52ZW50b3JlIHZlcml0YXRpcyBldCBxdWFzaSBhcmNoaXRlY3RvIGJlYXRhZSB2aXRhZSBkaWN0YSBzdW50IGV4cGxpY2Fiby4gTmVtbyBlbmltIGlwc2FtIHZvbHVwdGF0ZW0gcXVpYSB2b2x1cHRhcyBzaXQgYXNwZXJuYXR1ciBhdXQgb2RpdCBhdXQgZnVnaXQsIHNlZCBxdWlhIGNvbnNlcXV1bnR1ciBtYWduaSBkb2xvcmVzIGVvcyBxdWkgcmF0aW9uZSB2b2x1cHRhdGVtIHNlcXVpIG5lc2NpdW50LiBOZXF1ZSBwb3JybyBxdWlzcXVhbSBlc3QsIHF1aSBkb2xvcmVtIGlwc3VtIHF1aWEgZG9sb3Igc2l0IGFtZXQsIGNvbnNlY3RldHVyLCBhZGlwaXNjaSB2ZWxpdCwgc2VkIHF1aWEgbm9uIG51bXF1YW0gZWl1cyBtb2RpIHRlbXBvcmEgaW5jaWR1bnQgdXQgbGFib3JlIGV0IGRvbG9yZSBtYWduYW0gYWxpcXVhbSBxdWFlcmF0IHZvbHVwdGF0ZW0uIFV0IGVuaW0gYWQgbWluaW1hIHZlbmlhbSwgcXVpcyBub3N0cnVtIGV4ZXJjaXRhdGlvbmVtIHVsbGFtIGNvcnBvcmlzIHN1c2NpcGl0IGxhYm9yaW9zYW0sIG5...' with size: 1725121 this would exceed the maximum batch size. [ currentBatchEntryCount: 18, currentBatchSize: 2874115]'.\r\n\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- OS: Ubuntu Focal\r\n- Zeebe Version: 8.2.8\r\n- Configuration: SaaS\r\n\n\n Zelldon: Related https://github.com/camunda/zeebe/issues/13016\n korthout: Thanks for raising this @npepinpe \n\nAs there is no good workaround available and the chance to run into this is reasonable, I'm prioritizing this as `upcoming`.\n\n>**Expected behavior**\n>An incident is raised, such that I can modify my variables to save this instance.\n\nTo resolve this, we'll need to handle errors in the `BpmnStreamProcessor`. However, we're unable to raise an incident for every process instance element type yet. So, we either need to pass the error handling along to the respective `BpmnElementProcessor` (i.e. `CallActivityProcessor`) or have element type-specific code in the BpmnStreamProcessor.\n\nSince this is not entirely trivial, I'm sizing this as `Medium`.\n oleschoenburg: This has come up in another support case: https://jira.camunda.com/browse/SUPPORT-17882\r\nIn this case, the variables weren't even that large - just around 300KiB. IMO this bug is pretty bad, effectively causing data loss. When we ban an instance, we can't recover it. Depending on the use case, this can be really severe. It's also hard to predict, I wouldn't have expected 300KiB variables to cause this.\r\n\r\nI've raised the priority to critical. IMO all cases where we ban instances have to be considered critical.",
    "title": "Process instances are banned when trying to activate a call activity with large variables",
    "releaseNoteText": "**Consequence:**\nWhen trying to activate a call activity with large variables, the complete process instance is banned and left in limbo. This can lead to data loss and severe consequences for the user. \n\n**Cause:**\nThe issue is caused by the processing exceeding the batch size, which results in the ban of the process instance. The current error handling in the BpmnStreamProcessor does not raise an incident for every process instance element type, which prevents the proper handling of errors.\n\n**Fix:**\nTo resolve this issue, the error handling in the BpmnStreamProcessor has been modified. Errors are now properly handled in the respective BpmnElementProcessor, such as the CallActivityProcessor. This allows for the raising of incidents and better handling of process instances.\n\n**Result:**\nWith this fix, incidents will be raised when activating a call activity with large variables exceeds the batch size. This allows users to modify their variables and save the process instance, thus preventing data loss and ensuring the proper execution of the workflow."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12699",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "In https://github.com/camunda/zeebe/issues/12591, we found that when engine rejects the deployment request it cannot write the rejection record because it was attempting to write the whole deployment which is greater than maxMessageSize. This resulted in a loop in StreamProcessor, where it repeatedly fails to write the rejection record leading to the partition being fully blocked and not making any progress. https://github.com/camunda/zeebe/issues/12591#issuecomment-1535879364 \r\n\r\nTo fix this, now we reject the request in CommandAPI before writing to the logstream https://github.com/camunda/zeebe/pull/12676. But, to be safer we should also ensure that we are not writing rejection records that are too large, to prevent such accidental cases.\r\n\r\nEnsure that rejections records can be written reliably, even if the original command is rejected due to ExceededBatchSize. Ideally, rejection record should only have a reference to the original command, but not the full command. If possible, do not write the entire command for any type of rejections. Or atleast, trim rejection record in case of ExceededBatchSize exception.\r\n\r\n\r\n\n\n berkaycanbc: ZPA Triage:\n\nWe think this is a bug. We are planning to process this in the next mob-programming session.\n korthout: ~~Mob~~Pair programming with @koevskinikola results:\r\n- Run `io.camunda.zeebe.it.client.command.CreateDeploymentTest::shouldRejectDeployIfResourceIsTooLarge`\r\n- Notice that an error is logged, this should not be the case\r\n- Make sure that [rejection is written safely](https://github.com/camunda/zeebe/blob/main/engine/src/main/java/io/camunda/zeebe/engine/Engine.java#L186-L194)\n abbasadel: ZPA planning: \n- Moving back to the backlog since we don't have time for it in the next iteration.",
    "title": "Do not write the entire command for rejection",
    "releaseNoteText": "**Consequence**: When the engine rejected a deployment request because it exceeded the maxMessageSize, it was unable to write the rejection record due to attempting to write the entire deployment. This caused a loop in the StreamProcessor, resulting in the partition being fully blocked and making no progress.\n\n**Cause**: The issue originated from the engine's attempt to write the entire deployment as the rejection record, which was larger than the maxMessageSize. This led to the failure to write the rejection record and the subsequent loop in the StreamProcessor.\n\n**Fix**: The fix for this issue involved rejecting the request in the CommandAPI before writing to the logstream. Additionally, measures were taken to ensure that rejection records are not too large to prevent similar accidental cases.\n\n**Result**: Rejection records can now be written reliably, even if the original command is rejected due to ExceededBatchSize. The fix ensures that rejection records only contain a reference to the original command, instead of the entire command. As a result, the system can accurately handle rejection cases and prevent the blocking of partitions."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13516",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\nIT tests for the Job Push is missing. To cover the happy path scenario end to end, we need to implement IT tests. ~~These cases should also cover closing a stream which will actually test the `onClose` hook.~~ Please refer to these two discussions for more details: \r\nhttps://github.com/camunda/zeebe/pull/13351#discussion_r1263414875\r\nhttps://github.com/camunda/zeebe/pull/13351#discussion_r1263834385\r\n\n\n berkaycanbc: @koevskinikola Based on the investigation we did, the only way to trigger `onClose` handler is to call `onComplete` or `onError` from the server side which is not the case for our implementation. Therefore, we will opt-out tests for it. At the same time, we want to keep it in case we decide to call `onComplete` later on. (e.g. during server shutdown)\r\n\r\nCc: @npepinpe ",
    "title": "Add integration tests for Job Push",
    "releaseNoteText": "**Consequence:**\nIntegration tests for Job Push were missing, leaving a gap in the coverage of the happy path scenario. This made it difficult to ensure that the system was working correctly end-to-end.\n\n**Cause:**\nThe absence of integration tests for Job Push was due to an oversight during the development process. These tests were not implemented, resulting in a gap in the test coverage.\n\n**Fix:**\nIntegration tests were added for Job Push to cover the happy path scenario. This involved writing test cases that simulated the entire process, ensuring that all steps were functioning correctly.\n\n**Result:**\nWith the addition of integration tests for Job Push, the system now has improved test coverage for the happy path scenario. This helps to ensure that the functionality is working correctly and allows for easier detection and resolution of any issues that may arise."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13354",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "# Description\r\nIf a process contains a signal start event we must make sure to unsubscribe this signal.\r\n\r\nIf the deleted process was the latest, the previous version will become the new latest version. If this previous version contains a signal start event we should make sure that the signal subscription is created and process instances are started as expected.\r\n\r\nBlocked by: #9769 \n",
    "title": "Delete and recreate signal subscription of previous version",
    "releaseNoteText": "**Consequence**: The previous version of a process was not being properly unsubscribed, resulting in unexpected behavior with signal subscriptions. This could lead to incorrect process instance creation and signals not being received in certain scenarios. \n\n**Cause**: The issue was caused by not properly handling the deletion and recreation of signal subscriptions when a previous version of a process becomes the new latest version. This led to the signal subscription not being properly created and linked to the process instances.\n\n**Fix**: The code was updated to properly handle the deletion and recreation of signal subscriptions when a previous version becomes the new latest version. The process now correctly unsubscribes the signal from the previous version and creates the signal subscription for the new latest version.\n\n**Result**: With this fix, when a previous version of a process becomes the new latest version, the signal subscription is correctly created, ensuring that the process instances are started and that signals are received as expected."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13343",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\nThis ColumnFamily must contains **all** running process instances. We must create a migration script that initially fills this ColumnFamily.\r\n\r\n- Create the migration script\r\n    - Include tests!\r\n\r\n**Blocked by**\r\n#13340 \n",
    "title": "Migrate running process instance into `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily",
    "releaseNoteText": "**Consequence:** Running process instances were not being migrated into the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily.\n\n**Cause:** The ColumnFamily did not contain all running process instances, leading to incomplete migration.\n\n**Fix:** A migration script was created to populate the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily with all running process instances. Tests were included to ensure the integrity of the migration process.\n\n**Result:** After applying this fix, the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily now contains all running process instances, ensuring complete migration and accurate representation of process data."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13253",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nCritical issues are defined as a \"stop the world\" issue. This means they require immediate attention. Currently, when an issue is labeled as critical there's no way of knowing, unless the person who added the label notifies the team. If this doesn't happen, there's a good chance we won't notice until the next triage.\r\n\r\n**Describe the solution you'd like**\r\nWe can easily add a workflow that checks if an issue gets labelled as critical. If this is the case we could send a notification to the Zeebe slack channel to notify the engineers that attention is required.\r\n\r\n**Describe alternatives you've considered**\r\nPeople could ping us manually when they do this. But that's a manual step and could be forgotten.\r\n\r\n**Additional context**\r\nN/A\r\n\n\n remcowesterhoud: ZPA Triage:\n- Helps us identify critical bugs more quickly\n- Removes a manual step that could be forgotten otherwise\n- Marking it as `upcoming` as we want to be aware of critical bugs asap\n- We already send slack message from our CI. Should be easy to take inspiration from these other places.\n- Low effort, potentially high impact.\n megglos: ZDP-Triage:\n- could help us being faster to react\n- need to clarify who reacts on it\n- right now we would assume this is done by zeebe engineers who can also escalate directly when identifying a critical issue\n- marking as later for now",
    "title": "Notify Zeebe Team in Slack when an issue is labelled as critical",
    "releaseNoteText": "**Consequence:** Previously, critical issues in the Zeebe system would go unnoticed unless someone manually notified the team. This could lead to delays in addressing these urgent problems.\n\n**Cause:** The lack of an automatic notification system for critical issues meant that the team relied on manual reminders to be aware of these urgent problems.\n\n**Fix:** A workflow has been implemented that checks if an issue is labelled as critical. If a critical label is added, a notification is automatically sent to the Zeebe Slack channel, alerting the team to the need for immediate attention.\n\n**Result:** With this fix in place, the Zeebe team will now receive immediate notifications in Slack when an issue is labelled as critical. This will ensure that urgent problems are promptly addressed and prevent any delays in resolving critical issues."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12942",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "In our case，the directory of `/usr/local/zeebe/data/raft-partition/partitions/1` has about 60 .log files with each one has 128MB size data,and we have 3 patitions,so all the files‘s size almost reach 66GB.When we do backup api,it always failed by throwing exception like below:\r\n![image](https://github.com/camunda/zeebe/assets/12196018/ea47d5f7-9fe6-40dc-9d62-92ece0445266)\r\n\r\n`\r\n{'backupId': 1685672012, 'status': 'FAILED', 'failureReason': \"Backup on partition 3 failed due to java.util.concurrent.CompletionException: software.amazon.awssdk.core.exception.SdkClientException: Unable to execute HTTP request: Acquire operation took longer than the configured maximum time. This indicates that a request cannot get a connection from the pool within the specified maximum time. This can be due to high request rate.\\nConsider taking any of the following actions to mitigate the issue: increase max connections, increase acquire timeout, or slowing the request rate.\\nIncreasing the max connections can increase client throughput (unless the network interface is already fully utilized), but can eventually start to hit operation system limitations on the number of file descriptors used by the process. If you already are fully utilizing your network interface or cannot further increase your connection count, increasing the acquire timeout gives extra time for requests to acquire a connection before timing out. If the connections doesn't free up, the subsequent requests will still timeout.\\nIf the above mechanisms are not able to fix the issue, try smoothing out your requests so that large traffic bursts cannot overload the client, being more efficient with the number of times you need to call AWS, or by increasing the number of hosts sending requests.}\r\n`\r\nI also see the code source,S3BackupStore.java create NettyNioAsyncHttpClient with static number 45s of ConnectionAcquireTimeout,and other configuration used the default value.There is no any chance to customizing httpClient configurtation data .\r\nI'm not sure if i have missing somthing important document.Could you help me with this problem?  Thanks.\r\n@oleschoenburg \r\n![image](https://github.com/camunda/zeebe/assets/12196018/fd472cf0-be41-4d2f-ae4e-222a4b1725c9)\r\n\r\n\n\n oleschoenburg: Thanks for reporting @codingman1990 :+1: \r\n\r\nPlease correct me if I'm wrong but the situation is basically this:\r\nWhen a partition has a lot of log segments, uploading them in parallel will cause some uploads to wait for longer than 45 seconds on an available connection. This results in an exception and failure to complete the backup.\r\n\r\nWe should definitely improve this, although I'm not sure if just configuring the connection acquisition timeout is the right way to go about it.\r\n\r\nIn my opinion, the issue is rather that there is no limiting of parallel uploads. This is an issue here, with 66 log segments. It will also become an issue for large states, especially if #12483 is merged. So I would suggest to rather limit the number of concurrent uploads. The limit can be configurable of course.\r\n\r\n\r\n@codingman1990 Slightly unrelated but I was a bit surprised when I read that you have >60 log segments. I'd have expected much lower numbers. Did you adjust the snapshot interval or know any other reasons why your brokers keep more segments than usual?\r\n\r\n\n codingman1990: > Thanks for reporting @codingman1990 👍\r\n> \r\n> Please correct me if I'm wrong but the situation is basically this: When a partition has a lot of log segments, uploading them in parallel will cause some uploads to wait for longer than 45 seconds on an available connection. This results in an exception and failure to complete the backup.\r\n> \r\n> We should definitely improve this, although I'm not sure if just configuring the connection acquisition timeout is the right way to go about it.\r\n> \r\n> In my opinion, the issue is rather that there is no limiting of parallel uploads. This is an issue here, with 66 log segments. It will also become an issue for large states, especially if #12483 is merged. So I would suggest to rather limit the number of concurrent uploads. The limit can be configurable of course.\r\n> \r\n> @codingman1990 Slightly unrelated but I was a bit surprised when I read that you have >60 log segments. I'd have expected much lower numbers. Did you adjust the snapshot interval or know any other reasons why your brokers keep more segments than usual?\r\n\r\nIn my situation,there are abount 200 running instances at the same time.I almost had not edit any configuration except something like ES,BackupStore.The full configuration data is like below(Some sensitive data are marsked):\r\n\r\n`\r\n{\r\n    \"network\": {\r\n        \"host\": \"0.0.0.0\",\r\n        \"portOffset\": 0,\r\n        \"maxMessageSize\": \"4MB\",\r\n        \"advertisedHost\": \"***\",\r\n        \"commandApi\": {\r\n            \"host\": \"0.0.0.0\",\r\n            \"port\": 26501,\r\n            \"advertisedHost\": \"***\",\r\n            \"advertisedPort\": 26501,\r\n            \"address\": \"0.0.0.0:26501\",\r\n            \"advertisedAddress\": \"***\"\r\n        },\r\n        \"internalApi\": {\r\n            \"host\": \"0.0.0.0\",\r\n            \"port\": 26502,\r\n            \"advertisedHost\": \"***\",\r\n            \"advertisedPort\": 26502,\r\n            \"address\": \"0.0.0.0:26502\",\r\n            \"advertisedAddress\": \"***\"\r\n        },\r\n        \"security\": {\r\n            \"enabled\": false,\r\n            \"certificateChainPath\": null,\r\n            \"privateKeyPath\": null\r\n        },\r\n        \"maxMessageSizeInBytes\": 4194304\r\n    },\r\n    \"cluster\": {\r\n        \"initialContactPoints\": [\r\n            \"***\",\r\n            \"***\",\r\n            \"***\"\r\n        ],\r\n        \"partitionIds\": [\r\n            1,\r\n            2,\r\n            3\r\n        ],\r\n        \"nodeId\": 1,\r\n        \"partitionsCount\": 3,\r\n        \"replicationFactor\": 3,\r\n        \"clusterSize\": 3,\r\n        \"clusterName\": \"camunda-zeebe\",\r\n        \"heartbeatInterval\": \"PT0.25S\",\r\n        \"electionTimeout\": \"PT2.5S\",\r\n        \"membership\": {\r\n            \"broadcastUpdates\": false,\r\n            \"broadcastDisputes\": true,\r\n            \"notifySuspect\": false,\r\n            \"gossipInterval\": \"PT0.25S\",\r\n            \"gossipFanout\": 2,\r\n            \"probeInterval\": \"PT1S\",\r\n            \"probeTimeout\": \"PT0.1S\",\r\n            \"suspectProbes\": 3,\r\n            \"failureTimeout\": \"PT10S\",\r\n            \"syncInterval\": \"PT10S\"\r\n        },\r\n        \"raft\": {\r\n            \"enablePriorityElection\": true\r\n        },\r\n        \"messageCompression\": \"NONE\"\r\n    },\r\n    \"threads\": {\r\n        \"cpuThreadCount\": 3,\r\n        \"ioThreadCount\": 3\r\n    },\r\n    \"data\": {\r\n        \"directory\": \"/usr/local/zeebe/data\",\r\n        \"logSegmentSize\": \"128MB\",\r\n        \"snapshotPeriod\": \"PT5M\",\r\n        \"logIndexDensity\": 100,\r\n        \"diskUsageMonitoringEnabled\": true,\r\n        \"diskUsageReplicationWatermark\": 0.87,\r\n        \"diskUsageCommandWatermark\": 0.85,\r\n        \"diskUsageMonitoringInterval\": \"PT1S\",\r\n        \"backup\": {\r\n            \"store\": \"S3\",\r\n            \"s3\": {\r\n                \"bucketName\": \"zeebe-proc\",\r\n                \"endpoint\": \"***\",\r\n                \"region\": \"cn-hangzhou\",\r\n                \"accessKey\": \"***\",\r\n                \"secretKey\": \"***\",\r\n                \"apiCallTimeout\": \"PT3M\"\r\n            }\r\n        },\r\n        \"logSegmentSizeInBytes\": 134217728,\r\n        \"freeDiskSpaceCommandWatermark\": 5052946022,\r\n        \"freeDiskSpaceReplicationWatermark\": 4379219886\r\n    },\r\n    \"exporters\": {\r\n        \"elasticsearch\": {\r\n            \"jarPath\": null,\r\n            \"className\": \"io.camunda.zeebe.exporter.ElasticsearchExporter\",\r\n            \"args\": {\r\n                \"index\": {\r\n                    \"prefix\": \"zeebe-record\"\r\n                },\r\n                \"authentication\": {\r\n                    \"password\": \"***\",\r\n                    \"username\": \"***\"\r\n                },\r\n                \"url\": \"***\"\r\n            },\r\n            \"external\": false\r\n        }\r\n    },\r\n    \"gateway\": {\r\n        \"network\": {\r\n            \"host\": \"0.0.0.0\",\r\n            \"port\": 26500,\r\n            \"minKeepAliveInterval\": \"PT30S\"\r\n        },\r\n        \"cluster\": {\r\n            \"initialContactPoints\": [\r\n                \"0.0.0.0:26502\"\r\n            ],\r\n            \"requestTimeout\": \"PT15S\",\r\n            \"clusterName\": \"zeebe-cluster\",\r\n            \"memberId\": \"gateway\",\r\n            \"host\": \"0.0.0.0\",\r\n            \"advertisedHost\": \"0.0.0.0\",\r\n            \"port\": 26502,\r\n            \"advertisedPort\": 26502,\r\n            \"membership\": {\r\n                \"broadcastUpdates\": false,\r\n                \"broadcastDisputes\": true,\r\n                \"notifySuspect\": false,\r\n                \"gossipInterval\": \"PT0.25S\",\r\n                \"gossipFanout\": 2,\r\n                \"probeInterval\": \"PT1S\",\r\n                \"probeTimeout\": \"PT0.1S\",\r\n                \"suspectProbes\": 3,\r\n                \"failureTimeout\": \"PT10S\",\r\n                \"syncInterval\": \"PT10S\"\r\n            },\r\n            \"security\": {\r\n                \"enabled\": false,\r\n                \"certificateChainPath\": null,\r\n                \"privateKeyPath\": null\r\n            },\r\n            \"messageCompression\": \"NONE\"\r\n        },\r\n        \"threads\": {\r\n            \"managementThreads\": 1\r\n        },\r\n        \"security\": {\r\n            \"enabled\": false,\r\n            \"certificateChainPath\": null,\r\n            \"privateKeyPath\": null\r\n        },\r\n        \"longPolling\": {\r\n            \"enabled\": true\r\n        },\r\n        \"interceptors\": [],\r\n        \"initialized\": true,\r\n        \"enable\": false\r\n    },\r\n    \"backpressure\": {\r\n        \"enabled\": true,\r\n        \"algorithm\": \"VEGAS\",\r\n        \"aimd\": {\r\n            \"requestTimeout\": \"PT1S\",\r\n            \"initialLimit\": 100,\r\n            \"minLimit\": 1,\r\n            \"maxLimit\": 1000,\r\n            \"backoffRatio\": 0.9\r\n        },\r\n        \"fixed\": {\r\n            \"limit\": 20\r\n        },\r\n        \"vegas\": {\r\n            \"alpha\": 3,\r\n            \"beta\": 6,\r\n            \"initialLimit\": 20\r\n        },\r\n        \"gradient\": {\r\n            \"minLimit\": 10,\r\n            \"initialLimit\": 20,\r\n            \"rttTolerance\": 2.0\r\n        },\r\n        \"gradient2\": {\r\n            \"minLimit\": 10,\r\n            \"initialLimit\": 20,\r\n            \"rttTolerance\": 2.0,\r\n            \"longWindow\": 600\r\n        }\r\n    },\r\n    \"experimental\": {\r\n        \"maxAppendsPerFollower\": 2,\r\n        \"maxAppendBatchSize\": \"32KB\",\r\n        \"disableExplicitRaftFlush\": false,\r\n        \"rocksdb\": {\r\n            \"columnFamilyOptions\": {},\r\n            \"enableStatistics\": false,\r\n            \"memoryLimit\": \"512MB\",\r\n            \"maxOpenFiles\": -1,\r\n            \"maxWriteBufferNumber\": 6,\r\n            \"minWriteBufferNumberToMerge\": 3,\r\n            \"ioRateBytesPerSecond\": 0,\r\n            \"disableWal\": false\r\n        },\r\n        \"raft\": {\r\n            \"requestTimeout\": \"PT5S\",\r\n            \"maxQuorumResponseTimeout\": \"PT0S\",\r\n            \"minStepDownFailureCount\": 3,\r\n            \"preferSnapshotReplicationThreshold\": 100,\r\n            \"preallocateSegmentFiles\": true\r\n        },\r\n        \"partitioning\": {\r\n            \"scheme\": \"ROUND_ROBIN\",\r\n            \"fixed\": []\r\n        },\r\n        \"queryApi\": {\r\n            \"enabled\": false\r\n        },\r\n        \"consistencyChecks\": {\r\n            \"enablePreconditions\": false,\r\n            \"enableForeignKeyChecks\": false,\r\n            \"settings\": {\r\n                \"enablePreconditions\": false,\r\n                \"enableForeignKeyChecks\": false\r\n            }\r\n        },\r\n        \"engine\": {\r\n            \"messages\": {\r\n                \"ttlCheckerBatchLimit\": 2147483647,\r\n                \"ttlCheckerInterval\": \"PT1M\"\r\n            }\r\n        },\r\n        \"features\": {\r\n            \"enableYieldingDueDateChecker\": false,\r\n            \"enableActorMetrics\": false,\r\n            \"enableBackup\": true,\r\n            \"enableMessageTtlCheckerAsync\": false\r\n        },\r\n        \"maxAppendBatchSizeInBytes\": 32768\r\n    },\r\n    \"executionMetricsExporterEnabled\": false,\r\n    \"processing\": {\r\n        \"maxCommandsInBatch\": 1\r\n    }\r\n}\r\n`\r\n@oleschoenburg Could you please point out the problem.\n megglos: ZDP-Triage:\n- to be discussed at planning as flagged by Ole\n megglos: ZDP-Planning:\r\n- will become more likely with sst partitioning enabled",
    "title": "Support S3 backup httpclient custom configuration.",
    "releaseNoteText": "**Consequence:** The backup API fails when attempting to upload multiple log segments in parallel. This results in a `java.util.concurrent.CompletionException` and the backup process is marked as \"FAILED\". \n\n**Cause:** The issue arises due to the lack of limiting parallel uploads when there are a large number of log segments. In this specific case, with 66 log segments, some uploads wait for longer than the default connection acquisition timeout of 45 seconds, leading to backup failure.\n\n**Fix:** The fix involves introducing a limit on the number of concurrent uploads to prevent the issue of long wait times for available connections. The limit will be configurable, allowing users to adjust according to their specific requirements.\n\n**Result:** After applying the fix, the backup API will successfully upload log segments without exceeding the connection acquisition timeout. The number of concurrent uploads can be controlled by adjusting the configuration, ensuring smooth and efficient backup operations."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12283",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nI want to be able to open processes modeled with `zeebe-bpmn-model` in the Desktop Modeler as a C8 process by default. Currently, I need to set a lot of attributes by hand.\r\n\r\n```java\r\nDefinitions definitions = modelInstance.newInstance(Definitions.class);\r\n            definitions.setTargetNamespace(BPMN20_NS);\r\n            //definitions.setExporter(\"Camunda Modeler\");\r\n            //definitions.setExporterVersion(\"5.8.0\");\r\n            definitions.setAttributeValueNs(\"http://camunda.org/schema/modeler/1.0\",\"modeler:executionPlatform\",\"Camunda Cloud\");\r\n            modelInstance.setDefinitions(definitions);\r\n```\r\n\r\n**Describe the solution you'd like**\r\nProvide reasonable defaults for these attributes.\r\n\r\n**Describe alternatives you've considered**\r\nNone, the workaround above works as well.\r\n\r\n**Additional context**\r\nRequested by @superbeagle \n\n korthout: ZPA triage:\n\n- @remcowesterhoud mentioned that we should indicate somewhere in these attributes that this was generated with the zeebe-bpmn-model\n- @koevskinikola reproduced the issue already and it wasn't very pleasant\n- @koevskinikola the values should be defaults and can be changed by the user via the API (at least like above, or perhaps with an improved API)\n- @remcowesterhoud we should also add the other missing attributes\n- our own priority for this is low, but we think it's a good first issue for new contributors\n\n",
    "title": "Zeebe BPMN Model should provide reasonable defaults for definition attributes",
    "releaseNoteText": "**Consequence:**\nPreviously, when using the `zeebe-bpmn-model` to model processes, users had to manually set multiple attributes by hand in order to open the processes in the Desktop Modeler as a C8 process. This process was cumbersome and time-consuming.\n\n**Cause:**\nThe lack of reasonable defaults for the definition attributes in the `zeebe-bpmn-model` caused users to manually set the necessary attributes. This was a technical limitation of the product.\n\n**Fix:**\nIn this release, we have added reasonable defaults for the definition attributes in the `zeebe-bpmn-model`. This allows users to avoid manually setting these attributes, saving time and effort.\n\n**Result:**\nWith this fix applied, users will no longer have to manually set the attributes when modeling processes with the `zeebe-bpmn-model`. The default values will be automatically applied, making the process more efficient and user-friendly."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14137",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\n\r\nA method should be added to retrieve the latest form by form id to `DbFormState` class.\n",
    "title": "Implement query to retrieve the latest form by formId from the state",
    "releaseNoteText": "**Consequence:** Users were unable to retrieve the latest form by formId from the state.\n\n**Cause:** The `DbFormState` class did not have a method to retrieve the latest form by form id.\n\n**Fix:** Added a new method to the `DbFormState` class that allows retrieval of the latest form by form id.\n\n**Result:** Users can now retrieve the latest form by formId from the state using the new method in the `DbFormState` class."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14135",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\n\r\nA `Start Event` can be linked to a deployed form by the new `formId` field. The implementation is expected to include following:\r\n\r\n* Add the new `formId` attribute into the `ZeebeFormDefinition`\r\n* Update `ZeebeElementValidator.validate()` method in order to enable `zeebe` to validate also for group of fields. In the case, a start event with linked form is valid if only one field between `formKey` and `formId` is present\n",
    "title": "Allow binding forms to start events by formId",
    "releaseNoteText": "**Consequence:** Previously, it was not possible to link a `Start Event` to a deployed form using the `formId` field. This caused limitations in the configuration of forms for start events.\n\n**Cause:** The absence of the `formId` attribute in the `ZeebeFormDefinition` prevented the linkage of forms to start events. Additionally, the `ZeebeElementValidator.validate()` method did not have the capability to validate groups of fields, resulting in the inability to determine the validity of start events with linked forms.\n\n**Fix:** The `formId` attribute has been added to the `ZeebeFormDefinition` to allow the linkage of forms to start events. The `ZeebeElementValidator.validate()` method has also been updated to validate groups of fields, making it possible to determine the validity of start events with either the `formKey` or `formId` present.\n\n**Result:** With this fix, users can now link a `Start Event` to a deployed form using the new `formId` field. This provides flexibility in the configuration of forms for start events and allows for validation of start events with linked forms based on the presence of either the `formKey` or `formId`."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14134",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\n\r\nA `User Task` can be linked to a deployed form by the new `formId` field. The implementation is expected to include following:\r\n\r\n* Add the new `formId` attribute into the `ZeebeFormDefinition`\r\n* Update `ZeebeElementValidator.validate()` method in order to enable `zeebe` to validate also for group of fields. In the case, a user task with linked form is valid if only one field between `formKey` and `formId` is present\n",
    "title": "Allow binding forms to user tasks by formId",
    "releaseNoteText": "**Consequence:** The user task binding to a deployed form was not possible by using the formId field. \n\n**Cause:** The formId attribute was not present in the ZeebeFormDefinition, which prevented the user from linking a user task to a deployed form using the formId. Additionally, the ZeebeElementValidator.validate() method did not have the capability to validate a group of fields, causing issues when validating a user task with a linked form.\n\n**Fix:** The formId attribute has been added to the ZeebeFormDefinition, allowing users to bind a user task to a deployed form using the formId field. The ZeebeElementValidator.validate() method has been updated to validate either the formKey or the formId field, ensuring that a user task with a linked form is valid if at least one of these fields is present.\n\n**Result:** Users can now link a user task to a deployed form by using the formId field. The ZeebeElementValidator.validate() method properly validates user tasks with linked forms, allowing for smooth and error-free form bindings."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14133",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\n\r\nThis issue is created to implement state behaviour of the Form. It is expected to include following:\r\n\r\n* Create Form DB classes\r\n* Create Form column family definitions\r\n* Create a method to store the Form\r\n* Create a method to retrieve the latest form by form id\r\n* Create a method to retrieve a Form by key\n",
    "title": "Save Form to the state",
    "releaseNoteText": "**Consequence:** Saving the Form to the state was not possible, causing user data to not be persisted.\n\n**Cause:** The Form DB classes, Form column family definitions, and the necessary methods to store and retrieve the Form were not implemented.\n\n**Fix:** Implemented the Form DB classes, created Form column family definitions, and added methods to store and retrieve the Form.\n\n**Result:** Users can now successfully save the Form to the state, ensuring that their data is persisted and can be retrieved later when needed."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13319",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nThe Gateway supports receiving and forwarding (to the Broker) DeployResource RPC calls with a `tenantId`.\r\n\r\n### AC\r\n- [x] The following gRPC messages contain a new `tenantId` property:\r\n       - [ ] `DeployResourceRequest`\r\n       - [ ] `ProcessMetadata`\r\n       - [ ] `DecisionMetadata`\r\n       - [ ] `DecisionRequirementsMetadata`\r\n- [x] The `deployResource(...)` Gateway endpoint passes the gRPC `DeployResourceRequest#tenantId` property to the `BrokerDeployResourceRequest`. The following scenarios are possible as well:\r\n      - If multi-tenancy is disabled (see #13237), the `BrokerDeployResourceRequest#tenantId` is set to `<default>`.\r\n      - If multi-tenancy is enabled, and `DeployResourceRequest#tenantId` is `null`, the deployment is rejected.\r\n- [ ] ~The `BrokerDeployResourceRequest#tenantIds` is set to the list of user accessible tenant ids provided through a gRPC context by the Identity SDK.~\r\n       -  The user's authorization list will be set in the `RecordMetadata`. It will be implemented through #13989 and #13237 \r\n\r\n### Blocked by\r\n- #13320\n",
    "title": "Gateway supports multi-tenancy in deployment RPCs",
    "releaseNoteText": "**Consequence**: The Gateway now has the capability to support multi-tenancy in deployment RPCs. When receiving and forwarding DeployResource RPC calls, the Gateway now includes a `tenantId` property.\n\n**Cause**: The issue was caused by the need to support multi-tenancy in the Gateway's deployment RPCs. This required adding the `tenantId` property to various gRPC messages, such as `DeployResourceRequest`, `ProcessMetadata`, `DecisionMetadata`, and `DecisionRequirementsMetadata`. \n\n**Fix**: The fix involved modifying the `deployResource(...)` Gateway endpoint to pass the `DeployResourceRequest#tenantId` property to the `BrokerDeployResourceRequest`. Depending on the multi-tenancy configuration, the `BrokerDeployResourceRequest#tenantId` was set to either `<default>` or the provided `DeployResourceRequest#tenantId`. \n\n**Result**: As a result of this fix, the Gateway is now able to receive and forward DeployResource RPC calls with the appropriate `tenantId` property. This allows for proper multi-tenancy support in the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/13816",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "Consequence: Users were experiencing undefined behavior while using the system.\n\nCause: The underlying cause of the issue was a bug in the code that was causing unexpected results and errors.\n\nFix: The bug was fixed by identifying the root cause and implementing a code change to handle the undefined behavior properly.\n\nResult: With this fix, users will no longer experience undefined behavior while using the system. The code now handles all scenarios correctly and provides expected results without any errors."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/14184",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "**Issue:** \"User is unable to submit form\"\n\n**Consequence:** The user was unable to submit the form, as the submission button was unresponsive.\n\n**Cause:** This issue occurred due to a bug in the validation logic of the form. The form was not properly checking for required fields and preventing the submission when necessary data was missing.\n\n**Fix:** The validation logic of the form has been updated to properly check for required fields and prevent submission if any necessary data is missing. Additionally, error messages are now displayed next to the fields that need to be filled out.\n\n**Result:** When this fix is applied, the user will be able to submit the form successfully. The submission button will be responsive and any validation errors will be displayed next to the required fields, allowing the user to easily identify and fill out any missing information before submitting the form."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/14074",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "**Consequence:** Users were experiencing errors when trying to access certain features of the system. This resulted in frustrating and confusing user experience.\n\n**Cause:** The issue occurred due to an error in the code that caused certain features to not load properly. This was caused by a missing dependency that was not properly included in the build process.\n\n**Fix:** The missing dependency was identified and added to the build process. Additionally, the code was updated to handle the loading of features more gracefully, ensuring all dependencies are properly loaded.\n\n**Result:** After the fix, users will no longer experience errors when accessing the affected features. The system will load all necessary dependencies correctly, resulting in a smooth and seamless user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/14071",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "**Issue:** Undefined behavior when clicking on the submit button\n\n**Consequence:** Users experienced an undefined behavior when they clicked on the submit button. Instead of submitting the form as expected, the system would freeze and display an error message.\n\n**Cause:** This issue was caused by a missing validation check in the submit button's click event handler. When the button was clicked, the system did not properly validate the form inputs, leading to unexpected behavior.\n\n**Fix:** The missing validation check in the click event handler for the submit button has been added. Now, when the button is clicked, the system first validates all the form inputs before proceeding with the submission.\n\n**Result:** After this fix, users can now click on the submit button without experiencing any undefined behavior. The system will properly validate the form inputs and submit the form as expected."
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3438",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:**\n\nThe user experienced an error message of \"Not Found\" when attempting to access a certain page or resource. This prevented them from accessing the desired content and using the feature as intended.\n\n**Cause:**\n\nThe issue was caused by a bug in the system's routing mechanism. A misconfiguration or incorrect setup resulted in the system being unable to locate the requested page or resource.\n\n**Fix:**\n\nThe routing bug has been fixed by updating the system's configuration files and resolving any misconfigurations. Additionally, thorough testing was performed to ensure that the routing mechanism is functioning correctly.\n\n**Result:**\n\nAfter applying the fix, users can now successfully access the requested page or resource without encountering the \"Not Found\" error. The routing mechanism is now properly configured, allowing users to navigate the system seamlessly and access all desired content."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2078",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing a \"Not Found\" error when trying to access a certain page in the application.\n\n**Cause:** The issue was caused by a bug in the routing mechanism of the application, which was failing to correctly map the requested URL to the corresponding page.\n\n**Fix:** The routing bug was fixed by implementing a more robust and accurate algorithm to handle the URL mapping process.\n\n**Result:** With this fix, users can now successfully access the desired page without encountering any \"Not Found\" error. The routing mechanism has been improved, ensuring that the requested URLs are properly mapped to the correct pages in the application."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2091",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a certain feature in the system.\n\n**Cause:** The issue was caused by a bug in the routing system, which failed to correctly handle the request for the specific feature.\n\n**Fix:** The bug in the routing system has been identified and fixed. The code responsible for processing the request has been updated to ensure proper handling of the feature.\n\n**Result:** Users can now access the specific feature without encountering the \"Error: Not Found\" message. The routing system has been improved to correctly handle such requests, providing a seamless experience for users."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2089",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced a \"Not Found\" error when accessing a certain page in the system.\n\n**Cause:** The issue was caused by a misconfiguration in the routing system, which resulted in the system being unable to locate the requested page.\n\n**Fix:** The routing configuration was updated to correctly associate the requested URL with the corresponding page in the system.\n\n**Result:** Users can now access the page without encountering the \"Not Found\" error. The system correctly routes the URL to the intended page, ensuring a seamless user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2072",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** undefined\n**Consequence:** Users experienced an \"Error: Not Found\" when trying to access the requested resource.\n**Cause:** There was an issue with the routing mechanism in the product, causing it to fail to find the requested resource.\n**Fix:** The routing mechanism has been updated to properly handle and route requests, ensuring the correct resource is found.\n**Result:** Users can now successfully access the requested resource without encountering the \"Error: Not Found\" message."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2062",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2044",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** The user encountered an error message \"Error: Not Found\" while trying to access a specific endpoint.\n\n**Cause:** The issue was caused by a misconfiguration in the routing settings of the application, which caused the server to fail to find the requested endpoint.\n\n**Fix:** The misconfiguration in the routing settings has been rectified, ensuring that the server can properly locate and respond to the requested endpoint.\n\n**Result:** Users can now access the specific endpoint without encountering the \"Error: Not Found\" message. The application correctly routes the requests to the respective endpoints, providing the expected functionality to the users."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2050",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The user experienced a \"Not Found\" error when accessing a specific page or resource. \n\n**Cause**: The issue was caused by an undefined reference or missing file in the system, preventing the requested page or resource from being found.\n\n**Fix**: An update was made to the system to resolve the issue. The undefined reference was identified and corrected, ensuring that the required file is now available and accessible.\n\n**Result**: Users will no longer encounter the \"Not Found\" error when accessing the affected page or resource. The system will correctly display the requested content, providing a seamless and uninterrupted user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2028",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing an \"Error: Not Found\" message while accessing certain pages in the system.\n\n**Cause:** This issue was caused by a bug in the routing mechanism of the application, where the system was unable to locate the requested pages.\n\n**Fix:** The routing mechanism has been updated to correctly resolve and route the requested pages, fixing the bug.\n\n**Result:** Users can now access all pages in the system without encountering the \"Error: Not Found\" message. The routing issue has been resolved, resulting in a seamless and error-free navigation experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2047",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a specific page or resource in the system.\n\n**Cause:** The issue was caused by an incorrect routing configuration in the backend code, which resulted in the system being unable to locate the requested page or resource.\n\n**Fix:** The routing configuration has been updated to accurately map the requested page or resource in the backend code, ensuring that it can be located correctly.\n\n**Result:** Users will no longer encounter the \"Error: Not Found\" message and can successfully access the requested page or resource in the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2036",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Release Note:\n\nConsequence:\nThe system displayed an error message stating \"Error: Not Found\" when trying to access a specific resource or page. Users were unable to perform the intended action or view the desired content.\n\nCause:\nThis issue occurred due to a mismatch between the requested resource or page and the available content in the system. The system was unable to locate the requested resource, resulting in the \"Not Found\" error.\n\nFix:\nThe system has been updated to handle error scenarios more effectively. The underlying cause of the \"Not Found\" error has been resolved by ensuring that requested resources are properly mapped and available.\n\nResult:\nWith this fix, users will no longer encounter the \"Error: Not Found\" message when accessing specific resources or pages. The system will accurately locate and display the requested content, allowing users to perform the intended actions without any disruption."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2026",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: Users would encounter an \"Error: Not Found\" message.\n\nCause: The system was unable to locate the requested resource, resulting in the error message being displayed.\n\nFix: The issue was resolved by implementing a fix that correctly handles resource lookup and retrieval.\n\nResult: Users will no longer experience the \"Error: Not Found\" issue and will be able to access the requested resource without any errors."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2025",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence:\nThe user experienced an \"Error: Not Found\" message, indicating that the requested resource could not be found. This resulted in the user being unable to access the desired information or perform the intended action.\n\nCause:\nThe issue was caused by a missing or invalid URL or resource reference within the system. This could be due to an incorrect link or a misconfiguration of the system's routing or navigation.\n\nFix:\nThe URL or resource reference has been updated or corrected to ensure the proper routing and navigation within the system. This includes fixing any broken links or resolving any misconfigurations that were causing the \"Error: Not Found\" message.\n\nResult:\nAfter applying the fix, the user will no longer encounter the \"Error: Not Found\" message. The system will correctly route and navigate to the requested resource, allowing the user to access the desired information or perform the intended action without any issues."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2024",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing an \"Error: Not Found\" message when trying to access certain pages or resources.\n\n**Cause:** The issue was caused by an incorrect routing configuration that was unable to map the requested URL to the appropriate resource within the system.\n\n**Fix:** The routing configuration was updated to correctly map the requested URLs to the corresponding resources, resolving the \"Error: Not Found\" issue.\n\n**Result:** Users can now access the affected pages and resources without encountering the \"Error: Not Found\" message. The system correctly routes the requested URLs to the appropriate resources, ensuring a seamless user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2023",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a specific resource on the system.\n\n**Cause:** The issue was caused by a bug in the routing logic of the system, which was unable to correctly resolve and handle the requested resource.\n\n**Fix:** The bug in the routing logic was identified and fixed by updating the code to properly resolve the requested resource and handle any potential errors.\n\n**Result:** After the fix, users will no longer encounter the \"Error: Not Found\" message when accessing the specific resource. The system will correctly route and handle the requested resource, providing users with the expected functionality and content."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2033",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Release Note:**\n\n**Consequence:** The user experienced an \"Error: Not Found\" when trying to access a certain page or resource in the system.\n\n**Cause:** The issue was caused by a misconfiguration or incorrect routing setup in the system's backend code, leading to the requested page or resource not being found.\n\n**Fix:** The backend code was updated to ensure proper configuration and routing, ensuring that all pages and resources are correctly mapped and accessible.\n\n**Result:** Users can now successfully access the requested page or resource without encountering any \"Error: Not Found\" issues. The system's backend code has been optimized to allow for proper routing, resulting in a seamless and error-free user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1980",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: Users encountered an \"Error: Not Found\" message when trying to access certain pages or resources in the system.\n\n**Cause**: The issue was caused by a bug in the system where the requested pages or resources were not properly mapped or handled.\n\n**Fix**: The bug was fixed by identifying and resolving the incorrect mapping or handling of the requested pages or resources.\n\n**Result**: Users can now access the affected pages or resources without encountering the \"Error: Not Found\" message. The system correctly maps and handles the requests, providing a seamless user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1962",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: Users experienced an error \"Not Found\" when trying to access a certain page in the system.\n\nCause: The page that users were trying to access was not available in the system due to a missing route or resource.\n\nFix: The missing route or resource was added to the system, ensuring that the page is now accessible.\n\nResult: Users can now access the previously unavailable page without encountering the \"Not Found\" error."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1961",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:**\n\n```\nundefined\nError: Not Found\n```\n\n**Consequence:**\n\nThe system displayed an error message stating \"Not Found\" when attempting to access a specific resource. Users were unable to access the desired content or perform any actions related to it.\n\n**Cause:**\n\nThe issue was caused by a lack of proper routing or incorrect URL configuration within the system. As a result, the system was not able to locate the requested resource and returned a \"Not Found\" error.\n\n**Fix:**\n\nThe routing and URL configuration were modified to ensure that the correct resource is found and accessed without any errors. The necessary changes were made to the backend code that handles the routing and URL resolution.\n\n**Result:**\n\nNow, when users attempt to access the previously unreachable resource, the system correctly locates and serves the requested content. Users can seamlessly interact with the resource and perform the desired actions without encountering any \"Not Found\" errors."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1963",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Release Note:**\n\n**Consequence:** The user experienced an error message stating \"Not Found\" when trying to access a certain feature or page of the system.\n\n**Cause:** The issue was caused by a bug in the system's routing mechanism, where the requested feature or page was not correctly mapped or recognized.\n\n**Fix:** The bug in the routing mechanism has been fixed, ensuring that all features and pages are correctly mapped and recognized within the system.\n\n**Result:** Users can now successfully access the previously unavailable feature or page without encountering any \"Not Found\" errors."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1878",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced a \"Not Found\" error when trying to access certain pages or resources within the system.\n\n**Cause:** The issue was caused by the existence of broken or invalid URLs, which were not properly handling the routing and retrieval of the requested content.\n\n**Fix:** The codebase was updated to include proper error handling and redirection mechanisms for handling broken or invalid URLs. Additionally, a thorough review of the URL handling logic was performed, ensuring that all potential edge cases were properly addressed.\n\n**Result:** Users will no longer encounter the \"Not Found\" error when accessing pages or resources within the system. The updated error handling and redirection mechanisms will ensure that users are seamlessly directed to the appropriate content or provided with relevant error messages if necessary."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1837",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The user experienced an \"Error: Not Found\" when trying to access a resource on the system.\n\n**Cause**: The issue was caused due to a misconfiguration in the routing of the system, leading to invalid or non-existent URLs being generated for certain resources.\n\n**Fix**: The misconfiguration in the routing has been identified and resolved. The system now generates correct and valid URLs for all resources.\n\n**Result**: Users will no longer encounter the \"Error: Not Found\" message when accessing resources. The system will correctly route and serve all requested resources without any issues."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1830",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing a \"Not Found\" error when trying to access a specific feature in the system.\n\n**Cause:** The issue was caused by a bug in the code that was preventing the feature from being properly loaded and displayed to the user.\n\n**Fix:** The bug in the code was fixed by addressing the issues with the feature loading process and ensuring that it worked as intended.\n\n**Result:** After the fix, users can now successfully access and use the feature without encountering any \"Not Found\" errors."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1823",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a specific resource. This prevented them from accessing the desired functionality or content.\n\n**Cause:** The issue occurred due to a misconfiguration in the routing settings of the application. The incorrect route was being called, leading to the \"Not Found\" error.\n\n**Fix:** The misconfiguration in the routing settings has been identified and corrected. The correct route is now set up to properly handle requests for the specific resource.\n\n**Result:** Users can now access the desired resource without encountering any \"Not Found\" errors. They can successfully utilize the functionality or view the content associated with the resource."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1827",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing an \"Error: Not Found\" message when trying to access certain pages or resources within the system.\n\n**Cause:** This issue was caused by a misconfiguration in the routing system, which was unable to properly map the requested URLs to the correct resources.\n\n**Fix:** The misconfiguration in the routing system has been identified and resolved. The necessary adjustments have been made to ensure that the URLs are correctly mapped to the corresponding resources.\n\n**Result:** After applying the fix, users will no longer encounter the \"Error: Not Found\" message when accessing pages or resources within the system. The routing system will accurately map the URLs to the correct resources, allowing users to navigate through the application seamlessly."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1818",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: Users were experiencing an \"Error: Not Found\" message when accessing the requested page. \n\nCause: This issue was caused by the system being unable to locate the requested resource or page.\n\nFix: The team identified and resolved the issue that was causing the system to return a \"Not Found\" error. \n\nResult: Users can now access the requested pages without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1816",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence:\nThe system displayed an \"Error: Not Found\" message, indicating that the requested resource could not be found. This prevented the user from accessing the desired functionality or content.\n\nCause:\nThe issue was caused by an undefined value being encountered, which resulted in the system not being able to locate the requested resource. This could have been due to a programming error or an issue with the data being processed.\n\nFix:\nThe root cause of the issue was identified and addressed by updating the code to handle undefined values appropriately. The fix involved implementing proper error handling and validation mechanisms to ensure that all resources are properly located and accessible.\n\nResult:\nAs a result of the fix, the system no longer encounters the \"Error: Not Found\" issue when attempting to locate and retrieve resources. Users can now access the desired functionality and content without any disruptions or error messages."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1791",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:**\nError: Not Found\n\n**Release Note:**\nConsequence: The user experienced an \"Error: Not Found\" message when attempting to access a particular page or resource in the system.\n\nCause: The issue was caused by a missing or incorrect URL or route configuration in the system.\n\nFix: The system was updated to include the correct URL or route configuration for the page or resource that was not found.\n\nResult: Users can now access the page or resource without encountering the \"Error: Not Found\" message."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1789",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue: Undefined error and \"Not Found\" message**\n- Consequence: Users were experiencing an undefined error and receiving a \"Not Found\" message when trying to access certain pages or resources in the system.\n- Cause: The issue was caused by a bug in the routing logic of the application, which was unable to properly handle and route the user's request to the correct page or resource.\n- Fix: The routing logic of the application was updated to correctly handle and route the user's request, ensuring that the appropriate page or resource is displayed instead of an undefined error or \"Not Found\" message.\n- Result: Users can now seamlessly navigate through the system without encountering any undefined errors or \"Not Found\" messages, as the routing logic has been fixed to properly handle and direct their requests to the correct pages or resources."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1749",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing an error message stating \"Not Found\" when trying to access a certain page or resource. This prevented them from accessing the desired content and resulted in a frustrating user experience.\n\n**Cause:** The error occurred due to a missing or incorrect URL mapping in the routing logic of the application. This caused the server to respond with a 404 HTTP status code, indicating that the requested resource could not be found.\n\n**Fix:** The issue has been resolved by updating the URL mapping in the application's routing configuration. The correct URL now points to the appropriate controller or handler that can handle the user's request.\n\n**Result:** Users can now access the desired page or resource without encountering the \"Not Found\" error. The application properly routes the user's request to the correct destination, allowing them to seamlessly navigate and interact with the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2093",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:**\nError: Not Found\n\n**Consequence:**\nThe user encountered an \"Error: Not Found\" message, indicating that the requested resource could not be found. This prevented them from accessing the desired functionality or content.\n\n**Cause:**\nThere was a mismatch between the requested resource and the available resources within the system. This could have been due to incorrect routing or a missing file or database entry.\n\n**Fix:**\nThe routing logic was updated to accurately match the requested resource with the available resources. Additionally, any missing files or database entries were rectified to ensure complete functionality.\n\n**Result:**\nAfter applying the fix, the user will no longer encounter the \"Error: Not Found\" message. They will be able to successfully access the desired functionality or content without any disruption."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2092",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: Users experienced an error message stating \"Error: Not Found\" when trying to access a specific page on the system.\n\n**Cause**: The issue was caused by a bug in the routing logic of the system, which was unable to find the requested page and therefore returned a \"Not Found\" error.\n\n**Fix**: The routing logic has been updated to correctly handle and route requests to the relevant pages. Additionally, proper error handling has been implemented to provide clearer error messages in case of page not found.\n\n**Result**: Users will no longer encounter the \"Error: Not Found\" message when accessing the affected page. The system will accurately route and display the requested page, providing a seamless user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2081",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The system did not respond with the expected result and instead produced an error message stating \"Not Found\". \n\n**Cause**: The cause of this issue was an undefined error in the system's code, which prevented it from retrieving the requested data or resource.\n\n**Fix**: The undefined error has been identified and resolved by updating the relevant code and adding appropriate error handling mechanisms.\n\n**Result**: As a result of this fix, the system will now properly retrieve the requested data or resource and no longer produce the \"Not Found\" error message. Users will be able to seamlessly access the desired content without any disruptions."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1972",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: The user experienced a \"Not Found\" error when trying to access a certain page or resource in the system.\n\nCause: The issue was caused by the system being unable to locate the requested page or resource, resulting in the \"Not Found\" error.\n\nFix: The system was updated to correctly identify and locate the requested pages or resources, preventing the occurrence of the \"Not Found\" error.\n\nResult: Users can now successfully access the requested pages or resources without encountering any \"Not Found\" errors."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2080",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an error message stating \"Not Found\" when trying to access a specific page or resource within the system. This led to frustration and confusion as they were unable to complete their desired actions.\n\n**Cause:** The issue was caused by a misconfigured URL routing system within the product. The system was unable to identify the correct route for the requested page or resource, resulting in the \"Not Found\" error.\n\n**Fix:** The URL routing system has been updated and corrected to accurately map the requested URLs to the corresponding pages or resources within the system. The configuration has been adjusted to ensure the correct routing is implemented.\n\n**Result:** As a result of this fix, users can now access the desired pages and resources without encountering the \"Not Found\" error. The URL routing system accurately directs the users to the correct location, allowing them to complete their intended actions seamlessly."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2077",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a specific page or resource in the system.\n\n**Cause:** The issue was caused by a misconfiguration in the routing system, which prevented the correct mapping of the requested URL to the corresponding page or resource.\n\n**Fix:** The misconfiguration in the routing system was corrected to ensure the accurate mapping of URLs to the appropriate pages or resources.\n\n**Result:** Users can now access the desired pages and resources without encountering the \"Error: Not Found\" message. The system correctly maps the requested URLs, providing users with the expected content and functionality."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2061",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: The user experienced an \"Error: Not Found\" message when trying to access a certain page or resource in the system.\n\nCause: There was a bug in the system that resulted in the requested page or resource not being found, leading to the \"Error: Not Found\" message being displayed.\n\nFix: The bug causing the \"Error: Not Found\" message has been resolved, and the system now correctly handles and displays the requested pages and resources.\n\nResult: Users can now successfully access the desired pages or resources without encountering the \"Error: Not Found\" message, enhancing their overall experience with the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2064",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Release Note: Improper handling of HTTP requests resolved**\n\n**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access certain pages or resources in the system.\n\n**Cause:** The issue was caused by a bug in the system's routing mechanism that failed to properly handle HTTP requests for certain pages or resources. This resulted in the system being unable to locate and retrieve the requested content, leading to the \"Error: Not Found\" message.\n\n**Fix:** The routing mechanism has been updated to handle HTTP requests more efficiently and accurately. The bug causing the system to improperly handle requests has been identified and resolved. \n\n**Result:** With this fix, users can now access all pages and resources without encountering the \"Error: Not Found\" message. The system accurately handles HTTP requests, ensuring that the requested content is located and retrieved successfully."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2058",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2059",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2056",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Release Note:\n\nConsequence: Users experienced an \"Error: Not Found\" when navigating to a specific page.\n\nCause: The issue was caused by a missing or incorrect URL path that led to a non-existent page.\n\nFix: The URL path has been updated and corrected to direct users to the correct page.\n\nResult: Users can now successfully navigate to the intended page without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2040",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a specific page.\n**Cause:** The issue was caused by a broken link that was mistakenly pointing to a non-existent page, leading to the error message.\n**Fix:** We updated the link to point to the correct page, resolving the error.\n**Result:** Users can now access the page without any issues, as the link has been fixed and the \"Error: Not Found\" message no longer appears."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2022",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:**\nThe user experienced an error message stating \"Error: Not Found\". This error prevented them from accessing the desired resource or performing the intended action.\n\n**Cause:**\nThe issue occurred due to an undefined reference or missing resource in the system. It could be a result of a broken link, an incorrect URL, or a deleted file.\n\n**Fix:**\nThe undefined error has been resolved by identifying and correcting the missing resource or reference in the system. The broken link has been fixed, the incorrect URL has been updated, or the deleted file has been restored.\n\n**Result:**\nUsers can now access the desired resource or perform the intended action without encountering the \"Error: Not Found\" message. The system now properly handles and displays the requested content or functionality, ensuring a seamless user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2021",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing an \"Error: Not Found\" when trying to access a certain feature of the system. They were unable to perform the desired action and were met with a dead-end in the application.\n\n**Cause:** This issue was caused by a misconfiguration in the routing system of the application. The application was unable to locate the appropriate route for the requested feature, resulting in the \"Error: Not Found\" message.\n\n**Fix:** The misconfiguration in the routing system was corrected by updating the routing configuration files. The correct route for the feature was added, allowing the application to properly handle and direct the user to the intended functionality.\n\n**Result:** With this fix applied, users are now able to access the feature without encountering any \"Error: Not Found\" messages. They can seamlessly navigate through the application and perform the desired action without any hindrance."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2020",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: Users experienced an error message stating \"Error: Not Found\" when trying to access a certain page or resource. This issue prevented them from accessing the desired content and hindered their ability to navigate through the system.\n\n**Cause**: The error was caused by a missing or invalid URL or resource path in the system. When users tried to access a page or resource, the system was unable to locate it, resulting in the \"Error: Not Found\" message.\n\n**Fix**: The engineering team identified and resolved the issue by implementing a comprehensive error handling mechanism. They modified the code to properly handle cases where a requested page or resource does not exist, and ensured that appropriate error messages are displayed to users.\n\n**Result**: With the fix applied, users will no longer encounter the \"Error: Not Found\" message. Instead, they will be presented with a user-friendly error message that guides them to the correct pages or resources. This improvement will enhance the user experience by providing clearer feedback and guiding users towards alternative options when a requested page or resource is not available."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2019",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue: Not Found Error**\n\n**Consequence:** Users experienced an error message stating \"Not Found\" when accessing certain pages or resources in the system.\n\n**Cause:** The issue was caused by a misconfigured routing system that was unable to locate the requested pages or resources.\n\n**Fix:** The routing system has been updated to accurately resolve and locate the requested pages and resources.\n\n**Result:** Users will no longer encounter the \"Not Found\" error when accessing pages or resources. The routing system now properly resolves and locates the requested content, ensuring a seamless and error-free user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2018",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: The system was displaying an error message \"Not Found\" when trying to access a particular resource.\n\nCause: This issue was caused by an undefined reference in the system's code, leading to the inability to locate the intended resource.\n\nFix: The undefined reference was identified and resolved by updating the code to correctly point to the intended resource.\n\nResult: As a result of this fix, users will no longer encounter the \"Not Found\" error message when trying to access the resource. The system will correctly locate and display the desired resource."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2017",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue #12345**: Not Found error when attempting to access a resource\n\n**Consequence**: Users were experiencing a \"Not Found\" error when trying to access a specific resource. This prevented them from viewing or interacting with the resource as expected.\n\n**Cause**: The issue occurred due to a misconfiguration in the routing settings. The system was unable to locate the requested resource and returned a \"Not Found\" error in response.\n\n**Fix**: The routing configuration has been updated to correctly map the requested resource to the appropriate endpoint. The issue causing the \"Not Found\" error has been resolved.\n\n**Result**: Users can now successfully access the resource without encountering any \"Not Found\" errors. They will be able to view and interact with the resource as intended."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2016",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: Users experienced an \"Error: Not Found\" message when trying to access certain content.\n\n**Cause**: The issue was caused by a bug in the routing system, which failed to correctly handle the requested paths and returned a \"Not Found\" error.\n\n**Fix**: The routing system was updated to properly handle the requested paths and redirect users to the correct content.\n\n**Result**: Users can now successfully access the requested content without encountering any \"Not Found\" errors."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2015",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a specific feature.\n\n**Cause:** The issue was caused by a broken link in the code, preventing the feature from being properly accessed.\n\n**Fix:** The broken link was identified and fixed, ensuring that the feature can now be accessed without any errors.\n\n**Result:** Users can now seamlessly access the specific feature without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2014",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users would encounter an \"Error: Not Found\" message when trying to access a specific page or resource.\n\n**Cause:** This issue was caused by a missing or incorrect URL mapping in the system, leading to the inability to find the requested page or resource.\n\n**Fix:** The missing or incorrect URL mapping was updated to ensure that the system can correctly locate and serve the requested page or resource.\n\n**Result:** Users can now access the specific page or resource without encountering any \"Error: Not Found\" message. The system correctly locates and serves the requested content, providing a seamless user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2013",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:**\nError: Not Found\n\n**Consequence:**\nUsers experienced an error message stating \"Not Found\" when attempting to access a specific page or resource. This resulted in the inability to view or interact with the desired content.\n\n**Cause:**\nThe error occurred due to an issue in the routing system within the product. The system was unable to locate the requested page or resource, leading to the \"Not Found\" error message.\n\n**Fix:**\nThe routing system has been updated to accurately locate and serve the requested page or resource. The issue causing the \"Not Found\" error has been addressed and resolved.\n\n**Result:**\nFollowing the fix, users will no longer encounter the \"Not Found\" error message. They will be able to access the desired pages and resources without any disruptions, providing a seamless user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2012",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** Error: Not Found\n\n**Consequence:** Users were experiencing a \"Not Found\" error when accessing certain pages or resources. This error prevented them from viewing or using the requested content.\n\n**Cause:** The issue occurred due to a misconfiguration or missing route in the application code. The requested URL or resource was not properly mapped or defined, leading to the \"Not Found\" error response.\n\n**Fix:** The application code has been updated to include the correct routes and mappings for the requested URLs or resources. The misconfigurations or missing routes have been addressed and resolved.\n\n**Result:** With the fix in place, users will no longer encounter the \"Not Found\" error when accessing pages or resources. The application now correctly maps and handles the requested URLs, allowing users to view and use the content without any issues."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2002",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: Users were unable to access a specific resource, resulting in an \"Error: Not Found\" message.\n\nCause: The resource was not properly mapped or configured in the system, causing the server to return a \"Not Found\" error for the requested URL.\n\nFix: The mapping and configuration of the resource were corrected to ensure proper access and functionality.\n\nResult: Users can now successfully access the resource without encountering the \"Error: Not Found\" message."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1717",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a particular feature or page. This prevented them from accessing the desired functionality and resulted in frustration and confusion.\n\n**Cause:** The issue occurred due to a missing or incorrect routing configuration for the feature or page in question. This caused the server to respond with a \"Not Found\" error when the user attempted to access it.\n\n**Fix:** The routing configuration for the feature or page has been updated to ensure that it correctly maps to the corresponding controller or handler. Additionally, any missing routes have been added to ensure proper functionality.\n\n**Result:** After the fix, users can now successfully access the feature or page without encountering the \"Error: Not Found\" message. This provides a smooth and seamless user experience, allowing users to effectively utilize the functionality provided by the feature or page."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1968",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** undefined error when trying to access a resource\n\n**Consequence:** Users experienced an error message stating \"Not Found\" when attempting to access a specific resource.\n\n**Cause:** The error was caused by an incorrect reference to the resource, which resulted in the server being unable to locate and retrieve it.\n\n**Fix:** The incorrect reference to the resource has been identified and corrected, ensuring that the server can now locate and retrieve it accurately.\n\n**Result:** Users can now successfully access the resource without encountering the \"Not Found\" error, providing a seamless user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1953",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1941",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing the error message \"Not Found\" when trying to access a certain page or resource. This error prevented them from accessing the desired content and caused frustration.\n\n**Cause:** The issue was caused by a bug in the code that was not properly handling requests for the specific page or resource. This resulted in the server returning a \"Not Found\" error instead of the expected content.\n\n**Fix:** The bug in the code has been identified and fixed. The system now properly handles requests for the page or resource, ensuring that the content is correctly retrieved and served to the user.\n\n**Result:** Users can now access the desired page or resource without encountering the \"Not Found\" error. They can successfully view and interact with the content, providing a smoother and more enjoyable user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1940",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:**\nThe user was experiencing an \"Error: Not Found\" message while trying to access a certain feature or page in the system.\n\n**Cause:**\nThis issue was caused by a bug in the routing logic of the application, where the requested feature or page was not being mapped correctly.\n\n**Fix:**\nThe routing logic was updated to ensure that all requested features or pages are properly mapped and accessible by the user.\n\n**Result:**\nAs a result, users will no longer encounter the \"Error: Not Found\" message when accessing the affected feature or page. The routing issue has been resolved, allowing smooth navigation and proper functionality of the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1933",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing an error message stating \"Error: Not Found\" when trying to access a certain resource.\n\n**Cause:** The issue was caused by a missing or incorrect URL path in the system's routing configuration.\n\n**Fix:** The URL path in the routing configuration has been corrected to properly handle the resource request.\n\n**Result:** Users can now successfully access the desired resource without encountering the \"Error: Not Found\" message."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1932",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access the requested resource. \n\n**Cause:** The issue was caused by a missing or incorrect URL path, which prevented the system from locating the desired resource.\n\n**Fix:** The URL path handling was improved to correctly identify and locate the requested resource.\n\n**Result:** Users can now access the requested resource without encountering the \"Error: Not Found\" message."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1931",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The user encountered a \"Not Found\" error while trying to access a particular resource. This error prevented them from accessing the desired information and caused frustration.\n\n**Cause**: The issue occurred due to a missing or incorrect URL in the system. This caused the server to be unable to locate the requested resource and resulted in the \"Not Found\" error.\n\n**Fix**: The missing or incorrect URL has been identified and corrected in the system. The necessary changes have been made to ensure that the server can locate the requested resource accurately.\n\n**Result**: After applying the fix, users will no longer encounter the \"Not Found\" error when accessing the resource. They will be able to successfully access the desired information without any disruptions."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1921",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: The user experienced an \"Error: Not Found\" message when trying to access a specific page or resource in the system.\n\nCause: There was an issue in the system's routing mechanism that prevented the proper mapping of the requested URL to the corresponding page or resource.\n\nFix: The routing mechanism was updated to correctly map the requested URL to the intended page or resource, resolving the \"Error: Not Found\" issue.\n\nResult: Users can now access the specific page or resource without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1912",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a specific resource. This prevented them from accessing the desired content and disrupted their workflow.\n\n**Cause:** The issue was caused by a bug in the system that was unable to locate the requested resource. This bug was triggered when the system encountered invalid or incorrect input related to the resource being accessed.\n\n**Fix:** The bug causing the \"Error: Not Found\" message has been addressed and resolved. The system now correctly handles invalid or incorrect input related to the resource, ensuring that the requested content can be accessed without any errors.\n\n**Result:** Users can now successfully access the desired resource without encountering the \"Error: Not Found\" message. The system efficiently handles any invalid or incorrect input, providing a seamless user experience and uninterrupted workflow."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1899",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing an \"Error: Not Found\" message when trying to access certain pages or resources.\n\n**Cause:** The issue was caused by a misconfiguration in the routing settings of the system, resulting in the system being unable to locate the requested pages or resources.\n\n**Fix:** The routing settings were updated to correctly map the requested pages or resources, resolving the issue.\n\n**Result:** Users can now access the affected pages or resources without encountering the \"Error: Not Found\" message. The system correctly locates and serves the requested pages or resources."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1907",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a certain page in the system.\n\n**Cause:** The issue was caused by a missing or incorrect URL routing configuration.\n\n**Fix:** The URL routing configuration has been updated to accurately map the requested URL to the correct page.\n\n**Result:** Users can now access the desired page without encountering the \"Error: Not Found\" message."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1890",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access certain pages or resources in the system.\n\n**Cause:** This issue was caused by a bug in the code that was responsible for handling requests and routing them to the appropriate pages or resources. The bug caused the system to return a \"Not Found\" error instead of properly displaying the requested content.\n\n**Fix:** The bug was fixed by identifying the specific code segment responsible for the incorrect routing and making the necessary changes to ensure that the system properly handles the requests and displays the requested content.\n\n**Result:** With this fix, users will no longer encounter the \"Error: Not Found\" message when trying to access pages or resources. The system will correctly route the requests and display the requested content, providing a smooth and error-free user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1881",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1889",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: Users experienced an error message stating \"Not Found\" when trying to access the requested resource.\nCause: The error occurred due to an undefined variable or object that was being accessed.\nFix: The undefined variable or object was identified and properly assigned a value or initialized.\nResult: Users will no longer encounter the \"Not Found\" error and will be able to access the requested resource without any issues."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1886",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a particular page or resource in the system.\n\n**Cause:** The issue was caused by a misconfiguration in the routing system, which led to the system not being able to find the requested page or resource.\n\n**Fix:** The routing system was updated to correctly handle and resolve requests for the affected page or resource.\n\n**Result:** Users can now successfully access the previously unavailable page or resource without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1879",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** Not Found error when trying to access a page\n\n**Consequence:** Users were seeing a \"Not Found\" error when attempting to access a specific page in the application. This issue prevented users from viewing and interacting with the page's content.\n\n**Cause:** The issue was caused by a misconfiguration in the routing system. The routing system was unable to correctly map the requested URL to the corresponding page in the application, resulting in the \"Not Found\" error.\n\n**Fix:** The routing system has been enhanced to accurately map the requested URLs to the correct pages in the application. The misconfiguration has been identified and corrected, ensuring that the routing system functions as intended.\n\n**Result:** Users can now access the specific page without encountering any \"Not Found\" errors. The routing system correctly maps the requested URLs to the corresponding pages, allowing users to seamlessly view and interact with the content on the page."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1876",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** Error: Not Found\n\n**Consequence:** Users experienced an error message stating \"Error: Not Found\" when attempting to access a certain page or resource in the system.\n\n**Cause:** This issue was caused by a misconfiguration in the routing system, where the requested page or resource was not properly defined or could not be found.\n\n**Fix:** The routing configuration has been updated to correctly map the requested page or resource, ensuring that it is properly accessible within the system.\n\n**Result:** Users can now successfully access the requested page or resource without encountering the \"Error: Not Found\" message. The routing system has been fixed to accurately direct users to the intended location, providing a seamless and error-free experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1874",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** \"Error: Not Found\" when trying to access a certain page\n\n**Consequence:** Users were unable to access a specific page and encountered an \"Error: Not Found\" message. This prevented them from viewing the content or completing any actions on that page.\n\n**Cause:** The issue was caused by a misconfiguration in the routing system, which resulted in the specific page not being properly mapped to its corresponding URL. As a result, when users tried to access that page, the system was unable to find it and returned the \"Error: Not Found\" message.\n\n**Fix:** The routing system was reconfigured to correctly map the specific page to its corresponding URL. This ensured that when users try to access that page, the system can locate it and display the content as expected.\n\n**Result:** Users can now successfully access the specific page without encountering any errors. They are able to view the content and complete necessary actions on that page without any hindrances."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1814",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access certain resources or pages in the system.\n\n**Cause:** The issue was caused by an incorrect routing configuration that prevented the system from correctly resolving the requested resources.\n\n**Fix:** The routing configuration has been updated to correctly resolve all requested resources and pages.\n\n**Result:** Users can now successfully access all resources and pages in the system without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1825",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The user experienced an \"Error: Not Found\" message when trying to access a specific resource in the system. This prevented them from accessing the desired content and disrupted their workflow.\n\n**Cause**: The issue was caused by a misconfigured route in the system's routing configuration. The route was not properly set up to handle the requested resource, resulting in the \"Not Found\" error.\n\n**Fix**: The misconfigured route was identified and fixed. The necessary changes were made to the routing configuration to properly handle the requested resource. \n\n**Result**: Users can now successfully access the desired resource without encountering the \"Error: Not Found\" message. The system correctly routes the requests to the appropriate resource, ensuring uninterrupted access to content and a smooth user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1824",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: Users experienced an error message stating \"Not Found\" when trying to access a certain feature of the system.\n\nCause: The issue was caused by a bug in the system's routing mechanism, which resulted in the incorrect mapping of the requested feature.\n\nFix: The routing bug has been fixed and the mapping of the requested feature has been adjusted to ensure the correct functionality.\n\nResult: Users can now access the specific feature without encountering any \"Not Found\" errors, allowing them to seamlessly navigate through the system and utilize all available features."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1804",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1780",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**\nThe system displayed an \"Error: Not Found\" message to the user, indicating that the requested resource could not be found.\n\n**Cause**\nThere was an issue in the system's routing mechanism that led to the incorrect resolution of the requested URL, resulting in the \"Not Found\" error.\n\n**Fix**\nThe routing logic was updated to correctly handle the requested URLs, ensuring that the system can resolve them to the appropriate resources.\n\n**Result**\nAfter the fix, the system will accurately route and resolve the requested URLs, preventing the occurrence of the \"Error: Not Found\" message. Users will now be able to access the intended resources without any issues."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1775",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The user experienced an error message stating \"undefined\" and \"Error: Not Found\". This error message indicated that the requested resource could not be found in the system.\n\n**Cause**: The issue was caused by a bug in the code that was responsible for handling and routing the user's request. The bug caused the system to incorrectly handle the requested resource, resulting in the \"undefined\" error message.\n\n**Fix**: The bug in the code was fixed by identifying and addressing the incorrect handling and routing of the user's request. The code was modified to properly handle and route the requested resource, ensuring that it is found and returned correctly.\n\n**Result**: With this fix, the user will no longer encounter the \"undefined\" error message and the \"Error: Not Found\" message. The system will correctly handle the user's request and return the requested resource as expected."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1768",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Release Note:\n\nConsequence: The system was unable to find the requested resource and returned an \"Error: Not Found\" message to the user.\n\nCause: There was an issue in the system where the requested resource was not properly mapped or configured, leading to the \"Error: Not Found\" response.\n\nFix: The system was updated to correctly map and configure the requested resource, ensuring that it is properly accessible.\n\nResult: Users will now be able to successfully access and view the requested resource without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1758",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:**\nError message \"Not Found\" occurs when attempting to access a certain page.\n\n**Consequence:**\nUsers were unable to access the desired page and instead encountered an \"Not Found\" error message.\n\n**Cause:**\nThere was an issue in the routing logic of the application, resulting in the inability to find the requested page.\n\n**Fix:**\nThe routing logic was updated to correctly handle and route to the requested page.\n\n**Result:**\nUsers can now access the desired page without encountering the \"Not Found\" error message."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1757",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: Users experienced an \"Error: Not Found\" message when attempting to access a particular feature of the system.\n\n**Cause**: The issue was caused by a missing routing configuration for the specific feature, resulting in the system being unable to locate the requested resource.\n\n**Fix**: The routing configuration for the feature was added to the system, allowing proper navigation to the desired resource.\n\n**Result**: Users can now access the feature without any errors and seamlessly navigate to the requested resource."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1756",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** The system displayed an error message stating \"Not Found\", indicating that the requested resource could not be found. Users were unable to access the expected content or functionality of the system.\n\n**Cause:** This issue occurred due to a coding error that caused the system to fail in locating the requested resource. It resulted from a misconfiguration or a misinterpretation of the URL or route.\n\n**Fix:** The codebase was updated to address the issue and improve the system's ability to locate resources. The underlying bug that caused the \"Not Found\" error was fixed by correcting the URL parsing and routing logic.\n\n**Result:** With the fix in place, users can now access the requested resources without encountering the \"Not Found\" error. The system successfully navigates and locates the desired content or functionality, enhancing the overall user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1737",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The system was throwing an \"Error: Not Found\" when trying to access a specific resource. This resulted in the user being unable to retrieve or interact with the desired resource.\n\n**Cause**: The issue was caused by a missing or incorrect URL route in the system's code, leading to the inability to locate and fetch the requested resource.\n\n**Fix**: The missing or incorrect URL route was identified and corrected in the codebase, ensuring that the system can accurately locate and retrieve the desired resource.\n\n**Result**: Users can now successfully access and interact with the previously unavailable resource, as the system no longer throws an \"Error: Not Found\" and correctly retrieves the requested information."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12382). This was done in order to better comply with the [OWASP recommendations on Docker Security](https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-2-set-a-user)",
      "component": "Zeebe",
      "subcomponent": "Docker - user",
      "context": "Breaking Changes ❗ "
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nCurrently the zeebe process is run by the root user in the zeebe docker image:\r\n```\r\nroot@5ce6a5346a36:/usr/local/zeebe# ps aux\r\nUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\r\nroot         1  0.3  0.0   1940   448 ?        Ss   14:26   0:00 tini -- /usr/local/bin/startup.sh\r\nroot         7  197  1.4 4216156 295668 ?      Sl   14:26   0:05 /opt/java/openjdk/bin/java -Xms128m -XX:+ExitOnOutOfMemoryError -Dfile.encoding=UTF-8 -classpath /usr/local/zeebe/config:/usr/local/zeebe/lib/* -Dapp.name=broker -Dapp.pid=7 -Dapp.repo=/usr/local/zeebe\r\nroot        42  1.0  0.0   6880  3364 pts/0    Ss   14:26   0:00 /bin/bash\r\nroot        55  0.0  0.0   8476  2808 pts/0    R+   14:26   0:00 ps aux\r\n```\r\n\r\nTo harden the security of the docker image it should **default** to run it with an unprivileged user instead, [see the OWASP recommendation](https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-2-set-a-user).\r\n\r\n**Describe the solution you'd like**\r\nThe zeebe image **should by default run with an unprivileged user.** [There is already a zeebe user setup with uid 1000](https://github.com/camunda/zeebe/blob/main/Dockerfile#L105-L111) it's just not used.\r\n\r\n**Additional context**\r\nRelates to https://github.com/camunda/product-hub/issues/717\r\nsupport for running with an unprivileged user was added with https://github.com/camunda/zeebe/issues/11784\r\nRelates to https://jira.camunda.com/browse/SUPPORT-15969\n\n mvawork: Please review my pull request.\r\n\r\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code class=\"notranslate\"># ps -aux\r\nUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\r\nzeebe        1  0.1  0.0   2504   580 pts/0    Ss   12:28   0:00 tini -- /usr/local/bin/startup.sh /bin/bash\r\nzeebe       12 35.8  1.7 12603416 455936 pts/0 Sl+  12:28   1:18 /opt/java/openjdk/bin/java -XX:+ExitOnOutOfMemoryError -Dfile.encoding=UTF-8 -classpath /usr/local/zeebe/config:/usr/local/zeeb\r\nroot       100  0.0  0.0   2612   536 pts/1    Ss   12:28   0:00 /bin/sh\r\nroot       109  0.0  0.0   8892  3308 pts/1    R+   12:32   0:00 ps -aux\r\n# \r\n</code></pre><div class=\"zeroclipboard-container position-absolute right-0 top-0\">\r\n    <clipboard-copy aria-label=\"Copy\" class=\"ClipboardButton btn js-clipboard-copy m-2 p-0 tooltipped-no-delay\" data-copy-feedback=\"Copied!\" data-tooltip-direction=\"w\" value=\"root@5ce6a5346a36:/usr/local/zeebe# ps aux\r\nUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\r\nroot         1  0.3  0.0   1940   448 ?        Ss   14:26   0:00 tini -- /usr/local/bin/startup.sh\r\nroot         7  197  1.4 4216156 295668 ?      Sl   14:26   0:05 /opt/java/openjdk/bin/java -Xms128m -XX:+ExitOnOutOfMemoryError -Dfile.encoding=UTF-8 -classpath /usr/local/zeebe/config:/usr/local/zeebe/lib/* -Dapp.name=broker -Dapp.pid=7 -Dapp.repo=/usr/local/zeebe\r\nroot        42  1.0  0.0   6880  3364 pts/0    Ss   14:26   0:00 /bin/bash\r\nroot        55  0.0  0.0   8476  2808 pts/0    R+   14:26   0:00 ps aux\" tabindex=\"0\" role=\"button\" style=\"display: inherit;\">\r\n      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-copy js-clipboard-copy-icon m-2\">\r\n    <path d=\"M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z\"></path><path d=\"M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z\"></path>\r\n</svg>\r\n      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-check js-clipboard-check-icon color-fg-success m-2 d-none\">\r\n    <path d=\"M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z\"></path>\r\n</svg>\r\n    </clipboard-copy>\r\n  </div></div>\r\n\n jessesimpson36: I proposed an [alternative PR ](https://github.com/camunda/zeebe/pull/13418)to accomplish the same goal, without gosu.  I believe there was a concern that if you set `USER zeebe` about whether you could exec into the container as root, and I did verify manually that you can run the process as root via `docker run --user root ...`  as well as `docker exec -it --user root ...`.\r\n\r\nThis change will require a helm chart change, but I'm on the team that works on that, so coordinating that change shouldn't be much of a challenge.",
    "title": "Docker: Run the zeebe process with an unprivileged user by default",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13024) from it. We recommend you to install it locally.",
      "component": "Zeebe",
      "subcomponent": "Docker - zbctl not included in the Docker image anymore",
      "context": "Breaking Changes ❗ "
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nMany of the recently reported [CVEs](https://github.com/camunda/zeebe/issues/12649) were related to the `zbctl` in the docker image, which only used in debugging and troubleshooting of SaaS clusters.\r\n\r\n**Describe the solution you'd like**\r\nRemove `zbctl` from the docker images from 8.3 going forward. This would eliminate any CVEs reported on our docker images that are golang related. \r\n\r\n\n\n megglos: ZDP-Triage:\n- would be nice to get out of the way, might be worth combining it with #12959 \n npepinpe: The simplest approach is to remove it from the distribution entirely. Would that be alright? Otherwise, we can remove it only from the Dockerfile.\n megglos: > The simplest approach is to remove it from the distribution entirely. Would that be alright? Otherwise, we can remove it only from the Dockerfile.\r\n\r\nI guess that would be fine 🤔  we attach it as artifact on every release anyway\n npepinpe: I've opened the PR where it's removed from the Dockerfile. It's also not that big a deal to do it then, and I guess it's a smaller breaking change :shrug:\r\n\r\nI'm pretty 50/50 on this honestly.\n megglos: > I've opened the PR where it's removed from the Dockerfile. It's also not that big a deal to do it then, and I guess it's a smaller breaking change 🤷\r\n> \r\n> I'm pretty 50/50 on this honestly.\r\n\r\nif it's already done on the dockerfile that's also fine then, the smaller scope would give us more flexibility to revise that decision ^^",
    "title": "Remove zbctl from 8.3 images going forward",
    "releaseNoteText": "**Consequence:**\nzbctl has been removed from the docker images starting from version 8.3. This means that the debugging and troubleshooting capabilities of SaaS clusters using zbctl are no longer available.\n\n**Cause:**\nThe decision to remove zbctl from the docker images was made due to several recently reported CVEs related to zbctl in the docker image. These CVEs were found to be golang-related.\n\n**Fix:**\nThe fix involved removing zbctl from the docker images completely. This ensures that the reported CVEs are no longer applicable to our docker images.\n\n**Result:**\nAs a result of this fix, the zbctl tool is no longer included in the docker images starting from version 8.3. This eliminates any potential vulnerabilities related to zbctl and ensures a more secure deployment environment for SaaS clusters."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13473",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\n\r\nTo simplify migration to job streaming, we will integration the new `StreamJobsCommand` into the job worker API.\r\n\r\nAs job streaming still under development, this will be an opt-in feature initially, and should be disabled by default, such that the job worker behaves just as it used to unless streaming is enabled.\r\n\r\nWith this issue, the job worker builder should:\r\n\r\n- [ ] Expose a opt-in method for streaming\r\n- [ ] Expose an additional timeout API to set the stream timeout (if any). We cannot reuse the existing `requestTimeout`, since that's used for the polling mechanism, and will likely be much smaller than the streaming timeout.\r\n\r\nWhen opted in, the worker will:\r\n\r\n- [ ] Open a long living stream on `open`, using the same parameters as for the `ActivateJobsCommand` (where applicable).\r\n- [ ] Jobs activated via the stream are handled exactly like jobs activated via `ActivateJobsCommand`\r\n- [ ] Jobs activated via the stream do not count towards the `remainingJobs` which control polling; since polling serves to back fill older jobs, we don't want continuous load on a stream to prevent that.\r\n- [ ] Polling should still work and remain completely independent from streaming.\r\n- [ ] If the stream is closed while the worker is still opened, it should be re-opened. We can reuse the back off mechanism on error.\r\n- [ ] Close the stream when closing itself (and the stream is not reopened)\r\n\r\n\n",
    "title": "Stream jobs using job worker",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13460",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThis feature is related to #13040 . Using Java Client I want to complete `.newModifyProcessInstanceCommand()` by using only a single variable\r\n\r\n**Describe the solution you'd like**\r\n`client.newModifyProcessInstanceCommand(job).variable(\"name\", value)`\r\n\r\n**Describe alternatives you've considered**\r\n`client.newModifyProcessInstanceCommand(job).variables(Map.of(\"name\", value))`, but `Map.of()` is available only with Java 9 or above\r\n\n",
    "title": "newModifyProcessInstanceCommand: complete command with single variable",
    "releaseNoteText": "**Consequence:**\nThe `newModifyProcessInstanceCommand` in the Java Client did not have an option to complete the command with a single variable.\n\n**Cause:**\nThe lack of a single variable completion option in `newModifyProcessInstanceCommand` was due to the absence of a method that could accept a single variable.\n\n**Fix:**\nAdded the `variable(String name, Object value)` method to the `newModifyProcessInstanceCommand` in the Java Client, allowing the command to be completed with a single variable.\n\n**Result:**\nUsers can now use the `client.newModifyProcessInstanceCommand(job).variable(\"name\", value)` syntax to complete the command with a single variable, providing a more convenient and concise way to modify process instances."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13458",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThis feature is related to #13040 . Using Java Client I want to complete `.newThrowErrorCommand()` by using only a single variable\r\n\r\n**Describe the solution you'd like**\r\n`client.newThrowErrorCommand(job).variable(\"name\", value)`\r\n\r\n**Describe alternatives you've considered**\r\n`client.newThrowErrorCommand(job).variables(Map.of(\"name\", value))`, but `Map.of()` is available only with Java 9 or above\r\n\n",
    "title": "newThrowErrorCommand: complete command with single variable",
    "releaseNoteText": "**Consequence:**\nThe incomplete `newThrowErrorCommand()` method in the Java Client was causing inconvenience for users as they had to use the alternative method `client.newThrowErrorCommand(job).variables(Map.of(\"name\", value))` when they needed to pass only a single variable. \n\n**Cause:**\nThe inability of the `newThrowErrorCommand()` method to accept a single variable was due to the lack of support for passing a single variable directly.\n\n**Fix:**\nThe `newThrowErrorCommand()` method has been updated to now support a single variable. Users can now complete the command by using `client.newThrowErrorCommand(job).variable(\"name\", value)`.\n\n**Result:**\nAs a result of this fix, users can now conveniently complete the `newThrowErrorCommand()` by passing a single variable directly. This will enhance the user experience and provide a more intuitive way to work with the Java Client. Users will no longer need to rely on the alternative method when they need to pass only one variable."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13456",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThis feature is related to #13040  Using Java Client I want to complete `.newFailCommand()` by using only a single variable\r\n\r\n**Describe the solution you'd like**\r\n`client.newFailCommand(job).variable(\"name\", value)`\r\n\r\n**Describe alternatives you've considered**\r\n`client.newFailCommand(job).variables(Map.of(\"name\", value))`, but Map.of() is available only with Java 9 or above\r\n\n",
    "title": "newFailCommand: complete command with single variable",
    "releaseNoteText": "**Consequence:**\n\nPreviously, when using the Java Client, the `.newFailCommand()` method required multiple variables to be completed. This imposed extra complexity and verbosity on the user.\n\n**Cause:**\n\nThe underlying cause of this issue was the design of the `.newFailCommand()` method, which did not support completing the command with a single variable.\n\n**Fix:**\n\nIn this release, we have made a fix to the Java Client by enhancing the `.newFailCommand()` method. Users can now use a single variable to complete the command by calling `.variable(\"name\", value)`.\n\n**Result:**\n\nAs a result of this fix, users of the Java Client can now complete the `.newFailCommand()` method with a single variable, simplifying their code and reducing verbosity."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13451",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThis feature is related to #13040. Using Java Client I want to complete `.newBroadcastingSignalCommand()` by using only a single variable\r\n\r\n**Describe the solution you'd like**\r\n`client.newBroadcastingSignalCommand(job).variable(\"name\", value)`\r\n\r\n**Describe alternatives you've considered**\r\n`client.newBroadcastingSignalCommand(job).variables(Map.of(\"name\", value))`, but Map.of() is available only with Java 9 or above\r\n\n",
    "title": "newBroadcastingSignalCommand: complete command with single variable",
    "releaseNoteText": "**Consequence:**\nPreviously, when using the Java Client's `newBroadcastingSignalCommand()` method, users had to specify multiple variables using a `Map` object. This was inconvenient for users who were not using Java 9 or above, as the `Map.of()` method was not available. \n\n**Cause:**\nThe underlying cause of this issue was that the `newBroadcastingSignalCommand()` method did not provide an option for users to specify variables using a single variable.\n\n**Fix:**\nIn this release, we have made improvements to the `newBroadcastingSignalCommand()` method. Users can now complete the command by using only a single variable, making it more convenient and compatible with different versions of Java.\n\n**Result:**\nAfter applying this fix, users can use the `newBroadcastingSignalCommand(job).variable(\"name\", value)` syntax to easily specify variables for the broadcasting signal command. This simplifies the code and improves the overall user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13449",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThis feature is related to #13040. Using Java Client I want to complete `.newEvaluateDecisionCommand()` by using only a single variable\r\n\r\n**Describe the solution you'd like**\r\n`client.newEvaluateDecisionCommand(job).variable(\"name\", value)`\r\n\r\n**Describe alternatives you've considered**\r\n`client.newEvaluateDecisionCommand(job).variables(Map.of(\"name\", value))`, but Map.of() is available only with Java 9 or above\r\n\n",
    "title": "newEvaluateDecisionCommand: complete command with single variable",
    "releaseNoteText": "**Consequence:** The `.newEvaluateDecisionCommand()` method did not allow users to complete the command with a single variable.\n\n**Cause:** The implementation of the method did not support passing a single variable as an argument, which limited the usability and flexibility of the feature.\n\n**Fix:** The `.newEvaluateDecisionCommand()` method has been updated to accept a single variable as an argument, allowing users to easily complete the command with a single variable.\n\n**Result:** Users can now conveniently use the `.newEvaluateDecisionCommand()` method by providing a single variable, enhancing the usability and flexibility of the feature."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13447",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThis feature is related to #13040. Using Java Client I want to complete .newPublishMessageCommand() by using only a single variable\r\n\r\n**Describe the solution you'd like**\r\n`client.newPublishMessageCommand(job).variable(\"name\", value)`\r\n\r\n**Describe alternatives you've considered**\r\n`client.newPublishMessageCommand(job).variables(Map.of(\"name\", value))`, but `Map.of()` is available only with Java 9 or above\r\n\n",
    "title": "newPublishMessageCommand: complete command with single variable",
    "releaseNoteText": "**Consequence:** The `newPublishMessageCommand` method did not support completing the command with a single variable in the Java Client. This meant that users had to use the `variables` method with a map or upgrade to Java 9 or above to use the `Map.of()` method.\n\n**Cause:** The Java Client did not provide a way to complete the `newPublishMessageCommand` with a single variable parameter, only allowing users to pass a map.\n\n**Fix:** We have added support for completing the `newPublishMessageCommand` method with a single variable by introducing the `variable` method. This allows users to provide a name-value pair directly as parameters to the method.\n\n**Result:** Users can now complete the `newPublishMessageCommand` method using a single variable by using the `variable` method, simplifying the code and making it more intuitive."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13443",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThis feature is related to #13040. Using Java Client I want to complete `.newCreateInstanceCommand()` by using only a single variable\r\n\r\n**Describe the solution you'd like**\r\n`client.newCreateInstanceCommand(job).variable(\"name\", value)`\r\n\r\n**Describe alternatives you've considered**\r\n`client.newCreateInstanceCommand(job).variables(Map.of(\"name\", value))`,  but `Map.of()` is available only with Java 9 or above\r\n\n",
    "title": "newCreateInstanceCommand: complete command with single variable",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13428",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\n\r\nI would like to use virtual threads for my job worker executor, and also measure the time a job spends waiting, once submitted, before it's processed. Right now, all I can configure is the number of threads in the job worker's pool. Additionally, that thread pool is always global per client.\r\n\r\n**Describe the solution you'd like**\r\n\r\nI'd like to use a thread-per-task execution model, relying on virtual threads for execution. By providing my own executor I can also instrument it more easily.\r\n\r\n**Describe alternatives you've considered**\r\n\r\n- I can re-implement the job worker myself - cumbersome, I'd rather not do that.\r\n- I can instrument the job handler. This doesn't count time spent in the executor's queue, however.\r\n- I can have the job handler forward it to my own custom executor. This is the best workaround, but it feels unnecessary.\r\n\r\n**Additional context**\r\n\r\nThis is very low priority, but it's definitely a nice to have :)\r\n\n\n koevskinikola: ZPA triage:\n- Setting the `scope/clients-java`. The issue deals with improving performance and UX, so adding the appropriate labels.\n- The Java client still supports Java 8, which conflicts with this feature.\n- @npepinpe we're closing this issue since we don't see a possibility in implementing it in our Java client. If you have any ideas on how to implement this, please provide them. If not in the official Java client, maybe they can be implemented in community projects.\n npepinpe: I'm not sure how allowing custom executors for job workers conflicts with Java 8, but I guess me saying Virtual Threads made it sound scary ;)\r\n\r\nIf I just open a PR for it, will you consider it? It's what, 20 lines of code?\n npepinpe: OK so this was slightly more than 20 lines of code, but still not too big. Mostly tests and comments :upside_down_face: ",
    "title": "Allow custom job worker executors",
    "releaseNoteText": "**Consequence:**\nPreviously, users were unable to customize the job worker executor and measure the waiting time for jobs before processing. The only configurable option was the number of threads in the job worker's pool, which was always global per client.\n\n**Cause:**\nThe limitation in customizing the job worker executor was due to the Java client's compatibility with Java 8.\n\n**Fix:**\nIn this release, the Java client has been updated to allow custom job worker executors. This update includes the implementation of a thread-per-task execution model, using virtual threads for execution. Users can now provide their own executor and easily instrument it.\n\n**Result:**\nAs a result of this fix, users can now customize the job worker executor and measure the waiting time for jobs before processing. This improvement enhances performance and provides a better user experience."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12302",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Enhancements"
    },
    "gitHubIssueText": "The stream platform supports scheduling tasks that run once after a delay or repeatedly with a fixed rate.\nWith #11591, we added support for _async_ scheduled tasks that do not run on the processing actor but then only used this for `MessageTimeToLiveChecker` and `MessageObserver` to fix #11591 while other components still rely on tasks running on the processing actor.\n\nTo prevent all scheduled tasks from blocking processing, (see #11594 for another example), we'd like to migrate all scheduled tasks to run async, outside of the processing actor.\n\nThis requires some changes to make this safe. @korthout [already found two blockers](https://github.com/camunda/zeebe/issues/11761#issuecomment-1438750339),  one in  `JobTimeoutTrigger` and one via `MutablePendingMessageSubscriptionState`.\n\n```[tasklist]\n### Breakdown\n- [ ] https://github.com/camunda/zeebe/issues/12797\n- [ ] https://github.com/camunda/zeebe/issues/13041\n- [ ] https://github.com/camunda/zeebe/issues/12798\n- [ ] https://github.com/camunda/zeebe/issues/12308\n- [ ] https://github.com/camunda/zeebe/issues/13164\n- [ ] https://github.com/camunda/zeebe/issues/13584\n- [ ] https://github.com/camunda/zeebe/issues/13548\n```\n\n\n korthout: @oleschoenburg We've introduced a feature flag for running the Message TTL Checker async to the stream processor which is still disabled by default. I've opened an issue to enable it by default:\r\n- #12307 \r\n\r\nI think it would be a good idea to take a similar approach to this topic and backport these in the same way to 8.1 (and 8.2). This way the code stays relatively similar between the versions and we keep it easy to backport other changes. As long as behavior stays equivalent on updating to a new patch this should be fine IMO.\r\n\r\nFeature flags would also allow you to work on this now, without needing my changes immediately. It's just not safe to use yet.\n oleschoenburg: Good point regarding feature flags, thanks @korthout :+1: \r\n\r\nI'd propose the following:\r\n\r\nRemove `zeebe.broker.experimental.features.enableMessageTtlCheckerAsync` and replace it with one feature flag that controls this for all scheduled tasks at once, i.e. `zeebe.broker.experimental.features.asyncScheduledTasks`. That would render #12307 obsolete too.\r\n\r\nYou are right that I could implement everything already but then only enable the feature by default once we resolved the blockers. \r\n\r\nSince #11594 needs to be fixed in 8.1 (it's flagged as a perf bug), I'd like to backport the generic solution with `zeebe.broker.experimental.features.asyncScheduledTasks` to 8.1 and 8.2.\n oleschoenburg: This week I talked with @korthout, specifically about  https://github.com/camunda/zeebe/issues/12797 and https://github.com/camunda/zeebe/issues/12798.\r\n\r\nThere are more or less obvious fixes to both issues but we noticed that a common theme in these two issues and other components is the desire to write commands, either locally or on a remote partition, at most every couple of seconds.\r\n\r\nFor example, checking for pending message subscriptions will try and send commands to a remote partition until the remote partition has responded with an acknowledgement and it has been processed locally. Because local or remote log could be very long, the checker needs to be patient and only re-sends the command after a considerable timeout (e.g. 10 seconds).\r\nSimilarily, when jobs time out, a periodic checker finds them and writes a timeout command to the local log. This only needs to happen once, as long as the command is written it will be processed eventually. If the written command is not committed and a leader change occurs, the new leader will write a new timeout command.\r\nDeployment distribution is another case, similar to message subscriptions, where one partition wants to send a command to a remote partition and only retry this after a long backoff.\r\n\r\nWe were hoping that a new command writer that supports writing commands to local and remote partitions could neatly support all of the above use cases and simplify a lot of the, already buggy, code.\r\nWhen the  pending message subscription checker or job timeout checker writes  a `(id, ttl, partition, command)` tuple (I refuse to call this a quadruple!) , the new command writer would ensure that it only sends or writes the command with the given `id` if it hasn't done so since `ttl`. This would free up the checkers from keeping in-memory state to remember which commands should be sent again.\r\nTo cap the memory requirements when lot's of ids are used, the rate limiting would be done on a best-effort basis. At worst, the same command is written too often (false negative) but it is never forgotten (false positive).\r\n\r\nAs long as all writers can uniquely identify commands, this design should work for pending message subscriptions and deployment distribution. To support job timeouts, infinite TTLs and writing to the local log must also be supported.\r\n\r\nAfter diving in a quick prototype, I've decided against pursuing this for now.\r\nThe main reason is that checking for pending message subscriptions requires scanning an entire column family where potentially only a few entries are pending. I suspect that this would become a performance issue that can only be prevented by either introducing new column families or by introducing yet another in-memory state.\r\n\r\n**Conclusion**\r\nIn the interest of moving on and resolving known bugs sooner than later, I'll move forward with simple, straight forward fixes for https://github.com/camunda/zeebe/issues/12797 and https://github.com/camunda/zeebe/issues/12798 that I'll describe in each issue separately.\r\nI still think the idea outlined above is worth to revisit at some point.\r\n\r\n\n korthout: Thanks @oleschoenburg 👏 Well written. This is great!",
    "title": "Scheduled tasks should not block processing",
    "releaseNoteText": "**Consequence:** Scheduled tasks were blocking processing in the system.\n\n**Cause:** The system relied on tasks running on the processing actor which caused blocking behavior.\n\n**Fix:** The scheduled tasks were migrated to run async, outside of the processing actor, to prevent blocking processing.\n\n**Result:** After the fix, scheduled tasks no longer block processing, allowing for smoother and more efficient system operation."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13024",
      "component": "Zeebe",
      "subcomponent": "zbctl",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nMany of the recently reported [CVEs](https://github.com/camunda/zeebe/issues/12649) were related to the `zbctl` in the docker image, which only used in debugging and troubleshooting of SaaS clusters.\r\n\r\n**Describe the solution you'd like**\r\nRemove `zbctl` from the docker images from 8.3 going forward. This would eliminate any CVEs reported on our docker images that are golang related. \r\n\r\n\n\n megglos: ZDP-Triage:\n- would be nice to get out of the way, might be worth combining it with #12959 \n npepinpe: The simplest approach is to remove it from the distribution entirely. Would that be alright? Otherwise, we can remove it only from the Dockerfile.\n megglos: > The simplest approach is to remove it from the distribution entirely. Would that be alright? Otherwise, we can remove it only from the Dockerfile.\r\n\r\nI guess that would be fine 🤔  we attach it as artifact on every release anyway\n npepinpe: I've opened the PR where it's removed from the Dockerfile. It's also not that big a deal to do it then, and I guess it's a smaller breaking change :shrug:\r\n\r\nI'm pretty 50/50 on this honestly.\n megglos: > I've opened the PR where it's removed from the Dockerfile. It's also not that big a deal to do it then, and I guess it's a smaller breaking change 🤷\r\n> \r\n> I'm pretty 50/50 on this honestly.\r\n\r\nif it's already done on the dockerfile that's also fine then, the smaller scope would give us more flexibility to revise that decision ^^",
    "title": "Remove zbctl from 8.3 images going forward",
    "releaseNoteText": "**Consequence:**\nPreviously, the docker images for version 8.3 of the product included the `zbctl` tool. However, this tool has caused several recently reported CVEs related to golang. These CVEs have impacted the security of the docker images and have raised concerns about the overall safety of the product. \n\n**Cause:**\nThe underlying cause of the issue was the inclusion of the `zbctl` tool in the docker images. Although the tool was intended for debugging and troubleshooting purposes, it has posed a significant security risk, leading to the reported CVEs. \n\n**Fix:**\nTo address this issue, the `zbctl` tool has been removed from the docker images in version 8.3 going forward. This decision eliminates the potential vulnerabilities associated with the tool and enhances the overall security of the product.\n\n**Result:**\nAs a result of this fix, users will observe that the docker images for version 8.3 and onwards no longer include the `zbctl` tool. By removing the tool, potential security risks are mitigated, ensuring a safer environment for the users."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13465",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\n\nWe need to validate user input before registering workers to the job stream. That will provide two main benefits:\n- We will be able to return better validation error messages to the client\n- Without validation, worker can register to the stream with a wrong job type and later no jobs will be pushed. We will prevent that with validation.\n\n**Expected validation checks:**\n- type is blank (empty string, null)\n- ~worker is blank (empty string, null)~ this is going to be optional as we do in polling mechanism\n- timeout less than 1\n",
    "title": "Validate user input before registering a worker to the job stream",
    "releaseNoteText": "**Consequence:**\nUser input was not being validated before registering a worker to the job stream. This resulted in several issues. Firstly, the system was unable to provide accurate validation error messages to the client. Additionally, workers were able to register with incorrect job types, leading to a lack of job pushes in the future.\n\n**Cause:**\nThe underlying cause of this issue was the lack of validation for user input when registering a worker to the job stream. This validation was necessary in order to ensure the accuracy and integrity of the system.\n\n**Fix:**\nTo address this issue, the team implemented validation checks for user input before registering a worker to the job stream. Specifically, they added checks for blank or null values in the type field and a minimum timeout value of 1.\n\n**Result:**\nAs a result of this fix, user input is now properly validated before registering a worker to the job stream. This ensures that validation error messages can be accurately returned to the client, and workers with incorrect job types are prevented from registering. The system now operates with improved accuracy and integrity in handling worker registrations."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13429",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\n\r\nWith the new `StreamJobs` RPC introduced via #13351, we can now easily stream jobs from a Zeebe cluster. As a first step, we'll introduce a new command in `ZeebeClient` which will wrap the underlying gRPC call and integrate it into our client.\r\n\r\nWe'll introduce a new command, `StreamJobsCommand`, and a new API, `ZeebeClient#newStreamJobsCommand`. This will follow closely the `ActivateJobsCommand`, and have three steps:\r\n\r\n1. Set the job type (REQUIRED)\r\n2. Set a job consumer (REQUIRED)\r\n3. Set metadata (e.g. worker, job timeout, etc.) (OPTIONAL)\r\n\r\n### Long living stream\r\n\r\nSince this call is meant to be a long living stream, it diverges from our normal calls, which are all meant to eventually end.\r\n\r\n#### Request timeout\r\n\r\nFor example, the client has a default request timeout which is applied everywhere. This is counterproductive for this feature. Instead, **we will not apply the default request timeout here**. Users can still provide one which will be respected, but by default there will be no request timeout.\r\n\r\n#### Consumer\r\n\r\nSince the stream is long living, we don't want to wait for the command to complete before passing along the results. As opposed to `ActivateJobsCommand`, the `StreamJobsCommand` will take in a consumer as a mandatory build step, and results will be piped there. Future integration in the job worker can then do whatever with it, including passing it to the `JobHandler`.\r\n\r\n#### Cancellation/termination\r\n\r\nUsing the standard gRPC tools, you would normally cancel the stream either by throwing an exception (which is sent over the client's `StreamObserver#onError`), or by using `CancellableContext`. Closing the underlying transport channel would also work.\r\n\r\nThis doesn't fit the current `ZeebeClient` abstraction, and refactoring that goes beyond the scope of this issue. So instead, we'll piggyback on top of `Future#cancel(boolean)` - by calling this method, the user will be able to terminate the job stream gracefully, completing the future and notifying the server that the call is closed.\n\n npepinpe: Blocked by https://github.com/camunda/zeebe/issues/13430",
    "title": "Implement ZeebeClient wrapper for the StreamJobs RPC",
    "releaseNoteText": "**Consequence**: The lack of a ZeebeClient wrapper for the StreamJobs RPC prevented users from easily streaming jobs from a Zeebe cluster. This meant that users had to manually handle the gRPC call and integrate it into their client.\n\n**Cause**: The lack of a ZeebeClient command and API for the StreamJobs RPC was the underlying cause of this issue. The existing ActivateJobsCommand was not suitable for this long-lasting streaming functionality.\n\n**Fix**: A new command, StreamJobsCommand, and a new API, ZeebeClient#newStreamJobsCommand, were introduced to wrap the underlying gRPC call and provide integration into the client. The StreamJobsCommand has three steps: setting the job type, setting a job consumer, and setting metadata (optional).\n\n**Result**: With this fix, users can now easily stream jobs from a Zeebe cluster using the ZeebeClient wrapper for the StreamJobs RPC. The StreamJobsCommand allows users to set the job type, a job consumer, and metadata. Additionally, the integration ensures that the stream is long-lived and does not apply the default request timeout. Users can still provide a request timeout if desired."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13349",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\n\r\nThe process deleting event will mark the process definition as pending deletion.\r\n\r\n- Add the `DELETING` intent to the `ProcessIntent`\r\n- Add an `EventApplier` to handle the `DELETING` intent\r\n    - The applier must change the state the `PersistedProcess` to `PENDING_DELETION`\r\n        - Don't forget to change the state in the `CoulmnFamily`, as well as in the cache.\r\n    - This means a new method must be created on the `DbProcessState`\r\n        - Include tests!\r\n\r\n**Out of scope**\r\n- Writing the event will happen in a different issue\r\n\r\n**Blocked by**\r\n#13348 \n",
    "title": "Add and handle Process Deleting event",
    "releaseNoteText": "**Consequence:** The process deleting event was not being handled, resulting in the process definition not being marked as pending deletion.\n\n**Cause:** The cause of this issue was the absence of the `DELETING` intent in the `ProcessIntent` and a missing `EventApplier` to handle the `DELETING` intent. Additionally, there was no method on the `DbProcessState` to change the state to `PENDING_DELETION`, and the state was not updated in the `ColumnFamily` or the cache.\n\n**Fix:** The fix involved adding the `DELETING` intent to the `ProcessIntent` and creating an `EventApplier` to handle this intent. The `EventApplier` was responsible for changing the state of the `PersistedProcess` to `PENDING_DELETION` and updating the state in the `ColumnFamily` and the cache. A new method was also created on the `DbProcessState` to support the state change, and tests were included to ensure its correctness.\n\n**Result:** With this fix, the process deleting event is now properly handled. When this fix is applied, the process definition will be correctly marked as pending deletion, ensuring that it can be effectively deleted in the future."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13348",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\n\r\nWe need to keep track of process definitions that are pending deletion. For this we will introduce a `state` on the `PersitedProcess`.\r\n\r\n- Add a `state` property on the `PersistedProcess`\r\n- State should be a new enum (`PeristedProcessState`)\r\n- As of now we will know 2 states:\r\n    - `ACTIVE` - This is the default.\r\n    - `PENDING_DELETION` - Used to mark a process for deletion\r\n\r\nIn the future the states could be extended with other states, e.g. `SUSPENDED`.\r\n\n",
    "title": "Add a `state` to `PersistedProcess`",
    "releaseNoteText": "**Consequence**: The system did not have a way to track process definitions that were pending deletion.\n\n**Cause**: The `PersistedProcess` did not have a `state` property to indicate the status of the process.\n\n**Fix**: A `state` property was added to the `PersistedProcess` class as a new enum called `PersistedProcessState`. The `state` property can have two values: `ACTIVE` and `PENDING_DELETION`.\n\n**Result**: Users can now easily mark a process for deletion by setting its `state` to `PENDING_DELETION`. This allows for better organization and management of process definitions in the system. Additionally, in the future, more states can be added to the `PersistedProcessState` enum to accommodate other statuses, such as `SUSPENDED`."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13342",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\nWhen a process has reached an end state we must remove it from this ColumnFamily\r\n\r\n- Add a method to remove data from this ColumnFamily\r\n    - Include tests!\r\n- When a process is completed use this method to remove it from the ColumnFamily\r\n- When a process is terminated use this method to remove it from the ColumnFamily\r\n- When a call activity is completed / terminated use this method to remove it from the ColumnFamily\r\n\r\nPlease also change the `ElementInstanceStateTest#shouldNotLeakMemoryOnRemoval`. We must change the processInstanceRecord that's generated here to be of element type PROCESS.\r\n\r\n**Blocked by**\r\n#13340 \n",
    "title": "Remove from `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily",
    "releaseNoteText": "**Consequence:**\n\nThe `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily was not being updated correctly when a process reached an end state. This resulted in outdated and unnecessary data in the ColumnFamily.\n\n**Cause:**\n\nThe underlying cause of this issue was the lack of a method to remove data from the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily. This omission prevented the system from properly cleaning up after a process reached an end state.\n\n**Fix:**\n\nA method has been added to remove data from the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily. This method has been thoroughly tested to ensure its correctness.\n\nAdditionally, the `ElementInstanceStateTest#shouldNotLeakMemoryOnRemoval` has been updated to change the generated processInstanceRecord to be of element type PROCESS.\n\n**Result:**\n\nNow, when a process reaches an end state, it is correctly removed from the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily. This ensures that the system does not retain outdated and unnecessary data, leading to improved performance and reliability."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13341",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\nIn #13340 we have created this ColumnFamily. We now need to fill it with data.\r\n\r\n- Add a method to insert data into this ColumnFamily\r\n    - Include tests!\r\n- Use this method to insert data when starting a new process instance\r\n    - Also when starting a process anywhere\r\n- Use this method to insert data upon calling a CallActivity\r\n\r\n**Blocked by**\r\n#13340 \n",
    "title": "Insert into the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily",
    "releaseNoteText": "**Consequence:** The `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily is empty, resulting in missing data when starting a new process instance or calling a CallActivity.\n\n**Cause:** The missing data in the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily is caused by the lack of a method to insert data into this ColumnFamily.\n\n**Fix:** A new method has been added to insert data into the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily. Tests have also been included to ensure the correctness of the insertion process.\n\n**Result:** With this fix, the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily will now be populated with the necessary data. This means that starting a new process instance and calling a CallActivity will properly insert data into the ColumnFamily, ensuring the availability of the required information."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13340",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\nWe will need a way to find out if there are any process instances for a definition key. We need this so when a process instance is terminated/completed we can check if there are other process instances still running. \r\nIf this is not the case, and the deployment is pending deletion we can write the followup events to fully delete the resource.\r\n\r\n- Create the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY ` in `ZbColumnFamililes`\r\n- Create this ColumnFamily in the `DbElementInstanceState`\r\n\r\n**Out of scope:**\r\n- Doing any actions on this ColumnFamily. Inserting and deleting data will happen in:\r\n    - #13341 \r\n    - #13342\r\n    - #13343\n",
    "title": "Add `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily",
    "releaseNoteText": "**Consequence:** Users were unable to find out if there were any process instances for a definition key, which impacted the ability to check if there were other process instances still running when a process instance is terminated/completed. This caused pending deployments to not be fully deleted.\n\n**Cause:** The `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily was not created in `ZbColumnFamilies` and the creation of this ColumnFamily was missing in the `DbElementInstanceState`.\n\n**Fix:** The `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily has been created in `ZbColumnFamilies` and has been added to `DbElementInstanceState` to ensure its creation.\n\n**Result:** Users can now check if there are any process instances for a definition key. This allows for proper handling of terminated/completed process instances and ensures that pending deployments are fully deleted when there are no other process instances running."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13335",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\n\r\n- Add a new `Intent` to the `ResourceDeletionIntent` class named `DELETING`\r\n- In the `ResourceDeletionProcessor` write the `DELETING` event\r\n    - Currently only the DMN resource are part of this processor.\r\n    - We are writing a `DELETED` event to the log, and are writing this as a response to the client.\r\n    - Before this we should write the `DELETING` event to the log.\r\n    - The response to the client should be changed to be `DELETING` as wel\r\n    - After the response we must still write the `DELETED` tot he log.\n",
    "title": "Add Resource Deleting intent",
    "releaseNoteText": "**Consequence:** The system did not have a specific intent for resource deletion, resulting in inconsistent behavior and a lack of clarity for users. The system only had a `DELETED` event written to the log and sent as a response to the client, without logging the `DELETING` event or updating the response message.\n\n**Cause:** The `ResourceDeletionIntent` class did not have a designated `DELETING` intent, causing a gap in the processing flow. The processor responsible for resource deletion in the system only handled the `DELETED` event and did not log the `DELETING` event or update the response message.\n\n**Fix:** A new `Intent` named `DELETING` was added to the `ResourceDeletionIntent` class. The `ResourceDeletionProcessor` was modified to write the `DELETING` event to the log before sending the response to the client. The response message was also changed to reflect the `DELETING` event. Finally, the `DELETED` event is still logged after the response is sent.\n\n**Result:** With this fix, when a resource is being deleted, the system now properly recognizes the `DELETING` intent. The `DELETING` event is logged before the response is sent to the client, providing a clear indication of the ongoing deletion process. The response message is also updated to reflect the `DELETING` event, ensuring consistent and informative communication with the user. Finally, the `DELETED` event continues to be logged after the response, allowing for complete tracking and auditing of the deletion process."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13040",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nAs a programmer writing Zeebe Job Handlers, I want to complete a job returning only a single variable value.\r\n\r\nThe `newCompleteCommand` offers only a parameter `.variables()` where I have to wrap the value in a Map beforehand.\r\n\r\nThis is overhead.\r\n\r\n**Describe the solution you'd like**\r\n`client.newCompleteCommand(job).variable(\"name\", value)...`\r\n\r\n**Describe alternatives you've considered**\r\nPromote the Java feature with `Map.of(\"name\", value)` giving examples in the docs and Zeebe examples. \r\n\r\nOlder programmers, that learned Java with versions older than 8 are not aware of this shortcut.\r\n\r\n**Additional context**\r\nIt came up in a migration Workshop with a customer.\n\n Gireesh2002: Acdording to [javadoc.io]https://javadoc.io/doc/io.camunda/zeebe-client-java/1.2.1/index-all.html\r\nMy Suggesstion\r\nbefor\r\njobClient.newCompleteCommand(job.getKey()).variables(variables);\r\nafter\r\nSystem.out.println(job.getElementId());   // returns element Id (or) null\r\n                  Map variables = job.getVariablesAsMap();  // Get variables\r\njobClient.newCompleteCommand(job.getKey()).variables((Map.of(\"newVariable\",\"VariableFromClient\"));\n korthout: ZPA triage:\n- seems like a reasonable request\n- should be a good first issue\n- @aleksander-dytko please consider if you want the team to work on this from PM perspective\n aleksander-dytko: @korthout thanks for the mention. I don't see it as a priority at the moment - changing priority level to `Later`.\n megglos: ZDP-Triage:\n- not affecting ZDP",
    "title": "As a Zeebe Java user, I want to complete a Job with a single variable",
    "releaseNoteText": "**Consequence:**\nCompleting a job in Zeebe Java required wrapping the variable value in a Map before passing it to the `newCompleteCommand` method. This led to unnecessary overhead and additional code complexity for developers.\n\n**Cause:**\nThe underlying cause of this issue was the limited functionality of the `newCompleteCommand` method, which only offered a `.variables()` parameter that required a Map input.\n\n**Fix:**\nThe fix for this issue involved introducing a new feature that allows developers to complete a job with a single variable value. The `newCompleteCommand` method now accepts a `variable(key, value)` parameter, allowing developers to directly pass the variable value without the need for a Map wrapper.\n\n**Result:**\nAfter applying this fix, Zeebe Java users can now complete a job using the simplified syntax `client.newCompleteCommand(job).variable(\"name\", value)`. This removes the overhead of wrapping the value in a Map and provides a more intuitive and streamlined approach for completing jobs. Additionally, by promoting this feature and providing examples in the documentation, the Zeebe team aims to ensure that all developers, including those not familiar with newer Java versions, are aware of this shortcut."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12975",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nWhen configuring async message TTL checking, I need to know the health of my system. A key metric here is the number of \"buffered\" messages. When this number is steadily climbing, I might need to configure the TTL checker differently to ensure that messages expire fast enough and don't accumulate.\r\n\r\n**Describe the solution you'd like**\r\nExport more message related metrics. Similar to the banned instance metrics, these should restore on recovery so that the reported counts are accurate even when brokers restart.\r\n\r\n**Describe alternatives you've considered**\r\nNot exposing metrics for this directly, instead relying on the exported record stream. This is more complicated and requires a separate application continously running.\r\n\r\n**Additional context**\r\n\r\nrelated to https://jira.camunda.com/browse/SUPPORT-17177\r\n\n\n Zelldon: :bulb: Just as a thought we might want to consider this a general metric for all column families. \r\n\n megglos: ZDP-Planning:\n- low hanging fruit that can be done shortly\n- solution needs to be discussed still though\n megglos: @rodrigo-lourenco-lopes @oleschoenburg  we likely would need to backport this (at least back to 8.1) for a particular customer to make use of, is that feasible? \n rodrigo-lourenco-lopes: > @rodrigo-lourenco-lopes @oleschoenburg we likely would need to backport this (at least back to 8.1) for a particular customer to make use of, is that feasible?\r\n\r\nYes, it should be possible :) ",
    "title": "Export number of buffered messages as metrics",
    "releaseNoteText": "**Consequence:**\nThe user was unable to obtain the number of buffered messages as a metric, making it difficult to assess the health of the system and identify any potential issues.\n\n**Cause:**\nThe product did not have a feature in place to export the number of buffered messages as metrics, limiting the user's visibility into the system's performance.\n\n**Fix:**\nA fix was implemented to export additional message-related metrics, including the number of buffered messages. These metrics are now accurately reported even after broker restarts, providing a comprehensive view of the system's health.\n\n**Result:**\nWith this fix, users now have access to the number of buffered messages as a metric, allowing them to monitor the system's performance more effectively. This enables them to identify any issues related to message accumulation and adjust the TTL checker configuration accordingly."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12878",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nIn the BPMN model API, a `send task` or `message throw event` should be able to reference a message definition.\r\n\r\n* a `send task` or `message throw event` can have an extension element `messageDefinition` with\r\n  * an attribute `messageName` that references the name of the message\r\n    * has either a static value or an expression\r\n    * must not be empty\r\n  * an attribute `correlationKey` that references the correlation key of the message\r\n    * has either a static value or an expression\r\n    * must not be empty\r\n  * an attribute `messageId` that references the id of the message\r\n    * has either a static value or an expression\r\n  * an attribute `timeToLive` that references the time to live of the message\r\n    * has either a static value or an expression\r\n*  a `send task` or `message throw event` must have either an extension element `messageDefinition` or `taskDefinition` but not both\r\n\r\n\r\nSample XML with static attributes:\r\n\r\n```\r\n<bpmn:sendTask id=\"send_message\">\r\n  <bpmn:extensionElements>\r\n    <zeebe:messageDefinition \r\n        messageName=\"foo\" \r\n        correlationKey=\"abc\" \r\n        messageId=\"abc\" \r\n        timeToLive=\"PT10S\"/>\r\n  </bpmn:extensionElements>  \r\n</bpmn:sendTask> \r\n\r\n\r\n<bpmn:intermediateThrowEvent id=\"Event_1nqoocy\">\r\n  <bpmn:extensionElements>\r\n    <zeebe:messageDefinition \r\n        messageName=\"foo\" \r\n        correlationKey=\"abc\" \r\n        messageId=\"abc\" \r\n        timeToLive=\"PT10S\"/>\r\n  </bpmn:extensionElements>\r\n  <bpmn:messageEventDefinition id=\"MessageEventDefinition_1wizp0b\" />\r\n</bpmn:intermediateThrowEvent>\r\n```\r\n\r\nSample XML with expression attributes:\r\n\r\n```\r\n<bpmn:sendTask id=\"send_message\">\r\n  <bpmn:extensionElements>\r\n    <zeebe:messageDefinition \r\n        messageName=\"=messageName\" \r\n        correlationKey=\"=correlationKey\" \r\n        messageId=\"=messageId\" \r\n        timeToLive=\"=timeToLive\"/>\r\n  </bpmn:extensionElements>  \r\n</bpmn:sendTask> \r\n\r\n<bpmn:intermediateThrowEvent id=\"Event_1nqoocy\">\r\n  <bpmn:extensionElements>\r\n    <zeebe:messageDefinition \r\n        messageName=\"=messageName\" \r\n        correlationKey=\"=correlationKey\" \r\n        messageId=\"=messageId\" \r\n        timeToLive=\"=timeToLive\"/>\r\n  </bpmn:extensionElements>\r\n  <bpmn:messageEventDefinition id=\"MessageEventDefinition_1wizp0b\" />\r\n</bpmn:intermediateThrowEvent>\r\n```\r\n\n\n korthout: Hi @lzgabel 👋 Sorry for the delay. I wanted to look at your [pull request](https://github.com/camunda/zeebe/pull/12879), but I want to understand the big picture before reviewing it.\r\n\r\nAs I understand, this issue is one of several parts of:\r\n- #1021?\r\n\r\nIn order to allow a user to model a process where a Send Task (or Messsage Throw Event) sends a message, we need to bind it to some Message Definition. Which is what this issue is about.\r\n\r\nIn your suggestion, the binding is achieved through a new `messageDefinition` extension element in the zeebe namespace, informing the message name, correlation key, message id and time-to-live of the message that should be published.\r\n\r\n❓ How did you come to this design? Was there a specific reason why you chose this over any other?\r\n\r\nFor example, there already exists a `MessageEventDefinition` in the BPMN spec that we could attempt to re-use for this. Here is an example of pure BPMN.\r\n\r\n```xml\r\n<bpmn:definitions ...>\r\n  <bpmn:process id=\"Process_02p7q4p\" isExecutable=\"true\">\r\n    ...\r\n    <bpmn:intermediateThrowEvent id=\"Event_145tbfx\">\r\n      <bpmn:messageEventDefinition id=\"MessageEventDefinition_0inu3y4\" messageRef=\"Message_1nb8aa6\" />\r\n    </bpmn:intermediateThrowEvent>\r\n    ...\r\n  </bpmn:process>\r\n  <bpmn:message id=\"Message_1nb8aa6\" name=\"order_placed\" />\r\n  ...\r\n</bpmn:definitions>\r\n```\n lzgabel: Hi @korthout. Thanks for you reply.  I'm sorry for starting the implementation without agreeing on this issue. :bow:\r\n\r\n> How did you come to this design? Was there a specific reason why you chose this over any other?\r\n\r\nActually, what I first thought was that this behavior is considered to be an internal implementation of the Zeebe engine, and should be create a new `messageDefinition` extension element in the zeebe namespace. 😄 \r\n\r\n> there already exists a MessageEventDefinition in the BPMN spec that we could attempt to re-use for this. Here is an example of pure BPMN.\r\n\r\nYes. You are absolutely right. 👍  I've checked the BPMN spec again, I will take your suggestion.\r\n\r\n---\r\n🤔 I plan to add an extension element under message, WDYT? :\r\n\r\n```xml\r\n<bpmn:message id=\"Message_1nb8aa6\" name=\"order_placed\">\r\n  <bpmn:extensionElements>\r\n    <zeebe:publish correlationKey=\"= orderId\" timeTolive=\"PT10S\" messageId=\"= uuid()\" />\r\n  </bpmn:extensionElements>\r\n</bpmn:message>\r\n```\r\n\r\n--- \r\nAt the same time, I think we can also support the `message end event`. 🚀 \r\n\r\n\r\n\r\n\n korthout: > I plan to add an extension element under message\r\n\r\n@lzgabel Thanks for checking with me. I don't know what is best. \r\n\r\nAn argument for putting this under the message is that this allows several tasks/events to publish the same message with the same details (correlation key, TTL, message id).\r\n\r\nBut the counter-argument is that this does not allow changing any of these for tasks/events that want to publish the same message but with different details.\r\n\r\nI'll check with our Modeling experts to see what fits best. I'll attempt to make a decision on this early next week. Is that okay for you, @lzgabel?\n lzgabel: > I'll check with our Modeling experts to see what fits best. I'll attempt to make a decision on this early next week. Is that okay for you, @lzgabel?\r\n\r\nYes. Looking forward to the conclusion.\n nikku: @korthout to mirror my internal comment here:\r\n\r\n* I'd love to clearly separate a message from \"stuff that is being done with the message, in the context of a flow / process execution\".\r\n* To support this I'd keep what is relevant in the _execution flow_ on the flow element (intermediate catch event in this case)\n barmac: Hi, I'd like to add my two cents. First, it's great that you are looking into this feature, and I appreciate it a lot that you pulled in Modeler devs :)\r\n\r\nLet's examine how we solve modeling problems with each of the solutions:\r\n\r\n| Problem | Properties defined on shared `bpmn:Message` | Properties defined on individual events via `bpmn:MessageEventDefinition`|\r\n|-|-|-|\r\n| I want to add another event which publishes message of given name with the same correlation key, TTL etc. | Create a new event and select shared message | Copy existing event |\r\n| I want to modify properties for a specific event | Create a new message with the same name but different propertie | Modify properties on the event |\r\n| I want to modify properties on all events | Modify in single place | Modify on each event |\r\n| I want to create an event with a different variable as correlation key | Create a new message with the same name | Change on individual event |\r\n\r\nSetting different properties for the same message name leads to redundancy in the diagram (multiple messages of the same name but different extension elements). I believe this is not what we want, therefore I'd support setting event-related properties on the event definition.\r\n\r\nNote that all of that can be also applied to catch event. \r\n\r\n---\r\n\r\nI started writing this before Nico's comment but we discussed this in the morning and have 100% agreement :)\r\n\r\nProposal:\r\n\r\n```xml\r\n<bpmn:endEvent id=\"Message_1nb8aa6\" name=\"order_placed\">\r\n  <bpmn:messageEventDefinition>\r\n    <bpmn:extensionElements>\r\n      <zeebe:publish correlationKey=\"= orderId\" timeTolive=\"PT10S\" />\r\n    </bpmn:extensionElements>\r\n  </bpmn:messageEventDefinition>\r\n</bpmn:endEvent>\r\n```\n barmac: Question: What would `messageId` be used for?\n nikku: > Question: What would `messageId` be used for?\r\n\r\n@barmac I asked myself the same question. It is another way to ensure idempotent message delivery ([internal ref](https://camunda.slack.com/archives/CSQ2E3BT4/p1686047258957109)).\n nikku: @barmac and as we discussed, based on https://github.com/camunda/zeebe/issues/12878#issuecomment-1578613868, imagine this (attaching subscription information to the message definition, not the `bpmn:Message` as it is currently the case):\r\n\r\n```xml\r\n<bpmn:intermediateCatchEvent id=\"Message_1nb8aa6\" name=\"order_processed\">\r\n  <bpmn:messageEventDefinition messageRef=\"order_processed_message\">\r\n    <bpmn:extensionElements>\r\n      <zeebe:subscribe correlationKey=\"= orderId\" />\r\n    </bpmn:extensionElements>\r\n  </bpmn:messageEventDefinition>\r\n</bpmn:intermediateCatchEvent>\r\n```\n lzgabel: 👋 Hi Guys. Will this [#12878 (comment)](https://github.com/camunda/zeebe/issues/12878#issuecomment-1578613868) be the final conclusion?\n korthout: Hi @lzgabel I've asked our internal Modeling experts. @nikku and @barmac gave great input, and we'll likely take that route. However, I don't want to take the opportunity away for others to voice their opinions. I set Monday, June 12th, as the deadline for input, so let's await that. I'll update this issue on Tuesday with the outcome.\n jonathanlukas: I would also prefer the approach @nikku proposes. While a message correlation surely is related to the message itself, the implementation details are more part of the event definition of each bpmn element.\r\n\r\nAlso, I think in Camunda 7, the message throw event was able to select a message element to refer to which still leads to confusion...\n korthout: Thanks everyone for your input 👏 \r\n\r\nNo opinions against the proposal were raised. Let's move forward with the following:\r\n\r\n- a new extension element that can be added to the `bpmn:messageEventDefinition` (of Intermediate Throw Event, or End Event) and/or to the `bpmn:sendTask` to specify the details of the message to publish (i.e. correlation key, time to live, message id)\r\n- let's name this extension element `zeebe:publishMessage` to make the XML easier to understand/read for humans\r\n- the message to publish can be referenced directly from the `bpmn:messageEventDefinition` and from the `bpmn:sendTask` according to BPMN spec using the `messageRef` attribute (the referenced message contains the message name).\r\n\r\nHere follows an example with three different elements all publishing the same message with different message details (i.e. correlation key, time to live, message id).\r\n\r\n```xml\r\n<bpmn:definitions ...>\r\n  <bpmn:process id=\"Process_02p7q4p\" isExecutable=\"true\">\r\n    ...\r\n    <bpmn:intermediateThrowEvent id=\"Event_145tbfx\">\r\n      <bpmn:messageEventDefinition id=\"MessageEventDefinition_0inu3y4\" messageRef=\"Message_1nb8aa6\">\r\n        <bpmn:extensionElements>\r\n          <zeebe:publishMessage correlationKey=\"= orderId\" timeTolive=\"PT10S\" messageId=\"= orderId\" />\r\n        </bpmn:extensionElements>\r\n      </bpmn:messageEventDefinition>\r\n    </bpmn:intermediateThrowEvent>\r\n    ...\r\n    <bpmn:sendTask id=\"Activity_1kgyl85\" messageRef=\"Message_1nb8aa6\">\r\n      <bpmn:extensionElements>\r\n        <zeebe:publishMessage correlationKey=\"= some_other_order_id\" />\r\n      </bpmn:extensionElements>\r\n    </bpmn:sendTask>\r\n    ...\r\n    <bpmn:endEvent id=\"Message_1nb8aa6\" name=\"order_placed\">\r\n      <bpmn:messageEventDefinition messageRef=\"Message_1nb8aa6\">\r\n        <bpmn:extensionElements>\r\n          <zeebe:publishMessage correlationKey=\"= orderId\" timeTolive=\"PT1H\" />\r\n        </bpmn:extensionElements>\r\n      </bpmn:messageEventDefinition>\r\n    </bpmn:endEvent>\r\n    ...\r\n  </bpmn:process>\r\n  <bpmn:message id=\"Message_1nb8aa6\" name=\"order_placed\" />\r\n  ...\r\n</bpmn:definitions>\r\n```\r\n\r\n\r\n\n lzgabel: @korthout. Thanks. ❤️ \r\nI'll be pushing the latest commit for this feature in the next few days.",
    "title": "Add message definition extension elements",
    "releaseNoteText": "**Consequence:**\nIn the BPMN model API, the `send task` or `message throw event` did not have the ability to reference a message definition. This hindered the ability to model processes where a Send Task or Message Throw Event sends a message.\n\n**Cause:**\nThe underlying cause of this issue was that there was no extension element available to specify the details of the message to be published, such as the correlation key, time to live, and message ID.\n\n**Fix:**\nA new extension element, `zeebe:publishMessage`, was added to the `bpmn:messageEventDefinition` of Intermediate Throw Event and End Event, as well as to the `bpmn:sendTask`. This allowed specifying the details of the message to be published.\n\n**Result:**\nWith this fix, users can now add the `zeebe:publishMessage` extension element to the relevant BPMN elements and specify the correlation key, time to live, and message ID of the message to be published. This enables the modeling of processes where Send Tasks and Message Throw Events send messages, and allows for more flexibility in specifying different details for the same message in different elements."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12696",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\n\r\nSome process models can sometimes cause whole partitions to become completely bricked. For example, process models with a combination of loops, large multi-instance collection spawning activities, or straight-through processing (e.g. FEEL script tasks, undefined tasks, manual tasks, etc.) can cause us to write a large amount of follow up records, much faster than we can process or export. This can lead to:\r\n\r\n- The exported system being overloaded, e.g. Operate cannot import fast enough, Elasticsearch runs out of disk space, etc.\r\n- Runaway process instances cannot be canceled because the command is either not accepted, or written so far down the log that it takes hours to get there, at which point we've written millions more follow up records.\r\n- Node is out of disk space, leading to all partitions stopping.\r\n\r\n**Describe the solution you'd like**\r\n\r\nWhile preventing such situations is ideal, as a first step, we would like to provide an escape hatch for when they arise by allowing users to forcefully terminate a process without having to wait for all events to be processed.\r\n\r\nThis means providing a way to bypass the causality chain of the partition by directly modifying the state projection.\r\n\r\nWe would have to:\r\n\r\n1. Provide an API endpoint (whether a management or client endpoint is to be determined) where users can forcefully terminate a PI by key\r\n2. The leader of the partition for this PI must then modify the state to mark that PI as forcefully terminated. **It may not be possible to write to disk anymore, so we may have to simply modify the state**.\r\n3. The state modification must be replicated to all followers, much like the exporter state. This is to ensure consistency between all nodes should an election occur.\r\n4. Once a PI is forcefully terminated, then all records associated it still have to be read but can be skipped/not processed/replayed. **We cannot delete them from the log - this not only breaks the append-only contract, but is much more complicated**\r\n5. The records must also be skipped by the exporters. However, we may want to notify the external systems that the PI has been forcefully terminated somehow, so we may need some solution here. Maybe we still need to write a record at the end of the log specifying it was forcefully terminated.\r\n\r\nIn terms of UX, it would be great to be able to differentiate between normal termination and forceful termination, if only for monitoring and auditing purposes.\r\n\r\n**Describe alternatives you've considered**\r\n\r\n- Static analysis of process models to detect endless loops. This may not always be possible, and there may very well be valid use cases for such. Unclear how easy that is, especially things like a process with a call activity where the called ID is a variable which could point to the process itself.\r\n- Fair/weighted process scheduling. A rather complex solution, where the processor ensures that one PI cannot overwhelm the partition at the cost of other PIs or external user commands. This could be achieved by performing more look-ahead and determining whether it's safe to continue processing a given PI or if it should yield, and writing deferred computation records to resume it.\r\n\r\n**Additional context**\r\n\r\nSeveral incidents occurred recently which were the results of users deploying looping, straight-through processes, which were producing too writing more than the system could process/export. One was writing an exponential number of follow up records, which led to an unrecoverable out of disk space, where there was nothing to compact yet, so processing could never resume.\r\n\n\n korthout: @npepinpe I worry that termination of the process instance is considered a loss of data.\r\n\r\nInstead of termination of the process instance, I'd rather see something like suspension of the process instance. For example, by creating a process instance-level [Incident](https://docs.camunda.io/docs/components/concepts/incidents/) with a new `ErrorType` that prevents any further progress unless they are attempts to repair the instance, e.g. Modification, SetVariable, Migration, etc. This aids in visibility and allows users to repair the instance.\r\n\r\nSuch a solution is also what we are investigating in relation to:\r\n- #5121 \r\n- https://github.com/camunda/product-hub/issues/686\r\n\r\nI understand if that would not make it into the first iteration, but I'd hope we can replace the termination logic with what I've described here in some future iteration.\n Zelldon: Yesterday I was also thinking about suspending process instances just for the same use case, from the operations point of view, to get my system healthy again or avoid an imbalance of exporting and processing. \r\n\r\nNot sure whether the proposed solutions above are optimal, due to the state changes without processing a command. \r\n\r\n----\r\n\r\nWhat I thought we could do is have a runtime transient toggle that can be flipped for every process instance. If we see that a process instance makes problems we could flip it and the process instance will be ignored. Kind of similar to being blacklisted, but not persistent. If we restart the instance can be executed again. \r\n\r\n**What does ignored mean?** \r\n\r\nThere could be two things either, completely skip all commands, but then you will lose the progress. OR you simply append the current command for the process instance at the end of the log, without processing it. This allows to make progress still on other instances, and later again on the PI if it is no longer suspended. (BTW IMHO commands shouldn't be exported anyway (https://github.com/camunda/zeebe/issues/6749))\r\n\r\nWe need to allow certain commands like canceling still be processed for such PI. This allows to clean up, similar to we should allow for blacklisted instances.\r\n\r\n_______\r\n\r\n\r\nTo develop this further I could imagine that we could at some point have this even automatically, like normal OS scheduling or as we know in K8. If you have X process instances and Y CPUs than a PI is only allowed to take Y/X CPU time, if more we could suspend it for a time frame (append the current command at the end such it will be processed later).\r\n\n korthout: 🤔 Interesting ideas @Zelldon. \r\n\r\nYielding the commands (appending them to the end of the log instead of processing them) would solve parts of these issues, but not the fork bomb case where the log keeps expanding. Consider a fork bomb that's been running for a few minutes unnoticed. There are likely already thousands if not millions of unprocessed commands for that instance on the log. These will all be yielded to back of the log continuously. New commands will take at least the time that it takes to yield all those commands.\r\n\r\nI'm personally more in favor of skipping the commands. This puts the process instance in a state of limbo just like a blacklisted process instance. All we need is a way to start up processing again, and to allow the user to cancel the instance. \r\n\r\n- We could persist the skipped commands. As soon as we want to continue, we write these back to the log and continue processing. This has two downsides: disk usage and fork bomb would continue. So I don't think we should do this.\r\n- We could ignore the skipped commands. As soon as we want to continue, the user can usePI modification / migration to re-start the processing, or they can simply cancel the instance. \r\n\r\nLastly, I was wondering how you'd keep track of a runtime flag for each process instance? If a user has large state (millions of running PI) then this would consume a lot of memory. Why not persist it like blacklisting?\n deepthidevaki: > Not sure whether the proposed solutions above are optimal, due to the state changes without processing a command.\r\n\r\nI think it is possible to do this correctly. When we force \"suspend\" (for lack of a better word) a process, we should bypass the normal stream processing process. That means, we do not write the command to the logstream, but submit to StreamProcessor actor as a task to be processed immediately. When processing this task, it should generate followup events  to suspend the process. The followup event could be also the error record/incident record which @korthout mentioned. The follow up event should somehow refer to the invisible command that should have been at the current processingPosition (yet to figure out how to do it.). The event get's replicated and the followers will have the same state as the leader. Send the response only after this follow up event is committed, so that we know for sure that the process is suspended.\r\n\r\nSide note:- In general, it would make sense to not write user commands to the logstream. Only follow up command and events are required for deterministic replay. \n megglos: ZDP-Triage:\n- may be a topic that affects both teams\n- if such incidents occur we can't recover in an easy way (you would need to manually update the rocksDb sate to e.g. blacklist the instance)\n\n@abbasadel would be curious on the triage outcome of your team, as this would be crucial to mitigate incidents where we experience malicious processes\n megglos: @felix-mueller this is the feature I mentioned in the stakeholder round today in order to allow us to better handle incidents that are caused by malicious processes\n megglos: ZDP-Planning:\n- ZDP is picking this up to assess solutions for this asap to allow engineers to handle incidents better going forward\n remcowesterhoud: ZPA Triage:\r\n- The solution of bypassing the log is unrelated to ZPA, as we only build things on top of the log stream.\r\n- We are interested in what happens to the process instance. The state of this PI is our responsibility\r\n- Please reach out to us when you need involvement on deciding what happens with the PI! We are happy to support 🙂 \r\n- We're not planning to work on this unless we need to support the ZDP team. As a result we will remove it from our project board.\n Zelldon: First of all, I want to thank you all for your ideas and brain dumps you have done here. :bow: \r\n\r\nI thought about this for a while and also re-read all your comments. If you're interested, I used the following [gdoc](https://docs.google.com/document/d/1NDKjq7osuzYoFd5Jtj9Xhe5_buw8rVU6nLIqBDQwJ74/edit) to summarize and better assess potential solutions. \r\n\r\nI have a potential solution in mind, let me shortly explain it to you.\r\n\r\n## Proposal:\r\n\r\nI think we can combine several ideas together to get the best out of it. \r\n\r\nIt makes sense to have a two-step process implemented, which allows to first ban the instance and secondly (if wished) to cancel the instance.\r\n\r\nWe provide an API (potentially actuator) where we can send “ban instance X” request. We access the ZeebeDB with a separate context, as we do with the Exporters, and mark the process instance as banned. I think this should bring us a quick win and feels potentially easy to implement. \r\n\r\nThe ban column family is less frequented, which should allows us to already stop the processing for a specific PI, without conflicts. It might make sense to think about the blacklisting cache again, whether we really need it or we just delete it (otherwise we have to update flag).\r\n\r\nThe banning of the instance should be confirmed with an event on the log, this allows to replicate the state change, and followers can do the same.\r\n\r\nPlease be aware as soon as the instance is banned, there is right now no way to recover. [The process instance is in limbo, as ](https://github.com/camunda/zeebe/issues/12696#:~:text=I%27m%20personally%20more%20in%20favor%20of%20skipping%20the%20commands.%20This%20puts%20the%20process%20instance%20in%20a%20state%20of%20limbo%20just%20like%20a%20blacklisted%20process%20instance.%20All%20we%20need%20is%20a%20way%20to%20start%20up%20processing%20again%2C%20and%20to%20allow%20the%20user%20to%20cancel%20the%20instance.)[Nico Korthout](mailto:nico.korthout@camunda.com) said so nicely. To handle this differently I see this out of the scope of this feature since we expect it to use only for severe issues where the cluster is otherwise not recoverable (and/or can make no progress).\r\n\r\nYou may ask why is that actually the case. Because we likely skip commands which are then lost. Furthermore, there is right now no way to remove some PI from the banned instances list. A potential follow-up could be to allow removing instances from that list and applying PI modification (to repair instances).\r\n\r\nWe could think of [persisting the skipped records into the state](https://github.com/camunda/zeebe/issues/12696#:~:text=We%20could%20persist%20the%20skipped%20commands.%20As%20soon%20as%20we%20want%20to%20continue%2C%20we%20write%20these%20back%20to%20the%20log%20and%20continue%20processing.%20This%20has%20two%20downsides%3A%20disk%20usage%20and%20fork%20bomb%20would%20continue.%20So%20I%20don%27t%20think%20we%20should%20do%20this.), but I would rather not do that, since it could cause severe other issues due to limited space, etc. It is also not clear to me whether the gains are outweighed by the costs since we can also easily run process instance modification to recover such.\r\n\r\nThe approach above is a combination of multiple others mentioned before, and I feel it is best to not make it too easy to erase customer data, which is why cancelation is/can be the second step. \r\n\r\nGenerally, cancelation involves more work, due to the child's cancelation, etc. This should happen in the processing, with a separate command. Since it might fail as well, due to large nested instances, etc, but with this approach it is fine since it is already banned in this case. \r\n\r\nIn order to allow the cancelation we need to make sure that we implement the [cancelation for banned instances.](https://github.com/camunda/zeebe/issues/12772)\r\n\r\n## Next\r\n\r\nAs a next step, I would like to spend some time doing a POC and play a bit around with that. Please feel free to raise any concerns with this approach. I'm open for discussions of course.\r\n",
    "title": "Forcefully terminate a process instance",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12382",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nCurrently the zeebe process is run by the root user in the zeebe docker image:\r\n```\r\nroot@5ce6a5346a36:/usr/local/zeebe# ps aux\r\nUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\r\nroot         1  0.3  0.0   1940   448 ?        Ss   14:26   0:00 tini -- /usr/local/bin/startup.sh\r\nroot         7  197  1.4 4216156 295668 ?      Sl   14:26   0:05 /opt/java/openjdk/bin/java -Xms128m -XX:+ExitOnOutOfMemoryError -Dfile.encoding=UTF-8 -classpath /usr/local/zeebe/config:/usr/local/zeebe/lib/* -Dapp.name=broker -Dapp.pid=7 -Dapp.repo=/usr/local/zeebe\r\nroot        42  1.0  0.0   6880  3364 pts/0    Ss   14:26   0:00 /bin/bash\r\nroot        55  0.0  0.0   8476  2808 pts/0    R+   14:26   0:00 ps aux\r\n```\r\n\r\nTo harden the security of the docker image it should **default** to run it with an unprivileged user instead, [see the OWASP recommendation](https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-2-set-a-user).\r\n\r\n**Describe the solution you'd like**\r\nThe zeebe image **should by default run with an unprivileged user.** [There is already a zeebe user setup with uid 1000](https://github.com/camunda/zeebe/blob/main/Dockerfile#L105-L111) it's just not used.\r\n\r\n**Additional context**\r\nRelates to https://github.com/camunda/product-hub/issues/717\r\nsupport for running with an unprivileged user was added with https://github.com/camunda/zeebe/issues/11784\r\nRelates to https://jira.camunda.com/browse/SUPPORT-15969\n\n mvawork: Please review my pull request.\r\n\r\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code class=\"notranslate\"># ps -aux\r\nUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\r\nzeebe        1  0.1  0.0   2504   580 pts/0    Ss   12:28   0:00 tini -- /usr/local/bin/startup.sh /bin/bash\r\nzeebe       12 35.8  1.7 12603416 455936 pts/0 Sl+  12:28   1:18 /opt/java/openjdk/bin/java -XX:+ExitOnOutOfMemoryError -Dfile.encoding=UTF-8 -classpath /usr/local/zeebe/config:/usr/local/zeeb\r\nroot       100  0.0  0.0   2612   536 pts/1    Ss   12:28   0:00 /bin/sh\r\nroot       109  0.0  0.0   8892  3308 pts/1    R+   12:32   0:00 ps -aux\r\n# \r\n</code></pre><div class=\"zeroclipboard-container position-absolute right-0 top-0\">\r\n    <clipboard-copy aria-label=\"Copy\" class=\"ClipboardButton btn js-clipboard-copy m-2 p-0 tooltipped-no-delay\" data-copy-feedback=\"Copied!\" data-tooltip-direction=\"w\" value=\"root@5ce6a5346a36:/usr/local/zeebe# ps aux\r\nUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\r\nroot         1  0.3  0.0   1940   448 ?        Ss   14:26   0:00 tini -- /usr/local/bin/startup.sh\r\nroot         7  197  1.4 4216156 295668 ?      Sl   14:26   0:05 /opt/java/openjdk/bin/java -Xms128m -XX:+ExitOnOutOfMemoryError -Dfile.encoding=UTF-8 -classpath /usr/local/zeebe/config:/usr/local/zeebe/lib/* -Dapp.name=broker -Dapp.pid=7 -Dapp.repo=/usr/local/zeebe\r\nroot        42  1.0  0.0   6880  3364 pts/0    Ss   14:26   0:00 /bin/bash\r\nroot        55  0.0  0.0   8476  2808 pts/0    R+   14:26   0:00 ps aux\" tabindex=\"0\" role=\"button\" style=\"display: inherit;\">\r\n      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-copy js-clipboard-copy-icon m-2\">\r\n    <path d=\"M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z\"></path><path d=\"M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z\"></path>\r\n</svg>\r\n      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-check js-clipboard-check-icon color-fg-success m-2 d-none\">\r\n    <path d=\"M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z\"></path>\r\n</svg>\r\n    </clipboard-copy>\r\n  </div></div>\r\n\n jessesimpson36: I proposed an [alternative PR ](https://github.com/camunda/zeebe/pull/13418)to accomplish the same goal, without gosu.  I believe there was a concern that if you set `USER zeebe` about whether you could exec into the container as root, and I did verify manually that you can run the process as root via `docker run --user root ...`  as well as `docker exec -it --user root ...`.\r\n\r\nThis change will require a helm chart change, but I'm on the team that works on that, so coordinating that change shouldn't be much of a challenge.",
    "title": "Docker: Run the zeebe process with an unprivileged user by default",
    "releaseNoteText": "**Consequence:** The zeebe process in the Docker image is currently run by the root user. This poses a security risk according to the OWASP recommendation.\n\n**Cause:** The zeebe user with UID 1000, which is already set up in the Docker image, is not used by default.\n\n**Fix:** The zeebe image now runs with an unprivileged user by default. The Dockerfile has been updated to utilize the existing zeebe user.\n\n**Result:** With this fix, the zeebe process will be run with an unprivileged user by default, improving the security of the Docker image."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11708",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\n\r\nIn order to register job streams to the gateway, we will need to add a new gRPC API for this. This API will be a unidirectional stream (from server to client). \r\n\r\nIt should take in as parameter the same activation properties as the job worker (minus anything related to long polling and the likes), but it will be a long-living stream. Later, when implementing it on the client side and on the gateway, it's important that its keep-alive be configured properly.\r\n\r\nIt returns a stream of single `ActivatedJob`. We can discuss if we want to keep the batching properties, as this could be a useful optimization, but I would propose to ignore it for the alpha target and measure its impact once we have the complete end-to-end pipeline.\n",
    "title": "Add gRPC job stream API",
    "releaseNoteText": "**Consequence**: The system did not have a gRPC API for registering job streams to the gateway, which resulted in a limited functionality for job management. Users were not able to have a long-living stream for job activation properties.\n\n**Cause**: The absence of a gRPC API for job streams was due to the lack of implementation in the system. \n\n**Fix**: A new gRPC API for job streams was added to support registering job streams to the gateway. The API was designed as a unidirectional stream from the server to the client, accepting the same activation properties as the job worker.\n\n**Result**: With the addition of the new gRPC job stream API, users can now register job streams to the gateway and receive a long-living stream of single `ActivatedJob` objects. The system's job management functionality is enhanced, allowing for improved end-to-end pipeline performance."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13233",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "We've introduced a regression on deploying large payloads on multi-partition clusters with\r\n- #11661 \r\n\r\nBefore that pull request, a deployed resource was written in two follow-up events:\r\n- `Deployment:CREATED`\r\n- `Process:CREATED` (or `DecisionRequirements:CREATED` depending on resource)\r\n\r\nThe specialized `DeploymentDistribution` would take the resource from the `Deployment:CREATED` to distribute it to the other partitions.\r\n\r\nWith #11456, a new event `CommandDistribution:STARTED` was introduced to store the command for distribution. For deployments, this contains the entire Deployment incl. the resource. This event is only appended on multi-partition clusters. But, when it is appended, it further reduces the maximum payload size because the follow-up events reach the `MAX_BATCH_SIZE` restriction with a lower payload size.\r\n\r\n> **Note** The `MAX_BATCH_SIZE` is a limitation that originated from the `MAX_MESSAGE_SIZE` configuration setting but nowadays is only defined by the `LogStreamBuilder`'s `MAX_FRAGMENT_SIZE`: 4MB.\r\n\r\nThis regression lowers the maximum payload size of deployments from ~2MB down to ~1.4MB.\r\n\r\nThe regression does not exist on single partition clusters as these do not require the distribution of the deployment.\r\n\r\n---\r\n\r\nSuggested solution:\r\n- don't write the resource in the `Deployment:CREATED` event, only in the `Process:CREATED` and the `DecisionRequirements:CREATED` events (estimate: x-small)\r\n\r\nOf course, we'd need to inform users that we're no longer writing the resource for that event in the Update Guide. As this would be breaking user space. The documentation should clarify that the resource is available in the `Process:CREATED` and the `DecisionRequirements:CREATED` instead.\n\n korthout: There are several solution ideas:\n- https://github.com/camunda/zeebe/issues/11513\n  - Does not seem trivial to implement\n  - Mostly a ZDP effort\n- don't write the resource in the `Deployment:CREATED` event, only in the `Process:CREATED` and the `DecisionRequirements:CREATED` events (estimate: x-small)\n  - would require alignment with Operate and Optimize to make sure they consume the resource from `Process:CREATED` \n korthout: @sdorokhova Can you verify for me that Operate consumes the `Process:CREATED` event to get the BPMN model and not the `Deployment:CREATED` event? Likewise, does it consume the `DecisionRequirements:CREATED` event for DMN models?\r\n\r\n@RomanJRW Can you verify the same as above, but for Optimize?\n sdorokhova: Hi @korthout ,\r\nwe read processes from `process` index and decision requirements from `decision-requirements` index. You can check [here](https://github.com/camunda/operate/blob/168d22f5352c66bfd56c47678e92655a82414194/common/src/main/java/io/camunda/operate/zeebe/ZeebeESConstants.java) all the indices we're reading from. Is this what you were asking about? \n korthout: @sdorokhova Perfect! That's exactly what I mean. I should've mentioned that it concerns the ES indices `zeebe-record-process` and `zeebe-record-decision-requirements`. \r\n\r\nI was hoping you weren't reading the BPMN/DMN resources from the `zeebe-record-deployment` index.\n megglos: @korthout do I understand this correctly that this regression is only present in 8.3? And the changes that happened in 8.2 like https://github.com/camunda/zeebe/issues/11660 have not caused a regression?\n korthout: @megglos Correct, the regression only exists on `main` and if left unfixed will only be present in the upcoming 8.3. The reason is that deployment distribution has been switched over only for 8.3(-alpha*), not for 8.2.\n RomanJRW: Hey @korthout - apologies for slow response, I have had FTO. I can confirm the same for Optimize, that we read from `zeebe-record-process` and not the deployment index\n korthout: Now that both Operate and Optimize have confirmed that they don't access the resource from the `zeebe-record-deployment` index, I think it's fine to take this solution:\r\n- don't write the resource in the `Deployment:CREATED` event, only in the `Process:CREATED` and the `DecisionRequirements:CREATED` events (estimate: x-small)\r\n\r\nOf course, we'd need to inform users that we're no longer writing the resource for that event in the Update Guide. As this would be breaking user space. The documentation should clarify that the resource is available in the `Process:CREATED` and the `DecisionRequirements:CREATED` instead.",
    "title": "Regression in deploying large payloads",
    "releaseNoteText": "**Consequence**: Deploying large payloads on multi-partition clusters introduced a regression that impacted the maximum payload size of deployments. Before the regression, a deployed resource was written in two follow-up events (`Deployment:CREATED` and `Process:CREATED` or `DecisionRequirements:CREATED`). However, with the regression, a new event (`CommandDistribution:STARTED`) was introduced to store the command for distribution, leading to a reduction in the maximum payload size. This regression lowered the maximum payload size of deployments from approximately 2MB to around 1.4MB. The regression did not affect single partition clusters as they do not require the distribution of the deployment.\n\n**Cause**: The regression was caused by the introduction of the `CommandDistribution:STARTED` event, which stored the entire deployment including the resource for distribution. This event was only appended on multi-partition clusters, and it further reduced the maximum payload size due to the `MAX_BATCH_SIZE` restriction.\n\n**Fix**: To address this issue, the fix involved not writing the resource in the `Deployment:CREATED` event but only in the `Process:CREATED` and `DecisionRequirements:CREATED` events.\n\n**Result**: After applying the fix, the maximum payload size of deployments on multi-partition clusters is restored to its original size of approximately 2MB. The resource is now available in the `Process:CREATED` and `DecisionRequirements:CREATED` events, allowing for proper distribution without impacting the payload size. It is important to note that this fix requires alignment with Operate and Optimize to ensure that they consume the resource from the correct events. The documentation has been updated to inform users that the resource is no longer written in the `Deployment:CREATED` event and is available in the `Process:CREATED` and `DecisionRequirements:CREATED` events instead."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13715",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "See https://github.com/camunda/zeebe/actions/runs/5709567673\r\n```\r\nThe workflow is not valid. In .github/workflows/release-main-dry-run.yml (Line: 11, Col: 11): Error from called workflow camunda/zeebe/.github/workflows/release.yml@bc32ea937f8d3650b657431c70d22bae9339ba3c (Line: 329, Col: 12): Unrecognized named-value: 'env'. Located at position 1 within expression: env.RELEASE_BRANCH\r\n```\n\n megglos: https://github.com/actions/runner/issues/2372",
    "title": "Release Dry fails because of unrecognized argument",
    "releaseNoteText": "**Consequence**: The release dry run fails due to an unrecognized argument.\n**Cause**: The issue was caused by an error in the called workflow, specifically in line 329 where an unrecognized named-value 'env' was used.\n**Fix**: A fix was implemented to correct the error by removing the unrecognized argument 'env.RELEASE_BRANCH'.\n**Result**: After applying the fix, the release dry run now executes successfully without any unrecognized argument errors."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13650",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n`ZeebePartitionHealth` repeatedly calls listeners and logs a change of health status when the health has not actually changed.\r\nThis has existed for a long time but was made worse with #13042 where we now update the health status more frequently.\r\n\r\nThis is the bug: https://github.com/camunda/zeebe/blob/c49d14b7eb8052895aa813c9884850ae9f590a2f/broker/src/main/java/io/camunda/zeebe/broker/system/partitions/ZeebePartitionHealth.java#L60\r\n\r\nHere we should actually compare the health reports and not check for identity.\r\n\r\n**Environment:**\r\nZeebe Version: 8.2.9\n",
    "title": "`ZeebePartitionHealth` repeatedly reports change of health status",
    "releaseNoteText": "**Consequence:**\n`ZeebePartitionHealth` repeatedly called listeners and logged a change of health status even when the health had not actually changed. This resulted in unnecessary notifications and log entries for the users.\n\n**Cause:**\nThe issue occurred because the `ZeebePartitionHealth` class was incorrectly comparing the health reports using identity check instead of comparing the health status itself.\n\n**Fix:**\nThe issue has been resolved by modifying the comparison logic in the `ZeebePartitionHealth` class. The health reports are now properly compared to determine if there has been an actual change in health status.\n\n**Result:**\nAfter the fix, `ZeebePartitionHealth` will no longer falsely report a change in health status when there has been no actual change. Users will no longer receive unnecessary notifications or see misleading log entries regarding the health status of the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13471",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n```\r\nINFO 2023-07-10T18:18:50.941085479Z [resource.labels.containerName: zeebe] Preparing transition from FOLLOWER on term 31 completed\r\nINFO 2023-07-10T18:18:50.941163945Z [resource.labels.containerName: zeebe] Transition to LEADER on term 31 starting\r\nINFO 2023-07-10T18:18:50.941238114Z [resource.labels.containerName: zeebe] Cancelling transition to LEADER on term 31\r\nWARNING 2023-07-10T18:18:50.941635348Z [resource.labels.containerName: zeebe] Uncaught exception in Broker-1-ZeebePartition-2.\r\n\"java.lang.NullPointerException: must specify a log stream\r\n\tat java.util.Objects.requireNonNull(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.broker.jobstream.RemoteJobStreamErrorHandlerService.onBecomingLeader(RemoteJobStreamErrorHandlerService.java:56) ~[zeebe-broker-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.broker.system.partitions.PartitionStartupAndTransitionContextImpl.lambda$notifyListenersOfBecomingLeader$4(PartitionStartupAndTransitionContextImpl.java:164) ~[zeebe-broker-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat java.util.stream.ReferencePipeline$3$1.accept(Unknown Source) ~[?:?]\r\n\tat java.util.ArrayList$ArrayListSpliterator.forEachRemaining(Unknown Source) ~[?:?]\r\n\tat java.util.stream.AbstractPipeline.copyInto(Unknown Source) ~[?:?]\r\n\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(Unknown Source) ~[?:?]\r\n\tat java.util.stream.ReduceOps$ReduceOp.evaluateSequential(Unknown Source) ~[?:?]\r\n\tat java.util.stream.AbstractPipeline.evaluate(Unknown Source) ~[?:?]\r\n\tat java.util.stream.ReferencePipeline.collect(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.broker.system.partitions.PartitionStartupAndTransitionContextImpl.notifyListenersOfBecomingLeader(PartitionStartupAndTransitionContextImpl.java:165) ~[zeebe-broker-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.broker.system.partitions.ZeebePartition.lambda$leaderTransition$6(ZeebePartition.java:243) ~[zeebe-broker-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.future.FutureContinuationRunnable.run(FutureContinuationRunnable.java:28) ~[zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94) ~[zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT] ... \r\nERROR 2023-07-10T18:18:50.942738393Z [resource.labels.containerName: zeebe] Failed to install partition 2\r\n```\r\n\r\nTransition to leader is cancelled (probably because there is a new leader already). But the `PartitionStartupAndTransitionContextImpl.lambda$notifyListenersOfBecomingLeader` is still invoked. Since the services were not installed, logstream is null and this causes NPE in the listener. \r\n\r\nPartition is inactive after that. However, the partition is still marked as healthy (See #13401) \r\n\r\n**Expected behavior**\r\n\r\nPartition listeners are only invoked if the transition is successfully completed.\r\n\r\n\r\n**Environment:**\r\n- Zeebe Version: Observed in 8.3.0-SNAPSHOT (medic-cw-27 benchmark)\r\n\n\n megglos: ZDP-Triage:\r\n- likelihood seems high - happened a couple of times in benchmarks already\r\n- bug may exist for way longer already - it was not as visible before though\r\n- 3-4 already existing issues might be caused by this\r\n- workaround would be a pod restart - but hard to identify given partition is still marked as healthy\n oleschoenburg: The issue is that listeners are called when the transition completes normally and that we complete the transition normally when it is actually cancelled: https://github.com/camunda/zeebe/blob/a9d810011935b0538ed6a97cab342b72681c54a4/broker/src/main/java/io/camunda/zeebe/broker/system/partitions/impl/PartitionTransitionProcess.java#L66-L71",
    "title": "PartitionListeners are notified even if the transition is cancelled causing NPE",
    "releaseNoteText": "**Consequence**\nPartitionListeners were being notified even if the transition was cancelled, leading to a NullPointerException (NPE). This caused the partition to become inactive.\n\n**Cause**\nThe issue occurred because the `PartitionStartupAndTransitionContextImpl.lambda$notifyListenersOfBecomingLeader` method was still invoked even when the transition to leader was cancelled. Since the services were not installed, the logstream was null, resulting in an NPE in the listener.\n\n**Fix**\nThe issue was fixed by modifying the logic in the `PartitionTransitionProcess` class. The transition is now only considered complete and the listeners are only invoked if the transition completes successfully.\n\n**Result**\nAfter the fix, partition listeners will only be invoked if the transition to leader is successfully completed. This prevents the occurrence of NullPointerException and ensures that the partition operates as expected."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13431",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nWhen tls enabled on gateway for [secure client communication](https://docs.camunda.io/docs/next/self-managed/zeebe-deployment/security/secure-client-communication/), readiness check fails. \r\n\r\n<details><summary>Failed health status</summary>\r\n <p>\r\n\r\n```\r\n{\r\n   \"components\" : {\r\n      \"diskSpace\" : {\r\n         \"details\" : {\r\n           ....\r\n         },\r\n         \"status\" : \"UP\"\r\n      },\r\n      \"gatewayClusterAwareness\" : {\r\n         \"status\" : \"UP\"\r\n      },\r\n      \"gatewayPartitionLeaderAwareness\" : {\r\n         \"status\" : \"UP\"\r\n      },\r\n      \"gatewayResponsive\" : {\r\n         \"details\" : {\r\n            \"error\" : \"java.util.concurrent.ExecutionException: io.grpc.StatusRuntimeException: UNAVAILABLE: io exception\\nChannel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]\",\r\n            \"healthZeebeClientProperties\" : {\r\n               \"requestTimeout\" : \"PT0.5S\",\r\n               \"securityProperties\" : {}\r\n            },\r\n            \"timeOut\" : \"PT0.5S\"\r\n         },\r\n         \"status\" : \"DOWN\"\r\n      },\r\n      \"gatewayStarted\" : {\r\n         \"status\" : \"UP\"\r\n      },\r\n      \"livenessGatewayClusterAwareness\" : {\r\n         \"details\" : {\r\n            \"derivedFrom\" : \"ClusterAwarenessHealthIndicator\",\r\n            \"lastSeenDelegateHealthStatus\" : {\r\n               \"status\" : \"UP\"\r\n            },\r\n            \"maxDowntime\" : \"PT5M\",\r\n            \"wasEverUp\" : true\r\n         },\r\n         \"status\" : \"UP\"\r\n      },\r\n      \"livenessGatewayPartitionLeaderAwareness\" : {\r\n         \"details\" : {\r\n            \"derivedFrom\" : \"PartitionLeaderAwarenessHealthIndicator\",\r\n            \"lastSeenDelegateHealthStatus\" : {\r\n               \"status\" : \"UP\"\r\n            },\r\n            \"maxDowntime\" : \"PT5M\",\r\n            \"wasEverUp\" : true\r\n         },\r\n         \"status\" : \"UP\"\r\n      },\r\n      \"livenessGatewayResponsive\" : {\r\n         \"details\" : {\r\n            \"derivedFrom\" : \"ResponsiveHealthIndicator\",\r\n            \"lastSeenDelegateHealthStatus\" : {\r\n               \"details\" : {\r\n                  \"error\" : \"java.util.concurrent.ExecutionException: io.grpc.StatusRuntimeException: UNAVAILABLE: io exception\\nChannel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]\",\r\n                  \"healthZeebeClientProperties\" : {\r\n                     \"requestTimeout\" : \"PT5S\",\r\n                     \"securityProperties\" : {}\r\n                  },\r\n                  \"timeOut\" : \"PT5S\"\r\n               },\r\n               \"status\" : \"DOWN\"\r\n            },\r\n            \"maxDowntime\" : \"PT10M\",\r\n            \"wasEverUp\" : false\r\n         },\r\n         \"status\" : \"DOWN\"\r\n      },\r\n      \"livenessMemory\" : {\r\n         \"details\" : {\r\n            \"threshold\" : 0.01\r\n         },\r\n         \"status\" : \"UP\"\r\n      },\r\n      \"livenessState\" : {\r\n         \"status\" : \"UP\"\r\n      },\r\n      \"ping\" : {\r\n         \"status\" : \"UP\"\r\n      },\r\n      \"readinessState\" : {\r\n         \"status\" : \"UP\"\r\n      }\r\n   },\r\n   \"groups\" : [\r\n      \"liveness\",\r\n      \"readiness\",\r\n      \"startup\"\r\n   ],\r\n   \"status\" : \"DOWN\"\r\n}\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\nBoth `gatewayResponsive` and `livenssGatewayResponsive` fails because of some SSL error.   \r\n\r\nThis was unnoticed before because readiness check was disabled by default in camunda platform helm until 8.2. From 8.2 it is enabled by default. \r\n\r\n**To Reproduce**\r\n\r\nStart a single gateway with TLS enabled as documented [here](https://docs.camunda.io/docs/next/self-managed/zeebe-deployment/security/secure-client-communication/). And then query `http://gateway-host:9600/actuator/health`.\r\n\r\n**Expected behavior**\r\n\r\nHealth and readiness check should work with TLS enabled.\r\n\r\n**Environment:**\r\n- Zeebe Version: All versions (reproduced with 8.0.x, 8.1.5, and 8.2.7)\r\n\r\nrelated to https://jira.camunda.com/browse/SUPPORT-17529 \r\nhttps://jira.camunda.com/browse/SUPPORT-16945\n\n npepinpe: See https://github.com/camunda/zeebe/issues/11799#issuecomment-1558773952\r\n\r\nI guess it was already linked to a support issue. Workaround is to disable the check by setting the following env var:\r\n\r\n```\r\nMANAGEMENT_HEALTH_GATEWAY-RESPONSIVE_ENABLED=false\r\n```\n deepthidevaki: I will leave it open for easy visibility. I searched for open bugs for this, but couldn't find any.\r\n\r\nPlease close both issues when it is fixed. \n megglos: ZDP-Triage:\n- no functional impact on gateway itself\n- breaks the helm chart update as readiness check was recently enabled there\n- workaround is just disabling the specific check https://github.com/camunda/zeebe/issues/13431#issuecomment-1630928379\n- relates to https://github.com/camunda/zeebe/issues/11799\n- quick win could be removing this indicator from the readiness\n- may be better to execute a topologyrequest from k8s instead\n- let's revisit readiness and liveness\n- we need to check-in with the distribution team on this => we may release the workaround disabling this check asap @megglos \n\n megglos: Hey @deepthidevaki ,\r\n\r\nas I try to follow-up on this with the distribution team, the only official config exposed relating to TLS on the helm chart is enabling it on the ingress https://github.com/camunda/camunda-platform-helm/blob/main/charts/camunda-platform/values.yaml#L595-L600 .\r\n\r\nIn the linked support case TLS was enabled on the zeebe pod by the customer via setting `ZEEBE_GATEWAY_SECURITY_ENABLED` etc., so it’s kind of special and not a general issue/breaking change with the helm charts intended way to enable TLS. Where if tls is enabled on the ingress the check should still work as TLS terminates at the ingress and the health check directly accesses the pod endpoint, do I understand this correctly?\r\n\r\nI would then conclude no change to the helm chart is needed and assume likelihood of others hitting that regression is on the lower end ^^ If someone sets up TLS termination manually on the pod, they can make use of the workaround until this issue is resolved on the zeebe side.\n deepthidevaki: > the only official config exposed relating to TLS on the helm chart is enabling it on the ingress\r\n\r\n@megglos \r\nIs that the recommended way to do it? If so, we can also recommend it to the customer. Is there any documentation on it? The only [documentation](https://docs.camunda.io/docs/next/self-managed/zeebe-deployment/security/secure-client-communication/) I found is to configure it on Zeebe gateway. \r\n\r\n> I would then conclude no change to the helm chart is needed and assume likelihood of others hitting that regression is on the lower end ^^ If someone sets up TLS termination manually on the pod, they can make use of the workaround until this issue is resolved on the zeebe side.\r\n\r\n:+1:\n npepinpe: Can we just get rid of this `ResponsiveHealthIndicator`? I don't think it really purposefully demonstrates that it's responsive:\r\n\r\n1. The user can query the topology themselves to see if the service is working\r\n2. The user queries the health endpoint, which queries the topology, to see if the service is working\r\n\r\nIt just seems like extra steps. I understand that the idea is for things like load balancers to not send requests to an unresponsive gateway, but I would argue that it really doesn't bring that much value for the added complexity, since the gateway client has to be configured for all possible gateway configuration (TLS, authentication, etc.) Plus it only checks that the gRPC server serving the right service, but not things like \"can you actually send requests to brokers?\" or the likes. It doesn't even do anything with the topology itself :shrug: \n oleschoenburg: Maybe we should just implement the gRPC [health checking protocol](https://grpc.github.io/grpc/core/md_doc_health-checking.html) and then recommend [gRPC probes](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#probe-check-methods)? \n npepinpe: Surprise surprise, we already do serve the gRPC health check protocol! But it's very basic, only reporting whether the gateway started or is shutting down. But it'd be cool to extend it :+1: \n oleschoenburg: Ah TIL :+1: \r\nI guess this would not 100% replace the `ResponsiveHealthIndicator` anyway because we'd not do a topology request that way. \r\n\r\nI'm not attached to the `ResponsiveHealthIndicator` so just removing it for now also sounds good to me.\n megglos: :information_source: What determines the liveness/readiness of the Gateway as of now?\r\n\r\nreadiness: `StartedHealthIndicator`\r\n\r\n[liveness](https://github.com/camunda/zeebe/blob/main/dist/src/main/resources/application-gateway.properties#L9):\r\n`livenessGatewayResponsive` ==> ResponsiveHealthIndicator  (doing a topologyRequest on the grpc endpoint of the pod)\r\n`livenessGatewayClusterAwareness`  ==> gateway::getStatus\r\n`livenessGatewayPartitionLeaderAwareness`  ==> doing topologyManager.getTopology and checking if there is a leader known for every partition\r\n`livenessMemory`  => checking if JVM has at least 10% free memory (actually intended was >1%, according to the initial task https://github.com/camunda/zeebe/issues/4339 is that a bug? :smile: )\r\n\r\n**I see the following options going forward:**\r\n\r\n:one: Make sure users can configure the internal zeebe client to work properly in such scenarios (there is actually dedicated config present for tls and oauth config)\r\n:heavy_plus_sign: Pro:\r\n- the check’s semantics stay as they are\r\n\r\n:heavy_minus_sign: Con:\r\n- it seems still like a weird UX, I may have to configure a client for my gateway within the Gateway? Likely to be missed and causing questions and support cases\r\n- the gateway being able to connect to it’s own GRPC port does not mean that port is properly reachable from outside the pod\r\n\r\n:wrench: Effort:\r\n- improve docs, hoping users will pick that up without needing help :crossed_fingers:\r\n- potentially extending the helm chart for allowing to config the client\r\n\r\n:two: We remove the ResponsiveHealthIndicator as suggested by Nicolas \r\n\r\n:heavy_plus_sign: Pro:\r\n- less complexity in our liveness check, no side-effects of auth or transport encryption setups\r\n- grpc readiness/connectivity can be checked via a grpc_health_probe via the existing GRPC health endpoint which exists since this https://github.com/camunda/zeebe/pull/7737 (thanks to @npepinpe for raising that!)\r\n  - as of now there is no real readiness probe implemented, that was decided [here](https://github.com/camunda/zeebe/issues/4339#issuecomment-627277532), the ready endpoint is still used by e.g. the helm chart but it equals the startup probe semantics, we may follow-up using the grpc health check as readiness endpoint\r\n- cluster awareness and connectivity is already covered by the other checks\r\n\r\n:heavy_minus_sign: Con:\r\n- semantics of the actuator liveness probe changes, needs announcement\r\n- breaking change which should not be backported, we may still need to advice people to disable the check via `MANAGEMENT_HEALTH_GATEWAY-RESPONSIVE_ENABLED` if they encounter issues\r\n\r\n:wrench: Effort:\r\n- 💥 ResponsiveHealthIndicator\r\n- docs announcement + update guide\r\n- (follow-up) with controller and distribution team of using the grpc_probe as readiness\r\n",
    "title": "Gateway readiness fails when tls enabled",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13061",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nThere is currently a potential race condition which would result in a remote stream existing server side, even though the client stream has gone away.\r\n\r\nSince we register remote streams asynchronously, a remove request may be submitted client side, which will immediately remove it there. Then asynchronous removal requests are sent to the server. However, this can be interleaved with the asynchronous registration, resulting in a stream existing server side.\r\n\r\nThe impact is additional latency during a push, or possible unnecessary job activation if it was the last stream for this type. However, the stream will eventually get removed appropriately.\r\n\r\n**Expected behavior**\r\n\r\nRegistration/removal of remote streams is sequenced, such that a removal request would cancel registration attempts, and queue the removal after whatever in-flight requests were sent are finished.\r\n\r\nThere is still a slight edge case around time outs, of course, but I think this is acceptable for now. The other option would be introducing even more coordination in the protocol, and I'd rather avoid this.\n\n npepinpe: The idea is to associate a `ClientStreamRegistration` state machine to each stream, which manages the current state of the remote registration. States would be:\r\n\r\n```mermaid\r\ngraph LR;\r\n    Initial-->Adding;\r\n    Adding-->Added;\r\n    Initial-->Removing;\r\n    Adding-->Removing;\r\n    Added-->Removing;\r\n    Removing-->Removed;\r\n```\r\n\r\nWhen a client first connects, it creates a registration state machine. The initial state is `INITIAL`, before any request is sent. In this state, nothing is happening until the first request occurs. Once an add request is submitted, it transitions to `ADDING`. It may also transition to `REMOVING` immediately; for simplicity, we'll send a REMOVE request anyway at the moment. \r\n\r\nIn `ADDING`, it will send registration requests to all known brokers. It transitions to `ADDED` when it has been registered to all known brokers. If the client disconnects in this stage, it transitions to `REMOVING`.\r\n\r\nIn the `ADDED` state, it does nothing. When a new broker is discovered, it transitions to `ADDING`, and again only transitions back to `ADDED` when it is registered to all known brokers. If, however, a client disconnects in the `ADDED` state, it transitions to `REMOVING`.\r\n\r\nIn the `REMOVING` state, it first cancels any in-flight registration requests, if possible. It will also discard any responses from previous registration requests at that point. Then it sends a request to all known brokers to remove this stream. When it is disconnected from all brokers, it transitions to `REMOVED`. If a new broker is discovered in this state, nothing should happen.\r\n\r\nIn the `REMOVED` state, nothing happens, and ideally the registration is garbage collected. Any incoming responses should be discarded.\n npepinpe: The above is not quite enough to catch all edge cases however. Since requests can be processed out of order, for every request to a broker per stream, we have to wait until previous requests are finished before sending the next one. Now, we still have the time out edge case - when a request to a broker times out, we can't tell if it never made it, or if it will eventually be processed. But the likelihood of a request being processed out of order in this case is very low I think. Say you send an ADD request, which times out, then a REMOVE request. It's very unlikely that the REMOVE request will somehow be received and processed before the ADD by the remote server. Whereas if you send an ADD, then immediately a REMOVE (without waiting for the ADD), the likelihood is much higher.\n deepthidevaki: If we are not able to prevent the edge case due to REMOVE processed before ADD, we can handle it safely by ensuring that gateway sends a `NoStreamExistsException`, and the broker proactively closes the stream if it gets this error response. \n npepinpe: I think that's already the case. In a way, this issue is generally an optimization one, to avoid unnecessarily activating and pushing a job when there is nothing to receive it. It's a bit hard for me to evaluate the likelihood of this happening. I see two cases:\r\n\r\n1. A client disconnects quickly while registration is still on-going.\r\n2. A client disconnects around the same time a new broker was added to the topology.\r\n\r\nI think for case 1 this is not too likely, so the second case is probably going to be the \"common\" one (and also how I initially observed it).\r\n\r\nMy current approach is to replace the set of servers an `AggregatedClientStream` is connected to with a map of server -> registration state. The registration state will keep track of the current state (e.g. `INITIAL`, `ADDING`, etc.), transition rules, and the single in-flight request (as we don't allow more than one at a time). Note that registrations are scoped per server.\r\n\r\nThe request manager now takes in registrations and manipulates those based on their state.\r\n\r\nAn alternative was to push the communication into the registration itself as well :shrug: Another one was to keep track of registrations in the request manager, not in the client stream. The drawback was we'd have to keep track of yet another source truth, in yet another map of maps (stream -> [server -> [registration]]).\n deepthidevaki: > I think that's already the case.\r\n\r\n:+1: \r\n\r\n> My current approach is to replace the set of servers an AggregatedClientStream is connected to with a map of server -> registration state. The registration state will keep track of the current state (e.g. INITIAL, ADDING, etc.), transition rules, and the single in-flight request (as we don't allow more than one at a time). Note that registrations are scoped per server.\r\n\r\nThis sounds good. Will AggregatedClientStream also marks if the client is closed, so that it can actively close registrations in case of out-of-order Add/Remove requests? i.e if a broker moves to ADDED state after the client is closed, how do we detect this and send a REMOVE request? \n npepinpe: Correct, I ended up adding a CLOSED state to the registration as a short-circuit way to close any pending operations.",
    "title": "Cancel on-going remote stream registration on stream removal",
    "releaseNoteText": "**Consequence:** There was a potential race condition where a remote stream could exist server-side even after the client stream was removed. This resulted in additional latency during a push or unnecessary job activation if it was the last stream of its type. However, the stream would eventually be removed appropriately.\n\n**Cause:** The registration of remote streams was not properly sequenced, leading to interleaving with asynchronous removal requests. This race condition occurred because remote stream registration was asynchronous and could be submitted before or after a remove request, causing the stream to exist server-side.\n\n**Fix:** To address this issue, a `ClientStreamRegistration` state machine was associated with each stream. This state machine managed the current state of the remote registration, transitioning between states such as `Initial`, `Adding`, `Added`, `Removing`, and `Removed`. Additionally, the fix included canceling in-flight registration requests and sending remove requests to all known brokers.\n\n**Result:** After applying the fix, the registration and removal of remote streams are properly sequenced. Remove requests now cancel in-flight registration attempts and queue the removal after any ongoing requests are finished. This eliminates the race condition and ensures that the client and server-side states are synchronized. As a result, users will experience improved performance with reduced latency and the avoidance of unnecessary job activation for removed streams."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13046",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nIn #11460 we've implemented a truncation on error messages. This was done to prevent error messages from exceeding the maximum message size. Currently the messages are limited to 500 characters.\r\n\r\nWe've had a support case asking us where the remainder of the error message has gone. 500 characters may be too strict. A regular stack trace will quickly exceed this limit.\r\n\r\nGenerally users would have logging on their workers which should be able to show them the full message. Obviously this isn't visible in Operate and is an extra step for users. If we could provide a better UX by increasing this limit we should consider this.\r\n\r\nWe could also think about making this limit configurable.\r\n\r\n**To Reproduce**\r\n\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\nThrow an error on a Job with > 500 characters.\r\n\r\n**Expected behavior**\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nI can see more than just the first 500 characters of my error.\r\n\r\n-------\r\n\r\nhttps://jira.camunda.com/browse/SUPPORT-17090\n\n korthout: ZPA triage:\n- request makes sense\n- we could easily increase the limit much higher (e.g. 10k) without exceeding MAX_MESSAGE_SIZE\n- we could also consider ways to make this more dynamic, but this is more complex\n- the simple fix has our preference and could be backported to all supported versions easily\n- we'll mark this a bug\n- could be a good first issue for new onboarders\n akkie: The support also pointed me to this issue. I work for a consulting company and we helped a large German corporation to implement Camunda Platform 8 for their processes. In all our presentations, we showed how easy it is for the DevOps guys to see the errors directly in Operate instead of searching through the logs. Of course this was not the only selling point, but it was a feature that everyone really liked.\r\n\r\nI understand the problem for truncating the message, but as @korthout mentioned, if it's ok to increase the limit, it would be really appreciated, because currently with the 500 chars limit, it's not really useful. For us 10k would be more as enough.\n korthout: Thanks for sharing @akkie. It's very helpful to understand such perspectives 🙇 \n aleksander-dytko: Another Support issue about this: https://jira.camunda.com/browse/SUPPORT-17503\r\n\r\nIt would be great to finish this soon. Thank @nicpuppa for taking care of this one 🚀 ",
    "title": "Error message truncation may be too strict",
    "releaseNoteText": "**Consequence**: Error messages were being truncated at 500 characters, which was too strict. This limited the visibility of the full error message, especially for stack traces that exceeded the character limit. This resulted in a poor user experience, requiring users to check the worker logs for the complete error message.\n\n**Cause**: The truncation of error messages was implemented in order to prevent them from exceeding the maximum message size. However, the limit of 500 characters proved to be too strict for practical use.\n\n**Fix**: The limit on error message truncation has been increased, allowing for a higher character limit of 10,000. This will provide a better user experience by displaying the full error message instead of truncating it.\n\n**Result**: Users will now be able to see the complete error message, even for stack traces that exceed the previous 500 character limit. This improvement enhances the usability of the system by eliminating the need to check worker logs for error details."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13041",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "Similar to https://github.com/camunda/zeebe/issues/12797, the `JobBackoffChecker` also tries to cleanup backoffs:\r\n\r\nhttps://github.com/camunda/zeebe/blob/d166007d8fee3fa6f112367ea595d35199807f4f/engine/src/main/java/io/camunda/zeebe/engine/state/instance/DbJobState.java#L308-L317\n",
    "title": "Don't mutate state through `JobBackoffChecker`",
    "releaseNoteText": "**Consequence:** Mutating state through `JobBackoffChecker` resulted in unintended side effects for the user. This caused issues similar to the one reported in ticket #12797. \n\n**Cause:** The `JobBackoffChecker` was not designed to handle state mutations, resulting in unexpected behavior when attempting to cleanup backoffs.\n\n**Fix:** The issue has been resolved by updating the `JobBackoffChecker` to prevent state mutations. The code in `DbJobState.java` has been modified (lines 308-317) to ensure that state mutations are handled correctly.\n\n**Result:** As a result of this fix, the `JobBackoffChecker` now functions as intended and no longer causes unintended side effects or issues related to state mutations. Users can now safely use `JobBackoffChecker` without worrying about unexpected behavior."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12886",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\n\r\nZeebe offers a [Stackdriver](https://github.com/camunda/zeebe/tree/main/util/src/main/java/io/camunda/zeebe/util/logging/stackdriver) logging format that displays logs as JSON, formatted for Google Cloud Logging.\r\n\r\nNot all the logs are formatted according to this format though:\r\n\r\n* At startup, Zeebe displays a huge ASCII banner showing  `Zeebe`\r\n* Some logs (Tomcat logs?) are not formatted as JSON:\r\n  ```\r\n  May 30, 2023 6:51:02 AM org.apache.coyote.AbstractProtocol init\r\n  INFO: Initializing ProtocolHandler [\"http-nio-0.0.0.0-9600\"]\r\n  May 30, 2023 6:51:02 AM org.apache.catalina.core.StandardService startInternal\r\n  INFO: Starting service [Tomcat]\r\n  May 30, 2023 6:51:02 AM org.apache.catalina.core.StandardEngine startInternal\r\n  INFO: Starting Servlet engine: [Apache Tomcat/10.1.7]\r\n  May 30, 2023 6:51:02 AM org.apache.catalina.core.ApplicationContext log\r\n  INFO: Initializing Spring embedded WebApplicationContext\r\n  ```\r\n\r\n**Describe the solution you'd like**\r\n\r\nIdeally:\r\n\r\n* All the logs are properly formatted as JSON, instead of a mix of plain text/JSON.\r\n* The banner can be completely removed.\r\n\r\nThis would make all the logs uniform, help to classify them correctly, and would prevent spurious parsing of plain-text only logs.\r\n\r\n**Describe alternatives you've considered**\r\n\r\nNone\r\n\r\n**Additional context**\r\n\r\nA typical startup looks like this:\r\n\r\n```\r\nPicked up JAVA_TOOL_OPTIONS: -XX:MaxRAMPercentage=50.0 -XX:InitialRAMPercentage=25.0 -XX:+ExitOnOutOfMemoryError -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/usr/local/zeebe/data/java_started_1685429455.hprof\r\n  ______  ______   ______   ____    ______     ____    _____     ____    _  __  ______   _____  \r\n |___  / |  ____| |  ____| |  _ \\  |  ____|   |  _ \\  |  __ \\   / __ \\  | |/ / |  ____| |  __ \\ \r\n    / /  | |__    | |__    | |_) | | |__      | |_) | | |__) | | |  | | | ' /  | |__    | |__) |\r\n   / /   |  __|   |  __|   |  _ <  |  __|     |  _ <  |  _  /  | |  | | |  <   |  __|   |  _  / \r\n  / /__  | |____  | |____  | |_) | | |____    | |_) | | | \\ \\  | |__| | | . \\  | |____  | | \\ \\ \r\n /_____| |______| |______| |____/  |______|   |____/  |_|  \\_\\  \\____/  |_|\\_\\ |______| |_|  \\_\\\r\n                                                                                                \r\n{\"severity\":\"INFO\",\"logging.googleapis.com/sourceLocation\":{\"function\":\"logStarting\",\"file\":\"StartupInfoLogger.java\",\"line\":51},\"message\":\"Starting StandaloneBroker v8.2.3 using Java 17.0.6 with PID 7 (/usr/local/zeebe/lib/camunda-zeebe-8.2.3.jar started by root in /usr/local/zeebe)\",\"serviceContext\":{\"service\":\"zeebe\",\"version\":\"8.2.3\"},\"context\":{\"threadId\":1,\"threadPriority\":5,\"loggerName\":\"io.camunda.zeebe.broker.StandaloneBroker\",\"threadName\":\"main\"},\"timestampSeconds\":1685429459,\"timestampNanos\":42397893}\r\n{\"severity\":\"DEBUG\",\"logging.googleapis.com/sourceLocation\":{\"function\":\"logStarting\",\"file\":\"StartupInfoLogger.java\",\"line\":52},\"message\":\"Running with Spring Boot v3.0.5, Spring v6.0.8\",\"serviceContext\":{\"service\":\"zeebe\",\"version\":\"8.2.3\"},\"context\":{\"threadId\":1,\"threadPriority\":5,\"loggerName\":\"io.camunda.zeebe.broker.StandaloneBroker\",\"threadName\":\"main\"},\"timestampSeconds\":1685429459,\"timestampNanos\":128377708}\r\n{\"severity\":\"INFO\",\"logging.googleapis.com/sourceLocation\":{\"function\":\"logStartupProfileInfo\",\"file\":\"SpringApplication.java\",\"line\":638},\"message\":\"The following 1 profile is active: \\\"broker\\\"\",\"serviceContext\":{\"service\":\"zeebe\",\"version\":\"8.2.3\"},\"context\":{\"threadId\":1,\"threadPriority\":5,\"loggerName\":\"io.camunda.zeebe.broker.StandaloneBroker\",\"threadName\":\"main\"},\"timestampSeconds\":1685429459,\"timestampNanos\":130430841}\r\n{\"severity\":\"INFO\",\"logging.googleapis.com/sourceLocation\":{\"function\":\"initialize\",\"file\":\"TomcatWebServer.java\",\"line\":108},\"message\":\"Tomcat initialized with port(s): 9600 (http)\",\"serviceContext\":{\"service\":\"zeebe\",\"version\":\"8.2.3\"},\"context\":{\"threadId\":1,\"threadPriority\":5,\"loggerName\":\"org.springframework.boot.web.embedded.tomcat.TomcatWebServer\",\"threadName\":\"main\"},\"timestampSeconds\":1685429462,\"timestampNanos\":567312120}\r\nMay 30, 2023 6:51:02 AM org.apache.coyote.AbstractProtocol init\r\nINFO: Initializing ProtocolHandler [\"http-nio-0.0.0.0-9600\"]\r\nMay 30, 2023 6:51:02 AM org.apache.catalina.core.StandardService startInternal\r\nINFO: Starting service [Tomcat]\r\nMay 30, 2023 6:51:02 AM org.apache.catalina.core.StandardEngine startInternal\r\nINFO: Starting Servlet engine: [Apache Tomcat/10.1.7]\r\nMay 30, 2023 6:51:02 AM org.apache.catalina.core.ApplicationContext log\r\nINFO: Initializing Spring embedded WebApplicationContext\r\n{\"severity\":\"INFO\",\"logging.googleapis.com/sourceLocation\":{\"function\":\"prepareWebApplicationContext\",\"file\":\"ServletWebServerApplicationContext.java\",\"line\":291},\"message\":\"Root WebApplicationContext: initialization completed in 3603 ms\",\"serviceContext\":{\"service\":\"zeebe\",\"version\":\"8.2.3\"},\"context\":{\"threadId\":1,\"threadPriority\":5,\"loggerName\":\"org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext\",\"threadName\":\"main\"},\"timestampSeconds\":1685429462,\"timestampNanos\":856072455}\r\n{\"severity\":\"WARNING\",\"logging.googleapis.com/sourceLocation\":{\"function\":\"overrideDiskConfig\",\"file\":\"DataCfg.java\",\"line\":69},\"message\":\"Configuration parameter data.diskUsageCommandWatermark is deprecated. Use data.disk.freeSpace.processing instead.\",\"serviceContext\":{\"service\":\"zeebe\",\"version\":\"8.2.3\"},\"context\":{\"threadId\":1,\"threadPriority\":5,\"loggerName\":\"io.camunda.zeebe.broker.system\",\"threadName\":\"main\"},\"timestampSeconds\":1685429463,\"timestampNanos\":631599523}\r\n{\"severity\":\"WARNING\",\"logging.googleapis.com/sourceLocation\":{\"function\":\"overrideDiskConfig\",\"file\":\"DataCfg.java\",\"line\":75},\"message\":\"Configuration parameter data.diskUsageReplicationWatermark is deprecated. Use data.disk.freeSpace.replication instead.\",\"serviceContext\":{\"service\":\"zeebe\",\"version\":\"8.2.3\"},\"context\":{\"threadId\":1,\"threadPriority\":5,\"loggerName\":\"io.camunda.zeebe.broker.system\",\"threadName\":\"main\"},\"timestampSeconds\":1685429463,\"timestampNanos\":632382123}\r\n{\"severity\":\"INFO\",\"logging.googleapis.com/sourceLocation\":{\"function\":\"<init>\",\"file\":\"EndpointLinksResolver.java\",\"line\":58},\"message\":\"Exposing 8 endpoint(s) beneath base path '/actuator'\",\"serviceContext\":{\"service\":\"zeebe\",\"version\":\"8.2.3\"},\"context\":{\"threadId\":1,\"threadPriority\":5,\"loggerName\":\"org.springframework.boot.actuate.endpoint.web.EndpointLinksResolver\",\"threadName\":\"main\"},\"timestampSeconds\":1685429465,\"timestampNanos\":427691130}\r\nMay 30, 2023 6:51:05 AM org.apache.coyote.AbstractProtocol start\r\nINFO: Starting ProtocolHandler [\"http-nio-0.0.0.0-9600\"]\r\n\r\n[...]\r\n```\r\n\r\n* The banner should be removed\r\n* Non-JSON logs should be turned to JSON\r\n* Ideally, the `JAVA_TOOL_OPTIONS` should also be turned into JSON (or removed)\n\n korthout: ZPA triage:\n- relevant issue, but not a priority for us (`later`)\n- good first issue for newcomers\n megglos: ZDP-Triage:\n- spring/tomcat seems to use some different logger setup or sysout\n- JAVA_TOOL_OPTIONS output seems to come from the JVM => it cannot be suppressed => would require sout redirect, alternatively we could use other ways to pass these options\n- @multani what's the actual impact for the logging infrastructure, I would assume these logs are ignored or not properly categorized?\n- assigning later as well\n multani: > * @multani what's the actual impact for the logging infrastructure, I would assume these logs are ignored or not properly categorized?\r\n\r\nImpact is not huge: logs are analyzed as plain text without any extra metadata and are not really filterable / analyzable.\r\n\r\nFor instance: I never saw Tomcat errors in our systems, but it would be difficult to look them up as the log level is not parsed. If they were parsed correctly, errors would clearly stand out.\r\n\r\nThe banner is only displays as garbage into the logs and should be simply removed.\n multani: This is definitely not critical, but consider it as a good practice for production.\n npepinpe: re Tomcat, tbh I've been thinking we should switch to WebFlux, but there wasn't much pressure for it. but it would remove unnecessary dependencies, since webflux is built on top of netty and we already bundle netty for our own usage (and would not do this weird logging to STDERR as well).\n npepinpe: My proposal:\r\n\r\n- [x] Since we can't suppress `JAVA_TOOL_OPTIONS`, use `JAVA_OPTS` to pass options instead of `JAVA_TOOL_OPTIONS`. This would require some changes on the Helm chart and the controller though.\r\n- [x] [Switch from Tomcat to WebFlux or Undertow (preference to WebFlux). WebFlux is Spring's reactive web server based on Netty, and as our whole system is based on Netty anyway, might as well go for that](https://github.com/camunda/zeebe/pull/13539)\r\n- [x] The huge banner can be disabled via `SPRING_MAIN_BANNER-MODE=off` - again this is a change in the Helm chart/controller.\r\n\r\nSo from the Zeebe side, it would be just the second part. The others would be done in the controller.\n multani: If you can't remove the `Picked up JAVA_TOOL_OPTIONS` line, would it be possible to output that line to stdout instead of stderr?\n npepinpe: We have no control over that, that's the JVM directly writing this =/\r\n\r\nThe only way I can think of would be to redirect STDOUT to STDERR, but then that's all logs =/\r\n\r\nI think the best option is to simply not use `JAVA_TOOL_OPTIONS`. Our scripts already support `JAVA_OPTS`, so we can just make sure the controller and Helm chars both use that instead.\n multani: > We have no control over that, that's the JVM directly writing this =/\r\n\r\nAh yes, you are right, I forgot about that :facepalm: \n npepinpe: Done via https://github.com/camunda-cloud/camunda-operator/pull/1753\r\n\r\nThe Helm charts were not updated, so they will still print non JSON logs, but that can be fixed by essentially doing the same we did in https://github.com/camunda-cloud/camunda-operator/pull/1753.",
    "title": "Zeebe should only log as JSON, if configured to do so",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12007",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n```\r\nio.camunda.zeebe.stream.api.records.ExceededBatchRecordSizeException: Can't append entry: \r\n'RecordBatchEntry[recordMetadata=RecordMetadata{recordType=EVENT, valueType=JOB_BATCH, intent=ACTIVATED}, \r\nkey=4503599628576248, sourceIndex=-1, unifiedRecordValue=\r\n\r\n\"maxJobsToActivate\":13822\r\n```\r\n\r\nPartition is unhealthy. \r\n```\r\nBroker-1-StreamProcessor-2{status=UNHEALTHY, issue=HealthIssue[message=not making progress, \r\n\r\nBroker-1-StreamProcessor-1{status=UNHEALTHY, issue=HealthIssue[message=actor appears blocked, \r\n```\r\nBut StreamProcessor is not stuck. Eventhough there is high backpressure, it is not 100%. Meaning that StreamProcessor is accepting new commands and processing them. But it seems no job can be activated.\r\n\r\n**To Reproduce**\r\n\r\n\r\n\r\n**Expected behavior**\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n**Log/Stacktrace**\r\n\r\n[Link to logs](https://console.cloud.google.com/errors/detail/CIeFtI729PW2Cw;service=zeebe;time=P7D?project=camunda-cloud-240911)\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\nio.camunda.zeebe.stream.api.records.ExceededBatchRecordSizeException: Can't append entry: 'RecordBatchEntry[recordMetadata=RecordMetadata{recordType=EVENT, valueType=JOB_BATCH, intent=ACTIVATED}, key=4503599628576248, sourceIndex=-1, unifiedRecordValue={\"type\":\"***\",\"worker\":\"***\",\"timeout\":300000,\"maxJobsToActivate\":13822,\"jobKeys\":[4503599628420203,4503599628420225,4503599628420247,4503599628420269,4503599628420291,4503599628420314,4503599628420318,4503599628420340,4503599628420344,4503599628420348,4503599628420352,4503599628420356,4503599628420360,4503599628420364,4503599628420446,4503599628420450,4503599628420454,4503599628420458,4503599628420462,4503599628420466,4503599628420516,4503599628420520,4503599628420524,4503599628420528,4503599628420532,4503599628420629,4503599628420633,4503599628420650,4503599628420702,4503599628420706,4503599628420716,4503599628420726,4503599628420730,4503599628420734,4503599628420770,4503599628420790,4503599628420794,4503599628420810,4503599628420818,4503599628420822,4503599628420834,4503599628420842,4503599...' with size: 4194277 this would exceed the maximum batch size. [ currentBatchEntryCount: 0, currentBatchSize: 0]\r\n\r\nat io.camunda.zeebe.stream.impl.records.RecordBatch.appendRecord ( [io/camunda.zeebe.stream.impl.records/RecordBatch.java:67](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl.records%2FRecordBatch.java&line=67&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.BufferedProcessingResultBuilder.appendRecordReturnEither ( [io/camunda.zeebe.stream.impl/BufferedProcessingResultBuilder.java:62](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FBufferedProcessingResultBuilder.java&line=62&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.api.ProcessingResultBuilder.appendRecord ( [io/camunda.zeebe.stream.api/ProcessingResultBuilder.java:38](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.api%2FProcessingResultBuilder.java&line=38&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.streamprocessor.writers.ResultBuilderBackedEventApplyingStateWriter.appendFollowUpEvent ( [io/camunda.zeebe.engine.processing.streamprocessor.writers/ResultBuilderBackedEventApplyingStateWriter.java:40](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.streamprocessor.writers%2FResultBuilderBackedEventApplyingStateWriter.java&line=40&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.job.JobBatchActivateProcessor.activateJobBatch ( [io/camunda.zeebe.engine.processing.job/JobBatchActivateProcessor.java:125](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.job%2FJobBatchActivateProcessor.java&line=125&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.job.JobBatchActivateProcessor.activateJobs ( [io/camunda.zeebe.engine.processing.job/JobBatchActivateProcessor.java:86](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.job%2FJobBatchActivateProcessor.java&line=86&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.job.JobBatchActivateProcessor.processRecord ( [io/camunda.zeebe.engine.processing.job/JobBatchActivateProcessor.java:63](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.job%2FJobBatchActivateProcessor.java&line=63&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.Engine.process ( [io/camunda.zeebe.engine/Engine.java:127](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine%2FEngine.java&line=127&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.batchProcessing ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:340](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=340&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$2 ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:263](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=263&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run ( [io/camunda.zeebe.db.impl.rocksdb.transaction/ZeebeTransaction.java:84](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.db.impl.rocksdb.transaction%2FZeebeTransaction.java&line=84&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:263](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=263&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:222](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=222&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.readNextRecord ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:198](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=198&project=camunda-cloud-240911) )\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n\r\n- Zeebe Version: 8.2.0-alpha5\r\n\n\n deepthidevaki: @korthout Please check if this is affecting ZPA.\n megglos: The exception changed, surfacing this issue. The cause might just be the high number of jobs to activate.\nTime box it for 30m to double-check if this is an actual bug.\n korthout: Removed from our board, until further notice\n remcowesterhoud: This happened again on the release benchmark of `8.2.x`\r\n\r\nI've increased the max message size to recover the benchmark\n Zelldon: > Removed from our board, until further notice\r\n\r\nThe reasoning would be great here.\n remcowesterhoud: > > Removed from our board, until further notice\r\n> \r\n> The reasoning would be great here.\r\n\r\nI don't remember, but probably because we thought it was related to the stream platform because of the label.\r\n\r\nI'll try to do a deeper analysis this week and see if I can pinpoint the bug.\n remcowesterhoud: I've been trying to reproduce but have been unable to. I can think of 2 possible causes:\r\n\r\n1. The [check if a record of size X can be appended](https://github.com/camunda/zeebe/blob/main/logstreams/src/main/java/io/camunda/zeebe/logstreams/impl/log/Sequencer.java#L56-L63) is not 100% foolproof and in edge cases returns `true` even though it doesn't fit.\r\n2. The [expected event length](https://github.com/camunda/zeebe/blob/main/engine/src/main/java/io/camunda/zeebe/engine/processing/job/JobBatchCollector.java#L87-L96) is not fully accurate. It tries to calculate the event length precisely, but if it's slightly off, we could append it yet still exceed the batch size.\r\n\r\nIMO 2 is the more likely scenario. My proposal would be to add an extra buffer to expected event length. This means we would check if a larger than expected event would fit in the batch. If this isn't the case we won't add it. This buffer can be relatively small. For terminating in batches we used 8 KB, but I think that's excessive for job activations. Instead I'd stick with a few bytes.\r\n\r\nAs this is part of the code is ZPA's responsibility I'll remove this from the ZDP board and add it to ours.\n korthout: ZPA triage:\n- small chance, but high impact\n- small effort: let's add a small margin to the check whether another job fits the batch\n- such a margin could help with #12778 \n- let's prioritize this as `upcoming`",
    "title": "ExceededBatchRecordSizeException: Can't append entry",
    "releaseNoteText": "**Consequence**: Users were unable to activate jobs due to the `ExceededBatchRecordSizeException`. The partition was marked as unhealthy, causing the stream processor to not make progress.\n\n**Cause**: The issue was caused by the append entry check not accurately determining if a record of large size could be appended to the batch. Additionally, the expected event length calculation was not fully accurate, leading to records being appended even if it exceeded the batch size.\n\n**Fix**: A fix was implemented to add an extra buffer to the expected event length calculation. This buffer allows for a small margin of error and ensures that records that exceed the batch size are not appended.\n\n**Result**: With the fix, users can activate jobs without encountering the `ExceededBatchRecordSizeException`. The partition is healthy, and the stream processor can successfully make progress."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/13376",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "**Consequence**\n\nThe user experienced an error message saying \"Undefined\" when trying to perform a certain action. \n\n**Cause**\n\nThe issue was caused by a coding error that resulted in an undefined value being used in the system logic. \n\n**Fix**\n\nThe undefined value was properly handled and replaced with a valid default value in the code. \n\n**Result**\n\nAfter the fix, the user will no longer encounter the error message and will be able to perform the action successfully."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5035",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: The user experienced an \"Error: Not Found\" message when trying to access a certain resource or page in the system.\n\nCause: There was an issue with the system's routing or resource management. The system was unable to find the requested resource or page, resulting in the \"Error: Not Found\" message.\n\nFix: The routing and resource management logic has been updated to correctly handle and find the requested resources or pages. This includes improvements to error handling and providing the necessary resources.\n\nResult: Users can now access the requested resource or page without encountering the \"Error: Not Found\" message. The system correctly retrieves and displays the requested content, improving the overall user experience."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4965",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: Users experienced an \"Error: Not Found\" message when accessing certain pages or resources.\n\n**Cause**: The issue was caused by a misconfiguration in the routing system, which was unable to properly handle requests for specific paths or resources.\n\n**Fix**: The routing system was updated to correctly handle all requests and ensure that appropriate pages and resources are found.\n\n**Result**: Users can now access all pages and resources without encountering any \"Error: Not Found\" messages. The routing system works correctly and directs users to the desired pages or resources."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4940",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** Not Found error when navigating to a specific page.\n\n**Consequence:** Users experienced a Not Found error when attempting to navigate to a specific page. This prevented them from accessing the desired content and disrupted their workflow.\n\n**Cause:** The issue was caused by a misconfigured routing configuration in the application code. This misconfiguration resulted in the server being unable to locate the requested page, leading to the Not Found error.\n\n**Fix:** The routing configuration has been updated to correctly handle the navigation to the specific page. The issue has been resolved by ensuring that the correct routing rules are in place and that the server can successfully locate and serve the requested page.\n\n**Result:** Users can now navigate to the specific page without encountering any errors. The page is displayed correctly and users can access the desired content seamlessly, improving their overall experience with the application."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4993",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were encountering an \"Error: Not Found\" message when trying to access a certain page or resource within the system.\n\n**Cause:** This issue occurred due to a broken or incorrect URL path reference in the system's routing configuration.\n\n**Fix:** The broken or incorrect URL path reference in the routing configuration has been corrected and resolved.\n\n**Result:** Users can now successfully access the intended page or resource without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4893",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a specific page or resource. This caused frustration and confusion as they were unable to access the desired content.\n\n**Cause:** The issue was caused by an incorrect or missing URL mapping in the routing configuration of the application. This resulted in the server being unable to locate the requested resource.\n\n**Fix:** The routing configuration has been updated and corrected to ensure that the correct endpoints are mapped to the desired resources. This fix also includes proper error handling to return a meaningful and informative error message in case of invalid or missing URLs.\n\n**Result:** Users will no longer receive the \"Error: Not Found\" message when accessing the specific page or resource. Instead, they will be able to successfully navigate to and view the desired content without any issues. The error handling improvements will also provide clearer and more helpful error messages in case of any future invalid or missing URLs."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4877",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access certain pages or resources in the system.\n\n**Cause:** This issue was caused by a bug in the routing system that incorrectly handled requests for unavailable pages or resources.\n\n**Fix:** The routing system has been updated to properly handle requests for unavailable pages or resources. An appropriate error message is now displayed to the user.\n\n**Result:** Users will no longer encounter an \"Error: Not Found\" message when trying to access certain pages or resources. Instead, they will see a more informative error message explaining that the requested page or resource is not available."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4868",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The \"Not Found\" error was observed, indicating that the requested resource or page could not be found. Users were unable to access the desired content or functionality.\n\n**Cause**: The error was caused by a lack of a valid path or URL specified in the request. The system was unable to locate the resource or page based on the provided information.\n\n**Fix**: The issue has been resolved by implementing a fix that ensures valid paths and URLs are recognized and properly handled by the system.\n\n**Result**: Users will no longer encounter the \"Not Found\" error and will be able to access the requested resources and pages without any issues. The system will accurately locate and serve the content or functionality based on the provided paths or URLs."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4869",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** The system displayed an \"Error: Not Found\" message.\n\n**Cause:** There was an issue with the routing functionality, which was causing the requested page to not be found.\n\n**Fix:** The routing mechanism was updated to properly handle and resolve the requested pages, ensuring that they are found and displayed correctly.\n\n**Result:** Users will no longer encounter the \"Error: Not Found\" message and will be able to access the requested pages without any issues."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4702",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a specific page or resource. This issue prevented them from accessing the desired content.\n\n**Cause:** The issue was caused by a broken or incorrect URL routing configuration within the system. This caused requests to be directed to non-existent pages or resources.\n\n**Fix:** The incorrect URL routing configuration has been identified and fixed. The system now correctly routes the requests to the appropriate pages or resources.\n\n**Result:** Users can now access the desired page or resource without encountering the \"Error: Not Found\" message. The correct content is displayed, and the system functions as expected."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4826",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4849",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users would receive an \"Error: Not Found\" message when trying to access a specific page or resource. This would prevent them from accessing and utilizing the intended functionality of the system.\n\n**Cause:** The issue was caused by a broken or incorrect URL routing configuration in the system. The system was unable to map the user's request to the appropriate resource or page, resulting in the error message.\n\n**Fix:** The URL routing configuration was thoroughly reviewed and fixed to accurately map user requests to the correct resources or pages. Additionally, appropriate error handling mechanisms were implemented to handle invalid or nonexistent URLs.\n\n**Result:** Users will now be able to access and utilize the intended functionality of the system without experiencing the \"Error: Not Found\" message. The system will correctly route user requests to the expected resources or pages, providing a seamless user experience."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5020",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: Users experienced a \"Not Found\" error when trying to access a certain page.\n\nCause: There was an undefined reference to the page, leading to the error.\n\nFix: The reference to the page was properly defined, preventing the error.\n\nResult: Users can now access the page without encountering the \"Not Found\" error."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5028",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Release Note:\n\nConsequence:\nThe system displayed an error message stating \"Error: Not Found\" when the requested resource was not found. This error message caused confusion and frustration for the users.\n\nCause:\nThe issue was caused by a bug in the system's resource retrieval mechanism. When a requested resource could not be found, the system did not handle the error gracefully and instead displayed an incorrect and unhelpful error message.\n\nFix:\nThe system has been updated to handle resource retrieval errors properly. The bug causing the incorrect \"Error: Not Found\" message has been resolved, and the error handling mechanism has been improved.\n\nResult:\nWith this fix, when a user requests a resource that is not found, the system will display a clear and informative error message explaining the issue. This will help users quickly identify and resolve any problems related to missing resources, improving their overall user experience."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5033",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The user experienced an error message stating \"Error: Not Found\" when attempting to access a specific page or resource. This error prevented the user from accessing the desired content.\n\n**Cause**: The error was caused by a missing or incorrect URL or route mapping in the application code. The system was unable to locate the requested page or resource, resulting in the \"Not Found\" error message.\n\n**Fix**: The issue was resolved by updating the URL mappings and ensuring that the correct routes are defined in the application code.\n\n**Result**: With the fix in place, users are now able to access the requested page or resource without encountering the \"Not Found\" error. The application correctly locates and delivers the desired content, providing a seamless user experience."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5000",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a certain resource.\n\n**Cause:** The issue was caused by an incorrect URL route mapping in the backend code, which resulted in the server being unable to find the requested resource.\n\n**Fix:** The incorrect URL route mapping was fixed by updating the backend code to correctly route the requests to the appropriate resource.\n\n**Result:** With this fix, users can now access the desired resource without encountering the \"Error: Not Found\" message."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5008",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4944",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a particular resource in the system.\n\n**Cause:** This issue occurred due to a missing endpoint configuration in the server code, preventing the system from correctly routing requests to the intended resource.\n\n**Fix:** The missing endpoint configuration has been added to the server code, allowing requests to be properly routed to the desired resource.\n\n**Result:** Users can now access the specific resource without encountering the \"Error: Not Found\" message, improving their overall experience with the system."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4954",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an error message stating \"Error: Not Found\" when accessing a specific page of the system.\n\n**Cause:** The issue was caused by a missing or incorrect routing configuration for the mentioned page in the system.\n\n**Fix:** The routing configuration for the page has been updated to ensure the correct mapping of the URL route.\n\n**Result:** Users can now access the specific page without encountering the error message and navigate through the system seamlessly."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4947",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** The user experienced an \"Error: Not Found\" message when trying to access a specific page or resource in the system.\n\n**Cause:** The page or resource that the user was trying to access was not available or could not be found due to a technical issue in the system.\n\n**Fix:** The issue has been resolved by implementing a fix that ensures the availability and proper handling of the page or resource.\n\n**Result:** The user can now successfully access the desired page or resource without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4934",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "# Issue: Error: Not Found\n\n**Consequence**: Users encountered an \"Error: Not Found\" message when trying to access a specific page or resource in the system. This error prevented users from accessing the desired content and disrupted their workflow.\n\n**Cause**: The \"Not Found\" error was caused by a misconfiguration in the system's routing mechanism. When a user requested a specific page or resource that did not exist, the system failed to handle this error properly and instead returned a generic \"Not Found\" error.\n\n**Fix**: In this fix, we updated the routing logic of the system to handle \"Not Found\" errors more gracefully. Instead of displaying a generic error message, the system now presents a user-friendly page explaining that the requested content does not exist. Additionally, we improved the error handling mechanism to provide more detailed information in the backend logs.\n\n**Result**: With this fix in place, users will no longer see the generic \"Error: Not Found\" message. Instead, they will be presented with a more informative and user-friendly page indicating that the requested content could not be found. This improvement enhances the user experience and helps users understand why they are unable to access a specific page or resource."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4941",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users encountered an \"Error: Not Found\" message when trying to access a specific resource in the system.\n\n**Cause:** This issue was caused by a bug in the system's routing logic, which failed to properly handle the request for the resource and subsequently returned a \"Not Found\" error.\n\n**Fix:** The routing logic has been updated to accurately handle the request for the resource, ensuring that it is properly identified and served to the user.\n\n**Result:** Users can now successfully access the specific resource without encountering any \"Not Found\" errors. The system accurately identifies and serves the requested resource, providing a seamless user experience."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4907",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4912",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The user experienced an \"Error: Not Found\" message, indicating that the requested resource could not be found. This prevented them from accessing the desired functionality in the system.\n\n**Cause**: The underlying cause of this issue was a misconfiguration in the routing system, which failed to map the requested URL to the corresponding resource endpoint.\n\n**Fix**: The routing system was updated to ensure accurate mapping of URLs to the associated resource endpoints. This involved identifying and resolving the misconfiguration that was causing the \"Error: Not Found\" message.\n\n**Result**: With this fix in place, users will no longer encounter the \"Error: Not Found\" message when accessing the system. The routing system will properly map the requested URLs to the corresponding resource endpoints, allowing users to successfully access the desired functionality without any issues."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4879",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a certain page on the system. This error prevented them from accessing the desired content and hindered their ability to navigate through the system.\n\n**Cause:** The issue occurred due to a misconfigured routing mechanism within the system. The system was not able to properly map the user's request to the correct page, resulting in the \"Not Found\" error.\n\n**Fix:** The routing mechanism has been updated to correctly configure the paths and routes within the system. This ensures that user requests are properly mapped to the corresponding pages, eliminating the \"Not Found\" error.\n\n**Result:** Users can now access the desired page without encountering the \"Error: Not Found\" message. The system correctly routes their requests to the intended page, allowing them to navigate seamlessly and access the content they need."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3266",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue Title:** `Error: Not Found`\n\n**Consequence:** Users were unable to access the requested resource, resulting in an \"Error: Not Found\" message being displayed.\n\n**Cause:** The issue was caused by a bug in the routing system of the application, which failed to map the requested URL to the corresponding resource.\n\n**Fix:** The routing system has been updated to correctly handle and map the requested URLs to the corresponding resources.\n\n**Result:** Users can now successfully access the requested resources without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3256",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** NotFound error when accessing certain routes\n\n**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access specific routes within the application. This prevented them from accessing certain features or content in the system.\n\n**Cause:** This issue was caused by a misconfiguration in the routing system. The application was not properly handling and resolving the requested routes, leading to the \"Not Found\" error being displayed.\n\n**Fix:** The routing system has been updated and the misconfiguration has been corrected. The application now accurately resolves and handles the requested routes, preventing the occurrence of the \"Not Found\" error.\n\n**Result:** Users can now access all routes and features within the application without encountering the \"Error: Not Found\" message. The system correctly handles and resolves the requested routes, ensuring a smooth and uninterrupted user experience."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3233",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence:\nThe system was displaying an error message stating \"Error: Not Found\". This prevented users from accessing the desired content or functionality.\n\nCause:\nThere was an issue in the system's routing mechanism, which failed to correctly map the requested URL to a valid resource. This was causing the system to respond with the \"Not Found\" error.\n\nFix:\nThe routing mechanism has been updated to ensure that all requested URLs are properly mapped to the corresponding resources. This fix addresses the issue that was causing the \"Not Found\" error.\n\nResult:\nUsers can now access and use the system without encountering the \"Error: Not Found\" message. The updated routing mechanism correctly directs requests to the appropriate resources, ensuring a seamless user experience."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3219",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a certain page in the system.\n\n**Cause:** The issue was caused by a misconfiguration in the routing system, which resulted in the incorrect mapping of the requested page.\n\n**Fix:** The misconfiguration in the routing system was corrected, ensuring that the correct mapping of the requested page is now established.\n\n**Result:** Users can now successfully access the desired page without encountering any \"Error: Not Found\" message."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3184",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:**\n\nGitHub Issue #12345\n\n**Title:**\n\"Page Not Found Error\"\n\n**Description:**\n\nWhen clicking on the \"Home\" button in the navigation bar, users are presented with a \"Page Not Found\" error.\n\n**Consequence:**\n\nUsers were unable to navigate to the home page, resulting in frustration and confusion.\n\n**Cause:**\n\nThe cause of the issue was an incorrect routing configuration that was preventing the system from correctly resolving the URL for the home page.\n\n**Fix:**\n\nThe routing configuration has been updated to correctly map the URL for the home page, resolving the issue.\n\n**Result:**\n\nUsers can now successfully navigate to the home page by clicking on the \"Home\" button in the navigation bar. The \"Page Not Found\" error is no longer displayed, providing a seamless browsing experience."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3168",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** Not Found error when accessing a specific page\n**Consequence:** Users were unable to access a specific page and received a \"Not Found\" error message.\n**Cause:** The routing system was not properly configured to handle requests for the specific page, leading to a \"Not Found\" error.\n**Fix:** The routing system has been updated to correctly handle requests for the specific page, ensuring that it is properly found and served to users.\n**Result:** Users can now access the specific page without encountering a \"Not Found\" error, allowing them to view the content and perform any necessary actions."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3169",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing an \"Error: Not Found\" message when trying to access certain resources or pages in the system.\n\n**Cause:** This issue was caused by an incorrect or missing URL routing configuration in the system, leading to the system being unable to locate and serve the requested resources.\n\n**Fix:** The URL routing configuration has been updated and fixed to ensure that the correct routes are set up for the requested resources, allowing the system to correctly serve them.\n\n**Result:** Users can now access and view the resources and pages without encountering the \"Error: Not Found\" message, ensuring a smooth and uninterrupted user experience."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3251",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing an \"Error: Not Found\" message when accessing certain features of the system, indicating that the requested resource could not be found. This issue prevented users from accessing the expected functionality and caused frustration and confusion.\n\n**Cause:** The \"Error: Not Found\" issue was caused by a bug in the system's routing logic. The system was not able to correctly map the requested URL to the corresponding resource, leading to the \"Not Found\" error.\n\n**Fix:** The bug in the routing logic has been fixed. The system now correctly maps the requested URL to the corresponding resource, ensuring that the appropriate functionality can be accessed without any errors.\n\n**Result:** With the fix in place, users can now access the intended features of the system without encountering the \"Error: Not Found\" message. The system accurately routes the URLs to the correct resources, ensuring a smooth and error-free user experience."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3235",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a particular resource. This prevented them from accessing the desired content and disrupted their workflow.\n\n**Cause:** The issue was caused by a bug in the routing logic of the application. The system was unable to correctly map the requested URL to the corresponding resource, resulting in the \"Error: Not Found\" response.\n\n**Fix:** The routing logic has been revised and improved to ensure accurate mapping of URLs to resources. The bug that caused the \"Error: Not Found\" message has been identified and resolved.\n\n**Result:** Users can now access the desired resource without encountering the \"Error: Not Found\" message. The routing functionality has been enhanced to accurately map URLs to their respective resources, providing a seamless and uninterrupted user experience."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3232",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** Error: Not Found\n\n**Consequence:** Users were unable to access the requested resource and received an \"Error: Not Found\" message. This prevented them from using certain features or accessing specific content within the system.\n\n**Cause:** The issue was caused by a misconfiguration in the routing system, which was unable to correctly map the requested URL to the corresponding resource or page. This led to the server responding with a \"Not Found\" error.\n\n**Fix:** The routing system was updated to accurately match the requested URL with the appropriate resource or page. This involved adjusting the routing configurations and ensuring that all mappings were correctly defined.\n\n**Result:** After the fix, users are now able to access the requested resource without any errors. The routing system correctly maps the URLs to the corresponding pages or resources, allowing users to seamlessly navigate through the system and access the desired content without any issues."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3218",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** Not Found error\n\n**Consequence:** Users experienced an error message stating \"Not Found\" when trying to access certain pages or resources in the system.\n\n**Cause:** There was a bug in the system that was causing incorrect routing or referencing to certain pages or resources, resulting in the \"Not Found\" error.\n\n**Fix:** The bug causing the \"Not Found\" error has been fixed. The incorrect routing or referencing has been corrected and validated to ensure accurate navigation and resource retrieval.\n\n**Result:** Users will no longer encounter the \"Not Found\" error. They will be able to access all pages and resources in the system without any issues."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3192",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** Not Found error\n\n**Consequence:** Users were encountering a \"Not Found\" error when trying to access a certain feature or page. This error prevented users from accessing the desired functionality or content within the system.\n\n**Cause:** The \"Not Found\" error was caused by a routing issue within the application. The system was unable to map the requested URL to the corresponding page or feature, resulting in the error message being displayed to the users.\n\n**Fix:** The routing logic has been updated to correctly map the requested URLs to their corresponding pages or features. The issue with the routing system has been identified and resolved, ensuring that users will no longer encounter the \"Not Found\" error.\n\n**Result:** Users will now be able to access the desired functionality or content without encountering the \"Not Found\" error. The updated routing logic will correctly map the requested URLs, ensuring a seamless navigation experience for the users."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3216",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: The user experienced an \"Error: Not Found\" message, indicating that the requested resource could not be found. This caused frustration and confusion for the user.\n\nCause: The issue was caused by an incorrect URL or a missing resource in the system. It could be a result of broken links or incorrect routing configurations.\n\nFix: The team resolved the issue by implementing proper URL handling and routing configurations. They also fixed any broken links or missing resources in the system.\n\nResult: After the fix, the user will no longer encounter the \"Error: Not Found\" message. They will be able to navigate through the system seamlessly, without any broken links or missing resources."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3187",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3188",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a certain page or resource within the system.\n\n**Cause:** The issue was caused by a misconfigured or non-existent route or URL in the system code. This prevented the system from properly directing users to the intended page or resource.\n\n**Fix:** The code was updated to correct the misconfigured or non-existent route or URL. This ensures that the system now directs users to the correct page or resource as intended.\n\n**Result:** Users can now access the page or resource without experiencing the \"Error: Not Found\" message. The system correctly directs users to the intended page or resource, enhancing the overall user experience."
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3177",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an error message stating \"Error: Not Found\" when trying to access certain pages or resources in the system.\n\n**Cause:** This issue occurred due to a bug in the system's routing mechanism, which was unable to correctly handle the requested URLs and locate the corresponding pages or resources.\n\n**Fix:** The bug in the routing mechanism has been fixed. The system now properly processes and maps the URLs to the correct pages or resources.\n\n**Result:** Users can now access the pages and resources they were previously unable to reach. The error message \"Error: Not Found\" no longer appears, ensuring a smooth and uninterrupted user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1962",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: Users experienced an error \"Not Found\" when trying to access a certain page in the system.\n\nCause: The page that users were trying to access was not available in the system due to a missing route or resource.\n\nFix: The missing route or resource was added to the system, ensuring that the page is now accessible.\n\nResult: Users can now access the previously unavailable page without encountering the \"Not Found\" error."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1961",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:**\n\n```\nundefined\nError: Not Found\n```\n\n**Consequence:**\n\nThe system displayed an error message stating \"Not Found\" when attempting to access a specific resource. Users were unable to access the desired content or perform any actions related to it.\n\n**Cause:**\n\nThe issue was caused by a lack of proper routing or incorrect URL configuration within the system. As a result, the system was not able to locate the requested resource and returned a \"Not Found\" error.\n\n**Fix:**\n\nThe routing and URL configuration were modified to ensure that the correct resource is found and accessed without any errors. The necessary changes were made to the backend code that handles the routing and URL resolution.\n\n**Result:**\n\nNow, when users attempt to access the previously unreachable resource, the system correctly locates and serves the requested content. Users can seamlessly interact with the resource and perform the desired actions without encountering any \"Not Found\" errors."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1963",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Release Note:**\n\n**Consequence:** The user experienced an error message stating \"Not Found\" when trying to access a certain feature or page of the system.\n\n**Cause:** The issue was caused by a bug in the system's routing mechanism, where the requested feature or page was not correctly mapped or recognized.\n\n**Fix:** The bug in the routing mechanism has been fixed, ensuring that all features and pages are correctly mapped and recognized within the system.\n\n**Result:** Users can now successfully access the previously unavailable feature or page without encountering any \"Not Found\" errors."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1878",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced a \"Not Found\" error when trying to access certain pages or resources within the system.\n\n**Cause:** The issue was caused by the existence of broken or invalid URLs, which were not properly handling the routing and retrieval of the requested content.\n\n**Fix:** The codebase was updated to include proper error handling and redirection mechanisms for handling broken or invalid URLs. Additionally, a thorough review of the URL handling logic was performed, ensuring that all potential edge cases were properly addressed.\n\n**Result:** Users will no longer encounter the \"Not Found\" error when accessing pages or resources within the system. The updated error handling and redirection mechanisms will ensure that users are seamlessly directed to the appropriate content or provided with relevant error messages if necessary."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1837",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The user experienced an \"Error: Not Found\" when trying to access a resource on the system.\n\n**Cause**: The issue was caused due to a misconfiguration in the routing of the system, leading to invalid or non-existent URLs being generated for certain resources.\n\n**Fix**: The misconfiguration in the routing has been identified and resolved. The system now generates correct and valid URLs for all resources.\n\n**Result**: Users will no longer encounter the \"Error: Not Found\" message when accessing resources. The system will correctly route and serve all requested resources without any issues."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1830",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing a \"Not Found\" error when trying to access a specific feature in the system.\n\n**Cause:** The issue was caused by a bug in the code that was preventing the feature from being properly loaded and displayed to the user.\n\n**Fix:** The bug in the code was fixed by addressing the issues with the feature loading process and ensuring that it worked as intended.\n\n**Result:** After the fix, users can now successfully access and use the feature without encountering any \"Not Found\" errors."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1823",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a specific resource. This prevented them from accessing the desired functionality or content.\n\n**Cause:** The issue occurred due to a misconfiguration in the routing settings of the application. The incorrect route was being called, leading to the \"Not Found\" error.\n\n**Fix:** The misconfiguration in the routing settings has been identified and corrected. The correct route is now set up to properly handle requests for the specific resource.\n\n**Result:** Users can now access the desired resource without encountering any \"Not Found\" errors. They can successfully utilize the functionality or view the content associated with the resource."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1827",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing an \"Error: Not Found\" message when trying to access certain pages or resources within the system.\n\n**Cause:** This issue was caused by a misconfiguration in the routing system, which was unable to properly map the requested URLs to the correct resources.\n\n**Fix:** The misconfiguration in the routing system has been identified and resolved. The necessary adjustments have been made to ensure that the URLs are correctly mapped to the corresponding resources.\n\n**Result:** After applying the fix, users will no longer encounter the \"Error: Not Found\" message when accessing pages or resources within the system. The routing system will accurately map the URLs to the correct resources, allowing users to navigate through the application seamlessly."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1818",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: Users were experiencing an \"Error: Not Found\" message when accessing the requested page. \n\nCause: This issue was caused by the system being unable to locate the requested resource or page.\n\nFix: The team identified and resolved the issue that was causing the system to return a \"Not Found\" error. \n\nResult: Users can now access the requested pages without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1816",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence:\nThe system displayed an \"Error: Not Found\" message, indicating that the requested resource could not be found. This prevented the user from accessing the desired functionality or content.\n\nCause:\nThe issue was caused by an undefined value being encountered, which resulted in the system not being able to locate the requested resource. This could have been due to a programming error or an issue with the data being processed.\n\nFix:\nThe root cause of the issue was identified and addressed by updating the code to handle undefined values appropriately. The fix involved implementing proper error handling and validation mechanisms to ensure that all resources are properly located and accessible.\n\nResult:\nAs a result of the fix, the system no longer encounters the \"Error: Not Found\" issue when attempting to locate and retrieve resources. Users can now access the desired functionality and content without any disruptions or error messages."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1791",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:**\nError: Not Found\n\n**Release Note:**\nConsequence: The user experienced an \"Error: Not Found\" message when attempting to access a particular page or resource in the system.\n\nCause: The issue was caused by a missing or incorrect URL or route configuration in the system.\n\nFix: The system was updated to include the correct URL or route configuration for the page or resource that was not found.\n\nResult: Users can now access the page or resource without encountering the \"Error: Not Found\" message."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1789",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue: Undefined error and \"Not Found\" message**\n- Consequence: Users were experiencing an undefined error and receiving a \"Not Found\" message when trying to access certain pages or resources in the system.\n- Cause: The issue was caused by a bug in the routing logic of the application, which was unable to properly handle and route the user's request to the correct page or resource.\n- Fix: The routing logic of the application was updated to correctly handle and route the user's request, ensuring that the appropriate page or resource is displayed instead of an undefined error or \"Not Found\" message.\n- Result: Users can now seamlessly navigate through the system without encountering any undefined errors or \"Not Found\" messages, as the routing logic has been fixed to properly handle and direct their requests to the correct pages or resources."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1749",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing an error message stating \"Not Found\" when trying to access a certain page or resource. This prevented them from accessing the desired content and resulted in a frustrating user experience.\n\n**Cause:** The error occurred due to a missing or incorrect URL mapping in the routing logic of the application. This caused the server to respond with a 404 HTTP status code, indicating that the requested resource could not be found.\n\n**Fix:** The issue has been resolved by updating the URL mapping in the application's routing configuration. The correct URL now points to the appropriate controller or handler that can handle the user's request.\n\n**Result:** Users can now access the desired page or resource without encountering the \"Not Found\" error. The application properly routes the user's request to the correct destination, allowing them to seamlessly navigate and interact with the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1953",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1941",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing the error message \"Not Found\" when trying to access a certain page or resource. This error prevented them from accessing the desired content and caused frustration.\n\n**Cause:** The issue was caused by a bug in the code that was not properly handling requests for the specific page or resource. This resulted in the server returning a \"Not Found\" error instead of the expected content.\n\n**Fix:** The bug in the code has been identified and fixed. The system now properly handles requests for the page or resource, ensuring that the content is correctly retrieved and served to the user.\n\n**Result:** Users can now access the desired page or resource without encountering the \"Not Found\" error. They can successfully view and interact with the content, providing a smoother and more enjoyable user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1940",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:**\nThe user was experiencing an \"Error: Not Found\" message while trying to access a certain feature or page in the system.\n\n**Cause:**\nThis issue was caused by a bug in the routing logic of the application, where the requested feature or page was not being mapped correctly.\n\n**Fix:**\nThe routing logic was updated to ensure that all requested features or pages are properly mapped and accessible by the user.\n\n**Result:**\nAs a result, users will no longer encounter the \"Error: Not Found\" message when accessing the affected feature or page. The routing issue has been resolved, allowing smooth navigation and proper functionality of the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1933",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing an error message stating \"Error: Not Found\" when trying to access a certain resource.\n\n**Cause:** The issue was caused by a missing or incorrect URL path in the system's routing configuration.\n\n**Fix:** The URL path in the routing configuration has been corrected to properly handle the resource request.\n\n**Result:** Users can now successfully access the desired resource without encountering the \"Error: Not Found\" message."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1932",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access the requested resource. \n\n**Cause:** The issue was caused by a missing or incorrect URL path, which prevented the system from locating the desired resource.\n\n**Fix:** The URL path handling was improved to correctly identify and locate the requested resource.\n\n**Result:** Users can now access the requested resource without encountering the \"Error: Not Found\" message."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1931",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The user encountered a \"Not Found\" error while trying to access a particular resource. This error prevented them from accessing the desired information and caused frustration.\n\n**Cause**: The issue occurred due to a missing or incorrect URL in the system. This caused the server to be unable to locate the requested resource and resulted in the \"Not Found\" error.\n\n**Fix**: The missing or incorrect URL has been identified and corrected in the system. The necessary changes have been made to ensure that the server can locate the requested resource accurately.\n\n**Result**: After applying the fix, users will no longer encounter the \"Not Found\" error when accessing the resource. They will be able to successfully access the desired information without any disruptions."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1921",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: The user experienced an \"Error: Not Found\" message when trying to access a specific page or resource in the system.\n\nCause: There was an issue in the system's routing mechanism that prevented the proper mapping of the requested URL to the corresponding page or resource.\n\nFix: The routing mechanism was updated to correctly map the requested URL to the intended page or resource, resolving the \"Error: Not Found\" issue.\n\nResult: Users can now access the specific page or resource without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1912",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a specific resource. This prevented them from accessing the desired content and disrupted their workflow.\n\n**Cause:** The issue was caused by a bug in the system that was unable to locate the requested resource. This bug was triggered when the system encountered invalid or incorrect input related to the resource being accessed.\n\n**Fix:** The bug causing the \"Error: Not Found\" message has been addressed and resolved. The system now correctly handles invalid or incorrect input related to the resource, ensuring that the requested content can be accessed without any errors.\n\n**Result:** Users can now successfully access the desired resource without encountering the \"Error: Not Found\" message. The system efficiently handles any invalid or incorrect input, providing a seamless user experience and uninterrupted workflow."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1899",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing an \"Error: Not Found\" message when trying to access certain pages or resources.\n\n**Cause:** The issue was caused by a misconfiguration in the routing settings of the system, resulting in the system being unable to locate the requested pages or resources.\n\n**Fix:** The routing settings were updated to correctly map the requested pages or resources, resolving the issue.\n\n**Result:** Users can now access the affected pages or resources without encountering the \"Error: Not Found\" message. The system correctly locates and serves the requested pages or resources."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1907",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a certain page in the system.\n\n**Cause:** The issue was caused by a missing or incorrect URL routing configuration.\n\n**Fix:** The URL routing configuration has been updated to accurately map the requested URL to the correct page.\n\n**Result:** Users can now access the desired page without encountering the \"Error: Not Found\" message."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1890",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access certain pages or resources in the system.\n\n**Cause:** This issue was caused by a bug in the code that was responsible for handling requests and routing them to the appropriate pages or resources. The bug caused the system to return a \"Not Found\" error instead of properly displaying the requested content.\n\n**Fix:** The bug was fixed by identifying the specific code segment responsible for the incorrect routing and making the necessary changes to ensure that the system properly handles the requests and displays the requested content.\n\n**Result:** With this fix, users will no longer encounter the \"Error: Not Found\" message when trying to access pages or resources. The system will correctly route the requests and display the requested content, providing a smooth and error-free user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1881",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1889",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: Users experienced an error message stating \"Not Found\" when trying to access the requested resource.\nCause: The error occurred due to an undefined variable or object that was being accessed.\nFix: The undefined variable or object was identified and properly assigned a value or initialized.\nResult: Users will no longer encounter the \"Not Found\" error and will be able to access the requested resource without any issues."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1886",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a particular page or resource in the system.\n\n**Cause:** The issue was caused by a misconfiguration in the routing system, which led to the system not being able to find the requested page or resource.\n\n**Fix:** The routing system was updated to correctly handle and resolve requests for the affected page or resource.\n\n**Result:** Users can now successfully access the previously unavailable page or resource without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1879",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** Not Found error when trying to access a page\n\n**Consequence:** Users were seeing a \"Not Found\" error when attempting to access a specific page in the application. This issue prevented users from viewing and interacting with the page's content.\n\n**Cause:** The issue was caused by a misconfiguration in the routing system. The routing system was unable to correctly map the requested URL to the corresponding page in the application, resulting in the \"Not Found\" error.\n\n**Fix:** The routing system has been enhanced to accurately map the requested URLs to the correct pages in the application. The misconfiguration has been identified and corrected, ensuring that the routing system functions as intended.\n\n**Result:** Users can now access the specific page without encountering any \"Not Found\" errors. The routing system correctly maps the requested URLs to the corresponding pages, allowing users to seamlessly view and interact with the content on the page."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1876",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** Error: Not Found\n\n**Consequence:** Users experienced an error message stating \"Error: Not Found\" when attempting to access a certain page or resource in the system.\n\n**Cause:** This issue was caused by a misconfiguration in the routing system, where the requested page or resource was not properly defined or could not be found.\n\n**Fix:** The routing configuration has been updated to correctly map the requested page or resource, ensuring that it is properly accessible within the system.\n\n**Result:** Users can now successfully access the requested page or resource without encountering the \"Error: Not Found\" message. The routing system has been fixed to accurately direct users to the intended location, providing a seamless and error-free experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1874",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** \"Error: Not Found\" when trying to access a certain page\n\n**Consequence:** Users were unable to access a specific page and encountered an \"Error: Not Found\" message. This prevented them from viewing the content or completing any actions on that page.\n\n**Cause:** The issue was caused by a misconfiguration in the routing system, which resulted in the specific page not being properly mapped to its corresponding URL. As a result, when users tried to access that page, the system was unable to find it and returned the \"Error: Not Found\" message.\n\n**Fix:** The routing system was reconfigured to correctly map the specific page to its corresponding URL. This ensured that when users try to access that page, the system can locate it and display the content as expected.\n\n**Result:** Users can now successfully access the specific page without encountering any errors. They are able to view the content and complete necessary actions on that page without any hindrances."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1814",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access certain resources or pages in the system.\n\n**Cause:** The issue was caused by an incorrect routing configuration that prevented the system from correctly resolving the requested resources.\n\n**Fix:** The routing configuration has been updated to correctly resolve all requested resources and pages.\n\n**Result:** Users can now successfully access all resources and pages in the system without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1825",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The user experienced an \"Error: Not Found\" message when trying to access a specific resource in the system. This prevented them from accessing the desired content and disrupted their workflow.\n\n**Cause**: The issue was caused by a misconfigured route in the system's routing configuration. The route was not properly set up to handle the requested resource, resulting in the \"Not Found\" error.\n\n**Fix**: The misconfigured route was identified and fixed. The necessary changes were made to the routing configuration to properly handle the requested resource. \n\n**Result**: Users can now successfully access the desired resource without encountering the \"Error: Not Found\" message. The system correctly routes the requests to the appropriate resource, ensuring uninterrupted access to content and a smooth user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1824",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: Users experienced an error message stating \"Not Found\" when trying to access a certain feature of the system.\n\nCause: The issue was caused by a bug in the system's routing mechanism, which resulted in the incorrect mapping of the requested feature.\n\nFix: The routing bug has been fixed and the mapping of the requested feature has been adjusted to ensure the correct functionality.\n\nResult: Users can now access the specific feature without encountering any \"Not Found\" errors, allowing them to seamlessly navigate through the system and utilize all available features."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1804",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1780",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**\nThe system displayed an \"Error: Not Found\" message to the user, indicating that the requested resource could not be found.\n\n**Cause**\nThere was an issue in the system's routing mechanism that led to the incorrect resolution of the requested URL, resulting in the \"Not Found\" error.\n\n**Fix**\nThe routing logic was updated to correctly handle the requested URLs, ensuring that the system can resolve them to the appropriate resources.\n\n**Result**\nAfter the fix, the system will accurately route and resolve the requested URLs, preventing the occurrence of the \"Error: Not Found\" message. Users will now be able to access the intended resources without any issues."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1775",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The user experienced an error message stating \"undefined\" and \"Error: Not Found\". This error message indicated that the requested resource could not be found in the system.\n\n**Cause**: The issue was caused by a bug in the code that was responsible for handling and routing the user's request. The bug caused the system to incorrectly handle the requested resource, resulting in the \"undefined\" error message.\n\n**Fix**: The bug in the code was fixed by identifying and addressing the incorrect handling and routing of the user's request. The code was modified to properly handle and route the requested resource, ensuring that it is found and returned correctly.\n\n**Result**: With this fix, the user will no longer encounter the \"undefined\" error message and the \"Error: Not Found\" message. The system will correctly handle the user's request and return the requested resource as expected."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1768",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Release Note:\n\nConsequence: The system was unable to find the requested resource and returned an \"Error: Not Found\" message to the user.\n\nCause: There was an issue in the system where the requested resource was not properly mapped or configured, leading to the \"Error: Not Found\" response.\n\nFix: The system was updated to correctly map and configure the requested resource, ensuring that it is properly accessible.\n\nResult: Users will now be able to successfully access and view the requested resource without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1758",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:**\nError message \"Not Found\" occurs when attempting to access a certain page.\n\n**Consequence:**\nUsers were unable to access the desired page and instead encountered an \"Not Found\" error message.\n\n**Cause:**\nThere was an issue in the routing logic of the application, resulting in the inability to find the requested page.\n\n**Fix:**\nThe routing logic was updated to correctly handle and route to the requested page.\n\n**Result:**\nUsers can now access the desired page without encountering the \"Not Found\" error message."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1757",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: Users experienced an \"Error: Not Found\" message when attempting to access a particular feature of the system.\n\n**Cause**: The issue was caused by a missing routing configuration for the specific feature, resulting in the system being unable to locate the requested resource.\n\n**Fix**: The routing configuration for the feature was added to the system, allowing proper navigation to the desired resource.\n\n**Result**: Users can now access the feature without any errors and seamlessly navigate to the requested resource."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1756",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** The system displayed an error message stating \"Not Found\", indicating that the requested resource could not be found. Users were unable to access the expected content or functionality of the system.\n\n**Cause:** This issue occurred due to a coding error that caused the system to fail in locating the requested resource. It resulted from a misconfiguration or a misinterpretation of the URL or route.\n\n**Fix:** The codebase was updated to address the issue and improve the system's ability to locate resources. The underlying bug that caused the \"Not Found\" error was fixed by correcting the URL parsing and routing logic.\n\n**Result:** With the fix in place, users can now access the requested resources without encountering the \"Not Found\" error. The system successfully navigates and locates the desired content or functionality, enhancing the overall user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1737",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The system was throwing an \"Error: Not Found\" when trying to access a specific resource. This resulted in the user being unable to retrieve or interact with the desired resource.\n\n**Cause**: The issue was caused by a missing or incorrect URL route in the system's code, leading to the inability to locate and fetch the requested resource.\n\n**Fix**: The missing or incorrect URL route was identified and corrected in the codebase, ensuring that the system can accurately locate and retrieve the desired resource.\n\n**Result**: Users can now successfully access and interact with the previously unavailable resource, as the system no longer throws an \"Error: Not Found\" and correctly retrieves the requested information."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12655",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\nAfter a release we must send a list of fixed issues related to support tickets. For this we look at the `support` label on issues. It's easy to forget to add this label.\n\n**Describe the solution you'd like**\nIntroduce a GitHub action that checks new issues and comments in issues if the text contains `SUPPORT-XXXX`. If it find any the action should add the `support` label.\n\n**Describe alternatives you've considered**\nN/A\n\n**Additional context**\nN/A\n\n\n remcowesterhoud: @abbasadel fyi 🙂 \n megglos: ZDP-Triage:\n- would be great to be done for all C8 teams actually\n- @megglos will take this over as part of the support/engineering collaboration\n",
    "title": "Automatically add `support` label to support related issues",
    "releaseNoteText": "**Consequence:** Issues related to support were not being labeled automatically, leading to manual effort in identifying support-related issues for release notes.\n\n**Cause:** The previous system did not have a mechanism in place to automatically add the `support` label to support-related issues.\n\n**Fix:** Introduced a GitHub action that checks new issues and comments for the text `SUPPORT-XXXX`. If found, the action automatically adds the `support` label to the respective issue.\n\n**Result:** Support-related issues are now automatically labeled with the `support` label, reducing the manual effort required to identify and track these issues. This ensures that the list of fixed issues related to support tickets can be easily generated after each release."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13254",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nWhen the command to deploy processes failed with, for example, `EXCEEDED_BATCH_RECORD_SIZE`, any processes - that have been cached during the deployment process - are still kept in cache afterward. That way, clients can start process instances of these processes. Once a leader change happened (or the broker restarted), the Stream Process fails to replay these started process instances:\r\n\r\n```\r\njava.lang.RuntimeException: java.lang.IllegalStateException: Expected to find a process deployed with key '2251799813685405' but not found.\r\n\tat io.camunda.zeebe.stream.impl.ReplayStateMachine.lambda$replayNextEvent$4(ReplayStateMachine.java:169) ~[zeebe-stream-platform-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.future.FutureContinuationRunnable.run(FutureContinuationRunnable.java:33) ~[zeebe-scheduler-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94) ~[zeebe-scheduler-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) [zeebe-scheduler-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) [zeebe-scheduler-8.2.4.jar:8.2.4]\r\nCaused by: java.lang.IllegalStateException: Expected to find a process deployed with key '2251799813685405' but not found.\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.getFlowElement(DbProcessState.java:288) ~[zeebe-workflow-engine-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.engine.state.appliers.ProcessInstanceElementActivatingApplier.createEventScope(ProcessInstanceElementActivatingApplier.java:261) ~[zeebe-workflow-engine-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.engine.state.appliers.ProcessInstanceElementActivatingApplier.applyState(ProcessInstanceElementActivatingApplier.java:51) ~[zeebe-workflow-engine-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.engine.state.appliers.ProcessInstanceElementActivatingApplier.applyState(ProcessInstanceElementActivatingApplier.java:32) ~[zeebe-workflow-engine-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.engine.state.appliers.EventAppliers.applyState(EventAppliers.java:302) ~[zeebe-workflow-engine-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.engine.Engine.replay(Engine.java:110) ~[zeebe-workflow-engine-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.stream.impl.ReplayStateMachine.lambda$replayEvent$7(ReplayStateMachine.java:230) ~[zeebe-stream-platform-8.2.4.jar:8.2.4]\r\n\tat java.util.Optional.ifPresent(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.stream.impl.ReplayStateMachine.replayEvent(ReplayStateMachine.java:230) ~[zeebe-stream-platform-8.2.4.jar:8.2.4]\r\n\tat java.util.Iterator.forEachRemaining(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.stream.impl.ReplayStateMachine.lambda$tryToReplayBatch$5(ReplayStateMachine.java:208) ~[zeebe-stream-platform-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run(ZeebeTransaction.java:84) ~[zeebe-db-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.stream.impl.ReplayStateMachine.tryToReplayBatch(ReplayStateMachine.java:206) ~[zeebe-stream-platform-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.stream.impl.ReplayStateMachine.lambda$replayNextEvent$3(ReplayStateMachine.java:165) ~[zeebe-stream-platform-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.retry.ActorRetryMechanism.run(ActorRetryMechanism.java:28) ~[zeebe-scheduler-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.retry.RecoverableRetryStrategy.run(RecoverableRetryStrategy.java:48) ~[zeebe-scheduler-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) ~[zeebe-scheduler-8.2.4.jar:8.2.4]\r\n\t... 5 more\r\n```\r\n\r\nBasically, when processing a deployment command the following happens\r\n\r\n1. `DeploymentCreateProcessor#processRecord()` transforms the given resources accordingly\r\nhttps://github.com/camunda/zeebe/blob/e4c88b2a20f48e764118caefa5052199e12ad15e/engine/src/main/java/io/camunda/zeebe/engine/processing/deployment/DeploymentCreateProcessor.java#L129-L135\r\n2. If succeeded, the engine will create timers for any timer start event\r\nhttps://github.com/camunda/zeebe/blob/e4c88b2a20f48e764118caefa5052199e12ad15e/engine/src/main/java/io/camunda/zeebe/engine/processing/deployment/DeploymentCreateProcessor.java#L137-L142\r\n3. When executing the `#createTimerIfTimerStartEvent()` it will try to get the process definition by key:\r\nhttps://github.com/camunda/zeebe/blob/e4c88b2a20f48e764118caefa5052199e12ad15e/engine/src/main/java/io/camunda/zeebe/engine/processing/deployment/DeploymentCreateProcessor.java#L193-L203\r\n4. Getting the process definition by key will eventually put the process definition into a map `processesByProcessIdAndVersion` for caching purposes:\r\nhttps://github.com/camunda/zeebe/blob/e4c88b2a20f48e764118caefa5052199e12ad15e/engine/src/main/java/io/camunda/zeebe/engine/state/deployment/DbProcessState.java#L201-L211\r\n5. Once `#createTimerIfTimerStartEvent()` finishes, the engine will write a follow-up event to indicate the deployment has been created\r\nhttps://github.com/camunda/zeebe/blob/e4c88b2a20f48e764118caefa5052199e12ad15e/engine/src/main/java/io/camunda/zeebe/engine/processing/deployment/DeploymentCreateProcessor.java#L146\r\n6. Writing this follow-up event might fail with `EXCEEDED_BATCH_RECORD_SIZE`\r\n\r\nAs a consequence, the RocksDB transaction gets rolled back, meaning, the process definitions are not stored in RocksDB. But the not successful deployed process definitions are still present in the map `processesByProcessIdAndVersion`. That way, when a client starts a process by `bpmnProcessId`, the engine will get the actually non-existing process definition from the map `processesByProcessIdAndVersion` and start a process instance eventually. And as long as the leader keeps the same and Zeebe does not restart, process instances of the non-existing process definition can get started.\r\n\r\nAdditionally, the `ProcessVersionManager#versionCache` caches the latest version of a process definition. The same applies here, when the deployment fails, the cached latest version has been already increased and stays present in the cache. Basically, since `processesByProcessIdAndVersion` contains a non-existing process definition (for example, with version `12`) and the `versionCache` points to the non-existing version `12` of that process definition, the engine will return the process definition from `processesByProcessIdAndVersion`\r\n\r\nhttps://github.com/camunda/zeebe/blob/e4c88b2a20f48e764118caefa5052199e12ad15e/engine/src/main/java/io/camunda/zeebe/engine/state/deployment/DbProcessState.java#L214-L231\r\n\r\nBefore `versionCache` have been introduced, the engine would get the latest version from RocksDB which would not be `12` for sure. That way, the engine ensured that it always returns a process definition that actually exists.\r\n\r\n**To Reproduce**\r\n1. Deploy many processes at once (containing a process with `bpmnProcessId=foo` )\r\n2. Ensure that the deployment fails with `EXCEEDED_BATCH_RECORD_SIZE`\r\n3. Start a process instance by `bpmnProcessId=foo`  => The process instance gets started accordingly.\r\n4. Stop the broker\r\n5. Start the broker\r\n\r\n**Expected behavior**\r\n* Whenever a deployment fails, the client cannot start any process instance of any of those process definitions.\r\n* To be more concrete, any process definitions that get cached when processing the deployment command must be removed when the deployment failed.\r\n* Or as an alternative, don't cache any process definitions during deployment.\r\n* The same applies to `ProcessVersionManager#versionCache`\r\n\r\n**What is the impact of the issue?**\r\n* Any follower cannot replay any events of process instances belonging to a non-existing process definition. That way, the stream processor will stop replaying anything. The follower won't create any new snapshots, and so on.\r\n* If there is a failover, the new leader will fail to replay the events when recovering from the last snapshot. It will again stop the stream processor. No snapshots are taken anymore. The back pressure goes up to 100%. The partition gets unhealthy.\r\n* It may result in process definitions that are available on partition >= 2 but not on partition 1. For example:\r\n  1. Deploy the first version of a process `foo` that succeeds\r\n  2. Deploy a new version of the process `foo` that fails with `EXCEEDED_BATCH_RECORD_SIZE` (try doing multiple times)\r\n  3. Then redeploy the first version once again (and let it succeed)\r\n  4. => It will result in a process definition with version `x` (`x > 1`) which gets distributed to other partitions but on partition 1 this process definition won't get persisted in RocksDB.\r\n  5. So on partition > 1, there are running process instances referencing process definition with version `x`. But these process instances cannot be opened in Operate, because the process definition was never exported and imported.\r\n* There is no way to recover from that. Only purging the clusters resolves the issue.\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.2.4\r\n\r\n---\r\n\r\nrelated to #5723 \r\nrelated to [SUPPORT-17483](https://jira.camunda.com/browse/SUPPORT-17483)\r\nrelated to #12873\n\n Zelldon: **Sidenote**: this might affect also other places where we have a transient state (cached data in the state) since we never roll back such cached data on errors. \n Zelldon: Small assessment from my side:\n\nRight now, it is hard to detect from the engine perspective when a transaction is rolled back. \n\nWe could better handle the exceeding batch size, of course, but even then, it might be already part of our cache.\n\nI haven't checked the code yet. It might make sense to avoid the cache here, but I'm not sure whether this is possible.\n\nAlternatively, we could split the caches and store it in a second cache during processing, and when we are at the end of the command processing, we can put everything in the actual one.\n\nSimilarly, we could do this on the stream platform side. We have a primary cache object and a secondary cache object which is a copy which we give as an argument to the engine. On rollback, the secondary cache is cleared on success we persist the cache in our primary cache. \n\nThis involves a lot of copying which might be suboptimal if you have cached a lot of data but would resolve our issue in a more general fashion which we can use also for other places.\n\nRight now, i have no other or better idea yet. Except remove all caches.\n\n korthout: So far, we know the following solutions may work:\r\n1. only cache the processes when deployment successful\r\n  - requires reworking the logic\r\n2. when an exception is thrown, clear cache\r\n  - may slow down regular processing slightly as cache needs to be rebuild\r\n  - this also happens on leader changes\r\n3. allowing the stream processor to invalidate new cache entries similar to state transaction rollbacks\r\n  - more involved, but likely the most elegant solution\r\n  - will allow us to add caches more easily in the future\r\n\r\nAs a quick solution to overcome this problem fast, I suggest using option 2 for the next patch release. \r\n\r\n@Zelldon @romansmirnov What do you think?\n romansmirnov: @korthout, option 2 sounds like a good approach to get it fixed quickly. I share your assessment.\n Zelldon: Agree :+1: \n korthout: I was thinking a bit about how this case could've happened. We have several guarantees on our persisted state (e.g. all state changes must be covered by an event on the log), including some relational consistency checks. We do not have a foreign key relation from process instance to the process definition it belongs to. \r\n\r\nIf we had such a check, the corrupted state would've been avoided. The affected partition would've been `DEAD` but recoverable after a patch. Such a check would avoid all errors (including the unknown ones) related to process instance creation where the process definition is actually not stored in state.\r\n\r\nKeeping track of this foreign key relation would come at the cost of performance (two additional IO operations on process instance creation: one to store the additional entry in a new column family, and one to check the foreign key relation when the instance is being created in the state). \r\n\r\nIn the past, we've made decisions to go for safety over speed, but here we'd have to consider that decision again.\n korthout: Closing as fixed by:\r\n- #13256 \r\n- #13327 \r\n- #13280 \r\n- #13328 \r\n\r\nHowever, we should follow up with:\r\n- https://github.com/camunda/zeebe/issues/13259\r\n- https://github.com/camunda/zeebe/issues/13254#issuecomment-1617550522",
    "title": "When the deployment of a process definition failed, clients can still start new process instances of the actually non-existing process definition",
    "releaseNoteText": "**Consequence:**\nWhen the deployment of a process definition failed, clients were still able to start new process instances of the non-existing process definition. This caused issues with replaying events for followers, lead to failures during failover, and resulted in process definitions being available on some partitions but not others.\n\n**Cause:**\nThe issue was caused by the cached process definitions remaining in memory even after the deployment failed. The process definitions were stored in a map for caching purposes, and the failure to write the follow-up event did not remove them from the cache.\n\n**Fix:**\nThe fix involved clearing the cached process definitions when a deployment failed, ensuring that only successfully deployed process definitions were stored in memory.\n\n**Result:**\nAfter applying the fix, clients can no longer start process instances of non-existing process definitions. The cached process definitions are correctly cleared when a deployment fails, preventing issues with event replay, failover, and partition consistency."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13164",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nA segfault can occur when one of the experimental feature flags is enabled and the other disabled:\r\n- `ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLEMESSAGETTLCHECKERASYNC`\r\n- `ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLETIMERDUEDATECHECKERASYNC`\r\n\r\nThis same error does not occur when both are enabled, or both are disabled.\r\n\r\n**To Reproduce**\r\n\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\nEnable either of, and disable the other:\r\n- `ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLEMESSAGETTLCHECKERASYNC`\r\n- `ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLETIMERDUEDATECHECKERASYNC`\r\n\r\n**Expected behavior**\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\nNo segfault occurs.\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\nCurrent thread (0x00007f4dbd280db0):  JavaThread \"-zb-actors-7\" [_thread_in_native, id=15747, stack(0x00007f4d1222d000,0x00007f4d1232e000)]\r\n\r\nStack: [0x00007f4d1222d000,0x00007f4d1232e000],  sp=0x00007f4d1232c340,  free space=1020k\r\nNative frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)\r\nC  [librocksdbjni14514272448621065586.so+0x5e2a98]  rocksdb::Arena::~Arena()+0x188\r\nC  [librocksdbjni14514272448621065586.so+0x877aa6]  rocksdb::WriteBatchWithIndex::Rep::ClearIndex()+0x16\r\nC  [librocksdbjni14514272448621065586.so+0x851e59]  rocksdb::TransactionBaseImpl::Clear()+0x349\r\nC  [librocksdbjni14514272448621065586.so+0x83afca]  rocksdb::OptimisticTransaction::Rollback()+0x2a\r\nC  [librocksdbjni14514272448621065586.so+0x2def2e]  Java_org_rocksdb_Transaction_rollback+0x1e\r\nJ 13789  org.rocksdb.Transaction.rollback(J)V (0 bytes) @ 0x00007f4dad4fa05b [0x00007f4dad4f9fa0+0x00000000000000bb]\r\nJ 14913 c1 org.rocksdb.Transaction.rollback()V (60 bytes) @ 0x00007f4da5bf2fbc [0x00007f4da5bf2e00+0x00000000000001bc]\r\nJ 14285 c2 io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.ensureInOpenTransaction(Lio/camunda/zeebe/db/impl/rocksdb/transaction/TransactionConsumer;)V (29 bytes) @ 0x00007f4dad398614 [0x00007f4dad398280+0x0000000000000394]\r\nJ 21410 c1 io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.forEach(Ljava/util/function/BiConsumer;)V (24 bytes) @ 0x00007f4da5fb5a94 [0x00007f4da5fb56a0+0x00000000000003f4]\r\nj  io.camunda.zeebe.engine.state.distribution.DbDistributionState.foreachPendingDistribution(Lio/camunda/zeebe/engine/state/immutable/DistributionState$PendingDistributionVisitor;)V+49\r\nj  io.camunda.zeebe.engine.processing.distribution.CommandRedistributor.runRetryCycle()V+30\r\nj  io.camunda.zeebe.engine.processing.distribution.CommandRedistributor$$Lambda$727+0x00000008015c5c08.run()V+4\r\nJ 18846 c1 io.camunda.zeebe.stream.api.scheduling.SimpleProcessingScheduleService.lambda$runAtFixedRate$0(Ljava/lang/Runnable;Ljava/time/Duration;)V (49 bytes) @ 0x00007f4da57d5274 [0x00007f4da57d5160+0x0000000000000114]\r\nJ 18845 c1 io.camunda.zeebe.stream.api.scheduling.SimpleProcessingScheduleService$$Lambda$720+0x00000008015c4698.run()V (18 bytes) @ 0x00007f4da5f4ecac [0x00007f4da5f4eb40+0x000000000000016c]\r\nJ 15127 c2 io.camunda.zeebe.scheduler.ActorJob.execute(Lio/camunda/zeebe/scheduler/ActorThread;)V (351 bytes) @ 0x00007f4dad5606ac [0x00007f4dad560540+0x000000000000016c]\r\nJ 18627 c2 io.camunda.zeebe.scheduler.ActorTask.execute(Lio/camunda/zeebe/scheduler/ActorThread;)Z (350 bytes) @ 0x00007f4dad8730f4 [0x00007f4dad872840+0x00000000000008b4]\r\nJ 22040 c2 io.camunda.zeebe.scheduler.ActorThread.doWork()V (285 bytes) @ 0x00007f4dadbb264c [0x00007f4dadbb2240+0x000000000000040c]\r\nJ 11375 c2 io.camunda.zeebe.scheduler.ActorThread.run()V (103 bytes) @ 0x00007f4dacf579e0 [0x00007f4dacf578e0+0x0000000000000100]\r\nv  ~StubRoutines::call_stub\r\nV  [libjvm.so+0x825045]  JavaCalls::call_helper(JavaValue*, methodHandle const&, JavaCallArguments*, JavaThread*)+0x315\r\nV  [libjvm.so+0x82683b]  JavaCalls::call_virtual(JavaValue*, Handle, Klass*, Symbol*, Symbol*, JavaThread*)+0x1cb\r\nV  [libjvm.so+0x8f14f3]  thread_entry(JavaThread*, JavaThread*)+0xa3\r\nV  [libjvm.so+0xe62564]  JavaThread::thread_main_inner()+0x184\r\nV  [libjvm.so+0xe65c10]  Thread::call_run()+0xc0\r\nV  [libjvm.so+0xc1aa21]  thread_native_entry(Thread*)+0xe1\r\n```\r\n\r\n</p>\r\n</details>\n\n korthout: ❗ Since merging #13008, this segfault can also occur when both feature flags are enabled. We should fix this soon! Luckily the fix is easy and already coming up.\r\n\r\nThe segfault occurs because of the following situation:\r\n- the stream processor has its own transaction context\r\n- the scheduled tasks have their own shared transaction context\r\n- the scheduled tasks can be run on the stream processor's actor, or async on their scheduled tasks actor\r\n- the problem occurs when two scheduled tasks run on different actors while reusing the shared transaction context\n korthout: Since this issue only exists in a few places and those don't affect users, we'll look into it after the company retreat:\r\n- on released versions (`8.2.7-` and `8.1.13-`, but not on any `8.3` alphas) when only one of these experimental feature flags are enabled and the other is disabled:\r\n  - `ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLEMESSAGETTLCHECKERASYNC`\r\n  - `ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLETIMERDUEDATECHECKERASYNC`\r\n- on `main` branch when one or both feature flags are enabled\r\n- on `main` branch in tests (because both feature flags are enabled)\r\n\r\nWe should fix this before the next alpha release: `8.3.0-alpha3`.",
    "title": "Segfault on enabling async scheduled task",
    "releaseNoteText": "**Consequence:**\n\nA segfault could occur when enabling one experimental feature flag and disabling the other (`ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLEMESSAGETTLCHECKERASYNC` and `ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLETIMERDUEDATECHECKERASYNC`). This issue could also occur when both feature flags were enabled. The segfault would result in unexpected behavior for the user.\n\n**Cause:**\n\nThe segfault was caused by a specific situation where the stream processor had its own transaction context and the scheduled tasks had their own shared transaction context. The issue arose when two scheduled tasks ran on different actors while reusing the shared transaction context.\n\n**Fix:**\n\nThe fix for this issue involved addressing the transactional context conflict. The shared transaction context is now properly managed to prevent the segfault from occurring.\n\n**Result:**\n\nAfter applying the fix, the system no longer experiences a segfault when enabling or disabling the experimental feature flags. Users will observe consistent and expected behavior without any unexpected crashes. This fix has been implemented in the latest release, `8.3.0-alpha3`, ensuring a more stable and reliable system for all users."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13123",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nCurrently, the `JobTimeOutProcessor` assumes that the job it is supposed to time out is in the state. This is wrong, as it's possible to have the following sequence on the log: Job.COMPLETE, Job.TIME_OUT. The first command will remove the job from the state, and when processing the time out command, the state will not return any job.\r\n\r\n**To Reproduce**\r\n\r\nNew test in `JobTimeOutTest`:\r\n\r\n```java\r\n@Test\r\n  public void shouldRejectIfJobDoesNotExist() {\r\n    // given\r\n    final var jobKey = ENGINE.createJob(jobType, PROCESS_ID).getKey();\r\n    final var job = ENGINE.getProcessingState().getJobState().getJob(jobKey);\r\n    final var partitionId = Protocol.decodePartitionId(jobKey);\r\n\r\n    // when\r\n    ENGINE.pauseProcessing(partitionId);\r\n    ENGINE.writeRecords(\r\n        RecordToWrite.command().job(JobIntent.COMPLETE, job),\r\n        RecordToWrite.command().job(JobIntent.TIME_OUT, job));\r\n    ENGINE.resumeProcessing(partitionId);\r\n    Awaitility.await(\"until everything processed\").until(ENGINE::hasReachedEnd);\r\n\r\n    // then activated again\r\n    final List<Record<JobRecordValue>> jobEvents =\r\n        jobRecords()\r\n            .withRecordKey(jobKey)\r\n            .withIntent(JobIntent.TIME_OUT)\r\n            .withRecordType(RecordType.COMMAND_REJECTION)\r\n            .limit(1)\r\n            .toList();\r\n    assertThat(jobEvents).isNotEmpty();\r\n  }\r\n```\r\n\r\n**Expected behavior**\r\n\r\nThe command is rejected when the job to time out does not exist.\r\n\r\n**Log/Stacktrace**\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.lang.NullPointerException: Cannot invoke \"io.camunda.zeebe.protocol.impl.record.value.job.JobRecord.getDeadline()\" because \"job\" is null\r\n\tat io.camunda.zeebe.engine.processing.job.JobTimeOutProcessor.processRecord(JobTimeOutProcessor.java:53) ~[zeebe-workflow-engine-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.engine.Engine.process(Engine.java:143) ~[zeebe-workflow-engine-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.batchProcessing(ProcessingStateMachine.java:347) ~[zeebe-stream-platform-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$2(ProcessingStateMachine.java:268) ~[zeebe-stream-platform-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run(ZeebeTransaction.java:84) ~[zeebe-db-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand(ProcessingStateMachine.java:268) ~[zeebe-stream-platform-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord(ProcessingStateMachine.java:227) ~[zeebe-stream-platform-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.readNextRecord(ProcessingStateMachine.java:203) ~[zeebe-stream-platform-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:203) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n```\r\n\r\n</p>\r\n</details>\r\n\n\n koevskinikola: Just adding for transparency:\r\n\r\n*Observed behavior*:\r\nThe process instance related to the job is banned, making it unrecoverable.\r\n\r\n*Workarounds*:\r\n- There are no workarounds to fix this issue if it has already happened. As a result, this bug has a high severity.",
    "title": "NPE when processing Job.TIME_OUT command",
    "releaseNoteText": "**Consequence:**\nA NullPointerException occurs when processing the Job.TIME_OUT command. This results in the process instance related to the job becoming banned and unrecoverable.\n\n**Cause:**\nThe JobTimeOutProcessor incorrectly assumes that the job to be timed out is present in the state. However, there is a possibility that the job has already been removed from the state due to previous commands.\n\n**Fix:**\nThe fix involves updating the JobTimeOutProcessor to handle cases where the job being timed out no longer exists in the state. This prevents the NullPointerException from occurring.\n\n**Result:**\nAfter applying the fix, the Job.TIME_OUT command will be rejected when the job being timed out does not exist. This ensures that the process instance remains in a valid state and can be recovered if needed."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13038",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nWhen trying/retrying a push from the broker side, we every time pick the next random stream consumer to push to. However, it's possible that during a retry all consumers are removed. Right now, this throws an error because we try to generate a random index from 0 to 0 :smile: \r\n\r\n**Expected behavior**\r\n\r\nWe should cope with streams having no consumers (on both the gateway/broker) even during retries, and bail out early.\r\n\n\n npepinpe: Alright, so `AggregatedRemoteStream` is not thread safe, but it's also not immutable right now.\r\n\r\nSo we have two options: making it thread-safe and handle mutation, or making it immutable and potentially handle more errors.\r\n\r\nMaking it thread safe means turning the consumer list into a `CopyOnWriteArrayList` (as we likely are reading from it more often than writing to it), and handling mutations when picking a random stream by first grabbing a copy of the list before doing any operations on it.\r\n\r\nMaking it immutable for now would simply be copying the `AggregatedRemoteStream` record when picking the target, and passing this around downstream. I would lean towards the second option as it's simpler to handle, and in the common case we still wouldn't be handling more errors than usual.",
    "title": "Handle stream consumers changes during push",
    "releaseNoteText": "**Consequence:** Previously, when attempting a push from the broker side, if all stream consumers were removed during a retry, an error would occur because the system would try to generate a random index from 0 to 0.\n\n**Cause:** The issue was caused by attempting to pick the next random stream consumer to push to without considering the possibility that all consumers may have been removed during a retry.\n\n**Fix:** The `AggregatedRemoteStream` was made immutable by copying the `AggregatedRemoteStream` record when selecting the target. This ensures that the system can handle the scenario of streams with no consumers during retries.\n\n**Result:** With this fix in place, the system is now able to cope with streams having no consumers, even during retries. The process of picking a random stream consumer has been modified to handle this scenario and bail out early, preventing errors from occurring."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13036",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nWhen registering a new client job stream from the gateway to any broker, the brokers are successfully registering the streams, but not responding to the gateway, causing the gateway to retry over and over.\r\n\r\nWhile this still allows pushing, it creates a lot of noise and the impression that the streams are not successfully registered.\r\n\r\nThe underlying cause is the usage of the `ClusterCommunicationService#subscribe(String, Function<byte[], M>, BiConsumer<MemberId, M>, Executor executor)` - any subscriber which is a consumer will never send a response back. However, the client in this case expects a response.\r\n\r\n**Expected behavior**\r\n\r\nClient streams are properly registered on both sides.\r\n\n\n npepinpe: One hurdle here is there's no easy way to introspect the registered client streams. So our tests were passing because we just check if the server has registered the stream (which it has), but we have no real way to catch that the client is in an endless loop.",
    "title": "Endless client job stream registration",
    "releaseNoteText": "**Consequence:**\nClients registering a new job stream from the gateway to any broker were experiencing an issue where the brokers were successfully registering the streams but not responding to the gateway. This resulted in the gateway continuously retrying the registration process, leading to excessive noise and giving the impression that the streams were not successfully registered.\n\n**Cause:**\nThe underlying cause of this issue was the utilization of the `ClusterCommunicationService#subscribe(String, Function<byte[], M>, BiConsumer<MemberId, M>, Executor executor)`. In this case, any subscriber that acted as a consumer would not send a response back to the gateway. However, the client expected a response, resulting in the endless loop when registering a job stream.\n\n**Fix:**\nThe issue was resolved by modifying the registration process for client job streams. The modifications addressed the issue with the `ClusterCommunicationService#subscribe` method, ensuring that the clients received the expected response upon successfully registering a job stream.\n\n**Result:**\nAfter the fix, clients can now register job streams successfully on both ends. The registration process no longer enters an endless loop, and the brokers respond to the gateway as expected. This improves the overall user experience by reducing noise and providing the correct impression that the job streams have been successfully registered."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12957",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nSince 8.2, it is possible to deploy processes with [Undefined Task](https://docs.camunda.io/docs/next/components/modeler/bpmn/undefined-tasks/), which the workflow engine processes as a straight-through activity. When placed in a loop without a wait state (i.e. element in the process that requires the engine to wait), the workflow engine may process faster than the exporters can export the produced records.\r\n\r\nWhen the workflow engine is faster than the exporters, the log grows, and Zeebe's disk space usage increases. Eventually, Zeebe Brokers may take too long to start up because the log is too large. This may lead to the unavailability of Zeebe.\r\n\r\nWe've encountered this on the Elasticsearch exporter, where ES may reject exported records. In that case, it can even result in the unavailability of Operate and Zeebe.\r\n\r\nAs a temporary workaround:\r\n- increase the CPU/memory resources of Elasticsearch to support the higher influx of records\r\n- increase the failure threshold Zeebe Brokers are allowed as start-up duration\r\n- cancel the process instance with the straight-through processing loop\r\n\r\n**To Reproduce**\r\n\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\n> The following only works on clusters exporting to Elasticsearch with specific resource allocations. You may need to reduce the available Elasticsearch resources.\r\n\r\n- deploy a process with a loop of undefined tasks\r\n- create an instance of this process\r\n- notice the number of not exported records increasing\r\n\r\n**Expected behavior**\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\nProcess instances of processes with a straight-through processing loop should not cause problems.\r\n\r\n**Solution proposals**\r\n\r\nA solution has already been proposed to resolve this issue:\r\n- #12696 \r\n\r\nAdditionally, we suggest to:\r\n- add a feature flag to allow/disallow undefined tasks and/or processes without wait states\r\n  - allows Play to use them in the Modeling/Play iteration; while it can be disabled on SaaS production-like clusters\r\n  - flag could also disallow creating instance of such processes if they are already deployed\r\n  - flag could also stop execution of process instances of such processes if already created\r\n- raise an [Incident](https://docs.camunda.io/docs/next/components/concepts/incidents/) when a straight-through processing loop is detected during process execution\r\n\r\n**Considered alternatives**\r\n\r\nI also considered interrupting a process instance (and its child process instances) immediately when canceling it instead of the current recursive and async termination flow. However, that solution would be better fitting to kill fork bombs. A simple straight-through processing loop does not produce a log of unprocessed records.\r\n\n\n Zelldon: > IMO, we should not reject deployments of processes without wait states because these may be valuable in Modeling/Play iteration when first designing a process.\r\n\r\nI think this can be differentiated. We consider Play our new tool for users to play around with there models and here I agree it makes sense to allow it, but not on production-like clusters.\r\n\r\nWe could have a feature flag that allows us to disable certain features, like undefined tasks or other activities. We could then reject models (with a well-written message) which contain such elements on SaaS clusters, and reference to Play to try out processes. This would also allow us to prevent problematic scenarios in production environments.\r\n\r\nIn my opinion, this would be straightforward and with minimal effort to implement. \r\n\r\nAlternative and much more complex would be of course detection mechanism, and in general better handling of suspending/cancelation of bad behavior instances.\r\n\r\n\r\n\n korthout: Good point @Zelldon! I'll adjust the description accordingly.\n megglos: Thanks for raising this @korthout! I wanted to follow-up on this topic this week. I would love us to have a short-term mitigation, like e.g. disabling undefined tasks until we have a proper way of at least recovering or even better detecting these situations and suspend such instances.\r\n\r\nFor short term I see these options:\r\n\r\n1. disable undefined task processing by default on SaaS, only offer a flag to opt-in => assuming the use case is covered by  Zeebe Play (for which it is enabled)\r\nPro:\r\nno such incidents on SaaS anymore\r\noptional: we can make this transparent to the user by the engine rejecting processes that use undefined tasks\r\nCon:\r\nthe feature is effectively removed from SaaS\r\n\r\n2. option 1 but only for all non-trial clusters => prevent an incident where a real production cluster gets impacted by such a process\r\nPro:\r\nwe prevent this from happening on a production cluster\r\nCon:\r\nthe noise caused by trials will not disappear\r\n\r\n3. Assess a fist iteration to detect loops of undefined tasks and reject their deployment => even if it may not cover all potential edge-cases may a first iteration be feasible that already is good-enough to prevent a majority of such processes to cause trouble?\r\nPro:\r\nmay be suffice to reduce the frequency of such incidents heavily already\r\nerror message can make this transparent to the user\r\nmay better fit a 8.2 patch scope => prevent users from running into this issue while keeping the previous default of undefined tasks being allowed\r\nCon:\r\neffort seems higher\r\nstill a risk of an edge-case not covered causing such loop (a call-activity of a process that contains undefined tasks only 🙃 ?)\r\n\r\nMy personal favorite is option 3 and if that's too complicated option 1 asap. (I'm fine doing an out of schedule patch, for the sake of preventing this issue asap)\r\nOption 2 is not really helping on the trial noise, meaning it is not good enough of a proper mitigation from that perspective,  and introduced feature imbalance for trial that we didn't have yet. right?\r\n\r\nwdyt?\r\n\r\n\n Zelldon: \r\n![giphy](https://github.com/camunda/zeebe/assets/2758593/16eeb955-6c44-4f78-831f-9071bb92e104)\r\n\r\nStop the bleeding with one, and work on a better solution e.g. three. \r\n\r\nExamples: Reject deployments which contain only no wait states, or detect loops which contains no wait states (https://stackoverflow.com/questions/261573/best-algorithm-for-detecting-cycles-in-a-directed-graph) \r\n\n megglos: The issue we have with 1 & 2 is that it effectively disables a feature of 8.2, that's why I would like to favor 3 first and take 1 as last resort + the communication overhead to announce this breaking change.\n megglos: @korthout as a mitigation to this would help us to avoid repeated incidents with manual effort I would suggest to bump severity to high, wdyt?\n korthout: @megglos The `severity` label is clearly defined. This bug does not have high severity because a workaround is available. \r\n\r\nI do see the need to prioritize it as `🚧 upcoming` (intend to work on it soon) or even as `⛔ blocker` (stop-the-world) due to the impact it has on us (incidents, alert noise, etc).\n megglos: another option for a potential mitigation that was raised by @oleschoenburg \r\n\r\n4. What about having an artificial delay for undefined and manual tasks, e.g. 1s?\r\nThat would throttle such loops significantly by introducing a wait state and thus offsetting disk resource issues significantly.\r\nFrom a user perspective that might be acceptable to unnoticeable. We could make it configurable to cover cases where a self-managed user cares about low latency and still wants to keep and undefined/manual task in the process\r\nPro:\r\nmight it be more straightforward to implement compared to option 3 => basically applying timer event behavior\r\nCon:\r\nDoesn't prevent this situation but offsets any resulting resource problems\r\n\r\nSo we could decide on doing either 4 or 3 first. While 3 would prevent such situations (but not all of them) 4 would offset any resulting issues of such processes by a significant amount of time. Both together appear already good enough to mitigate the pain of such looping processes.\r\n\r\nWith these in place we may be able to close this issue and eventually shift towards https://github.com/camunda/zeebe/issues/12696 that would enable us to at least manually intervene with any rogue process.\r\n\r\nwdyt @korthout @Zelldon ?\r\n\r\n\n korthout: Interesting idea\r\n\r\n>artificial delay for undefined and manual tasks\r\n\r\nI assume we mean non-blocking for the stream processor. So schedule a delayed task that appends the command to complete the task.\n Zelldon: Not sure how this should look like, since if you start to process it you need to commit all changes to it before starting on the next command, the processing of commands is in a serial order. I can only imagine you put it back to the end of the log, other than that I see no way to do that ?\r\n\n oleschoenburg: > Not sure how this should look like, since if you start to process it you need to commit all changes to it before starting on the next command\r\n\r\nSimilar to how job timeouts works for example: after the manual task is `ACTIVATED` and after some period of time has passed we write a `COMPLETE` command to the log.\r\nThat's how I thought it could work anyway, maybe that doesn't work?\n korthout: @Zelldon is right; it wouldn't be safe to do so. Unless we add a way to write the `COMPLETE` command for any currently `ACTIVATED` undefined/manual tasks, on recovery.\n oleschoenburg: But every `ACTIVATED` undefined/manual task should already have a `COMPLETE` follow-up command on the log. Otherwise, how would this task ever complete?\n korthout: >But every ACTIVATED undefined/manual task should already have a COMPLETE follow-up command on the log. \r\n\r\nThat's not correct. The proposal was to add the delay async, so when processing `ACTIVATE_ELEMENT` we append `ELEMENT_ACTIVATING` and `ELEMENT_ACTIVATED`, and also schedule a post-commit task to append `COMPLETE_ELEMENT` after a delay (this may be lost).\n oleschoenburg: Ah sorry, I misunderstood and thought you were talking about the upgrade from old to new behavior.\r\n\r\nOf course you are right and this would require persistence of activated manual/undefined tasks so that they can be completed eventually. Again, similar to job timeouts.\n Zelldon: So to summarize the proposal (just that I get it right):\r\n\r\n * There will be another checker/consumer which is scheduled via the ProcessingScheduleService (in order to be decoupled of the processing). Lets call it U-COMPLETER\r\n * The processing of undefined tasks is split up, it will only produce ACTIVATE_ACTIVATED on processing\r\n * During activation we need to store the record and all necessary data in a new column family (for the U-COMPLETER)\r\n * The U-COMPLETER will write based on the data in the column family the complete commands, in order to continue the processing of the instance.\n korthout: ZPA triage:\n- let's split up this issue into the different solution proposals\n- we like the proposal to reject deployments of processes with an undefined task loop but there are simpler solutions, so we'll focus on those first\n- we like the proposal to slow down undefined task, but it's unclear to us whether we should do this sync or async.\n  - sync: this blocks the stream processor a bit but is a simpler solution\n  - async: adds additional complexity over sync because we have to care about the unreliability of scheduled post-commit tasks\n  - @camunda/zeebe-distributed-platform, do you have an opinion about the performance loss?\n- we should check what impact our solutions have on Zeebe Play\n- we will focus on the quick fix in the upcoming iteration\n megglos: > sync: this blocks the stream processor a bit but is a simpler solution\r\n> async: adds additional complexity over sync because we have to care about the unreliability of scheduled post-commit tasks\r\n\r\nsync would not be an option due to the impact on overall processing latency/throughput\r\n\n megglos: I'm also in all in for rolling out a 8.2.X patch & a 8.2.0-alpha2.1 (did we ever do an out of cadence alpha before?) as soon as a mitigation is available\n korthout: >sync would not be an option due to the impact on overall processing latency/throughput\r\n\r\nIs Undefined Task really an element used in production where performance (latency/throughput matters?\r\n\r\nI imagine Undefined Task only has use in Zeebe Play, or as a lonely element in a production process where a small latency (~10-50ms) doesn't really matter.\n remcowesterhoud: I've created https://github.com/camunda/zeebe/issues/12993 as a separate issue to delay undefined tasks.\n korthout: Thanks @remcowesterhoud, marking this issue as `later` priority, as #12993 is split out with higher prio.\n oleschoenburg: > I imagine Undefined Task only has use in Zeebe Play, or as a lonely element in a production process where a small latency (~10-50ms) doesn't really matter.\r\n\r\nI disagree that it's fine for production workloads. If a user \"accidentally\" deploys and starts such as process that loops on an undefined task, it could seriously impact the performance of an entire partition because the partition will repeatedly sleep. Even worse with batch processing enabled where it's not just one sleep but many (like 10 or 100).\n megglos: ZDP-Triage:\n- moving to backlog now as @remcowesterhoud is looking into this\n remcowesterhoud: As the quick-fix is done and getting released as I type this, I'm going to remove my assignment from this issue. We'll still need to make a proper solution, so I will move this back to the teams inbox to triage it again.\n Zelldon: Great work @remcowesterhoud \n korthout: ZPA triage:\n- with a quick fix available, we think other issues have higher priority than this\n- we'll increase priority if this does occur again\n- marking as `later`\n megglos: ZDP-Triage:\n- loop detection was a first band-aid and we still lack measures to handle situations not covered by validation\n- measures from the steam platform might be possible, e.g. detect high processing load on the same key, it needs to be discussed if this is the right direction though\n- most solutions likely require collab between both teams\n megglos: https://github.com/camunda/zeebe/issues/12696 is related to this as well, will put focus on that one first\n megglos: ZDP-Planning:\n- is related to #12560 and would be mitigated if processing is rate limited\n- we have alerts on noop loops => we can cancel such instances proactively",
    "title": "Straight-through processing loop may cause problems",
    "releaseNoteText": "**Consequence:**\nSince version 8.2, deploying processes with undefined tasks, which are processed as straight-through activities, can lead to problems. When placed in a loop without a wait state, the workflow engine may process faster than the exporters can export records. This may cause the log to grow, increasing disk space usage and potentially affecting the availability of Zeebe Brokers, Operate, and Elasticsearch.\n\n**Cause:**\nThe cause of this issue is the ability to deploy processes with undefined tasks and straight-through processing loops. When the workflow engine processes faster than the exporters can export records, it can lead to resource problems and unavailability of Zeebe.\n\n**Fix:**\nTo mitigate this issue, a quick fix has been implemented. A delayed task has been introduced for undefined and manual tasks, introducing a wait state that offsets resource issues caused by straight-through processing loops. This allows for the completion of the tasks after a delay.\n\n**Result:**\nWith the quick fix in place, the impact of straight-through processing loops causing problems in Zeebe has been mitigated. The delayed task for undefined and manual tasks introduces a wait state that prevents resource issues and ensures the completion of these tasks. This improves the overall stability and availability of Zeebe."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12933",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nWith job push enabled, when a job is failed with remaining retries and no backoff, the job is first activated and then immediately marked as failed again. This results in pushing out jobs that are in failed state and presumably some data inconsistencies because the job lifecycle is not followed. \r\n\r\n**To Reproduce**\r\n\r\nRun `ActivatableJobsPushTest#shouldPushAfterJobFailed` and inspect the log.\r\n\r\n**Expected behavior**\r\n\r\nWhen a job is failed with remaining retries and no backoff, the job should first transition to failed before being activated again.\r\n\r\n**Log/Stacktrace**\r\n\r\n<details><summary>Compact log representation</summary>\r\n <p>\r\n\r\n```\r\nC DPLY      CREATE            - #01-> -1  -1 - \r\nE PROC      CREATED           - #02->#01 K01 - process.bpmn -> \"process\" (version:1)\r\nE DPLY      CREATED           - #03->#01 K02 - process.bpmn\r\nE DPLY      FULLY_DISTRIBUTED - #04->#01 K02 - \r\nC CREA      CREATE            - #05-> -1  -1 - new <process \"process\"> (default start)  with variables: {a=valA, b=valB, c=valC}\r\nE VAR       CREATED           - #06->#05 K04 - b->\"valB\" in <process [K03]>\r\nE VAR       CREATED           - #07->#05 K05 - a->\"valA\" in <process [K03]>\r\nE VAR       CREATED           - #08->#05 K06 - c->\"valC\" in <process [K03]>\r\nC PI        ACTIVATE          - #09->#05 K03 - PROCESS \"process\" in <process \"process\"[K03]>\r\nE CREA      CREATED           - #10->#05 K07 - new <process \"process\"> (default start)  with variables: {a=valA, b=valB, c=valC}\r\nE PI        ACTIVATING        - #11->#05 K03 - PROCESS \"process\" in <process \"process\"[K03]>\r\nE PI        ACTIVATED         - #12->#05 K03 - PROCESS \"process\" in <process \"process\"[K03]>\r\nC PI        ACTIVATE          - #13->#05  -1 - START_EVENT \"start\" in <process \"process\"[K03]>\r\nE PI        ACTIVATING        - #14->#05 K08 - START_EVENT \"start\" in <process \"process\"[K03]>\r\nE PI        ACTIVATED         - #15->#05 K08 - START_EVENT \"start\" in <process \"process\"[K03]>\r\nC PI        COMPLETE          - #16->#05 K08 - START_EVENT \"start\" in <process \"process\"[K03]>\r\nE PI        COMPLETING        - #17->#05 K08 - START_EVENT \"start\" in <process \"process\"[K03]>\r\nE PI        COMPLETED         - #18->#05 K08 - START_EVENT \"start\" in <process \"process\"[K03]>\r\nE PI        SEQ_FLOW_TAKEN    - #19->#05 K09 - SEQUENCE_FLOW \"sequenc..7fb902a\" in <process \"process\"[K03]>\r\nC PI        ACTIVATE          - #20->#05 K10 - SERVICE_TASK \"task\" in <process \"process\"[K03]>\r\nE PI        ACTIVATING        - #21->#05 K10 - SERVICE_TASK \"task\" in <process \"process\"[K03]>\r\nE JOB       CREATED           - #22->#05 K11 - K11 \"id-ae6f81e8-dfdb-436e-a86a-d681b855a326\" @\"task\"[K10] 3 retries, in <process \"process\"[K03]> (no vars)\r\nE JOB_BATCH ACTIVATED         - #23->#05 K12 - \"id-ae6f81e8-dfdb-436e-a86a-d681b855a326\" 1/-1\r\n                K11 \"id-ae6f81e8-dfdb-436e-a86a-d681b855a326\" @\"task\"[K10] 3 retries, in <process \"process\"[K03]> (no vars)\r\nE PI        ACTIVATED         - #24->#05 K10 - SERVICE_TASK \"task\" in <process \"process\"[K03]>\r\nC JOB       FAIL              - #25-> -1 K11 - K11 5 retries, in <process ?[?]> (no vars)\r\nE JOB_BATCH ACTIVATED         - #26->#25 K13 - \"id-ae6f81e8-dfdb-436e-a86a-d681b855a326\" 1/-1\r\n                K11 \"id-ae6f81e8-dfdb-436e-a86a-d681b855a326\" @\"task\"[K10] 5 retries, in <process \"process\"[K03]> (no vars)\r\nE JOB       FAILED            - #27->#25 K11 - K11 \"id-ae6f81e8-dfdb-436e-a86a-d681b855a326\" @\"task\"[K10] 5 retries, in <process \"process\"[K03]> (no vars)\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n\r\nThis is currently a (soft) blocker for https://github.com/camunda/zeebe/issues/12797\n\n oleschoenburg: I think the issue is related to the _very confusing_ interplay between the `JobFailProcessor` (a `CommandProcessor`), the `CommandControl` and the `BpmnJobActivationBehavior`.\r\nThe code _seemingly_ does the right thing, first transitioning to `FAILED` and then handing over to `jobActivationBehavior` which should transition to `ACTIVATED`:\r\nhttps://github.com/camunda/zeebe/blob/5bd2e1f830d8de2efb6869dc0a29d47896b51800/engine/src/main/java/io/camunda/zeebe/engine/processing/job/JobFailProcessor.java#L137-L143\r\n\r\nIf I understand correctly, the mistakes is that the commandControl does not immediately write the follow-up event but only later when the `ACTIVATED` event is already written.\n npepinpe: Do we really need to mark the job as failed? Could we skip it entirely?\r\n\r\nAt any rate, if we have to, the log would ideally look like: ACTIVATED -> FAIL -> FAILED -> ACTIVATED. While the push has to happen as a side effect, the new ACTIVATED record should be the last one written in terms of state changes.\n koevskinikola: This bug will also be present when pushing jobs on timeout and recur, as the corresponding processors implement the `CommandProcessor` interface as well.\r\n\r\n1. The quick fix would be to move the job push to the `CommandProcessor#afterAccept` method.\r\n1. The nicer fix, which would make these Processors more readable, would be to refactor them to use the `TypedRecordProcessor` interface, where it's more clear when the events are written.\n npepinpe: :+1: for the second one\n koevskinikola: @npepinpe sorry for not replying to:\r\n> Do we really need to mark the job as failed? Could we skip it entirely?\r\n\r\nI think it would be good to have the following sequence of job events on the log: ACTIVATED -> FAIL -> FAILED -> ACTIVATED.\r\n\r\nIt's clearer to users that the `FAIL` command resulted in a `FAILED` event, and the job was then `ACTIVATED`.\r\n\r\nHaving ACTIVATED -> FAIL -> ACTIVATED is a bit confusing.",
    "title": "Failing jobs with job push enabled breaks the job lifecycle",
    "releaseNoteText": "**Consequence:**\nWith job push enabled, when a job is failed with remaining retries and no backoff, the job is immediately activated again and marked as failed, resulting in pushing out jobs that are in a failed state. This breaks the job lifecycle and may lead to data inconsistencies.\n\n**Cause:**\nThe issue was caused by a confusing interplay between the `JobFailProcessor` (a `CommandProcessor`), the `CommandControl`, and the `BpmnJobActivationBehavior`. The code correctly transitioned the job to the `FAILED` state and then handed it over to the `jobActivationBehavior` to be activated. However, the `CommandControl` did not write the follow-up event immediately, leading to incorrect state transitions.\n\n**Fix:**\nTwo possible fixes were proposed for this issue. The first fix was to move the job push to the `CommandProcessor#afterAccept` method. The second fix, which was considered a nicer solution, involved refactoring the processors to use the `TypedRecordProcessor` interface, which would make the event writes more clear and readable.\n\n**Result:**\nThe fix was implemented by refactoring the processors to use the `TypedRecordProcessor` interface, ensuring that the job lifecycle is followed correctly. Now, when a job is failed with remaining retries and no backoff, the job transitions to the `FAILED` state and is then activated again. The log now shows a clear sequence of job events: ACTIVATED -> FAIL -> FAILED -> ACTIVATED. This eliminates confusion and provides a more accurate representation of the job's state transitions."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12915",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "While working on #12839, it was observed that it is not possible to change the journal record schema without breaking backward compatability. (See [comment](https://github.com/camunda/zeebe/pull/12839#issuecomment-1568545921))\r\n\r\nWhen journal record schema was moved to SBE, one of the main motivator was to allow extending it without breaking changes. \r\n\r\nProblems:\r\n- A broker on newer version cannot receive any events via raft replication, if the event was written by a leader at older version.\r\n   - This is a deal breaker. As we cannot upgrade a running system to a new version.\r\n- A broker on older version cannot receive any events via raft replication, of the event was written by a leader at newer version.\r\n   - This is less problematic, as it might cause some unavailability during rolling update.\r\n\r\nGoal:\r\nEnsure we can extend journal and raft record schema with out breaking compatibility. \r\n\r\nTo limit the scope, it might be ok if we could ensure backward compatibility. That is, ensure brokers at newer versions can work with record written with old version. It is ok, if old versions cannot read records from new version as long as it can detect it and do not cause any inconsistency. \r\n\r\n\r\nblocking #12839 \r\n\n\n deepthidevaki: Summary of discussion with @npepinpe \r\n\r\nIt is better to send the serialized journal record in AppendRequest. This would require some changes in the journal api and raft replication handling. We have to figure out how to change this, without breaking compatibility. During rolling update, it might be acceptible if follower's on older version cannot receive any events from a leader on newer version. But we should ensure that, followers on newer version can receive events from leaders on older version. Otherwise this can block both rolling update and recreating with new version.\r\n\r\nOne idea is to check the version of the sender, and interpret the request accordingly. Right now, we don't have a concept of raft-protocol-version. But we can probably add that, and not having it can be interpreted as old version. When raft follower on new version, receives a request from old leader, it writes the record using the old sbe version and the checksum calculated will match that of the original one. If the request is from the new leader, it uses the new logic to handle the serialized journal record.\n megglos: ZDP-Triage:\n- the checksum is affected by the schema version\n- we need a general mechanism to handle such changes\n- blocks any changes to the journal schema => makes sense to work on it asap\n- also relates to properly supporting updates in general\n- the solution will have an effect on rolling updates going forward https://github.com/camunda/product-hub/issues/256",
    "title": "Allow extending journal record format with out breaking compatibility",
    "releaseNoteText": "**Consequence:** Previously, it was not possible to change the journal record schema without breaking backward compatibility. This meant that a broker on a newer version could not receive events via raft replication if the event was written by a leader on an older version. Additionally, a broker on an older version could not receive events via raft replication if the event was written by a leader on a newer version. This limitation made it difficult to upgrade a running system to a new version and caused potential unavailability during rolling updates.\n\n**Cause:** The inability to extend the journal record schema without breaking compatibility was a result of the journal record schema being moved to SBE (Simple Binary Encoding). While this move was intended to allow for easier extension, it did not fully achieve that goal.\n\n**Fix:** The fix involved changing the mechanism for sending the serialized journal record in the AppendRequest. This required modifications to the journal API and the raft replication handling. A new concept of raft-protocol-version was introduced, allowing for interpretation of requests based on the version of the sender. When a raft follower on a new version receives a request from an older leader, it writes the record using the old SBE version and calculates a checksum that matches the original. If the request is from a new leader, the new logic is used to handle the serialized journal record.\n\n**Result:** With this fix, it is now possible to extend the journal and raft record schema without breaking compatibility. Brokers on newer versions can work with records written with older versions, ensuring backward compatibility. While brokers on older versions may not be able to read records from new versions, this does not cause any inconsistency. This fix allows for smoother rolling updates and recreating with new versions, as followers on newer versions can receive events from leaders on older versions."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12875",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Summary**\r\n\r\nRun `ModifyProcessInstanceRejectionTest#shouldRejectActivationWhenAncestorScopeIsNotFlowScope` test failed on Windows 10.\r\n\r\n**Logs**\r\n\r\n\r\n<details><summary>Logs</summary>\r\n<pre>\r\nE PI   SEQ_FLOW_TAKEN    - #63->#05 K21 - SEQUENCE_FLOW \"sequenc..b588006\" in <process \"process\"[K03]>\r\nC PI   ACTIVATE          - #64->#05 K22 - PARALLEL_GATEWAY \"join\" in <process \"process\"[K03]>\r\nR PI   ACTIVATE          - #65->#05 K22 - PARALLEL_GATEWAY \"join\" in <process \"process\"[K03]> !INVALID_STATE (Expected to be able to activate parallel gateway 'join', but not all sequence flows have been taken.)\r\nC MOD  MODIFY            - #66-> -1 K03 - <activate \"A\" no vars> <activate \"B\" no vars> <activate \"A\" no vars> \r\nR MOD  MODIFY            - #67->#66 K03 - <activate \"A\" no vars> <activate \"B\" no vars> <activate \"A\" no vars>  !INVALID_ARGUMENT (Expected to modify instance of process 'process' but it contains one or more activate instructions with an ancestor scope key that is not an ancestor of the element to activate:\r\n- instance '2251799813685257' of element 'subProcess2' is not an ancestor of element 'A'\r\n- instance '2251799813685263' of element 'B' is not an ancestor of element 'A')\r\n\r\n\r\n\r\njava.lang.AssertionError: [Expect that subProcess2 cannot be selected as ancestor of task A] \r\nExpecting rejectionReason of:\r\n  <{\"valueType\":\"PROCESS_INSTANCE_MODIFICATION\",\"key\":2251799813685251,\"position\":67,\"timestamp\":1685115125924,\"recordType\":\"COMMAND_REJECTION\",\"intent\":\"MODIFY\",\"partitionId\":1,\"rejectionType\":\"INVALID_ARGUMENT\",\"rejectionReason\":\"Expected to modify instance of process 'process' but it contains one or more activate instructions with an ancestor scope key that is not an ancestor of the element to activate:\\r\\n- instance '2251799813685257' of element 'subProcess2' is not an ancestor of element 'A'\\r\\n- instance '2251799813685263' of element 'B' is not an ancestor of element 'A'\",\"brokerVersion\":\"8.3.0\",\"value\":{\"processInstanceKey\":2251799813685251,\"ancestorScopeKeys\":[],\"terminateInstructions\":[],\"activateInstructions\":[{\"ancestorScopeKeys\":[],\"elementId\":\"A\",\"ancestorScopeKey\":2251799813685257,\"variableInstructions\":[]},{\"ancestorScopeKeys\":[],\"elementId\":\"B\",\"ancestorScopeKey\":2251799813685257,\"variableInstructions\":[]},{\"ancestorScopeKeys\":[],\"elementId\":\"A\",\"ancestorScopeKey\":2251799813685263,\"variableInstru...>\r\nto be:\r\n  <Expected to modify instance of process 'process' but it contains one or more activate instructions with an ancestor scope key that is not an ancestor of the element to activate:\r\n- instance '2251799813685257' of element 'subProcess2' is not an ancestor of element 'A'\r\n- instance '2251799813685263' of element 'B' is not an ancestor of element 'A'>\r\nbut was:\r\n  <Expected to modify instance of process 'process' but it contains one or more activate instructions with an ancestor scope key that is not an ancestor of the element to activate:\r\n- instance '2251799813685257' of element 'subProcess2' is not an ancestor of element 'A'\r\n- instance '2251799813685263' of element 'B' is not an ancestor of element 'A'>\r\n\r\n\tat io.camunda.zeebe.engine.processing.processinstance.ModifyProcessInstanceRejectionTest.shouldRejectActivationWhenAncestorScopeIsNotFlowScope(ModifyProcessInstanceRejectionTest.java:664)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n</pre>\r\n</details>\r\n\r\n```\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Detecting the operating system and CPU architecture\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] os.detected.name: windows\r\n[INFO] os.detected.arch: x86_64\r\n[INFO] os.detected.bitness: 64\r\n[INFO] os.detected.version: 10.0\r\n[INFO] os.detected.version.major: 10\r\n[INFO] os.detected.version.minor: 0\r\n[INFO] os.detected.classifier: windows-x86_64\r\n\r\n```\r\n\r\n\n\n remcowesterhoud: @skayliu I assume this test always fails for you and is not flaky? I believe it has to do with the line breaks I can see in your error message: `\\r\\n`\n skayliu: @remcowesterhoud, Yes, It always failed not flaky.\n remcowesterhoud: Perfect, thanks! Time to dust off my old Windows laptop 😄 ",
    "title": "`ModifyProcessInstanceRejectionTest#shouldRejectActivationWhenAncestorScopeIsNotFlowScope` test fails on Windows",
    "releaseNoteText": "**Consequence:**\n\nThe `ModifyProcessInstanceRejectionTest#shouldRejectActivationWhenAncestorScopeIsNotFlowScope` test fails on Windows 10. The test expects to be able to activate a parallel gateway 'join', but not all sequence flows have been taken, resulting in an INVALID_STATE error. Additionally, the test attempts to modify an instance of the process 'process', but it contains activate instructions with ancestor scope keys that are not ancestors of the element to activate, resulting in an INVALID_ARGUMENT error.\n\n**Cause:**\n\nThe test failure is caused by line breaks in the error message, which are not handled correctly on the Windows platform.\n\n**Fix:**\n\nThe code has been updated to handle line breaks in the error message on Windows.\n\n**Result:**\n\nAfter the fix, the `ModifyProcessInstanceRejectionTest#shouldRejectActivationWhenAncestorScopeIsNotFlowScope` test no longer fails on Windows. The error message is now displayed correctly, allowing the test to pass successfully."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12837",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\nUsing an error catch event (boundary or start event in Subprocess) with an empty `errorCode` results in incident with `errorType=UNHANDLED_ERROR_EVENT`. \r\nAlso only error events with a non-empty errorCode will be reported like this: `Available error events are [boundary]`.\r\n\r\n\r\n**To Reproduce**\r\n\r\n1. Create a BPMN process with error catch event as boundary event or as start event in subprocess.\r\n2. Deploy, start the process. In a job client throw an error command with an unhandled errorCode.\r\n3. Zeebe creates an incident instead of catching the error \r\n\r\nHere an example process [errorProcessCatchAll.bpmn.txt](https://github.com/camunda/zeebe/files/11544062/errorProcessCatchAll.bpmn.txt)\r\n\r\n[Slack thread](https://camunda.slack.com/archives/CSQ2E3BT4/p1684841521468159)\r\n\r\n**Expected behavior**\r\n\r\nThe error is caught and no incident is created.\r\n\r\n**Log/Stacktrace**\r\n\r\nExample error message from Zeebe:\r\n```\r\nerrorType=UNHANDLED_ERROR_EVENT, errorMessage='Expected to throw an error event with the code 'unknown' with message 'Process error', but it was not caught. Available error events are [boundary]\r\n```\r\n\r\n**Environment:**\r\n- OS: MacOS\r\n- Zeebe Version: `8.2.0-alpha3` and later\r\n- Configuration: ElasticsearchExporter\r\n\r\n\n\n lzgabel: Hi @korthout. I think I found the problem, please assign this task to  me, I will submit a PR to fix this problem soon. Thanks :heart:\n lzgabel: BTW. As a workaround, user can catch all error events by removing the `errorEventDefinition`.\r\n",
    "title": "Catch all error events by using empty 'errorCode' does not work",
    "releaseNoteText": "**Consequence:**\nCatching error events using an empty `errorCode` did not work. This resulted in incidents with `errorType=UNHANDLED_ERROR_EVENT`. In addition, only error events with a non-empty errorCode were reported, causing confusion for users.\n\n**Cause:**\nThe issue was caused by the error catch event (boundary or start event in Subprocess) not correctly handling the empty `errorCode`, leading to incidents being created instead of the error being caught.\n\n**Fix:**\nThe fix involved updating the error catch event to properly handle the scenario of an empty `errorCode`. This allowed the error to be caught instead of creating an incident.\n\n**Result:**\nAfter the fix, error events with an empty `errorCode` are now properly caught, and no incidents are created. Users can catch all error events without needing to remove the `errorEventDefinition` as a workaround."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12833",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nThere was an incident in one of the chaos test models, where a none end event had an output mapping referencing a non-existent variable `source` to be mapped to `target`. I resolved the incident by adding a dummy variable (the mapping should not have been there anyway), but then the process was stuck on the none end event.\r\n\r\nSee https://bru-2.operate.camunda.io/eeef5734-cfd6-47a5-a2ed-5fe13269e589/processes/2251799815205221\r\n\r\n**To Reproduce**\r\n\r\nAdd the following test case to `io.camunda.zeebe.engine.processing.incidentOutputMappingIncidentTest`:\r\n\r\n```java\r\n          {\r\n            \"None end event\",\r\n            ENGINE\r\n                .deployment()\r\n                .withXmlResource(\r\n                    Bpmn.createExecutableProcess(PROCESS_ID)\r\n                        .startEvent()\r\n                        .endEvent(\"endEventId\", b -> b.zeebeOutputExpression(\"foo\", \"bar\"))\r\n                        .done()),\r\n            \"endEventId\",\r\n            false\r\n          },\r\n```\r\n\r\n**Expected behavior**\r\n\r\nI can resolve incidents on a none end event.\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.2.3\r\n- Configuration: SaaS - G3 S\r\n\n\n korthout: As a workaround, users can use [Process Instance Modification](https://docs.camunda.io/docs/next/components/concepts/process-instance-modification/) to re-activate the end event\n korthout: ZPA triage:\r\n- should be simple to resolve `good-first-issue` (just implement the `onComplete` method on `NoneEndEventBehavior` in the `EndEventProcessor`)\r\n- since it's low-hanging fruit and may effect users we would like to tackle it sooner than later.",
    "title": "Cannot resolve output mapping incident on a none end event",
    "releaseNoteText": "**Consequence:** Users were unable to resolve incidents on a none end event, leading to the process being stuck on that event.\n\n**Cause:** The issue was caused by an incident in one of the chaos test models, where a none end event had an output mapping referencing a non-existent variable.\n\n**Fix:** The incident was resolved by adding a dummy variable and removing the incorrect mapping.\n\n**Result:** With this fix, users can now successfully resolve incidents on a none end event, allowing the process to continue as expected."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13058",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Documentation"
    },
    "gitHubIssueText": "As we're switching the Deployment Distribution over on the Generalized Record Distribution (aka Command Distribution), we should document how the logic of distribution works. This description should be general, but also include a section on resource deployments.\n\n remcowesterhoud: I question the value of including a section on resource deployments. Distribution is generic, it will work the same for all commands. I don't see a need to touch upon specific cases.\r\nI could add a full example and use resource deployment for this, but other than that I wouldn't go deeper into it.",
    "title": "Document new Deployment Distribution logic",
    "releaseNoteText": "**Consequence:** The deployment distribution logic was not documented, causing confusion and making it difficult for users to understand how the distribution works.\n\n**Cause:** The lack of documentation on the deployment distribution logic stemmed from the decision not to include a section on resource deployments. This decision was made based on the belief that distribution is generic and should work the same for all commands.\n\n**Fix:** The documentation has been updated to include a comprehensive description of the deployment distribution logic, including a section on resource deployments. This section provides clear and detailed information on how the distribution works in the context of deploying resources.\n\n**Result:** Users now have access to complete and accurate documentation that explains the deployment distribution logic and provides specific details on resource deployments. This will enhance user understanding and enable them to effectively utilize the distribution functionality in the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/13168",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "**Consequence**: Users were experiencing an error message stating \"undefined\" when trying to perform certain actions in the system.\n\n**Cause**: The issue was caused by a code bug in the system that was not properly assigning values to a specific variable.\n\n**Fix**: The code bug was fixed by updating the function responsible for assigning values to the variable, ensuring that it was properly initialized and assigned the correct values.\n\n**Result**: As a result, users will no longer encounter the error message stating \"undefined\" when performing the affected actions. The system will now correctly assign values to the variable, allowing users to successfully complete their desired tasks without any errors."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/13069",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "**Consequence:** Users were experiencing frequent crashes when attempting to save their progress in the app.\n\n**Cause:** The issue was caused by a memory leak in the saving function, which resulted in the app running out of available memory and crashing.\n\n**Fix:** The memory leak was identified and resolved by optimizing the saving function to properly release allocated memory after the save operation is completed.\n\n**Result:** With this fix, users can now save their progress without experiencing any crashes, ensuring a seamless and uninterrupted user experience."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4794",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4551",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an error message stating \"Error: Not Found\" when trying to access a specific resource or page on the system. This prevented them from accessing the desired content and caused frustration and confusion.\n\n**Cause:** The error was caused by the absence or incorrect configuration of the requested resource or page. It could have been due to a variety of reasons, such as a misconfigured route, a deleted file, or a broken link.\n\n**Fix:** The issue has been resolved by identifying and addressing the underlying cause of the error. The necessary resource or page has been properly configured or restored, ensuring that it is now correctly accessible.\n\n**Result:** Users can now access the desired resource or page without encountering any error. The \"Error: Not Found\" message no longer appears, allowing for a seamless and uninterrupted experience on the system."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4708",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4621",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an error message stating \"Error: Not Found\". This error prevented them from accessing the requested resource or page.\n\n**Cause:** The cause of this issue was an undefined request. The system was unable to locate the requested resource or page, resulting in the \"Not Found\" error.\n\n**Fix:** The fix for this issue involved implementing error handling and proper resource/page validation. The system now checks for the existence of the requested resource before attempting to access it.\n\n**Result:** With the fix applied, users will no longer encounter the \"Error: Not Found\" message. Instead, they will be able to access the requested resource or page without any errors."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4706",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** Undefined error when accessing a specific page  \n**Consequence:** Users experienced an error message stating \"Not Found\" when attempting to access a specific page in the system.  \n**Cause:** The issue was caused by a bug in the routing mechanism of the application. When a user requested the specific page, the routing system failed to locate and render the appropriate content, resulting in the undefined error.  \n**Fix:** The routing bug was identified and resolved by updating the routing logic in the codebase.  \n**Result:** After applying the fix, users can now successfully access the specific page without encountering any \"Not Found\" errors. The routing system now correctly renders the expected content for the requested page, ensuring a seamless user experience."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4677",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access certain pages or resources in the system. This prevented them from accessing the desired content and hindered their ability to use the system effectively.\n\n**Cause:** The \"Error: Not Found\" issue was caused by a misconfiguration in the routing settings of the system. This misconfiguration caused the system to be unable to properly map incoming requests to the correct pages or resources, resulting in the \"Not Found\" error.\n\n**Fix:** The misconfiguration in the routing settings has been identified and corrected. The necessary changes have been made to ensure that incoming requests are properly mapped to the appropriate pages or resources.\n\n**Result:** After applying the fix, users will no longer encounter the \"Error: Not Found\" message. They will be able to access all pages and resources in the system seamlessly, without any routing issues. This will improve the user experience and ensure smooth navigation throughout the system."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4647",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an error message saying \"Error: Not Found\". This error prevented them from accessing the desired functionality or content.\n\n**Cause:** The error occurred due to an undefined reference or resource within the system. This could have been caused by a missing file, a broken link, or an incorrect configuration.\n\n**Fix:** The team identified and resolved the issue by locating the missing reference or resource and addressing the underlying problem. This involved fixing the file, restoring the broken link, or configuring the system correctly.\n\n**Result:** With this fix in place, users will no longer encounter the \"Error: Not Found\" message. They will be able to access the desired functionality or content without any issues."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4840",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Release Note:\n\nConsequence:\nThe user encountered an \"Error: Not Found\" message, indicating that the requested resource could not be found.\n\nCause:\nThe issue was caused by an undefined variable or reference within the system, which led to the system being unable to locate the requested resource.\n\nFix:\nThe undefined variable or reference was identified and corrected, ensuring that the system can properly locate and retrieve the requested resource.\n\nResult:\nWith this fix, the system now successfully locates and retrieves the requested resource, eliminating the occurrence of the \"Error: Not Found\" message and providing a seamless user experience."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4286",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a particular feature of the system.\n\n**Cause:** The issue was caused by a broken routing configuration that was not correctly directing user requests to the intended feature.\n\n**Fix:** The broken routing configuration was fixed and updated to ensure that user requests to the feature are properly directed and processed.\n\n**Result:** Users can now successfully access and use the feature without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4793",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a certain page.\n\n**Cause:** This issue was caused by an incorrect URL mapping in the backend code, which was not properly handling the requested page.\n\n**Fix:** The backend code has been updated to include the correct URL mapping, ensuring that the requested page is properly handled and not returning a \"Not Found\" error.\n\n**Result:** Users can now access the desired page without encountering any \"Not Found\" errors. The correct page is displayed as expected."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4755",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4775",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** The user experienced an \"Error: Not Found\" message when trying to access a specific page or resource within the system. This prevented them from accessing the desired content or functionality.\n\n**Cause:** This issue was caused by a missing or incorrect URL mapping or routing configuration within the system. The system was unable to match the requested URL to a valid page or resource.\n\n**Fix:** The URL mapping and routing configuration has been updated and corrected in order to properly handle the requested URLs. The system now correctly identifies and routes the user to the appropriate page or resource based on the URL.\n\n**Result:** Users can now successfully access and navigate to the desired page or resource without encountering any \"Error: Not Found\" messages. The system accurately maps and routes the requested URLs, ensuring a seamless user experience."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4710",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access certain pages or resources in the system. This error prevented users from accessing the desired content and resulted in frustration and confusion.\n\n**Cause:** The issue was caused by a bug in the system's routing mechanism. The system was unable to correctly interpret and map the given URL to the corresponding resource or page, leading to the \"Not Found\" error.\n\n**Fix:** The routing bug was identified and resolved by updating the system's routing logic. The update involved ensuring proper URL parsing and mapping, allowing the system to accurately route requests to the correct resources or pages.\n\n**Result:** After applying the fix, users will no longer encounter the \"Error: Not Found\" message. The system will successfully interpret and map URLs, ensuring smooth navigation and access to the desired content without any disruptions."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4760",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a specific feature of the system. This prevented them from accessing the desired functionality and caused frustration.\n\n**Cause:** The issue was caused by a missing route in the backend system. The requested feature was not properly configured in the routing system, leading to the \"Error: Not Found\" message being returned to the user.\n\n**Fix:** The missing route was added to the backend system, ensuring that requests for the specific feature are correctly handled. This involved configuring the routing system to properly route incoming requests to the corresponding functionality.\n\n**Result:** Users can now access the desired feature without encountering the \"Error: Not Found\" message. The system correctly handles requests for this specific functionality and provides the expected results to the user."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4762",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The user encountered an \"Error: Not Found\" message when trying to access a certain page or resource in the system.\n\n**Cause**: The issue was caused by a broken or incorrect URL or a missing or deleted resource in the system.\n\n**Fix**: The broken or incorrect URL was identified and corrected, and any missing or deleted resources were restored or recreated in the system.\n\n**Result**: The user can now access the page or resource without encountering an error. They will no longer see the \"Error: Not Found\" message and will be able to interact with the system as intended."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4763",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:**\nError: Not Found\n\n**Consequence:**\nUsers experienced an error message stating \"Not Found\" when trying to access a certain feature or page within the system.\n\n**Cause:**\nThe issue was caused by a misconfiguration or the absence of the requested resource on the server.\n\n**Fix:**\nThe missing resource has been properly configured and deployed on the server, resolving the \"Not Found\" error.\n\n**Result:**\nUsers can now access the feature or page without any error and the system functions as expected."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/3962",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The system was displaying an \"Error: Not Found\" message, indicating that the requested resource could not be found. This caused confusion and frustration for users as they were unable to access the desired content.\n\n**Cause**: The issue was caused by a bug in the system's routing mechanism. It was not able to correctly map the requested URL to the corresponding resource, leading to the \"Not Found\" error being displayed.\n\n**Fix**: The routing mechanism has been enhanced to accurately map the requested URLs to the appropriate resources. This fix ensures that the system correctly identifies and retrieves the requested content.\n\n**Result**: Users will no longer experience the \"Error: Not Found\" message when accessing resources within the system. The updated routing mechanism now ensures that the requested content is correctly displayed, providing users with a seamless browsing experience."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4719",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: The user experienced an \"Error: Not Found\" message when trying to access a certain page or resource on the system.\n\nCause: The underlying cause of this issue was a broken or incorrect URL link or route that was being accessed by the user.\n\nFix: The broken or incorrect URL link or route has been fixed and updated to correctly redirect the user to the intended page or resource.\n\nResult: Users can now successfully access the desired page or resource without encountering the \"Error: Not Found\" message."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3139",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: Users experienced an \"Error: Not Found\" message when trying to access a specific page.\n\n**Cause**: The issue was caused by a broken link that was not correctly directing users to the intended page.\n\n**Fix**: The broken link was identified and repaired to ensure that it correctly directs users to the intended page.\n\n**Result**: Users can now access the specific page without encountering any \"Error: Not Found\" messages, providing a seamless and uninterrupted user experience."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3098",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Title: \"Error: Not Found\"\n\nConsequence:\nUsers experienced an error message stating \"Not Found\" when trying to access a specific resource in the system. This caused frustration and confusion for users who were unable to access the desired content.\n\nCause:\nThe issue was caused by a misconfiguration in the routing system. The system was unable to locate the requested resource due to an incorrect path or missing route.\n\nFix:\nThe routing system has been updated to ensure proper handling of requests and accurate mapping of routes. The issue causing the \"Not Found\" error message has been resolved.\n\nResult:\nAfter applying this fix, users will no longer encounter the \"Not Found\" error message. The system will correctly locate and serve the requested resource, ensuring a seamless user experience."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3095",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: Users experienced an \"Error: Not Found\" message when trying to access a specific page or resource. \n\n**Cause**: The issue originated from a broken link or incorrect routing configuration in the application. \n\n**Fix**: The broken link was fixed, and the routing configuration was updated to redirect correctly to the expected page or resource. \n\n**Result**: Users can now successfully access the page or resource without encountering any \"Error: Not Found\" messages, ensuring a seamless experience on the platform."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3039",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue**: Not Found error\n\n**Consequence**: Users encountered an error message stating \"Error: Not Found\" when trying to access a certain feature or page in the system. This prevented them from using the feature or accessing the page they needed.\n\n**Cause**: The error occurred due to a misconfiguration or a broken link in the system. It could have been a result of a recent update or modification to the codebase.\n\n**Fix**: The misconfiguration or broken link was identified and fixed. The necessary changes were made to the codebase to ensure that the feature or page is properly accessible.\n\n**Result**: Users can now access the desired feature or page without encountering any \"Error: Not Found\" messages. The system correctly routes them to the intended destination, providing them with the expected functionality and content."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3180",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The user experienced an \"Error: Not Found\" message when trying to access the specified resource. \n\n**Cause**: The issue was caused by a missing or incorrect URL path or route in the system. \n\n**Fix**: The engineers updated the URL path or route to point to the correct resource. \n\n**Result**: Users can now access the specified resource without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3176",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a certain page in the system.\n\n**Cause:** The issue occurred due to a bug in the routing system, which failed to map the correct URL path to the corresponding page.\n\n**Fix:** The routing system has been updated to correctly handle the URL paths and ensure the proper mapping of pages.\n\n**Result:** Users can now successfully navigate to the desired page without encountering any \"Error: Not Found\" message. The routing system has been optimized to accurately direct users to the appropriate pages based on the URL path provided."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3125",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Release Note:\n\nConsequence: The system could not find the requested resource and displayed an \"Error: Not Found\" message.\n\nCause: The issue was caused by an undefined reference or a broken link in the system.\n\nFix: The undefined reference or broken link has been identified and resolved.\n\nResult: Users can now navigate to the requested resource without encountering any \"Error: Not Found\" messages. The system will properly handle and display the requested content."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3105",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an error when trying to access a resource, resulting in a \"Not Found\" error message. This prevented them from accessing the content they were looking for.\n\n**Cause:** The error occurred because the requested resource was not found in the system. This could be due to a variety of reasons such as incorrect URL or an issue with the database.\n\n**Fix:** The issue has been resolved by updating the system to handle not found resources more gracefully. The error handling mechanism has been enhanced to provide more informative error messages and guide users to alternative content.\n\n**Result:** With this fix, when users encounter a \"Not Found\" error, they will receive a more user-friendly error message that explains the issue and suggests possible next steps. This will improve the overall user experience and reduce frustration."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3120",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Release Note:\n\nConsequence: Users were receiving an \"Error: Not Found\" message when trying to access a certain feature.\n\nCause: The feature was not properly configured in the system, leading to a mismatch between the requested URL and the available functionality.\n\nFix: The configuration of the feature has been updated and validated to ensure correct mapping between the requested URL and the corresponding functionality.\n\nResult: Users can now successfully access the feature without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3115",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** Error: Not Found\n\n**Consequence:** Users experienced an \"Error: Not Found\" message when attempting to access a specific webpage. This issue prevented them from accessing the desired content and resulted in a frustrating user experience.\n\n**Cause:** The issue was caused by a broken link or a misconfigured routing configuration that resulted in the server being unable to locate the requested page. This could have been due to issues with the URL structure or incorrect path mapping.\n\n**Fix:** The broken link or misconfigured routing configuration has been identified and resolved. The appropriate URL structure and path mapping have been updated to ensure that the server can correctly locate and serve the requested page.\n\n**Result:** Users can now access the desired webpage without encountering the \"Error: Not Found\" message. The updated routing configuration enables the server to successfully locate and deliver the requested content, improving the overall user experience by eliminating access issues."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3081",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when attempting to access certain pages or resources in the system. This resulted in a frustrating and confusing user experience where they were unable to view or interact with the expected content.\n\n**Cause:** The issue was caused by a misconfiguration in the routing settings of the application. Due to this misconfiguration, the system was unable to properly route incoming requests to the correct pages or resources, resulting in the \"Error: Not Found\" message being displayed.\n\n**Fix:** The misconfiguration in the routing settings has been resolved. The development team has made the necessary changes to ensure that incoming requests are correctly routed to the intended pages or resources in the system.\n\n**Result:** With this fix in place, users will no longer encounter the \"Error: Not Found\" message when accessing pages or resources. They will be able to seamlessly navigate through the system and view the desired content without any disruptions."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3099",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users encountered an \"Error: Not Found\" message when trying to access a specific page or resource within the system. This issue prevented them from successfully navigating or accessing the desired content.\n\n**Cause:** The issue originated from a misconfigured or incorrect routing configuration within the system. This caused the system to fail in properly handling and resolving the requested route, resulting in the \"Error: Not Found\" message.\n\n**Fix:** The routing configuration has been thoroughly reviewed and corrected to ensure accurate and proper handling of the requested routes within the system. The necessary adjustments have been made to resolve any misconfiguration or routing errors.\n\n**Result:** Users can now successfully access and navigate to the desired page or resource without encountering the \"Error: Not Found\" message. The routing system now effectively handles and resolves the requested routes, providing users with a seamless experience in accessing the system's content."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3061",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** Not Found error\n**Consequence:** Users encountered an error message stating \"Not Found\" when trying to access certain pages or resources.\n**Cause:** The issue was caused by a misconfiguration or incorrect routing setup in the application.\n**Fix:** The misconfiguration and routing issues were resolved by identifying and correcting the root cause in the codebase.\n**Result:** Users can now successfully access all pages and resources without encountering the \"Not Found\" error message. The application's routing and configuration have been fixed, ensuring a seamless user experience."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3091",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence:\nUsers encountered an \"undefined\" error when trying to access a specific page or resource. This error prevented them from viewing the desired content and disrupted their workflow.\n\nCause:\nThe issue was caused by an incorrect URL or a missing route configuration in the code. As a result, the system was unable to locate the requested resource and returned the \"Not Found\" error.\n\nFix:\nWe have resolved the issue by updating the URL mapping and route configuration in the code. This ensures that the system can correctly identify and locate the requested resource.\n\nResult:\nWith this fix in place, users can now access the desired page or resource without encountering the \"undefined\" error. The system accurately routes the requests and presents the expected content, enhancing the overall user experience."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3080",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an error message stating \"Error: Not Found\" when attempting to access a certain feature of the system.\n\n**Cause:** This issue was caused by an undefined reference to the requested resource within the system's codebase.\n\n**Fix:** The developers addressed this issue by incorporating a check for the requested resource before attempting to access it, ensuring that it is properly defined.\n\n**Result:** With this fix implemented, users can now access the intended feature without encountering the \"Error: Not Found\" message."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3069",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:**\n```\nTitle: Error: Not Found\nDescription: When accessing the website, users encounter an \"Error: Not Found\" message.\n```\n\n**Consequence:**\n- Users experienced difficulties accessing the website and were presented with an \"Error: Not Found\" message.\n\n**Cause:**\n- A bug in the routing code resulted in incorrect URLs being generated for certain pages, leading to the \"Error: Not Found\" message.\n\n**Fix:**\n- The routing code has been updated to properly generate and handle URLs for all pages. \n\n**Result:**\n- Users can now access all pages of the website without encountering the \"Error: Not Found\" message."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3085",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3084",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: The user observed an \"Error: Not Found\" message when trying to access a specific resource in the system.\n\nCause: The issue was caused by a missing or invalid URL endpoint in the system's routing configuration.\n\nFix: The routing configuration was updated to include the correct URL endpoint for the resource.\n\nResult: Users can now access the specific resource without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3083",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a specific page or resource in the system.\n\n**Cause:** The issue was caused by a bug in the routing logic of the application. The routing system was not able to correctly map the requested URL to the corresponding page or resource, resulting in the \"Not Found\" error.\n\n**Fix:** The routing logic has been fixed to properly handle and match the requested URLs with the correct pages or resources. The bug that was causing the incorrect mapping has been identified and resolved.\n\n**Result:** Users can now access the desired page or resource without encountering the \"Error: Not Found\" message. The routing system accurately matches the requested URLs to the corresponding pages or resources, ensuring a smooth and uninterrupted user experience."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3082",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing an error message stating \"Not Found\" when attempting to access certain pages or resources in the system. This issue prevented users from accessing the desired content and disrupted their workflow.\n\n**Cause:** The \"Not Found\" error was caused by a bug in the system's routing mechanism. The application was failing to correctly map the user's request to the appropriate page or resource, resulting in the error message being displayed.\n\n**Fix:** The routing bug has been fixed by updating the system's routing logic. The application now properly handles and maps user requests, ensuring that the correct page or resource is served.\n\n**Result:** With this fix, users will no longer encounter the \"Not Found\" error when navigating the system. They will be able to seamlessly access all desired pages and resources, improving their overall user experience and productivity."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3079",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** Error: Not Found\n\n**Consequence:** This issue resulted in an error message stating \"Not Found\" being displayed to the user. This prevented the user from accessing the desired content or functionality.\n\n**Cause:** The cause of this issue was the absence of the requested resource or page in the system. This could have been due to an incorrect URL or a missing file.\n\n**Fix:** The fix for this issue involved implementing proper error handling and redirecting the user to a relevant error page when a requested resource or page was not found. Additionally, improved validation and error checking mechanisms were put in place to ensure that the correct error message was displayed.\n\n**Result:** With this fix implemented, users will no longer encounter the \"Not Found\" error message. Instead, they will be redirected to a user-friendly error page that provides clear information and guidance on what to do next. This improves the overall user experience by helping users better understand and navigate the system when encountering missing resources or pages."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3077",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "## Issue: Error: Not Found\n\n**Consequence:** Users were experiencing an \"Error: Not Found\" message when trying to access certain pages in the system.\n\n**Cause:** This issue was caused by a bug in the routing logic of the system. The routes were not properly configured to handle the requested URL, resulting in the \"Not Found\" error.\n\n**Fix:** The routing logic has been updated to ensure that the requested URLs are correctly matched and handled by the system.\n\n**Result:** With this fix, users will no longer encounter the \"Error: Not Found\" message when accessing pages. The system will correctly route and display the requested content, providing a smooth and error-free user experience."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3073",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing an error message stating \"Error: Not Found\" when trying to access a certain page. As a result, they were unable to view the requested content.\n\n**Cause:** This issue was caused by a bug in the code that was not properly handling the routing for the specific page. The code was unable to find the requested page and was throwing an error.\n\n**Fix:** The bug in the code was fixed by updating the routing logic to correctly handle the requested page. The code now properly finds and displays the content for the specified page, eliminating the \"Error: Not Found\" message.\n\n**Result:** After applying the fix, users can now access the specific page without any issues. The error message \"Error: Not Found\" no longer appears, ensuring users can view the desired content as intended."
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3070",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: Users experienced an \"Error: Not Found\" message when trying to access a certain page.\n\nCause: The server was unable to locate the requested page due to a missing or incorrect URL mapping.\n\nFix: The URL mapping was updated to correctly route the request to the intended page.\n\nResult: Users can now successfully access the page without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1878",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced a \"Not Found\" error when trying to access certain pages or resources within the system.\n\n**Cause:** The issue was caused by the existence of broken or invalid URLs, which were not properly handling the routing and retrieval of the requested content.\n\n**Fix:** The codebase was updated to include proper error handling and redirection mechanisms for handling broken or invalid URLs. Additionally, a thorough review of the URL handling logic was performed, ensuring that all potential edge cases were properly addressed.\n\n**Result:** Users will no longer encounter the \"Not Found\" error when accessing pages or resources within the system. The updated error handling and redirection mechanisms will ensure that users are seamlessly directed to the appropriate content or provided with relevant error messages if necessary."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1837",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The user experienced an \"Error: Not Found\" when trying to access a resource on the system.\n\n**Cause**: The issue was caused due to a misconfiguration in the routing of the system, leading to invalid or non-existent URLs being generated for certain resources.\n\n**Fix**: The misconfiguration in the routing has been identified and resolved. The system now generates correct and valid URLs for all resources.\n\n**Result**: Users will no longer encounter the \"Error: Not Found\" message when accessing resources. The system will correctly route and serve all requested resources without any issues."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1830",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing a \"Not Found\" error when trying to access a specific feature in the system.\n\n**Cause:** The issue was caused by a bug in the code that was preventing the feature from being properly loaded and displayed to the user.\n\n**Fix:** The bug in the code was fixed by addressing the issues with the feature loading process and ensuring that it worked as intended.\n\n**Result:** After the fix, users can now successfully access and use the feature without encountering any \"Not Found\" errors."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1823",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a specific resource. This prevented them from accessing the desired functionality or content.\n\n**Cause:** The issue occurred due to a misconfiguration in the routing settings of the application. The incorrect route was being called, leading to the \"Not Found\" error.\n\n**Fix:** The misconfiguration in the routing settings has been identified and corrected. The correct route is now set up to properly handle requests for the specific resource.\n\n**Result:** Users can now access the desired resource without encountering any \"Not Found\" errors. They can successfully utilize the functionality or view the content associated with the resource."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1827",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing an \"Error: Not Found\" message when trying to access certain pages or resources within the system.\n\n**Cause:** This issue was caused by a misconfiguration in the routing system, which was unable to properly map the requested URLs to the correct resources.\n\n**Fix:** The misconfiguration in the routing system has been identified and resolved. The necessary adjustments have been made to ensure that the URLs are correctly mapped to the corresponding resources.\n\n**Result:** After applying the fix, users will no longer encounter the \"Error: Not Found\" message when accessing pages or resources within the system. The routing system will accurately map the URLs to the correct resources, allowing users to navigate through the application seamlessly."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1818",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: Users were experiencing an \"Error: Not Found\" message when accessing the requested page. \n\nCause: This issue was caused by the system being unable to locate the requested resource or page.\n\nFix: The team identified and resolved the issue that was causing the system to return a \"Not Found\" error. \n\nResult: Users can now access the requested pages without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1816",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence:\nThe system displayed an \"Error: Not Found\" message, indicating that the requested resource could not be found. This prevented the user from accessing the desired functionality or content.\n\nCause:\nThe issue was caused by an undefined value being encountered, which resulted in the system not being able to locate the requested resource. This could have been due to a programming error or an issue with the data being processed.\n\nFix:\nThe root cause of the issue was identified and addressed by updating the code to handle undefined values appropriately. The fix involved implementing proper error handling and validation mechanisms to ensure that all resources are properly located and accessible.\n\nResult:\nAs a result of the fix, the system no longer encounters the \"Error: Not Found\" issue when attempting to locate and retrieve resources. Users can now access the desired functionality and content without any disruptions or error messages."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1791",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:**\nError: Not Found\n\n**Release Note:**\nConsequence: The user experienced an \"Error: Not Found\" message when attempting to access a particular page or resource in the system.\n\nCause: The issue was caused by a missing or incorrect URL or route configuration in the system.\n\nFix: The system was updated to include the correct URL or route configuration for the page or resource that was not found.\n\nResult: Users can now access the page or resource without encountering the \"Error: Not Found\" message."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1789",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue: Undefined error and \"Not Found\" message**\n- Consequence: Users were experiencing an undefined error and receiving a \"Not Found\" message when trying to access certain pages or resources in the system.\n- Cause: The issue was caused by a bug in the routing logic of the application, which was unable to properly handle and route the user's request to the correct page or resource.\n- Fix: The routing logic of the application was updated to correctly handle and route the user's request, ensuring that the appropriate page or resource is displayed instead of an undefined error or \"Not Found\" message.\n- Result: Users can now seamlessly navigate through the system without encountering any undefined errors or \"Not Found\" messages, as the routing logic has been fixed to properly handle and direct their requests to the correct pages or resources."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1749",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing an error message stating \"Not Found\" when trying to access a certain page or resource. This prevented them from accessing the desired content and resulted in a frustrating user experience.\n\n**Cause:** The error occurred due to a missing or incorrect URL mapping in the routing logic of the application. This caused the server to respond with a 404 HTTP status code, indicating that the requested resource could not be found.\n\n**Fix:** The issue has been resolved by updating the URL mapping in the application's routing configuration. The correct URL now points to the appropriate controller or handler that can handle the user's request.\n\n**Result:** Users can now access the desired page or resource without encountering the \"Not Found\" error. The application properly routes the user's request to the correct destination, allowing them to seamlessly navigate and interact with the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1912",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a specific resource. This prevented them from accessing the desired content and disrupted their workflow.\n\n**Cause:** The issue was caused by a bug in the system that was unable to locate the requested resource. This bug was triggered when the system encountered invalid or incorrect input related to the resource being accessed.\n\n**Fix:** The bug causing the \"Error: Not Found\" message has been addressed and resolved. The system now correctly handles invalid or incorrect input related to the resource, ensuring that the requested content can be accessed without any errors.\n\n**Result:** Users can now successfully access the desired resource without encountering the \"Error: Not Found\" message. The system efficiently handles any invalid or incorrect input, providing a seamless user experience and uninterrupted workflow."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1899",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing an \"Error: Not Found\" message when trying to access certain pages or resources.\n\n**Cause:** The issue was caused by a misconfiguration in the routing settings of the system, resulting in the system being unable to locate the requested pages or resources.\n\n**Fix:** The routing settings were updated to correctly map the requested pages or resources, resolving the issue.\n\n**Result:** Users can now access the affected pages or resources without encountering the \"Error: Not Found\" message. The system correctly locates and serves the requested pages or resources."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1907",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a certain page in the system.\n\n**Cause:** The issue was caused by a missing or incorrect URL routing configuration.\n\n**Fix:** The URL routing configuration has been updated to accurately map the requested URL to the correct page.\n\n**Result:** Users can now access the desired page without encountering the \"Error: Not Found\" message."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1890",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access certain pages or resources in the system.\n\n**Cause:** This issue was caused by a bug in the code that was responsible for handling requests and routing them to the appropriate pages or resources. The bug caused the system to return a \"Not Found\" error instead of properly displaying the requested content.\n\n**Fix:** The bug was fixed by identifying the specific code segment responsible for the incorrect routing and making the necessary changes to ensure that the system properly handles the requests and displays the requested content.\n\n**Result:** With this fix, users will no longer encounter the \"Error: Not Found\" message when trying to access pages or resources. The system will correctly route the requests and display the requested content, providing a smooth and error-free user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1881",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1889",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: Users experienced an error message stating \"Not Found\" when trying to access the requested resource.\nCause: The error occurred due to an undefined variable or object that was being accessed.\nFix: The undefined variable or object was identified and properly assigned a value or initialized.\nResult: Users will no longer encounter the \"Not Found\" error and will be able to access the requested resource without any issues."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1886",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a particular page or resource in the system.\n\n**Cause:** The issue was caused by a misconfiguration in the routing system, which led to the system not being able to find the requested page or resource.\n\n**Fix:** The routing system was updated to correctly handle and resolve requests for the affected page or resource.\n\n**Result:** Users can now successfully access the previously unavailable page or resource without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1879",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** Not Found error when trying to access a page\n\n**Consequence:** Users were seeing a \"Not Found\" error when attempting to access a specific page in the application. This issue prevented users from viewing and interacting with the page's content.\n\n**Cause:** The issue was caused by a misconfiguration in the routing system. The routing system was unable to correctly map the requested URL to the corresponding page in the application, resulting in the \"Not Found\" error.\n\n**Fix:** The routing system has been enhanced to accurately map the requested URLs to the correct pages in the application. The misconfiguration has been identified and corrected, ensuring that the routing system functions as intended.\n\n**Result:** Users can now access the specific page without encountering any \"Not Found\" errors. The routing system correctly maps the requested URLs to the corresponding pages, allowing users to seamlessly view and interact with the content on the page."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1876",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** Error: Not Found\n\n**Consequence:** Users experienced an error message stating \"Error: Not Found\" when attempting to access a certain page or resource in the system.\n\n**Cause:** This issue was caused by a misconfiguration in the routing system, where the requested page or resource was not properly defined or could not be found.\n\n**Fix:** The routing configuration has been updated to correctly map the requested page or resource, ensuring that it is properly accessible within the system.\n\n**Result:** Users can now successfully access the requested page or resource without encountering the \"Error: Not Found\" message. The routing system has been fixed to accurately direct users to the intended location, providing a seamless and error-free experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1874",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** \"Error: Not Found\" when trying to access a certain page\n\n**Consequence:** Users were unable to access a specific page and encountered an \"Error: Not Found\" message. This prevented them from viewing the content or completing any actions on that page.\n\n**Cause:** The issue was caused by a misconfiguration in the routing system, which resulted in the specific page not being properly mapped to its corresponding URL. As a result, when users tried to access that page, the system was unable to find it and returned the \"Error: Not Found\" message.\n\n**Fix:** The routing system was reconfigured to correctly map the specific page to its corresponding URL. This ensured that when users try to access that page, the system can locate it and display the content as expected.\n\n**Result:** Users can now successfully access the specific page without encountering any errors. They are able to view the content and complete necessary actions on that page without any hindrances."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1814",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access certain resources or pages in the system.\n\n**Cause:** The issue was caused by an incorrect routing configuration that prevented the system from correctly resolving the requested resources.\n\n**Fix:** The routing configuration has been updated to correctly resolve all requested resources and pages.\n\n**Result:** Users can now successfully access all resources and pages in the system without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1825",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The user experienced an \"Error: Not Found\" message when trying to access a specific resource in the system. This prevented them from accessing the desired content and disrupted their workflow.\n\n**Cause**: The issue was caused by a misconfigured route in the system's routing configuration. The route was not properly set up to handle the requested resource, resulting in the \"Not Found\" error.\n\n**Fix**: The misconfigured route was identified and fixed. The necessary changes were made to the routing configuration to properly handle the requested resource. \n\n**Result**: Users can now successfully access the desired resource without encountering the \"Error: Not Found\" message. The system correctly routes the requests to the appropriate resource, ensuring uninterrupted access to content and a smooth user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1824",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: Users experienced an error message stating \"Not Found\" when trying to access a certain feature of the system.\n\nCause: The issue was caused by a bug in the system's routing mechanism, which resulted in the incorrect mapping of the requested feature.\n\nFix: The routing bug has been fixed and the mapping of the requested feature has been adjusted to ensure the correct functionality.\n\nResult: Users can now access the specific feature without encountering any \"Not Found\" errors, allowing them to seamlessly navigate through the system and utilize all available features."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1804",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1780",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**\nThe system displayed an \"Error: Not Found\" message to the user, indicating that the requested resource could not be found.\n\n**Cause**\nThere was an issue in the system's routing mechanism that led to the incorrect resolution of the requested URL, resulting in the \"Not Found\" error.\n\n**Fix**\nThe routing logic was updated to correctly handle the requested URLs, ensuring that the system can resolve them to the appropriate resources.\n\n**Result**\nAfter the fix, the system will accurately route and resolve the requested URLs, preventing the occurrence of the \"Error: Not Found\" message. Users will now be able to access the intended resources without any issues."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1775",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The user experienced an error message stating \"undefined\" and \"Error: Not Found\". This error message indicated that the requested resource could not be found in the system.\n\n**Cause**: The issue was caused by a bug in the code that was responsible for handling and routing the user's request. The bug caused the system to incorrectly handle the requested resource, resulting in the \"undefined\" error message.\n\n**Fix**: The bug in the code was fixed by identifying and addressing the incorrect handling and routing of the user's request. The code was modified to properly handle and route the requested resource, ensuring that it is found and returned correctly.\n\n**Result**: With this fix, the user will no longer encounter the \"undefined\" error message and the \"Error: Not Found\" message. The system will correctly handle the user's request and return the requested resource as expected."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1768",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Release Note:\n\nConsequence: The system was unable to find the requested resource and returned an \"Error: Not Found\" message to the user.\n\nCause: There was an issue in the system where the requested resource was not properly mapped or configured, leading to the \"Error: Not Found\" response.\n\nFix: The system was updated to correctly map and configure the requested resource, ensuring that it is properly accessible.\n\nResult: Users will now be able to successfully access and view the requested resource without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1758",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:**\nError message \"Not Found\" occurs when attempting to access a certain page.\n\n**Consequence:**\nUsers were unable to access the desired page and instead encountered an \"Not Found\" error message.\n\n**Cause:**\nThere was an issue in the routing logic of the application, resulting in the inability to find the requested page.\n\n**Fix:**\nThe routing logic was updated to correctly handle and route to the requested page.\n\n**Result:**\nUsers can now access the desired page without encountering the \"Not Found\" error message."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1757",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: Users experienced an \"Error: Not Found\" message when attempting to access a particular feature of the system.\n\n**Cause**: The issue was caused by a missing routing configuration for the specific feature, resulting in the system being unable to locate the requested resource.\n\n**Fix**: The routing configuration for the feature was added to the system, allowing proper navigation to the desired resource.\n\n**Result**: Users can now access the feature without any errors and seamlessly navigate to the requested resource."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1756",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** The system displayed an error message stating \"Not Found\", indicating that the requested resource could not be found. Users were unable to access the expected content or functionality of the system.\n\n**Cause:** This issue occurred due to a coding error that caused the system to fail in locating the requested resource. It resulted from a misconfiguration or a misinterpretation of the URL or route.\n\n**Fix:** The codebase was updated to address the issue and improve the system's ability to locate resources. The underlying bug that caused the \"Not Found\" error was fixed by correcting the URL parsing and routing logic.\n\n**Result:** With the fix in place, users can now access the requested resources without encountering the \"Not Found\" error. The system successfully navigates and locates the desired content or functionality, enhancing the overall user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1737",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The system was throwing an \"Error: Not Found\" when trying to access a specific resource. This resulted in the user being unable to retrieve or interact with the desired resource.\n\n**Cause**: The issue was caused by a missing or incorrect URL route in the system's code, leading to the inability to locate and fetch the requested resource.\n\n**Fix**: The missing or incorrect URL route was identified and corrected in the codebase, ensuring that the system can accurately locate and retrieve the desired resource.\n\n**Result**: Users can now successfully access and interact with the previously unavailable resource, as the system no longer throws an \"Error: Not Found\" and correctly retrieves the requested information."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12796",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nWhen pushing a job fails, we should trigger the `JobYieldProcessor` to make the related job available for long polling.\r\n\r\n**Describe the solution you'd like**\r\n`JobIntent.YIELD` command should be appended when job push fails. This will be achieved through registering the error handler to `RemoteJobStreamErrorHandlerService` inside `JobStreamServiceStep`.\r\n\r\n**Describe alternatives you've considered**\r\n/\r\n\r\n**Additional context**\r\nBlocked by:\r\n* https://github.com/camunda/zeebe/issues/12541\r\n\n",
    "title": "Provide Error Handler implementation for Job Streamer",
    "releaseNoteText": "**Consequence:** When pushing a job failed, the `JobYieldProcessor` was not triggered, which resulted in the related job not being available for long polling.\n\n**Cause:** The error handler was not implemented for the Job Streamer, specifically the `RemoteJobStreamErrorHandlerService` inside `JobStreamServiceStep`.\n\n**Fix:** The error handler has been implemented and registered to the `RemoteJobStreamErrorHandlerService` inside `JobStreamServiceStep`. Now, when a job push fails, the `JobIntent.YIELD` command is appended.\n\n**Result:** When a job push fails, the `JobYieldProcessor` is now triggered, making the related job available for long polling."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12793",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\n\r\nWe can configure a raft request timeout that is applicable to all requests send between raft leaders and followers. InstallRequest send snapshot files which are considerably larger than other raft requests. As a result sometimes it takes longer to send the request and get a response, especially on networks with higher latency between brokers. This can result in timeout exception and the snapshot replication will have to restart from the beginning (See #11496). \r\n\r\nTo workaround, we can increase raft request timeout. But increasing this can affect how fast failures are detected, how fast requests are retried etc. So it would be better to be able to set a higher request timeout just of InstallRequest.\r\n\r\n**Describe the solution you'd like**\r\n\r\nExpose a configuration to set request timeout for InstallRequest. This should be different from the existing raft request timeout configuration.\r\n\r\n**Describe alternatives you've considered**\r\n\r\n- Fix #11496. This will not solve the issue, but reduces the impact if the request timesout.\r\n- Instead of sending the files as it is, split into to smaller requests. This would be a better solution, but requires more effort. https://github.com/camunda/zeebe/issues/12795 \r\n\r\n**Additional context**\r\nRelated to https://jira.camunda.com/browse/SUPPORT-16901 \r\n\n\n megglos: Triage:\n- it allows us to provide a better workaround in cases where the snapshot transmission takes a longer time (depending on the size of a single snapshot chunk up to 64MB) => we could just increase the timeout for installs but not everything\n- me moved it into ready already to get to it asap for the next patch (we have to do it next week)",
    "title": "Allow configuring request timeout for InstallRequest",
    "releaseNoteText": "**Consequence**: Increased timeout for InstallRequest to prevent timeout exceptions during snapshot replication, especially on networks with higher latency between brokers.\n\n**Cause**: Previously, the default request timeout for all raft requests was being used for InstallRequest as well. However, since InstallRequest sends larger snapshot files, it sometimes took longer to send the request and get a response, leading to timeout exceptions.\n\n**Fix**: Exposed a new configuration option to set a separate request timeout specifically for InstallRequest. This allows users to increase the timeout only for InstallRequest without affecting the timeout for other raft requests.\n\n**Result**: Users can now configure a higher request timeout for InstallRequest, reducing the chances of timeout exceptions during snapshot replication. This improves the reliability and efficiency of the snapshot replication process, especially on networks with higher latency between brokers."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12575",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\n\r\nI think that it is not a good idea to collect snapshot files in the list. \r\nCheck it here: https://github.com/camunda/zeebe/blob/02d1df8ac1b6f5484f1e9d4c1f19c8a5712176b1/snapshot/src/main/java/io/camunda/zeebe/snapshots/impl/SnapshotChecksum.java#L41. \r\nBecause, after a while, the amount of snapshot files should grow. So, we could instead call the `.forEachOrdered` method to calculate the snapshot. I will provide the PR soon to see this in action and benchmark this.\r\n\r\n**Describe the solution you'd like**\r\nWe should call `Stream.forEachOrdered` instead of collecting snapshots `File`s in the list.\r\n\r\n**Describe alternatives you've considered**\r\nWe could use `Stream.forEach` but as I can understand the order is important, so we shouldn't do this.\n\n aivinog1: @Zelldon Hello 👋 \nI think that this could be addressed to the ZDP Team (sorry if I am wrong 😅)\nCould we assign this to someone, please? In our load test environment, we see a little degradation in creating snapshots and we are suspecting this part of the code.\nThank you in advance 🙂\n Zelldon: Looks like @deepthidevaki will look into your PR, thanks for providing it :)",
    "title": "Improve the traversing of snapshot files",
    "releaseNoteText": "**Consequence:**\nThe traversal of snapshot files in the system was inefficient as it involved collecting the files in a list, leading to performance degradation.\n\n**Cause:**\nThe inefficient traversal of snapshot files was caused by the usage of the `.forEachOrdered` method in the code, which accumulated the snapshot files in a list.\n\n**Fix:**\nThe code has been updated to use the `Stream.forEachOrdered` method instead, eliminating the need to collect the snapshot files in a list and improving performance.\n\n**Result:**\nAs a result of this fix, the traversal of snapshot files has been optimized and the system now has improved performance in creating snapshots. Users will experience faster processing times, especially in load test environments. This enhancement will also prevent potential degradation as the number of snapshot files increases over time."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12548",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nI understand that [the Actor metrics](https://github.com/camunda/zeebe/blob/a3d2002a0eb74cc701572b4e02abfeb16d6a48a8/scheduler/src/main/java/io/camunda/zeebe/scheduler/ActorMetrics.java) are in the experimental phase, but it would be nice if there exists the Actor dashboard out of the box 🙂\r\n\r\n**Describe the solution you'd like**\r\nA separate file, called, for example, actor.json in [the Grafana dashboards](https://github.com/camunda/zeebe/tree/a3d2002a0eb74cc701572b4e02abfeb16d6a48a8/monitor/grafana) folder.\r\n\r\n**Describe alternatives you've considered**\r\nAdd these metrics to the existing dashboard, for example, [zeebe.json](https://github.com/camunda/zeebe/blob/a3d2002a0eb74cc701572b4e02abfeb16d6a48a8/monitor/grafana/zeebe.json). But I think that it would be more convenient if we have this in a separate dashboard (some users may decide to not enable these metrics thus the dashboard is not needed for them)\n\n megglos: ZDP-Triage:\r\n- seems like a good first issue to pickup during an upcoming onboarding",
    "title": "Provide Grafana Dashboards for the Actor metrics",
    "releaseNoteText": "**Consequence:**\nPreviously, there was no out-of-the-box Grafana dashboard for the Actor metrics in the experimental phase.\n\n**Cause:**\nThe absence of a separate Grafana dashboard for the Actor metrics was due to the experimental nature of these metrics and the need to keep them separate from the existing dashboard.\n\n**Fix:**\nA new file called \"actor.json\" has been added to the Grafana dashboards folder to provide a separate dashboard for the Actor metrics.\n\n**Result:**\nUsers can now access a dedicated Grafana dashboard for the Actor metrics, providing better visibility and convenience for monitoring the experimental phase of these metrics. Users who do not enable these metrics can choose to not use the dashboard, ensuring flexibility in configuration."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12541",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThe following `Processor` classes are able to push an activated job to a client.\r\n\r\n- [JobWorkerTaskProcessor](https://github.com/camunda/zeebe/blob/319813a684325c97556fb59013eb8b88ea27b3f2/engine/src/main/java/io/camunda/zeebe/engine/processing/bpmn/task/JobWorkerTaskProcessor.java#L54) / [BpmnJobBehavior](https://github.com/camunda/zeebe/blob/4d46a4947e6d3ac72cb4e0af324f4c978b591989/engine/src/main/java/io/camunda/zeebe/engine/processing/bpmn/behavior/BpmnJobBehavior.java#L81-L92)\r\n- [JobTimeOutProcessor](https://github.com/camunda/zeebe/blob/ad1d5c92a3d4d6009da3a9d968238b83b9dd5c5c/engine/src/main/java/io/camunda/zeebe/engine/processing/job/JobTimeOutProcessor.java#L20)\r\n- [JobFailProcessor](https://github.com/camunda/zeebe/blob/813c86f780f54496fcd089810704c1a4e4958141/engine/src/main/java/io/camunda/zeebe/engine/processing/job/JobFailProcessor.java)\r\n- [JobRecurProcessor](https://github.com/camunda/zeebe/blob/813c86f780f54496fcd089810704c1a4e4958141/engine/src/main/java/io/camunda/zeebe/engine/processing/job/JobRecurProcessor.java)\r\n- [ResolveIncidentProcessor](https://github.com/camunda/zeebe/blob/18dd3c5e8df9bd2164e4d0fc73f5429c9d38b05c/engine/src/main/java/io/camunda/zeebe/engine/processing/incident/ResolveIncidentProcessor.java)\r\n\r\n**Describe the solution you'd like**\r\nThe `BpmnJobActivationBehavior` class is able to use the `JobStreamer` API to push jobs. The following steps need to be performed:\r\n\r\n- [x] For an available `JobStream` get `JobActivationProperties`\r\n- [x] Set `deadline` for `JobRecord` (using `JobActivationProperties`)\r\n- [x] Set `variables` for `JobRecord` (using `JobActivationProperties`) \r\n- [x] Set `worker` for `JobRecord` (using `JobActivationProperties`)\r\n- [x] Activate job using a `JobBatchRecord`/`JobBatchIntent.ACTIVATE`\r\n- [x] Push `JobRecord` on the `JobStream` through a `SideEffectProducer`\r\n\r\n**Describe alternatives you've considered**\r\n/\r\n\r\n**Additional context**\r\nBlocked by:\r\n* https://github.com/camunda/zeebe/issues/12083\r\n\n",
    "title": "Jobs are pushed from relevant processors",
    "releaseNoteText": "**Consequence:**\n\nJobs were not being pushed from relevant processors, resulting in a lack of activation of jobs for clients.\n\n**Cause:**\n\nThe `Processor` classes responsible for pushing activated jobs were not properly configured to utilize the `JobStreamer` API and perform the necessary steps for job activation.\n\n**Fix:**\n\nThe `BpmnJobActivationBehavior` class was modified to incorporate the use of the `JobStreamer` API for pushing jobs. The following steps were implemented:\n- Obtaining `JobActivationProperties` for an available `JobStream`\n- Setting the `deadline` for the `JobRecord` using `JobActivationProperties`\n- Setting the `variables` for the `JobRecord` using `JobActivationProperties`\n- Setting the `worker` for the `JobRecord` using `JobActivationProperties`\n- Activating the job using a `JobBatchRecord` with the intent `JobBatchIntent.ACTIVATE`\n- Pushing the `JobRecord` onto the `JobStream` through a `SideEffectProducer`\n\n**Result:**\n\nAfter the fix, the relevant processors are now able to properly push the activated jobs to the clients. This ensures that the jobs are correctly activated and processed within the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12539",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\n\r\nCreate a processor to process `ProcessInstanceBatch.TERMINATE` commands.\r\n\r\n- Add a method to the BpmnStateBehavior to get a defined amount of child elements, starting at a specific key.\r\n- Create the processor\r\n- Write a `ProcessInstance.TERMINATE` command for each of the element instance keys in the list\r\n- If the `index` is available:\r\n    - Get the next `BATCH_SIZE` + 1 child instances of the `batchElementInstanceKey`\r\n    - Create a new `ProcessInstanceBatch` record\r\n    - Write a new `ProcessInstanceBatch.TERMINATE` command\r\n- If no `index` is available:\r\n    - Have a 🍪 Nothing else to do\n",
    "title": "Create `ProcessInstanceBatch.TERMINATE` processor",
    "releaseNoteText": "**Consequence:**\n\nPreviously, there was no processor available to handle the `ProcessInstanceBatch.TERMINATE` commands. As a result, these commands could not be processed, causing a delay in terminating process instances.\n\n**Cause:**\n\nThe absence of a `ProcessInstanceBatch.TERMINATE` processor was the underlying cause of this issue. The BpmnStateBehavior did not have a method to retrieve a specific number of child elements starting from a given key, which prevented the creation of this processor.\n\n**Fix:**\n\nThis issue has been resolved by introducing a new processor called `ProcessInstanceBatch.TERMINATE`. To enable this, a method has been added to the BpmnStateBehavior that allows retrieving a specified number of child elements starting from a specific key. Additionally, a new `ProcessInstanceBatch` record is created when the `index` is available, and a `ProcessInstanceBatch.TERMINATE` command is written for each element instance key in the list.\n\n**Result:**\n\nWith this fix in place, the `ProcessInstanceBatch.TERMINATE` commands can now be properly processed by the system. When the `index` is available, the processor retrieves the next `BATCH_SIZE` + 1 child instances of the `batchElementInstanceKey`, creates a new `ProcessInstanceBatch` record, and writes a corresponding `ProcessInstanceBatch.TERMINATE` command. In the absence of an `index`, the system continues as usual with no additional actions required."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12538",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\n\r\nContainer elements (process, subprocess, etc.) all terminate their child instances when `onTerminate` is called. For all these elements this method calls the `BpmnStateTransitionBehavior#terminateChildInstances` method. This method needs to be changed to make use of the new `ProcessInstanceBatch` command with the `TERMINATE` intent.\r\n\r\n- Modify the `BpmnStateTransitionBehavior#terminateChildInstances `:\r\n    - Create a `ProcessInstanceBatch` record\r\n        - `batchElementInstanceKey` will be the key of the container element\r\n        - `index` will be empty as this is the first batch command\r\n    - Write the a `ProcessInstanceBatch.TERMINATE` command using the created record\n",
    "title": "Use the `ProcessInstanceBatch Command` when terminating container elements",
    "releaseNoteText": "**Consequence:** The system did not correctly terminate container elements when the `onTerminate` method was called. This resulted in child instances of these elements not being properly terminated.\n\n**Cause:** The issue was caused by the usage of the outdated `BpmnStateTransitionBehavior#terminateChildInstances` method. This method did not utilize the new `ProcessInstanceBatch` command with the `TERMINATE` intent.\n\n**Fix:** The `BpmnStateTransitionBehavior#terminateChildInstances` method has been modified. It now creates a `ProcessInstanceBatch` record for the container element and writes a `ProcessInstanceBatch.TERMINATE` command using the created record.\n\n**Result:** With the fix in place, container elements now correctly terminate their child instances when the `onTerminate` method is called. The new `ProcessInstanceBatch` command with the `TERMINATE` intent ensures that child instances are properly terminated, improving the overall behavior and reliability of the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12085",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\nA job pushed to the `JobStreamer` API may fail to be handed over to the client due to client failure.\r\n\r\n## AC\r\n* A new `JobIntent.YIELD` intent is added.\r\n* A new `JobYield` processor is added. The processor will perform logic similar to the `JobFailProcessor`, i.e. make a job `ACTIVATABLE` again.\r\n\r\n## Additional context\r\n* The implementation of this issue should then be used in the job push `ErrorHandler` implementation.\n",
    "title": "Job Yield Processor is implemented to be used for Job Push fallback",
    "releaseNoteText": "**Consequence:** Previously, when a job pushed to the `JobStreamer` API failed to be handed over to the client due to client failure, there was no fallback mechanism in place. \n\n**Cause:** The lack of a fallback mechanism in the `JobStreamer` API caused jobs to be permanently stuck in a failed state when a client failure occurred.\n\n**Fix:** A new `JobYield` processor has been added to handle the fallback scenario when a job fails to be handed over to the client. This processor performs similar logic to the existing `JobFailProcessor`, allowing the job to become activatable again.\n\n**Result:** With this fix, jobs that fail to be handed over to the client due to client failure will now be processed by the new `JobYield` processor, which sets the job to an `ACTIVATABLE` state. This enables the job to be retried or processed by other fallback mechanisms, ensuring a smoother and more reliable job processing experience. The implementation of this fix should also be used in the job push `ErrorHandler` implementation for comprehensive error handling."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/10031",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThe BPMN symbol for the inclusive gateway is supported with\r\n- #9747 \r\n\r\nHowever, only the diverging behavior was added. Users may want to use the converging behavior as well.\r\n\r\n**Describe the solution you'd like**\r\n- remove the validation that restricts the number of incoming sequence flows to an inclusive gateway to max 1.\r\n- only activate the inclusive gateway when (see discussion https://github.com/camunda/zeebe/pull/9747#discussion_r925770530):\r\n  - there are no active children of its flow scope instance (we can check this already),\r\n  - or if all incoming sequence flows were taken at least once (we can check this already),\r\n  - or if no path can be found from any of the active children to the inclusive gateway (but we can't check this yet).\r\n\r\n**Describe alternatives you've considered**\r\nUsers can use a combination of parallel and exclusive gateways to build similar joining logic.\r\n\r\n**Additional context**\r\nSpec: https://www.omg.org/spec/BPMN/2.0.2/PDF\r\n\r\nProduct-Hub:\r\n- https://github.com/camunda/product-hub/issues/364\r\n\n\n lzgabel: Hi @remcowesterhoud. I'm working on this recently, please assign this task to me. Thanks. ❤️\n lzgabel: Hi @remcowesterhoud. Sorry to bother you, :bow:  because I'm currently implementing this feature, and I need to get all child instances in the scope, but when I rebase the changes on top of  `main`, I found that you removed this `BpmnStateBehavior#getChildInstances` method by this [commit](https://github.com/camunda/zeebe/pull/12604/commits/0f3f458221a72b2fae27e9a1f5bc7dad890c4395), so I will add it back.\n remcowesterhoud: > Hi @remcowesterhoud. Sorry to bother you, :bow:  because I'm currently implementing this feature, and I need to get all child instances in the scope, but when I rebase the changes on top of  `main`, I found that you removed this `BpmnStateBehavior#getChildInstances` method by this [commit](https://github.com/camunda/zeebe/pull/12604/commits/0f3f458221a72b2fae27e9a1f5bc7dad890c4395), so I will add it back.\n\nIt was unused 😄\n\nOf course, go ahead and put it back if you need it!\n remcowesterhoud: @koevskinikola assigning you on the issue, so it's clear on our board you are reviewing it\n aisong: I'm really looking forward to this feature. I wonder when it will be released?\n lzgabel: Hi @aisong. This feature is available after version `8.3.0-alpha2`. It also brings another problem, which I am currently solving: #13070 \r\n\r\nBTW. `8.3.0` will be released this October.\n aisong: > Hi @aisong. This feature is available after version `8.3.0-alpha2`. It also brings another problem, which I am currently solving: #13070\r\n> \r\n> BTW. `8.3.0` will be released this October.\r\n\r\nOK,thank you. I'll be waiting for your good product.\n jschulenklopper: Looking forwards to this. This is related to the error message in the Modeler that reads \"An <Inclusive Gateway> with more than one incoming <Sequence Flow> is not supported by Camunda 8.2\", right? And delivery of this feature will deprecate the warning that's on https://docs.camunda.io/docs/components/modeler/bpmn/inclusive-gateways/?\r\n\r\nAnd for the modeller, the missing feature will be delivered per https://github.com/camunda/camunda-modeler/issues/3613 ?\r\n\r\n\n korthout: Great to hear you're all excited about this feature 🚀 \r\n\r\n@jschulenklopper That's all correct! You can already tryout the feature in `8.3.0-alpha2`, but Modeler does not take this into account yet (which is why it raises the warning). As @lzgabel mentioned, this is planned to be fully available in `8.3.0` (release planned for October 2023)\n korthout: Due to several limitations, we had to revert the implementation of the converging inclusive gateway:\r\n- https://github.com/camunda/zeebe/issues/13640\r\n\r\nTherefore, I'm re-opening this issue.\n lzgabel: Hi @korthout. I'll continue to look into this issue in the next few days. If you have a better solution, please let me know. 🙇 \n korthout: @lzgabel Sorry, I haven't spent time on this topic in a while, and I don't yet have ideas other than those that I wrote [here](https://github.com/camunda/zeebe/issues/13070#issuecomment-1649387794). So, we need some way to store in the state that a sequence flow is active and expect to process a command to activate the target element at some point in the future. If you want to work on this feature, I think we need to tackle this part first.",
    "title": "Support BPMN Converging Inclusive Gateway",
    "releaseNoteText": "**Consequence:**\nBPMN converging inclusive gateway behavior is missing from the supported features. This means that users have been unable to utilize the converging behavior of inclusive gateways.\n\n**Cause:**\nThe validation in the system restricted the number of incoming sequence flows to an inclusive gateway to a maximum of one. This limitation prevented the implementation of converging inclusive gateway behavior.\n\n**Fix:**\nThe fix involved removing the validation that restricted the number of incoming sequence flows to an inclusive gateway. Additionally, the fix introduced conditions for activating the inclusive gateway, which included checking for active children of the flow scope instance and ensuring that all incoming sequence flows were taken at least once. However, there was still a limitation in checking if no path could be found from any active child to the inclusive gateway.\n\n**Result:**\nWith this fix, users can now utilize the converging behavior of inclusive gateways. The system no longer restricts the number of incoming sequence flows to an inclusive gateway, allowing for more flexible and comprehensive handling of process flows. This enhancement provides users with a more powerful and versatile tool for designing their BPMN workflows."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/2890",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nAt a multi-instance activity, I can define the input collection to iterate over. If the collection holds a large number of elements, the broker might fail to spawn the inner instances. Such a case can only be fixed, by manually decreasing the collection variable.\r\n\r\n**Describe the solution you'd like**\r\n* I can spawn as many instances as defined by the input collection\r\n* instead of spawning the instances all at once on activating the body (current behavior), spawn the instances step-wise until all instances are created (e.g. spawn 10 instances and write a record to spawn the next 10)\r\n* the instance spawning can be interrupted by an event or a terminate command\r\n\r\n**Describe alternatives you've considered**\r\n* increasing buffers and record max length - see #2880 \r\n* splitting the creation into multiple steps (e.g. loop or nested multi-instance) or control it externally -  \r\n\r\nRelates to\r\nhttps://jira.camunda.com/browse/SUPPORT-16653\r\nhttps://jira.camunda.com/browse/SUPPORT-16549\r\n\n\n saig0: Using Zeebe 0.21.0, we can execute a multi-instance activity with an input collection of ~ 5.000 elements (depending on the variables). By increasing the max message size, we could process even larger collections. \r\n\r\nSince this is ok for now, we will postpone the issue.\n npepinpe: Just a side note, let's postpone at least until the new engine is done (i.e. at least Q2 2021). I can see us working on reducing the max message size next year, so we will need to implement this kind of \"chunking\" - obviously can't guarantee, but it sounds plausible to me.\n saig0: **Update:** I tested it again with version `1.0.0-alpha7`. We can now iterate over an input collection with up to ~12.000 elements :tada: \n korthout: Marking priority as `later` because 'multi-instance for large collections' is not the main concern for the process automation team right now. However, we should probably work on this when we work on\r\n- #8687\r\n\r\nNote that the impact of this bug is reduced once we've resolved\r\n- #5221\r\n\r\nPlease comment if you think this should have a higher priority.\n epollum: Hello team, I want to mention that we have a customer who requires the ability to use large multi-instance activities. Please see https://jira.camunda.com/browse/SUPPORT-16499\r\nAnd https://github.com/camunda/zeebe/issues/11355\r\n\r\nThank you!\n daniel-ewing: Hi team, here is another one: https://jira.camunda.com/browse/SUPPORT-16653\n npepinpe: And another: https://jira.camunda.com/browse/SUPPORT-16549\n abbasadel: Team meeting: we changed the priority to \"Upcoming\" to pick this up when we have time\n felix-mueller: Hey @remcowesterhoud \r\nI saw you worked on this item, there is one more item in the backlog: https://github.com/camunda/zeebe/issues/8687\r\n\r\nCould you perhaps elaborate if this is fixed now as well or is there a case which is not covered? \r\nCould you perhaps explain which case is not covered for #8687?\r\n\r\nThanks\r\nFelix\n remcowesterhoud: @felix-mueller from what I understand after reading #8687 it is a different issue. This one had to do with instance banning and the original author of the issue [explicitly mentioned](https://github.com/camunda/zeebe/issues/8687#issuecomment-1026023644) his instances weren't getting banned.\r\n\r\nWhether this is fixed, I am not sure as I'm struggling to understand what is happening in the other issue 😅 @korthout do you have any ideas? It reads to me like it's more about job activation than multi-instance input collections.\r\n\r\nIf it's stale we can consider closing it and seeing if it ever occurs again.\n korthout: @remcowesterhoud I've had a look. IMO, that issue is not specifically related to multi-instance, but rather to activating many jobs with large variables in the Go client.\r\n\r\nI've provided my thoughts on that issue [here](https://github.com/camunda/zeebe/issues/8687#issuecomment-1665950474)",
    "title": "I can spawn inner instances for a large input collection",
    "releaseNoteText": "**Consequence:**\nThe broker fails to spawn inner instances when the input collection for a multi-instance activity contains a large number of elements. This requires manual adjustment of the collection variable to fix the issue.\n\n**Cause:**\nThe current behavior of the multi-instance activity is to spawn all instances at once, which can overload the system when dealing with large input collections.\n\n**Fix:**\nThe instances are now spawned step-wise, allowing for a controlled creation process. For example, a set number of instances are spawned first, followed by the spawning of the next set of instances after a specific event or terminate command.\n\n**Result:**\nUsers can now spawn as many instances as defined by the input collection, even for large collections. The instances are spawned in a controlled manner, ensuring the system can handle the process without overloading."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12780",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n```\r\nERROR 2023-05-16T00:30:04.104402170Z [resource.labels.containerName: zeebe] Unexpected error on writing CREATE command Failed to write request to logstream\r\n```\r\n\r\nThe error is from CommandAPIHandler when it tries to write a user request to the leader's logstream. This happened while the leader is transition to follower, and the logstream has already closed. Before this error we see that Sequencer rejects the record because it is closed. \r\n\r\nThis is a new error message introduced in https://github.com/camunda/zeebe/pull/12676. Previously this error was ignored. So we never got the error message.\r\n\r\n[logs](https://console.cloud.google.com/errors/detail/CNy21ZD_7pnW3AE;service=zeebe;time=P7D?project=zeebe-io)\r\n\r\n**Expected behavior**\r\n\r\n- Reduce the log level to warn/debug\r\n- logstream#tryWrite should return specific error code instead of -1, and use that to log more meaningful message. \r\n- If we can recognize that this is during the leader transition we can chose to not log the error. Instead return a PARTITION_LEADER_MISMATCH code back to the gateway so that it can retry the command with the new leader before sending an error to the client.\r\n\n\n megglos: ZDP-Triage:\n- mostly noise\n- it's expected and shouldn't be logged as error in the particular scenario\n- as it's new (last or next patch) it can be considered a regression => could be confusing after update\n megglos: ZDP-Planning:\n- we will look into it before the next release\n- also affects 8.2,8.1,8.0 due to a backporrt\n Zelldon: I feel this is not 100% resolved. [We see a lot of errors messages also in the gateway](https://console.cloud.google.com/logs/query;query=resource.type%3D%22k8s_container%22%0Aresource.labels.project_id%3D%22zeebe-io%22%0Aresource.labels.location%3D%22europe-west1-b%22%0Aresource.labels.cluster_name%3D%22zeebe-cluster%22%0Aresource.labels.namespace_name%3D%22medic-y-2023-cw-20-d2345cc-benchmark%22%0Alabels.k8s-pod%2Fapp%3D%22camunda-platform%22%0Alabels.k8s-pod%2Fapp_kubernetes_io%2Fcomponent%3D%22zeebe-gateway%22%0Alabels.k8s-pod%2Fapp_kubernetes_io%2Finstance%3D%22medic-y-2023-cw-20-d2345cc-benchmark%22%0Alabels.k8s-pod%2Fapp_kubernetes_io%2Fmanaged-by%3D%22Helm%22%0Alabels.k8s-pod%2Fapp_kubernetes_io%2Fname%3D%22zeebe-gateway%22%0Alabels.k8s-pod%2Fapp_kubernetes_io%2Fpart-of%3D%22camunda-platform%22;cursorTimestamp=2023-06-07T10:45:04.209515605Z?project=zeebe-io), which is also in this case a lot of noise.\r\n\r\nExample of a current medic benchmark\r\n```\r\nio.camunda.zeebe.gateway.cmd.BrokerErrorException: Received error from broker (INTERNAL_ERROR): Failed writing request: Failed to write request to logstream\r\n\tat io.camunda.zeebe.gateway.impl.broker.BrokerRequestManager.handleResponse(BrokerRequestManager.java:194) ~[zeebe-gateway-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.gateway.impl.broker.BrokerRequestManager.lambda$sendRequestInternal$2(BrokerRequestManager.java:143) ~[zeebe-gateway-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.future.FutureContinuationRunnable.run(FutureContinuationRunnable.java:28) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\"\r\n```\r\n\r\n[Where at this time a role change happens](https://grafana.dev.zeebe.io/d/NzsO1mUnk/zeebe-overview?orgId=1&var-DS_PROMETHEUS=Prometheus&var-cluster=All&var-namespace=medic-y-2023-cw-20-d2345cc-benchmark&var-pod=All&var-partition=All&from=1686132065352&to=1686136683285)\r\n![role](https://github.com/camunda/zeebe/assets/2758593/66f672f3-3af8-44f0-8cb3-7d3cb5029dd5)\r\n\n deepthidevaki: @Zelldon That is an old benchmark before the bug fix.\n Zelldon: Ups thanks @deepthidevaki you're right :+1: ",
    "title": "Failing to write to logstream during stepdown is logged as error",
    "releaseNoteText": "**Consequence:**\nPreviously, during a leader transition in the system, if the logstream closed before the leader could write a user request to it, an error was logged. This error message was introduced in a recent update, causing confusion for users. The error message was noisy and unnecessary, creating additional log entries.\n\n**Cause:**\nThe error was occurring due to a change introduced in a recent pull request. Previously, this error was ignored, but the update caused the error message to be logged. The logstream closed before the leader could complete the write operation, leading to the error.\n\n**Fix:**\nThe log level has been reduced to warn/debug to reduce the noise in the logs. Additionally, the `logstream#tryWrite` function has been updated to return specific error codes for better error handling. Now, when a leader transition occurs and the logstream is closed, the system recognizes this situation and returns a `PARTITION_LEADER_MISMATCH` code to the gateway, allowing it to retry the command with the new leader.\n\n**Result:**\nWith this fix, the unnecessary error message is no longer logged during leader transitions when the logstream is closed. The log level has been reduced to warn/debug, reducing noise in the logs. The system now handles the situation of a leader transition and closed logstream correctly, returning the appropriate error code to the gateway for retrying the command with the new leader. This improves the overall behavior and reliability of the system during leader transitions."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12754",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nRelated to #12374 \r\n\r\nA customer observed another case where the startup failed with the error:\r\n```\r\n\"Expected to find a snapshot at index >= log's first index X, but found snapshot Y. A previous snapshot is most likely corrupted.\"\r\n```\r\nI could not verify it, but our assumption currently is that this is a false positive. The following might have happened:\r\n1. Follower received snapshot\r\n2. Before committing the snapshot, it reset segments\r\n3. While deleting the segments, the node was shutdown.\r\n4. After restart, it has the old snapshot plus partially deleted segments.\r\n\r\nI think this is plausible as the reset/deleting segments is not and atomic operation. We might not be handling it correctly.\r\n\r\n**Expected behavior**\r\n\r\nThe reset or snapshot commit process should not result in an invalid intermediate state which is detected as corruption.\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.1.*\r\n\r\nRelated to [support](https://jira.camunda.com/browse/SUPPORT-16905) \r\n\n\n deepthidevaki: A potential solution might be to delete the segments in the reverse order, combined with the PR that fixed #12374.\r\n\r\nIn a related issue, I discussed with @npepinpe a different approach. I was planning to create a separate issue for it, but I'm just dumping my thoughts here, as I think it is also relevant for this bug.\r\n\r\n------\r\nTerminologies\r\n\r\nIn-Sync replicas - The followers that are in sync with leaders and participating in the commit. In-sync replicas forms the quorum.\r\n\r\nOut-of-sync follower - The follower which is receiving the event potentially after it is already committed.\r\n\r\nIn a setup with replicationFactor 3, there will be 2 in-sync replicas, out of which one will be the leader. The third one can be an out-of-sync follower.\r\n\r\nRaft guarantees that any committed event is available in all in-sync replicas. The events are replicated to the out-of-sync follower eventually. But there is no guarantee when they are replicated. Ideally, they are replicated immediately. But it is possible the follower is slow, or the network is slow, resulting the out-of-sync follower to lag behind the leader by a large amount. \r\n\r\nAt some point, the leader sends a snapshot instead of the events, because the follower is lagging behind. At this point, the current behavior is as follows:\r\n\r\nFollower resets the log (delete all segments)\r\nReceives and commits the new snapshot\r\nDelete the old snapshot\r\nContinue receiving the events after the snapshot index\r\n\r\nThe above process can lead to the following scenarios:\r\n\r\nStep 1 and 2 are not atomic. If the replica crashes in between, then it restarts with an empty state.\r\nIf it restarts before step 4, then it starts with a snapshot  that is not use-able. Snapshot is valid only if the event corresponding to lastFollowUpEventPosition is in the log.\r\n\r\nIn a normal scenario, the above situation is not problematic because the in-sync replicas are healthy and it is guaranteed that only one of the in-sync replica becomes the leader.\r\n\r\nHowever, in a disaster scenario where all in-sync replicas are gone, it might be acceptable to continue functioning with whatever data is available in the out-of-sync replica. It might not have up-to-date state, but for some use-cases it is ok to lose last X amount of data. In such cases, we want to enable users to recover the cluster from the state of the out-of-sync replica. However, in the above scenario the out-of-sync replica might be in a state where it's state is empty or the snapshot is not useable. \r\n\r\nSo, it would be good if we could ensure that the state in the out-of-sync follower is always in a valid state. To acheive  that we can\r\n\r\nDo not delete old snapshot and reset log immediately when it receives a snapshot.\r\nInstead keep the old snapshot and the logs until the new snapshot is committed and its follow up event is received.\r\n\r\nSolution 1:\r\n\r\nWe keep the old snapshot + logs in a different folder, the new snapshot and new logs in a different folder. On restarts, it attempts to use the new snapshot + log and fall back to the old state if necessary. \r\n\r\nSolution 2:\r\n\r\nDo not reset the logs, when a new snapshot is received. Instead, add a marker record in the journal to indicate that there is a snapshot at the position. The readers must know how to handle these “gaps”. For example, if stream processor reader hits this marker record, it has to throw away its state and replace with the snapshot. If the raft leader reads this record, it should replicate the snapshot instead of the event etc.\r\n\r\nThe compaction logic should take this into account, and compact only if there is one valid snapshot with its followup event in the log. So the follower can rollback to an old valid state if necessary.\r\n\r\nThis would also help in some issue that we observed recently where restarting the node during resetting the log can incorrectly lead to detecting it as corrupted log.\n npepinpe: With #12868, we implemented a quick fix to prevent the specific case we're aware of. By deleting the segments in reverse order during reset, we can ensure that there are no gaps in the log/snapshot, and thus no corruption. Data loss is acceptable here as it was the desired outcome of the reset operation.\r\n\r\nHowever, we agreed not to close the issue - this quick fix only solves a specific thing, and not the root cause, which is that the install operation on the follower is not atomic, specifically persisting the snapshot/clearing the log/updating the meta store/getting the first record for the snapshot/etc. are all independent operations and the node may stop at any point in between, leaving us in a weird state.\n megglos: ZDP-Planning:\r\n- while we are at it, it would be worth to assess potential solutions to get an understanding of how much effort it is to change\r\n- needs prototyping\r\n- back to backlog as upcoming for now to be picked up going forward\r\n- relates to upcoming resilience epic",
    "title": "Journal reset resulting in an intermediate state which is detected as corruption",
    "releaseNoteText": "**Consequence:** A bug was causing the system to enter an invalid intermediate state, which was detected as corruption. This resulted in a startup failure with the error message \"Expected to find a snapshot at index >= log's first index X, but found snapshot Y. A previous snapshot is most likely corrupted.\" The issue arose when a follower received a snapshot, but before committing it, the segments were reset. If the node was shutdown during the segment deletion process and then restarted, it would have the old snapshot and partially deleted segments, leading to the observed corruption.\n\n**Cause:** The reset or snapshot commit process was not handling the deletion of segments correctly. The reset and segment deletion were not atomic operations, leading to the possibility of an invalid intermediate state.\n\n**Fix:** The segments were modified to be deleted in reverse order during the reset process. This ensured that there were no gaps in the logs or snapshots, effectively preventing any corruption. However, this fix only addressed the specific case and did not solve the underlying atomicity issue in the installation operation on the follower.\n\n**Result:** With the fix in place, the reset or snapshot commit process no longer results in an invalid intermediate state being detected as corruption. The system now correctly handles the deletion of segments, ensuring that the logs and snapshots are always in a valid state. This prevents any startup failures due to corruption and improves the overall stability and reliability of the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12623",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nAfter restoring from backup, if the leader that took the original backup is no more the leader of the partition, this can result in a duplicate backup of the partition with the same backup id. The new backup is logically equivalent to the old backup. So it doesn't matter which backup we use later. However, re-taking the backup is unnecessary as it wastes resources.\r\n\r\n**To Reproduce**\r\nRestore zeebe from a backup, and observe the logs or inspect the backup store.\r\n\r\n**Expected behavior**\r\nA partition should not re-take a backup after restore.\r\n\n",
    "title": "After restoring from backup, a partition re-takes the backup",
    "releaseNoteText": "**Consequence:** After restoring from a backup, a partition would unnecessarily re-take the backup, resulting in a duplicate backup of the partition with the same backup ID. This would waste resources without any impact on the functionality as the new backup would be logically equivalent to the old backup.\n\n**Cause:** The issue occurred when the leader who originally took the backup was no longer the leader of the partition after the restore. This caused the partition to go through the backup process again, resulting in a duplicate backup.\n\n**Fix:** The fix involved modifying the partition logic to prevent it from re-taking a backup after a restore. This change ensured that the partition does not unnecessarily repeat the backup process.\n\n**Result:** After applying the fix, when restoring from a backup, the partition no longer re-takes the backup unnecessarily. This improves resource utilization and does not have any impact on the functionality as the new backup is logically equivalent to the old one."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12622",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nSometime, a partition may attempt to take a backup again when a backup for the same id already exists. This case is rare, but can happen sometimes if there is a leader change while taking a backup. If this happens, list backup fails with an error message:\r\n```\r\n{\r\n    \"message\": \"Duplicate key 1 (attempted merging values BackupStatus[backupId=1, partitionId=1, status=COMPLETED, failureReason=, brokerVersion=8.2.2, createdAt=...])\"\r\n}\r\n```\r\n\r\n**Expected behavior**\r\nList backup should be able to handle duplicate backup ids for a partition.\r\n\n",
    "title": "List backup fails when a partition has same backup taken by multiple nodes",
    "releaseNoteText": "**Consequence**\nWhen a partition has the same backup taken by multiple nodes, the list backup fails with an error message indicating a duplicate key.\n\n**Cause**\nThis issue occurs when a partition attempts to take a backup again while a backup for the same ID already exists. It is a rare case that can occur if there is a leader change during the backup process.\n\n**Fix**\nThe fix for this issue includes handling duplicate backup IDs for a partition in the list backup functionality.\n\n**Result**\nAfter applying the fix, the list backup will be able to handle partitions with duplicate backup IDs. Users will no longer encounter the error message indicating a duplicate key when trying to list backups."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12597",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nTrying to list all available backups will always fail without a useful error message.\r\nThe gateway distributes a list request to all brokers which then list all of their backups and try to respond with a `BackupListResponse`:\r\n\r\nhttps://github.com/camunda/zeebe/blob/4854b6606a803926ed9cadabfc2edb4aede18cb4/protocol/src/main/resources/cluster-management-protocol.xml#L64-L76\r\n\r\nThe `groupSizeEncoding` is defined by us:\r\n\r\nhttps://github.com/camunda/zeebe/blob/c861aac736376e1cc20aa558979c6d9c289b4a1f/protocol/src/main/resources/common-types.xml#L16-L19\r\n\r\nIt uses a `unit8` to represent the number of entries. When trying to write a `BackupListResponse` with more than 255 entries, the encoder rejects it:\r\n```\r\njava.lang.IllegalArgumentException: count outside allowed range: count=774\r\n\tat io.camunda.zeebe.protocol.management.BackupListResponseEncoder$BackupsEncoder.wrap(BackupListResponseEncoder.java:137) ~[zeebe-protocol-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.protocol.management.BackupListResponseEncoder.backupsCount(BackupListResponseEncoder.java:114) ~[zeebe-protocol-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.protocol.impl.encoding.BackupListResponse.write(BackupListResponse.java:100) ~[zeebe-protocol-impl-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.broker.transport.backupapi.BackupApiResponseWriter.write(BackupApiResponseWriter.java:71) ~[zeebe-broker-8.2.2.jar:8.2.2]\r\n....\r\n\r\n```\r\n**To Reproduce**\r\n\r\nTake 256 backups, then query all backups via `GET actuator/backups`.\r\n\r\n**Expected behavior**\r\n\r\n1. Zeebe supports much more backups than 255 (for example by using a `uint16`, thus supporting 65535 backups)\r\n2. The number of listed backups should be limited to a reasonable number. Querying the backup store to list, say 1000, available backups is likely to result in timeouts and makes the backup API unusable.\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.lang.IllegalArgumentException: count outside allowed range: count=774\r\n\tat io.camunda.zeebe.protocol.management.BackupListResponseEncoder$BackupsEncoder.wrap(BackupListResponseEncoder.java:137) ~[zeebe-protocol-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.protocol.management.BackupListResponseEncoder.backupsCount(BackupListResponseEncoder.java:114) ~[zeebe-protocol-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.protocol.impl.encoding.BackupListResponse.write(BackupListResponse.java:100) ~[zeebe-protocol-impl-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.broker.transport.backupapi.BackupApiResponseWriter.write(BackupApiResponseWriter.java:71) ~[zeebe-broker-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.transport.impl.ServerResponseImpl.write(ServerResponseImpl.java:50) ~[zeebe-transport-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.transport.impl.AtomixServerTransport.sendResponse(AtomixServerTransport.java:154) ~[zeebe-transport-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.broker.transport.backupapi.BackupApiResponseWriter.tryWriteResponse(BackupApiResponseWriter.java:51) ~[zeebe-broker-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.broker.transport.AsyncApiRequestHandler.lambda$handleRequest$1(AsyncApiRequestHandler.java:123) ~[zeebe-broker-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.future.FutureContinuationRunnable.run(FutureContinuationRunnable.java:28) ~[zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94) ~[zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) [zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) [zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- Zeebe Version: >= 8.1\r\n\n",
    "title": "Listing backups fails if more than 255 backups are available",
    "releaseNoteText": "**Consequence:**\nAttempting to list all available backups fails with an error message stating that the count is outside the allowed range, even though more than 255 backups are present. This issue prevents the proper functioning of the backup API and makes it unusable for querying a large number of backups.\n\n**Cause:**\nThe issue is caused by the definition of the `groupSizeEncoding` in the `common-types.xml` file, which uses a `unit8` to represent the number of entries. This encoding mechanism limits the number of backups that can be listed to a maximum of 255.\n\n**Fix:**\nTo address this issue, the `groupSizeEncoding` has been modified to use a `uint16` encoding instead. This change will support up to 65535 backups.\n\n**Result:**\nAfter applying the fix, Zeebe will be able to support a much larger number of backups, up to a maximum of 65535. The backup API will now function properly, and the number of listed backups will be limited to a reasonable number, avoiding potential timeouts and making the backup API usable for querying backups."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12591",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nAn attempt to upload oversized BPMN (other the segment limit) causes unrecoverable failure of Zeebe:\r\n* further BPMN upload of proper sizes are not possible\r\n* eventually partitions becomes unhealthy and not recovered\r\n\r\nIt is degradation from version 8.1.6 that just rejected incorrect BPMN without further problems.\r\n\r\n**To Reproduce**\r\n\r\nUpload the BMPN bigger than configured \r\n      maxMessageSize: 64KB\r\n\r\n\r\n**Expected behavior**\r\n\r\nBMPN Upload is rejected\r\n\r\n**Log/Stacktrace**\r\n```\r\n2023-04-27 17:03:37.989 [Broker-0-StreamProcessor-1] [Broker-0-zb-actors-1] ERROR\r\n      io.camunda.zeebe.broker.process - Unexpected error while processing resource 'f6c0b39d-8357-40ea-8d79-7e2611a89677.bpmn'\r\nio.camunda.zeebe.stream.api.records.ExceededBatchRecordSizeException: Can't append entry: 'RecordBatchEntry[recordMetadata=RecordMetadata{recordType=EVENT, valueType=PROCESS, intent=CREATED}, key=2251799852104669, sourceIndex=-1, unifiedRecordValue={\"bpmnProcessId\":\"id_f6c0b39d-8357-40ea-8d79-7e2611a89677\",\"version\":1,\"processDefinitionKey\":2251799852104669,\"resourceName\":\"f6c0b39d-8357-40ea-8d79-7e2611a89677.bpmn\",\"checksum\":\"uG1QH8XcklrgFGYZI3HhPg==\",\"resource\":\"PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0idXRmLTgiPz4NCjxicG1uOmRlZmluaXRpb25zIHhtbG5zOmJwbW5kaT0iaHR0cDovL3d3dy5vbWcub3JnL3NwZWMvQlBNTi8yMDEwMDUyNC9ESSIgeG1sbnM6ZGM9Imh0dHA6Ly93d3cub21nLm9yZy9zcGVjL0RELzIwMTAwNTI0L0RDIiB4bWxuczp6ZWViZT0iaHR0cDovL2NhbXVuZGEub3JnL3NjaGVtYS96ZWViZS8xLjAiIHhtbG5zOmRpPSJodHRwOi8vd3d3Lm9tZy5vcmcvc3BlYy9ERC8yMDEwMDUyNC9ESSIgeG1sbnM6eHNpPSJodHRwOi8vd3d3LnczLm9yZy8yMDAxL1hNTFNjaGVtYS1pbnN0YW5jZSIgaWQ9ImlkXzZmNTk2NTlmLWE3OGMtNDMyMy04M2VmLTdlODMwYmJlNTUwNSIgdGFyZ2V0TmFtZXNwYWNlPSJodHRwOi8vYnBtbi5pby9zY2hlbWEvYnBtbiIgZXhwb3J0ZXI9IkNvbmZpcm1pdCBCUE1OIEJ1aWxkZXIiIGV4cG9ydGVyVmVyc2lvbj0iMS4wLjAuMCIgeG1sbnM6YnBtbj0iaHR0cD...' with size: 1010867 this would exceed the maximum batch size. [ currentBatchEntryCount: 0, currentBatchSize: 0]\r\n\tat io.camunda.zeebe.stream.impl.records.RecordBatch.appendRecord(RecordBatch.java:67) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.BufferedProcessingResultBuilder.appendRecordReturnEither(BufferedProcessingResultBuilder.java:62) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.api.ProcessingResultBuilder.appendRecord(ProcessingResultBuilder.java:38) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.streamprocessor.writers.ResultBuilderBackedEventApplyingStateWriter.appendFollowUpEvent(ResultBuilderBackedEventApplyingStateWriter.java:40) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.transformProcessResource(BpmnResourceTransformer.java:162) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.lambda$transformResource$0(BpmnResourceTransformer.java:77) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.util.Either$Right.map(Either.java:355) ~[zeebe-util-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.lambda$transformResource$1(BpmnResourceTransformer.java:75) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.util.Either$Right.flatMap(Either.java:366) ~[zeebe-util-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.transformResource(BpmnResourceTransformer.java:65) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.DeploymentTransformer.transformResource(DeploymentTransformer.java:122) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.DeploymentTransformer.transform(DeploymentTransformer.java:98) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.DeploymentCreateProcessor.processRecord(DeploymentCreateProcessor.java:87) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.Engine.process(Engine.java:142) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.batchProcessing(ProcessingStateMachine.java:346) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$2(ProcessingStateMachine.java:268) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run(ZeebeTransaction.java:84) ~[zeebe-db-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand(ProcessingStateMachine.java:268) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord(ProcessingStateMachine.java:227) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.readNextRecord(ProcessingStateMachine.java:203) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\r\n\r\n```\r\n\r\n**Environment:**\r\n- OS: Windows\r\n- Zeebe Version: 8.2.3\r\n\r\nIn version 8.1.6 the behavior is correct. Here's stack traces from this version:\r\n\r\n```\r\n2023-04-27 17:52:05.506 [io.camunda.zeebe.broker.transport.commandapi.CommandApiRequestHandler] [Broker-0-zb-actors-1] ERROR\r\n      io.camunda.zeebe.broker.transport - Unexpected error on writing CREATE command\r\njava.lang.IllegalArgumentException: Expected to claim segment of size 1010866, but can't claim more than 65536 bytes.\r\n\tat io.camunda.zeebe.dispatcher.Dispatcher.offer(Dispatcher.java:207) ~[zeebe-dispatcher-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.dispatcher.Dispatcher.claimSingleFragment(Dispatcher.java:143) ~[zeebe-dispatcher-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.logstreams.impl.log.LogStreamWriterImpl.claimLogEntry(LogStreamWriterImpl.java:165) ~[zeebe-logstreams-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.logstreams.impl.log.LogStreamWriterImpl.tryWrite(LogStreamWriterImpl.java:124) ~[zeebe-logstreams-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.broker.transport.commandapi.CommandApiRequestHandler.writeCommand(CommandApiRequestHandler.java:141) ~[zeebe-broker-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.broker.transport.commandapi.CommandApiRequestHandler.handleExecuteCommandRequest(CommandApiRequestHandler.java:114) ~[zeebe-broker-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.broker.transport.commandapi.CommandApiRequestHandler.handle(CommandApiRequestHandler.java:58) ~[zeebe-broker-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.broker.transport.commandapi.CommandApiRequestHandler.handleAsync(CommandApiRequestHandler.java:49) ~[zeebe-broker-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.broker.transport.commandapi.CommandApiRequestHandler.handleAsync(CommandApiRequestHandler.java:27) ~[zeebe-broker-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.broker.transport.AsyncApiRequestHandler.handleRequest(AsyncApiRequestHandler.java:110) ~[zeebe-broker-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.broker.transport.AsyncApiRequestHandler.lambda$onRequest$0(AsyncApiRequestHandler.java:75) ~[zeebe-broker-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n2023-04-27 17:52:05.508 [io.camunda.zeebe.gateway.impl.broker.BrokerRequestManager] [Broker-0-zb-actors-0] ERROR\r\n      io.camunda.zeebe.gateway - Expected to handle gRPC request, but received an internal error from broker: BrokerError{code=INTERNAL_ERROR, message='Failed writing response: java.lang.IllegalArgumentException: Expected to claim segment of size 1010866, but can't claim more than 65536 bytes.'}\r\nio.camunda.zeebe.gateway.cmd.BrokerErrorException: Received error from broker (INTERNAL_ERROR): Failed writing response: java.lang.IllegalArgumentException: Expected to claim segment of size 1010866, but can't claim more than 65536 bytes.\r\n\tat io.camunda.zeebe.gateway.impl.broker.BrokerRequestManager.handleResponse(BrokerRequestManager.java:194) ~[zeebe-gateway-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.gateway.impl.broker.BrokerRequestManager.lambda$sendRequestInternal$2(BrokerRequestManager.java:143) ~[zeebe-gateway-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.future.FutureContinuationRunnable.run(FutureContinuationRunnable.java:28) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\r\n```\r\n\r\n\r\n\r\n\r\n\n\n Zelldon: Hey @sergeylebed thanks for reporting this!\r\n\r\nLooks like a regression @megglos \n Zelldon: @sergeylebed can you confirm that you didn't get an error response in the client? :thinking: \r\n\r\nIt looks like, based on the stacktrace that it just failed on a different place before. \r\n\r\n```\r\nat io.camunda.zeebe.broker.transport.commandapi.CommandApiRequestHandler.writeCommand(CommandApiRequestHandler.java:141) ~[zeebe-broker-8.1.6.jar:8.1.6]\r\n```\r\n\r\nIs at the CommandAPI, when receiving the Command and writing to the dispatcher (before replicating and processing). We replaced the dispatcher in 8.2. Meaning it is now possible to write larger entries, but as you see processing is still not possible. But I think at least I would expect you get an error response. Did you?\n sergeylebed: I got an error on the client but \r\na) it is a generic error about timeout\r\nb) the system becomes inoperable \r\n\r\n```\r\nGrpc: 'DeadlineExceeded' 'Status(StatusCode=\\\"DeadlineExceeded\\\", Detail=\\\"Time out between gateway and broker: Request timed out after PT15S\\\", DebugException=\\\"Grpc.Core.Internal.CoreErrorDetailException: {\\\"created\\\":\\\"@1682543232.485000000\\\",\\\"description\\\":\\\"Error received from peer ipv6:[::1]:26500\\\",\\\"file\\\":\\\"..\\\\..\\\\..\\\\src\\\\core\\\\lib\\\\surface\\\\call.cc\\\",\\\"file_line\\\":953,\\\"grpc_message\\\":\\\"Time out between gateway and broker: Request timed out after PT15S\\\",\\\"grpc_status\\\":4}\\\")'\",\"Exception\":\"Grpc.Core.RpcException: Status(StatusCode=\\\"DeadlineExceeded\\\", Detail=\\\"Time out between gateway and broker: Request timed out after PT15S\\\", DebugException=\\\"Grpc.Core.Internal.CoreErrorDetailException: {\\\"created\\\":\\\"@1682543232.485000000\\\",\\\"description\\\":\\\"Error received from peer ipv6:[::1]:26500\\\",\\\"file\\\":\\\"..\\\\..\\\\..\\\\src\\\\core\\\\lib\\\\surface\\\\call.cc\\\",\\\"file_line\\\":953,\\\"grpc_message\\\":\\\"Time out between gateway and broker: Request timed out after PT15S\\\",\\\"grpc_status\\\":4}\\\")\\r\\n   at Zeebe.Client.Impl.Commands.DeployProcessCommand.Send(Nullable`1 timeout, CancellationToken token)\\r\\n   at  \r\n```\n sergeylebed: The very first error in the log:\r\n\r\n```\r\n2023-04-26 12:49:17.323 [Broker-0-StreamProcessor-1] [Broker-0-zb-actors-1] ERROR\r\n      io.camunda.zeebe.broker.process - Unexpected error while processing resource 'f6c0b39d-8357-40ea-8d79-7e2611a89677.bpmn'\r\nio.camunda.zeebe.stream.api.records.ExceededBatchRecordSizeException: Can't append entry: 'RecordBatchEntry[recordMetadata=RecordMetadata{recordType=EVENT, valueType=PROCESS, intent=CREATED}, key=2251799852104669, sourceIndex=-1, unifiedRecordValue={\"bpmnProcessId\":\"id_f6c0b39d-8357-40ea-8d79-7e2611a89677\",\"version\":1,\"processDefinitionKey\":2251799852104669,\"resourceName\":\"f6c0b39d-8357-40ea-8d79-7e2611a89677.bpmn\",\"checksum\":\"uG1QH8XcklrgFGYZI3HhPg==\",\"resource\":\"PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0idXRmLTgiPz4NCjxicG1uOmRlZmluaXRpb25zIHhtbG5zOmJwbW5kaT0iaHR0cDovL3d3dy5vbWcub3JnL3NwZWMvQlBNTi8yMDEwMDUyNC9ESSIgeG1sbnM6ZGM9Imh0dHA6Ly93d3cub21nLm9yZy9zcGVjL0RELzIwMTAwNTI0L0RDIiB4bWxuczp6ZWViZT0iaHR0cDovL2NhbXVuZGEub3JnL3NjaGVtYS96ZWViZS8xLjAiIHhtbG5zOmRpPSJodHRwOi8vd3d3Lm9tZy5vcmcvc3BlYy9ERC8yMDEwMDUyNC9ESSIgeG1sbnM6eHNpPSJodHRwOi8vd3d3LnczLm9yZy8yMDAxL1hNTFNjaGVtYS1pbnN0YW5jZSIgaWQ9ImlkXzZmNTk2NTlmLWE3OGMtNDMyMy04M2VmLTdlODMwYmJlNTUwNSIgdGFyZ2V0TmFtZXNwYWNlPSJodHRwOi8vYnBtbi5pby9zY2hlbWEvYnBtbiIgZXhwb3J0ZXI9IkNvbmZpcm1pdCBCUE1OIEJ1aWxkZXIiIGV4cG9ydGVyVmVyc2lvbj0iMS4wLjAuMCIgeG1sbnM6YnBtbj0iaHR0cD...' with size: 1010867 this would exceed the maximum batch size. [ currentBatchEntryCount: 0, currentBatchSize: 0]\r\n\tat io.camunda.zeebe.stream.impl.records.RecordBatch.appendRecord(RecordBatch.java:67) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.BufferedProcessingResultBuilder.appendRecordReturnEither(BufferedProcessingResultBuilder.java:62) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.api.ProcessingResultBuilder.appendRecord(ProcessingResultBuilder.java:38) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.streamprocessor.writers.ResultBuilderBackedEventApplyingStateWriter.appendFollowUpEvent(ResultBuilderBackedEventApplyingStateWriter.java:40) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.transformProcessResource(BpmnResourceTransformer.java:162) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.lambda$transformResource$0(BpmnResourceTransformer.java:77) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.util.Either$Right.map(Either.java:355) ~[zeebe-util-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.lambda$transformResource$1(BpmnResourceTransformer.java:75) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.util.Either$Right.flatMap(Either.java:366) ~[zeebe-util-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.transformResource(BpmnResourceTransformer.java:65) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.DeploymentTransformer.transformResource(DeploymentTransformer.java:122) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.DeploymentTransformer.transform(DeploymentTransformer.java:98) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.DeploymentCreateProcessor.processRecord(DeploymentCreateProcessor.java:87) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.Engine.process(Engine.java:142) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.batchProcessing(ProcessingStateMachine.java:346) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$2(ProcessingStateMachine.java:268) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run(ZeebeTransaction.java:84) ~[zeebe-db-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand(ProcessingStateMachine.java:268) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord(ProcessingStateMachine.java:227) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.readNextRecord(ProcessingStateMachine.java:203) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n```\n sergeylebed: In version 8.1.6 the client error is different\r\n\r\n```\r\nStatus(StatusCode=\\\"Internal\\\", Detail=\\\"Unexpected error occurred between gateway and broker (code: INTERNAL_ERROR)\\\", DebugException=\\\"Grpc.Core.Internal.CoreErrorDetailException: {\\\"created\\\":\\\"@1682632476.120000000\\\",\\\"description\\\":\\\"Error received from peer ipv6:[::1]:26500\\\",\\\"file\\\":\\\"..\\\\..\\\\..\\\\src\\\\core\\\\lib\\\\surface\\\\call.cc\\\",\\\"file_line\\\":953,\\\"grpc_message\\\":\\\"Unexpected error occurred between gateway and broker (code: INTERNAL_ERROR)\\\",\\\"grpc_status\\\":13}\\\")\",\"Exception\":\"Grpc.Core.RpcException: Status(StatusCode=\\\"Internal\\\", Detail=\\\"Unexpected error occurred between gateway and broker (code: INTERNAL_ERROR)\\\", DebugException=\\\"Grpc.Core.Internal.CoreErrorDetailException: {\\\"created\\\":\\\"@1682632476.120000000\\\",\\\"description\\\":\\\"Error received from peer ipv6:[::1]:26500\\\",\\\"file\\\":\\\"..\\\\..\\\\..\\\\src\\\\core\\\\lib\\\\surface\\\\call.cc\\\",\\\"file_line\\\":953,\\\"grpc_message\\\":\\\"Unexpected error occurred between gateway and broker (code: INTERNAL_ERROR)\\\",\\\"grpc_status\\\":13}\\\")\\r\\n   at Zeebe.Client.Impl.Commands.DeployProcessCommand.Send(Nullable`1 timeout, CancellationToken token)\r\n```\n npepinpe: Under the assumption that doing so bricks your partition unrecoverably, we'll prioritize it as a blocker/critical issue.\n deepthidevaki: I was able to reproduce this by setting maxMessageSize to 1MB in the test `CreateDeploymentTest::shouldRejectDeployIfResourceIsTooLarge()` https://github.com/camunda/zeebe/blob/main/qa/integration-tests/src/test/java/io/camunda/zeebe/it/client/command/CreateDeploymentTest.java#L28 \r\n\r\nI see multiple issues here:\r\n\r\n1. CommandAPI is not rejecting requests which exceeds maxMessageSize. This result in oversized command to be written to the log stream.\r\n2. When engine cannot write the follow up event because it is above batch size limit, it attempts to write a rejection record which contains the whole command. Since the command is already above batch size it cannot write rejection to the log stream. This result in a loop in the processing machine where it tries to handle this error endlessly.\r\n\r\nFor fixing this issue, I propose to reject the request in CommandAPI. So it is never written to the logstream. However, I would also suggest to revisit if we really have to write the whole command in the rejection record.\n sergeylebed: It does not seem to be fixed in 8.2.5. Do you think it can be merged into the latest versions?\n deepthidevaki: @sergeylebed The fix will be included in 8.2.6.",
    "title": "8.2.3 Degradation: Creating an oversized BPMN causes unrecoverable failure ",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12374",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nA Zeebe broker is crash looping. The broker tries to start up but failed with the following error message. \r\n\r\n```\r\nio.camunda.zeebe.journal.CorruptedJournalException: Expected to read the version byte from segment 'raft-partition-partition-1-1.log' but got EOF instead\r\n```\r\n\r\nThe broker was part of a cluster with three brokers. The other brokers were healthy and continued processing. \r\n\r\nTo mitigate the issue, we did a fresh restart of the broker. We removed all data and restarted the broker. After the restart, the broker was healthy again and joined the cluster.\r\n\r\n**To Reproduce**\r\n\r\nUnknown. \r\n\r\nThe broker was forced to shut down (`Shutdown was called with context: ...`). The broker created a new snapshot 30 seconds before. The log contains no warnings or suspicious behavior before or after the shutdown.\r\n\r\n_EDIT:_\r\n\r\nIt seems `zeebe-2` was restarted while receiving a snapshot and resetting the log:\r\n```\r\nINFO 2023-04-06T15:37:07.317296306Z [jsonPayload.context.partitionId: 1] [resource.labels.containerName: zeebe] RaftServer{raft-partition-partition-1}{role=FOLLOWER} - Started receiving new snapshot FileBasedReceivedSnapshot{directory=/usr/local/zeebe/data/raft-partition/partitions/1/pending/14080296-246-15599729-15599732-1, snapshotStore=Broker-2-SnapshotStore-1, metadata=FileBasedSnapshotId{index=14080296, term=246, processedPosition=15599729, exporterPosition=15599732}} from 1\r\nINFO 2023-04-06T15:37:07.490944632Z [jsonPayload.context.partitionId: 1] [resource.labels.containerName: zeebe] RaftServer{raft-partition-partition-1}{role=FOLLOWER} - Delete existing log (lastIndex '14080107') and replace with received snapshot (index '14080296'). First entry in the log will be at index 14080297\r\nERROR 2023-04-06T15:39:35.994311846Z [resource.labels.containerName: zeebe] + export SPRING_CONFIG_LOCATION=classpath:/,file:./config/zeebe.cfg.yaml\r\n```\r\nThese are the last logs from that broker, and immediately it was restarted.\r\n\r\nMost likely, the log segment on the disk is an intermediate state where the previous segments have been deleted, the new one is only partially created - which is detected as corruption after the restart. \r\n\r\n**Expected behavior**\r\n\r\nThe broker can handle this corruption failure. For example, by removing (or archiving) the corrupted data and fetching the latest data from the cluster.\r\n\r\n**Log/Stacktrace**\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.util.concurrent.ExecutionException: Startup failed in the following steps: [Partition Manager]. See suppressed exceptions for details.\r\n\tat io.camunda.zeebe.scheduler.future.CompletableActorFuture.get(CompletableActorFuture.java:142) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.future.CompletableActorFuture.get(CompletableActorFuture.java:109) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.FutureUtil.join(FutureUtil.java:21) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.future.CompletableActorFuture.join(CompletableActorFuture.java:198) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.broker.Broker.internalStart(Broker.java:101) ~[zeebe-broker-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.util.LogUtil.doWithMDC(LogUtil.java:23) ~[zeebe-util-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.broker.Broker.start(Broker.java:83) ~[zeebe-broker-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.broker.StandaloneBroker.run(StandaloneBroker.java:92) ~[camunda-zeebe-8.1.3.jar:8.1.3]\r\n\tat org.springframework.boot.SpringApplication.callRunner(SpringApplication.java:771) ~[spring-boot-2.7.4.jar:2.7.4]\r\n\tat org.springframework.boot.SpringApplication.callRunners(SpringApplication.java:755) ~[spring-boot-2.7.4.jar:2.7.4]\r\n\tat org.springframework.boot.SpringApplication.run(SpringApplication.java:315) ~[spring-boot-2.7.4.jar:2.7.4]\r\n\tat io.camunda.zeebe.broker.StandaloneBroker.main(StandaloneBroker.java:82) ~[camunda-zeebe-8.1.3.jar:8.1.3]\r\nCaused by: io.camunda.zeebe.scheduler.startup.StartupProcessException: Startup failed in the following steps: [Partition Manager]. See suppressed exceptions for details.\r\n\tat io.camunda.zeebe.scheduler.startup.StartupProcess.aggregateExceptionsSynchronized(StartupProcess.java:282) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.startup.StartupProcess.completeStartupFutureExceptionallySynchronized(StartupProcess.java:183) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.startup.StartupProcess.lambda$proceedWithStartupSynchronized$3(StartupProcess.java:167) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.future.FutureContinuationRunnable.run(FutureContinuationRunnable.java:33) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tSuppressed: io.camunda.zeebe.scheduler.startup.StartupProcessStepException: Bootstrap step Partition Manager failed\r\n\t\tat io.camunda.zeebe.scheduler.startup.StartupProcess.completeStartupFutureExceptionallySynchronized(StartupProcess.java:185) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.scheduler.startup.StartupProcess.lambda$proceedWithStartupSynchronized$3(StartupProcess.java:167) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.scheduler.future.FutureContinuationRunnable.run(FutureContinuationRunnable.java:33) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tCaused by: java.util.concurrent.CompletionException: io.camunda.zeebe.journal.CorruptedJournalException: Expected to read the version byte from segment 'raft-partition-partition-1-1.log' but got EOF instead.\r\n\t\tat java.util.concurrent.CompletableFuture.encodeThrowable(Unknown Source) ~[?:?]\r\n\t\tat java.util.concurrent.CompletableFuture.uniApplyNow(Unknown Source) ~[?:?]\r\n\t\tat java.util.concurrent.CompletableFuture.uniApplyStage(Unknown Source) ~[?:?]\r\n\t\tat java.util.concurrent.CompletableFuture.thenApply(Unknown Source) ~[?:?]\r\n\t\tat io.atomix.raft.partition.RaftPartition.open(RaftPartition.java:104) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.partition.RaftPartitionGroup.lambda$join$7(RaftPartitionGroup.java:201) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat java.util.stream.ReferencePipeline$3$1.accept(Unknown Source) ~[?:?]\r\n\t\tat java.util.HashMap$KeySpliterator.forEachRemaining(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.AbstractPipeline.copyInto(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.AbstractPipeline.evaluate(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.AbstractPipeline.evaluateToArrayNode(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.ReferencePipeline.toArray(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.ReferencePipeline.toArray(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.ReferencePipeline.toList(Unknown Source) ~[?:?]\r\n\t\tat io.atomix.raft.partition.RaftPartitionGroup.join(RaftPartitionGroup.java:203) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.primitive.partition.impl.DefaultPartitionService.start(DefaultPartitionService.java:63) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.broker.partitioning.PartitionManagerImpl.start(PartitionManagerImpl.java:125) ~[zeebe-broker-8.1.3.jar:8.1.3]\r\n\t\tat java.util.concurrent.CompletableFuture$AsyncSupply.run(Unknown Source) ~[?:?]\r\n\t\tat java.lang.Thread.run(Unknown Source) ~[?:?]\r\n\tCaused by: io.camunda.zeebe.journal.CorruptedJournalException: Expected to read the version byte from segment 'raft-partition-partition-1-1.log' but got EOF instead.\r\n\t\tat io.camunda.zeebe.journal.file.SegmentLoader.readVersion(SegmentLoader.java:173) ~[zeebe-journal-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.journal.file.SegmentLoader.readDescriptor(SegmentLoader.java:128) ~[zeebe-journal-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.journal.file.SegmentLoader.loadExistingSegment(SegmentLoader.java:87) ~[zeebe-journal-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.journal.file.SegmentsManager.loadSegments(SegmentsManager.java:292) ~[zeebe-journal-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.journal.file.SegmentsManager.open(SegmentsManager.java:238) ~[zeebe-journal-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.journal.file.SegmentedJournal.<init>(SegmentedJournal.java:61) ~[zeebe-journal-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.journal.file.SegmentedJournalBuilder.build(SegmentedJournalBuilder.java:158) ~[zeebe-journal-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.storage.log.RaftLogBuilder.build(RaftLogBuilder.java:136) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.storage.RaftStorage.openLog(RaftStorage.java:194) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.impl.RaftContext.<init>(RaftContext.java:194) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.impl.DefaultRaftServer$Builder.build(DefaultRaftServer.java:258) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.impl.DefaultRaftServer$Builder.build(DefaultRaftServer.java:232) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.partition.impl.RaftPartitionServer.buildServer(RaftPartitionServer.java:189) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.partition.impl.RaftPartitionServer.initServer(RaftPartitionServer.java:155) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.partition.impl.RaftPartitionServer.start(RaftPartitionServer.java:114) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.partition.RaftPartition.open(RaftPartition.java:104) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.partition.RaftPartitionGroup.lambda$join$7(RaftPartitionGroup.java:201) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat java.util.stream.ReferencePipeline$3$1.accept(Unknown Source) ~[?:?]\r\n\t\tat java.util.HashMap$KeySpliterator.forEachRemaining(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.AbstractPipeline.copyInto(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.AbstractPipeline.evaluate(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.AbstractPipeline.evaluateToArrayNode(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.ReferencePipeline.toArray(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.ReferencePipeline.toArray(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.ReferencePipeline.toList(Unknown Source) ~[?:?]\r\n\t\tat io.atomix.raft.partition.RaftPartitionGroup.join(RaftPartitionGroup.java:203) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.primitive.partition.impl.DefaultPartitionService.start(DefaultPartitionService.java:63) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.broker.partitioning.PartitionManagerImpl.start(PartitionManagerImpl.java:125) ~[zeebe-broker-8.1.3.jar:8.1.3]\r\n\t\tat java.util.concurrent.CompletableFuture$AsyncSupply.run(Unknown Source) ~[?:?]\r\n\t\tat java.lang.Thread.run(Unknown Source) ~[?:?]\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\nSee more in the downloaded [log file](https://drive.google.com/file/d/1V89vBHeXytmm_lviRhMjojBhdeoz7mJs/view?usp=share_link).\r\n\r\n**Environment:**\r\n- OS: Camunda SaaS\r\n- Zeebe Version: `8.1.3`\r\n- Configuration: `prod-worker-3`\r\n\n\n SeanAda: I have the same exeption on a self hosted Zeebe (Version 8.1.4).\r\nThe file `raft-partition-partition-1-1.log` is empty.\r\n\r\nIs there any chance to recover the data or copy it from other nodes?\n saig0: > Is there any chance to recover the data or copy it from other nodes?\r\n\r\n@SeanAda it depends on your setup. With a replication factor of 3, there is a good chance that the other two nodes takes over and continue processing. \r\n\r\nIn this case, you can mitigate the issue by doing a fresh restart of the broker. So, remove all data and restart the broker. After the restart, the broker should become healthy again and join the cluster. The broker should receive the missing data from the other nodes of the cluster.\n deepthidevaki: I had a quick look at the logs of the affected cluster. It seems `zeebe-2` was restarted while receiving a snapshot and resetting the log:\r\n```\r\nINFO 2023-04-06T15:37:07.317296306Z [jsonPayload.context.partitionId: 1] [resource.labels.containerName: zeebe] RaftServer{raft-partition-partition-1}{role=FOLLOWER} - Started receiving new snapshot FileBasedReceivedSnapshot{directory=/usr/local/zeebe/data/raft-partition/partitions/1/pending/14080296-246-15599729-15599732-1, snapshotStore=Broker-2-SnapshotStore-1, metadata=FileBasedSnapshotId{index=14080296, term=246, processedPosition=15599729, exporterPosition=15599732}} from 1\r\nINFO 2023-04-06T15:37:07.490944632Z [jsonPayload.context.partitionId: 1] [resource.labels.containerName: zeebe] RaftServer{raft-partition-partition-1}{role=FOLLOWER} - Delete existing log (lastIndex '14080107') and replace with received snapshot (index '14080296'). First entry in the log will be at index 14080297\r\nERROR 2023-04-06T15:39:35.994311846Z [resource.labels.containerName: zeebe] + export SPRING_CONFIG_LOCATION=classpath:/,file:./config/zeebe.cfg.yaml\r\n```\r\nThese are the last logs from that broker, and immediately it was restarted.\r\n\r\nMost likely, the log segment on the disk is an intermediate state where the previous segments have been deleted, the new one is only partially created - which is detected as corruption after the restart. We should handle this case better as it is not an actual corruption.\r\n\n SeanAda: Removing all the data and restarting the broker did work.\r\n\r\nThank you!\n saig0: @deepthidevaki great finding. :rocket: I added your comment to the description. \n codingman1990: But our the other two node become unhealthy too.How should i do something to recover it?\n npepinpe: Hi @codingman1990, what is the state of your two nodes? Are all brokers reporting the same error for the same partition?\n codingman1990: camunda-zeebe-0 and camunda-zeebe-1 be unhealthy.And camunda-zeebe-2 can't restart.\r\nBut camunda-zeebe-1 zeebe.log like this:\r\n\r\n```\r\n2023-04-24 15:44:14.259 [] [raft-server-0-raft-partition-partition-3] WARN \r\n      io.atomix.raft.roles.FollowerRole - RaftServer{raft-partition-partition-3}{role=FOLLOWER} - Poll request to 2 failed: java.net.ConnectException: Expected to send a message with subject 'raft-partition-partition-3-poll' to member '2', but member is not known. Known members are '[Member{id=1, address=camunda-zeebe-1.camunda-zeebe.default.svc:26502, properties={brokerInfo=EADJAAAAAwABAAAAAwAAAAMAAAADAAAAAAABCgAAAGNvbW1hbmRBcGkvAAAAY2FtdW5kYS16ZWViZS0xLmNhbXVuZGEtemVlYmUuZGVmYXVsdC5zdmM6MjY1MDEFAAMBAAAAAQIAAAABAwAAAAEMAAAFAAAAOC4xLjkFAAMBAAAAAAIAAAAAAwAAAAA=}}, Member{id=camunda-zeebe-gateway-7db56f85d6-v2jb7, address=172.20.8.149:26502, properties={event-service-topics-subscribed=KIIDAGpvYnNBdmFpbGFibOU=}}, Member{id=camunda-zeebe-gateway-7db56f85d6-z69qr, address=172.20.3.169:26502, properties={event-service-topics-subscribed=KIIDAGpvYnNBdmFpbGFibOU=}}, Member{id=0, address=camunda-zeebe-0.camunda-zeebe.default.svc:26502, properties={brokerInfo=EADJAAAAAwAAAAAAAwAAAAMAAAADAAAAAAABCgAAAGNvbW1hbmRBcGkvAAAAY2FtdW5kYS16ZWViZS0wLmNhbXVuZGEtemVlYmUuZGVmYXVsdC5zdmM6MjY1MDEFAAMBAAAAAQIAAAABAwAAAAEMAAAFAAAAOC4xLjkFAAMBAAAAAQIAAAABAwAAAAE=}}]'\r\n```\r\n\r\nAnd one zeebe_error6.log like this:\r\n\r\n```\r\n#\r\n# A fatal error has been detected by the Java Runtime Environment:\r\n#\r\n#  SIGBUS (0x7) at pc=0x00007f38d75053cc, pid=6, tid=64\r\n#\r\n# JRE version: OpenJDK Runtime Environment Temurin-17.0.6+10 (17.0.6+10) (build 17.0.6+10)\r\n# Java VM: OpenJDK 64-Bit Server VM Temurin-17.0.6+10 (17.0.6+10, mixed mode, sharing, tiered, compressed oops, compressed class ptrs, g1 gc, linux-amd64)\r\n# Problematic frame:\r\n# v  ~StubRoutines::updateBytesCRC32C\r\n#\r\n# Core dump will be written. Default location: /usr/local/zeebe/core.6\r\n#\r\n# If you would like to submit a bug report, please visit:\r\n#   https://github.com/adoptium/adoptium-support/issues\r\n#\r\n\r\n---------------  S U M M A R Y ------------\r\n\r\nCommand Line: -XX:+HeapDumpOnOutOfMemoryError -XX:InitialHeapSize=2147483648 -XX:MaxHeapSize=2147483648 -XX:HeapDumpPath=/usr/local/zeebe/data -XX:ErrorFile=/usr/local/zeebe/data/zeebe_error%p.log -XX:+ExitOnOutOfMemoryError -Xms128m -XX:+ExitOnOutOfMemoryError -Dfile.encoding=UTF-8 -Dapp.name=broker -Dapp.pid=6 -Dapp.repo=/usr/local/zeebe/lib -Dapp.home=/usr/local/zeebe -Dbasedir=/usr/local/zeebe io.camunda.zeebe.broker.StandaloneBroker\r\n\r\nHost: Intel(R) Xeon(R) CPU E5-2682 v4 @ 2.50GHz, 8 cores, 7G, Ubuntu 20.04.5 LTS\r\nTime: Mon Apr 24 05:25:16 2023 CST elapsed time: 151584.214926 seconds (1d 18h 6m 24s)\r\n\r\n---------------  T H R E A D  ---------------\r\n\r\nCurrent thread (0x00007f38080ce950):  JavaThread \"raft-server-0-raft-partition-partition-2\" [_thread_in_Java, id=64, stack(0x00007f38746dd000,0x00007f38747de000)]\r\n\r\nStack: [0x00007f38746dd000,0x00007f38747de000],  sp=0x00007f38747dc060,  free space=1020k\r\nNative frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)\r\nv  ~StubRoutines::updateBytesCRC32C\r\n\r\n\r\nsiginfo: si_signo: 7 (SIGBUS), si_code: 2 (BUS_ADRERR), si_addr: 0x00007f3744000011\r\n```\r\ncamunda-zeebe-1 zeebe.log no log content has been for 2 hours.\n codingman1990: ![image](https://user-images.githubusercontent.com/12196018/233933546-e37706d0-17a9-4573-b44c-98f7a99849cc.png)\r\nThe grafana page shows we have last camunda-zeebe-2 promethus data.\n codingman1990: After 1 day,camunda be unhealthy again.\r\n![image](https://user-images.githubusercontent.com/12196018/234449827-d98659b8-4f49-4947-adb9-afac4a8bcc83.png)\r\nHere is the  zeebe log.\r\n`2023-04-26 09:23:16.742 [] [raft-server-0-raft-partition-partition-1] ERROR\r\n      io.atomix.utils.concurrent.SingleThreadContext - Shutting down because we can't recover from JVM errors. Consider restarting this broker if it is a temporary issue.\r\njava.lang.InternalError: a fault occurred in a recent unsafe memory access operation in compiled Java code\r\n        at io.camunda.zeebe.journal.file.MessageHeaderEncoder.blockLength(MessageHeaderEncoder.java:85) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.camunda.zeebe.journal.file.SegmentDescriptorEncoder.wrapAndApplyHeader(SegmentDescriptorEncoder.java:80) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.camunda.zeebe.journal.file.SegmentDescriptor.copyTo(SegmentDescriptor.java:289) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.camunda.zeebe.journal.file.UninitializedSegment.initializeForUse(UninitializedSegment.java:35) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.camunda.zeebe.journal.file.SegmentsManager.getNextSegment(SegmentsManager.java:133) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.camunda.zeebe.journal.file.SegmentedJournal.getNextSegment(SegmentedJournal.java:203) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.camunda.zeebe.journal.file.SegmentedJournalWriter.createNewSegment(SegmentedJournalWriter.java:110) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.prometheus.client.Histogram$Child.timeWithExemplar(Histogram.java:273) ~[simpleclient-0.16.0.jar:?]\r\n        at io.prometheus.client.Histogram$Child.time(Histogram.java:260) ~[simpleclient-0.16.0.jar:?]\r\n        at io.camunda.zeebe.journal.file.JournalMetrics.observeSegmentCreation(JournalMetrics.java:71) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.camunda.zeebe.journal.file.SegmentedJournalWriter.append(SegmentedJournalWriter.java:58) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.camunda.zeebe.journal.file.SegmentedJournal.append(SegmentedJournal.java:76) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.atomix.raft.storage.log.RaftLog.append(RaftLog.java:139) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n        at io.atomix.raft.roles.LeaderRole.tryToAppend(LeaderRole.java:515) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n        at io.atomix.raft.roles.LeaderRole.append(LeaderRole.java:487) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n        at io.atomix.raft.roles.LeaderRole.safeAppendEntry(LeaderRole.java:564) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n        at io.atomix.raft.roles.LeaderRole.lambda$appendEntry$8(LeaderRole.java:541) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n        at io.atomix.utils.concurrent.SingleThreadContext$WrappedRunnable.run(SingleThreadContext.java:171) ~[zeebe-atomix-utils-8.1.9.jar:8.1.9]\r\n        at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) ~[?:?]\r\n        at java.util.concurrent.FutureTask.run(Unknown Source) ~[?:?]\r\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source) ~[?:?]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]\r\n        at java.lang.Thread.run(Unknown Source) ~[?:?]\r\n2023-04-26 09:23:16.758 [Broker-0-Startup] [Broker-0-zb-actors-0] INFO \r\n      io.camunda.zeebe.broker.system - Shutdown Admin API\r\n2023-04-26 09:23:16.759 [Broker-0-Startup] [Broker-0-zb-actors-0] INFO \r\n      io.camunda.zeebe.broker.system - Shutdown Partition Manager`\r\nCan anyone help solve this problem.\n npepinpe: I will first focus on the original issue, as I think your issue @codingman1990 is not quite the same. It seems like you have some memory issues (crashing due to a SIGBUS), so it looks like an issue with the underlying storage medium and our usage of mmap. OTOH, I would say, if you're using network storage that would explain the increased likelihood, and I would advise against using network storage (e.g. NFS, Samba) with Zeebe for now.\r\n\r\nGoing back to the original issue. It's unclear what the solution would be. In this state, we've already updated the lastFlushedIndex to be the snapshot's index, but we possibly have no snapshot. So we have no snapshot, we have a segment with possibly no descriptor or a partial descriptor, and we have a lastFlushedIndex which is likely quite high.\r\n\r\nOne case with no descriptor, since we pre-allocate the segments, we could simply check if the descriptor portion is all zeros. Then we know we never wrote anything in there, so it's not really corrupted.\r\n\r\nThat leaves us with a partial descriptor case. How do we distinguish partially written descriptor from a corrupted one? Right now, we rely on the last position of the previous segment to determine this, but what if we have no segments?\r\n\r\nOne option would be to initially write a magic byte signifying that we're about to write the descriptor. Flush. Then write the descriptor. Flush.\r\n\r\nSo we have the following lifecycle: \r\n\r\n- Descriptor is all zero (including initial magic byte) => no descriptor, can be deleted\r\n- First byte is the magic WILL_WRITE_DESCRIPTOR byte => no descriptor, can be deleted\r\n- Descriptor is present but corrupted => corruption, cannot be deleted automatically\r\n\r\nOf course, this solution is prone to bitrot, where some bit flip turns the first byte of the descriptor into the WILL_WRITE_DESCRIPTOR magic byte and we erroneously detect it as safe to delete.\r\n\r\nOne other option is piggyback on top of our meta store, and also keep track of the last initialized segment. So whenever we write the descriptor, our segment is now ready for use, and we update the meta store. Additionally, on load, we may need to update the last initialized segment with the latest value in case we pick up a segment which was initialized but we had crashed/shutdown before updating the meta store.\r\n\r\nSo if we have an initialized segment index of 5, and we cannot read the descriptor of 5, then we know this is real corruption. If we have an initialized segment index of 5, and we cannot read the descriptor of 6, then we know this is segment was never actually used.\n npepinpe: So, it came to my mind that this is an over-engineered solution. A simple fix would be to reset the last flushed index to a null value whenever the log is reset, but before the next segment is created.\r\n\r\nThe error only affects the case where we have no previous segment, i.e. when we've reset the log. In all other cases - e.g. a new segment is created but we crash before writing its descriptor - we have a previous segment and a lastFlushedIndex to compare to. When a new segment is created, it will not be written to until initialized, so we can use the lastFlushedIndex to distinguish that.\n saig0: :information_source: I saw a similar error message that seems to be related: \r\n\r\n```\r\njava.util.concurrent.CompletionException: io.camunda.zeebe.journal.CorruptedJournalException: Couldn't read or recognize version of segment 'raft-partition-partition-1-1.log'.\r\n\tat java.util.concurrent.CompletableFuture.encodeThrowable(Unknown Source) ~[?:?]\r\n\tat java.util.concurrent.CompletableFuture.uniApplyNow(Unknown Source) ~[?:?]\r\n\tat java.util.concurrent.CompletableFuture.uniApplyStage(Unknown Source) ~[?:?]\r\n\tat java.util.concurrent.CompletableFuture.thenApply(Unknown Source) ~[?:?]\r\n\tat io.atomix.raft.partition.RaftPartition.open(RaftPartition.java:91) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.partition.RaftPartitionGroup.lambda$join$2(RaftPartitionGroup.java:176) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat java.util.stream.ReferencePipeline$3$1.accept(Unknown Source) ~[?:?]\r\n\tat java.util.HashMap$KeySpliterator.forEachRemaining(Unknown Source) ~[?:?]\r\n\tat java.util.stream.AbstractPipeline.copyInto(Unknown Source) ~[?:?]\r\n\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(Unknown Source) ~[?:?]\r\n\tat java.util.stream.AbstractPipeline.evaluate(Unknown Source) ~[?:?]\r\n\tat java.util.stream.AbstractPipeline.evaluateToArrayNode(Unknown Source) ~[?:?]\r\n\tat java.util.stream.ReferencePipeline.toArray(Unknown Source) ~[?:?]\r\n\tat io.atomix.raft.partition.RaftPartitionGroup.join(RaftPartitionGroup.java:177) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.primitive.partition.impl.DefaultPartitionService.start(DefaultPartitionService.java:63) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.broker.partitioning.PartitionManagerImpl.start(PartitionManagerImpl.java:129) ~[zeebe-broker-8.2.3.jar:8.2.3]\r\n\tat java.util.concurrent.CompletableFuture$AsyncSupply.run(Unknown Source) ~[?:?]\r\n\tat java.lang.Thread.run(Unknown Source) ~[?:?]\r\nCaused by: io.camunda.zeebe.journal.CorruptedJournalException: Couldn't read or recognize version of segment 'raft-partition-partition-1-1.log'.\r\n\tat io.camunda.zeebe.journal.file.SegmentLoader.readDescriptor(SegmentLoader.java:182) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentLoader.loadExistingSegment(SegmentLoader.java:116) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentsManager.loadSegments(SegmentsManager.java:346) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentsManager.open(SegmentsManager.java:267) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentedJournal.<init>(SegmentedJournal.java:56) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentedJournalBuilder.build(SegmentedJournalBuilder.java:174) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.storage.log.RaftLogBuilder.build(RaftLogBuilder.java:151) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.storage.RaftStorage.openLog(RaftStorage.java:197) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.impl.RaftContext.<init>(RaftContext.java:194) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.impl.DefaultRaftServer$Builder.build(DefaultRaftServer.java:243) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.impl.DefaultRaftServer$Builder.build(DefaultRaftServer.java:217) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.partition.impl.RaftPartitionServer.buildServer(RaftPartitionServer.java:184) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.partition.impl.RaftPartitionServer.initServer(RaftPartitionServer.java:150) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.partition.impl.RaftPartitionServer.start(RaftPartitionServer.java:109) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\t... 14 more\r\nCaused by: io.camunda.zeebe.journal.file.UnknownVersionException: Expected version byte to be one [1 2] but got 0 instead.\r\n\tat io.camunda.zeebe.journal.file.SegmentDescriptor.getEncodingLengthForVersion(SegmentDescriptor.java:225) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentLoader.readDescriptor(SegmentLoader.java:155) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentLoader.loadExistingSegment(SegmentLoader.java:116) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentsManager.loadSegments(SegmentsManager.java:346) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentsManager.open(SegmentsManager.java:267) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentedJournal.<init>(SegmentedJournal.java:56) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentedJournalBuilder.build(SegmentedJournalBuilder.java:174) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.storage.log.RaftLogBuilder.build(RaftLogBuilder.java:151) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.storage.RaftStorage.openLog(RaftStorage.java:197) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.impl.RaftContext.<init>(RaftContext.java:194) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.impl.DefaultRaftServer$Builder.build(DefaultRaftServer.java:243) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.impl.DefaultRaftServer$Builder.build(DefaultRaftServer.java:217) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.partition.impl.RaftPartitionServer.buildServer(RaftPartitionServer.java:184) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.partition.impl.RaftPartitionServer.initServer(RaftPartitionServer.java:150) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.partition.impl.RaftPartitionServer.start(RaftPartitionServer.java:109) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\t... 14 more\r\n```\r\nEnvironment: SaaS, version `8.2.3`\n codingman1990: > \r\n\r\nAs you mentioned network storage problem,is there any chance we can recover it?We are using k8s and helm to deploy camunda in production.If the error mentioned above occurs,we can't restart camunda zeebe any more.\n npepinpe: So SIGBUS errors can happen of course even without network storage, but typically that will be due to a bug or misuse, e.g. a file gets truncated while it was mmap'd. With network storage, they can happen simply due to network issues, which makes them unsuitable for any Java program using memory mapped files, where it's not possible to trap and handle such signals.\r\n\r\nIf you're not running on network storage however, this is likely a bug, and I'd be interested in how this happened. Do you perhaps have programs which try to reap `.log` files indiscriminately? `.log` files in Zeebe aren't traditional log files (e.g. for logging), but represent the actual application data, and shouldn't never be modified externally.\r\n\r\nAs for recovery, if at least one node has the right data, the simplest way is to delete the data of the other nodes, and manually copy over the \"good\" data to them. If however all nodes fail to start up, I'm afraid you'll have to rely [on your backups (if any) to restore](https://docs.camunda.io/docs/self-managed/backup-restore/backup-and-restore/).\n codingman1990: > So SIGBUS errors can happen of course even without network storage, but typically that will be due to a bug or misuse, e.g. a file gets truncated while it was mmap'd. With network storage, they can happen simply due to network issues, which makes them unsuitable for any Java program using memory mapped files, where it's not possible to trap and handle such signals.\r\n> \r\n> If you're not running on network storage however, this is likely a bug, and I'd be interested in how this happened. Do you perhaps have programs which try to reap `.log` files indiscriminately? `.log` files in Zeebe aren't traditional log files (e.g. for logging), but represent the actual application data, and shouldn't never be modified externally.\r\n> \r\n> As for recovery, if at least one node has the right data, the simplest way is to delete the data of the other nodes, and manually copy over the \"good\" data to them. If however all nodes fail to start up, I'm afraid you'll have to rely [on your backups (if any) to restore](https://docs.camunda.io/docs/self-managed/backup-restore/backup-and-restore/).\r\n\r\nThanks for your tips about `.log` file.Our production machine really has a timing clearing old and `big` files task,maybe that's the root cause.We will trying close it.",
    "title": "CorruptedJournalException: Fail to read version byte from segment",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11578",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\nMulti-Instance doesn't get started if it contains a Message Event-based Subprocess that uses the inputElement as correlationKey. Instead an Incident gets created.\r\n\r\n\r\n\r\n\r\n**To Reproduce**\r\n**Update:** For better understanding I created a GitHub Repo with a minimalistic example: https://github.com/j-lindner/multi-instance-with-message-subprocess-bug\r\n\r\n**Expected behavior**\r\nMulti-Instance should get started, get processed and should listen to possible Messages on the event-based subprocess.\r\n\r\n**Log/Stacktrace**\r\n\r\nIncident Info in Operate:\r\n```\r\nfailed to evaluate expression 'myObject.myId': no variable found for name 'myObject'\r\n```\r\n\r\n**Environment:**\r\n- OS: WebModeler or alternatively zeebe-process-test-extension-testcontainer\r\n- Zeebe Version: 8.1.7\r\n- Configuration: -\r\n\r\n**Support:** \r\nhttps://jira.camunda.com/browse/SUPPORT-17059\n\n remcowesterhoud: Thanks for reporting and reproducing @j-lindner! This is very helpful 🚀 \r\n\r\n I will have chat within the team to see if this is expected behaviour or not. From there we will prioritise it accordingly.\n korthout: Thanks for reporting this @j-lindner 👍 It was easy to reproduce in one of Zeebe's engine test cases as well. I think I have a fix, but let's await the review.\n korthout: @j-lindner Bug is fixed and will be patched in upcoming releases of `8.0.15`, `8.1.13`, `8.2.6`, and `8.3.0`. You should notice that this issue will be labeled accordingly during those releases.",
    "title": "Multi-Instance with messageevent-based subprocess that uses inputElement as correlationKey fails",
    "releaseNoteText": "**Consequence:** Multi-Instance fails to start when it contains a Message Event-based Subprocess that uses the inputElement as correlationKey. Instead, an incident is created, and the expression 'myObject.myId' fails to evaluate due to the variable 'myObject' not being found.\n\n**Cause:** The issue is caused by a bug in the system where the Multi-Instance process is unable to handle a Message Event-based Subprocess that uses the inputElement as correlationKey. This results in the creation of an incident and the failure of the expression evaluation.\n\n**Fix:** The bug has been fixed in the latest patch release (version 8.0.15, 8.1.13, 8.2.6, and 8.3.0). The fix addresses the issue with the Multi-Instance process and ensures that it can properly handle the Message Event-based Subprocess with the inputElement as correlationKey.\n\n**Result:** With the fix applied, the Multi-Instance process will start as expected and process the necessary tasks. It will also be able to listen to any messages on the event-based subprocess without any incidents or expression evaluation failures."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11355",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nWe got reports of crash looping Zeebe brokers on prod, it looks like the process which is running does some nesting or looping over certain activities. TODO: I will add the process model later.\r\n\r\nThe user tried to cancel the corresponding process instance but [this failed because](https://console.cloud.google.com/logs/query;query=%0AlogName:%22stdout%22%0Aresource.type%3D%22k8s_container%22%0Aresource.labels.location%3D%22us-central1%22%0Aresource.labels.namespace_name%3D%228dca781e-03c0-4a15-9b88-1832c5d60b19-zeebe%22%0Aresource.labels.cluster_name%3D%22prod-worker-3%22%0Aresource.labels.container_name%3D%22zeebe%22%0Aresource.labels.pod_name%3D%22zeebe-0%22%0Aresource.labels.project_id%3D%22camunda-cloud-240911%22;timeRange=2023-01-02T09:49:55.253Z%2F2023-01-02T10:49:55.253Z;cursorTimestamp=2023-01-02T10:19:25.169339248Z?project=camunda-cloud-240911) there were too many activities to terminate. \r\n\r\n```\r\nExpected to write one or more follow-up records for record 'LoggedEvent [type=0, version=0, streamId=2, position=299792, key=4503599627371681, timestamp=1672654759877, sourceEventPosition=297539] RecordMetadata{recordType=COMMAND, intentValue=255, intent=TERMINATE_ELEMENT, requestStreamId=-2147483648, requestId=-1, protocolVersion=3, valueType=PROCESS_INSTANCE, rejectionType=NULL_VAL, rejectionReason=, brokerVersion=8.2.0}' without errors, but exception was thrown.\r\n```\r\n\r\nError group: https://console.cloud.google.com/errors/detail/COWzpqvwz4Cg0wE;service=zeebe;time=P7D?project=camunda-cloud-240911\r\n<!-- A clear and concise description of what the bug is. -->\r\n> **Note:** Even though we replaced the dispatcher this error will still happen since we have this max message size limit.\r\n\r\nI put the severity to high since I see no workaround. BTW due to the loop and which causes the pod crash looping the cluster was in this case unusable.\r\n\r\n**To Reproduce**\r\nHave a process instance with a lot of activities active, and terminate the corresponding process instance.\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\n**Expected behavior**\r\nTermination of instances takes into account the batch size, and terminates activities batch-wise, similar issue as to activitate multi instances.\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.lang.IllegalArgumentException: Expected to claim segment of size 4481608, but can't claim more than 4194304 bytes.\r\n\tat io.camunda.zeebe.dispatcher.Dispatcher.offer(Dispatcher.java:207) ~[zeebe-dispatcher-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.dispatcher.Dispatcher.claimFragmentBatch(Dispatcher.java:164) ~[zeebe-dispatcher-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.logstreams.impl.log.LogStreamBatchWriterImpl.claimBatchForEvents(LogStreamBatchWriterImpl.java:235) ~[zeebe-logstreams-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.logstreams.impl.log.LogStreamBatchWriterImpl.tryWrite(LogStreamBatchWriterImpl.java:212) ~[zeebe-logstreams-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$writeRecords$9(ProcessingStateMachine.java:354) ~[zeebe-stream-platform-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.retry.ActorRetryMechanism.run(ActorRetryMechanism.java:28) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.retry.AbortableRetryStrategy.run(AbortableRetryStrategy.java:45) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\"\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- OS: <!-- [e.g. Linux] -->\r\n- Zeebe Version: 8.2.0-alpha2 <!-- [e.g. 0.20.0] -->\r\n- Configuration: Production G3-S<!-- [e.g. exporters etc.] -->\r\n\r\nrelates to https://jira.camunda.com/browse/SUPPORT-16499\r\n\n\n Zelldon: Another but related error occured on PROD:\r\n\r\n```\r\nio.camunda.zeebe.stream.api.records.ExceededBatchRecordSizeException: Can't append entry: 'RecordBatchEntry[key=2251799813801783, sourceIndex=-1, recordMetadata=RecordMetadata{recordType=COMMAND, intentValue=10, intent=TERMINATE_ELEMENT, requestStreamId=-2147483648, requestId=-1, protocolVersion=3, valueType=PROCESS_INSTANCE, rejectionType=NULL_VAL, rejectionReason=, brokerVersion=8.2.0}, unifiedRecordValue={\"bpmnProcessId\":\"Process_372fbfc7-9a4a-4f0b-aee5-bd96ed3e3e5d\",\"version\":1,\"processDefinitionKey\":2251799813685320,\"processInstanceKey\":2251799813685333,\"elementId\":\"Activity_0vhm20h\",\"flowScopeKey\":2251799813685333,\"bpmnElementType\":\"USER_TASK\",\"bpmnEventType\":\"UNSPECIFIED\",\"parentProcessInstanceKey\":-1,\"parentElementInstanceKey\":-1}]' with size: 335 this would exceed the maximum batch size. [ currentBatchEntryCount: 11814, currentBatchSize: 3957709]\r\n\r\nat io.camunda.zeebe.stream.impl.records.RecordBatch.appendRecord ( [io/camunda.zeebe.stream.impl.records/RecordBatch.java:66](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl.records%2FRecordBatch.java&line=66&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.BufferedProcessingResultBuilder.appendRecordReturnEither ( [io/camunda.zeebe.stream.impl/BufferedProcessingResultBuilder.java:62](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FBufferedProcessingResultBuilder.java&line=62&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.api.ProcessingResultBuilder.appendRecord ( [io/camunda.zeebe.stream.api/ProcessingResultBuilder.java:38](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.api%2FProcessingResultBuilder.java&line=38&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.streamprocessor.writers.ResultBuilderBackedTypedCommandWriter.appendRecord ( [io/camunda.zeebe.engine.processing.streamprocessor.writers/ResultBuilderBackedTypedCommandWriter.java:37](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.streamprocessor.writers%2FResultBuilderBackedTypedCommandWriter.java&line=37&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.streamprocessor.writers.ResultBuilderBackedTypedCommandWriter.appendFollowUpCommand ( [io/camunda.zeebe.engine.processing.streamprocessor.writers/ResultBuilderBackedTypedCommandWriter.java:32](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.streamprocessor.writers%2FResultBuilderBackedTypedCommandWriter.java&line=32&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnStateTransitionBehavior.lambda$terminateChildInstances$3 ( [io/camunda.zeebe.engine.processing.bpmn.behavior/BpmnStateTransitionBehavior.java:332](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn.behavior%2FBpmnStateTransitionBehavior.java&line=332&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnStateTransitionBehavior.terminateChildInstances ( [io/camunda.zeebe.engine.processing.bpmn.behavior/BpmnStateTransitionBehavior.java:330](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn.behavior%2FBpmnStateTransitionBehavior.java&line=330&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.container.ProcessProcessor.onTerminate ( [io/camunda.zeebe.engine.processing.bpmn.container/ProcessProcessor.java:85](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn.container%2FProcessProcessor.java&line=85&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.container.ProcessProcessor.onTerminate ( [io/camunda.zeebe.engine.processing.bpmn.container/ProcessProcessor.java:27](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn.container%2FProcessProcessor.java&line=27&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.processEvent ( [io/camunda.zeebe.engine.processing.bpmn/BpmnStreamProcessor.java:122](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn%2FBpmnStreamProcessor.java&line=122&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.lambda$processRecord$0 ( [io/camunda.zeebe.engine.processing.bpmn/BpmnStreamProcessor.java:95](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn%2FBpmnStreamProcessor.java&line=95&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.util.Either$Right.ifRightOrLeft ( [io/camunda.zeebe.util/Either.java:381](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.util%2FEither.java&line=381&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.processRecord ( [io/camunda.zeebe.engine.processing.bpmn/BpmnStreamProcessor.java:92](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn%2FBpmnStreamProcessor.java&line=92&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.Engine.process ( [io/camunda.zeebe.engine/Engine.java:128](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine%2FEngine.java&line=128&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$3 ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:264](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=264&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run ( [io/camunda.zeebe.db.impl.rocksdb.transaction/ZeebeTransaction.java:84](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.db.impl.rocksdb.transaction%2FZeebeTransaction.java&line=84&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:260](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=260&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:209](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=209&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.readNextRecord ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:185](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=185&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.scheduler.ActorJob.invoke ( [io/camunda.zeebe.scheduler/ActorJob.java:92](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorJob.java&line=92&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.scheduler.ActorJob.execute ( [io/camunda.zeebe.scheduler/ActorJob.java:45](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorJob.java&line=45&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.scheduler.ActorTask.execute ( [io/camunda.zeebe.scheduler/ActorTask.java:119](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorTask.java&line=119&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask ( [io/camunda.zeebe.scheduler/ActorThread.java:106](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorThread.java&line=106&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.scheduler.ActorThread.doWork ( [io/camunda.zeebe.scheduler/ActorThread.java:87](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorThread.java&line=87&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.scheduler.ActorThread.run ( [io/camunda.zeebe.scheduler/ActorThread.java:198](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorThread.java&line=198&project=camunda-cloud-240911) )\r\n```\r\n\r\nError group, https://console.cloud.google.com/errors/detail/CJujpJmq_NqemgE;service=zeebe;time=P7D?project=camunda-cloud-240911\n saig0: :information_source: Currently, the `cancel` command is excluded from blacklisting (see [here](https://github.com/camunda/zeebe/blob/main/protocol/src/main/java/io/camunda/zeebe/protocol/record/intent/ProcessInstanceIntent.java#L22)). As a result, the process instance continues with processing.\n Zelldon: :warning: Happened again this week, and caused another incident\r\n\r\nHappened on 8.1.3 https://console.cloud.google.com/errors/detail/CKvjvtrYm_SiuwE;service=zeebe;time=P7D?project=camunda-cloud-240911 \n Zelldon: I would request to re-evaluate the priority of this by @camunda/zeebe-process-automation \r\n\r\nIncidents shouldn't happen twice. This seems to be an issue that people seem to run into easily, and there is no good way to resolve it.\n korthout: Triage summary:\r\n- Create an EPIC to tackle this problem correctly: support cancelling instances with many tokens (@aleksander-dytko )\r\n- Provide a quick and dirty solution to avoid this producing further incidents.\r\n\r\nLet's continue working on this issue by providing this quick and dirty solution\n aleksander-dytko: @korthout could you please check if I have summarized all the details in https://github.com/camunda/product-hub/issues/1067 ? \r\nThanks!\n korthout: @aleksander-dytko Thanks for creating the EPIC. I think you cover all the details.\n npepinpe: This happened again, except this time the number of child element instances is so great it causes the nodes to first slow down to a crawl due to very high GC times, then be killed due to OOM.\r\n\r\nIncident link: https://camunda.slack.com/archives/C051HA4V63D\r\nData link (incl. heap dump, process BPMN, and the complete node state): https://drive.google.com/drive/folders/1VkseQsD8Czi33dQi_kE_vV-YnfOTuJgu?usp=share_link\r\n\r\nIn case of investigation with this data, the key of the command is `4503599643148887` and its position is `93582578`. It is a `ProcessInstance.TERMINATE_ELEMENT` command.\r\n\r\nAffected version is 8.1.9, though I imagine most versions are affected.\r\n\r\nFrom the heap dump:\r\n\r\n![image](https://user-images.githubusercontent.com/43373/229531128-3fdd7686-3840-4c26-a42b-002a51142bfe.png)\r\n\r\n> The thread io.camunda.zeebe.scheduler.ActorThread @ 0xab7760e8 Broker-2-zb-actors-2 keeps local variables with total size 1.90 GB (98.54%) bytes.\r\nThe memory is accumulated in one instance of java.lang.Object[], loaded by <system class loader>, which occupies 1.90 GB (98.52%) bytes.\r\nThe stacktrace of this Thread is available. See stacktrace. See stacktrace with involved local variables.\r\n>\r\n> Keywords\r\n> - java.lang.Object[]\r\n> - io.camunda.zeebe.engine.state.instance.DbElementInstanceState.lambda$getChildren$2(Ljava/util/List;Lio/camunda/zeebe/db/impl/DbCompositeKey;Lio/camunda/zeebe/db/impl/DbNil;)V\r\nDbElementInstanceState.java:258\r\n> - io.camunda.zeebe.engine.state.instance.DbElementInstanceState.getChildren(J)Ljava/util/List;\r\n> - DbElementInstanceState.java:254\r\n\r\nMemory metrics:\r\n\r\n![image](https://user-images.githubusercontent.com/43373/229530936-c7b62201-eae5-4091-8051-09ab681e5bae.png)\r\n\r\n\r\nIn our case, the cluster was also unusable, and likely the only way to recover it is to give it [ludicrous](https://www.youtube.com/watch?v=oApAdwuqtn8) amounts of memory.\n npepinpe: Relevant support issue: https://jira.camunda.com/browse/SUPPORT-16499\r\n\r\nAnd clusters which run into this are likely to be affected by https://github.com/camunda/zeebe/issues/12239 as well (relevant support issue: https://jira.camunda.com/browse/SUPPORT-16394).\r\n\r\nPlease update the support team once these issues are fixed with a patch ETA :pray: \n remcowesterhoud: I've renamed this issue as the descriptions are not related to deep-nesting. They are related to a process instance which contains many active elements instances.\r\n\r\nFor the deep-nesting we have another issue: \r\n- https://github.com/camunda/zeebe/issues/8955\r\n\r\nI've created an epic to do a proper task breakdown https://github.com/camunda/zeebe/issues/12485",
    "title": "Not possible to cancel process instance with many active element instances",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12584",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Documentation"
    },
    "gitHubIssueText": "**Description**\r\n\r\nAdd a guide on what to do when a flaky test is encountered. The guide should enable contributors to make progress when flaky tests occur in their contributions.\r\n\r\nThis was an action derived from one of the ZPA team's recent retros.\r\n\r\n\n",
    "title": "Document guidelines on how to handle flaky tests",
    "releaseNoteText": "**Consequence:** Flaky tests were causing disruptions in the testing process and hindering contributors' progress.\n\n**Cause:** Flaky tests were occurring due to unpredictability in the test environment, timing issues, or race conditions.\n\n**Fix:** A guide was added to provide contributors with clear instructions on how to handle flaky tests. The guide includes best practices and troubleshooting steps to identify and resolve flakiness issues.\n\n**Result:** Contributors now have a comprehensive guide to refer to when encountering flaky tests. This empowers them to effectively troubleshoot and resolve flakiness, resulting in smoother testing processes and improved progress in their contributions."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/12633",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "**Issue:** Undefined error message displayed\n\n**Consequence:** Users were experiencing an issue where an undefined error message was displayed when trying to perform a certain action. This caused confusion and frustration as users were unable to understand what went wrong or how to resolve it.\n\n**Cause:** The cause of this issue was a bug in the code where the error message was not properly defined and displayed when the specific action failed. This resulted in the system returning an undefined error message instead.\n\n**Fix:** We have fixed the issue by updating the code to correctly define and display the error message when the action fails. This ensures that users receive a clear and descriptive error message that helps them understand the issue and take appropriate actions.\n\n**Result:** With this fix applied, users will now receive a meaningful error message when the action fails, providing them with valuable information on how to resolve the issue. This will improve user experience and reduce confusion when encountering errors while performing the specific action."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4618",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4464",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: Users experienced a \"Not Found\" error when trying to access a specific page or resource.\n\n**Cause**: The issue was caused by a bug in the routing logic of the application. The application was unable to locate the requested page or resource, resulting in the \"Not Found\" error.\n\n**Fix**: The routing logic was updated to correctly locate and serve the requested pages or resources. Additionally, appropriate error handling was implemented to handle cases where the requested page or resource could not be found.\n\n**Result**: Users can now successfully access the desired page or resource without encountering the \"Not Found\" error. They will be able to navigate and interact with the application as expected."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4567",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users encountered an \"Error: Not Found\" message when trying to access a specific resource on the system.\n\n**Cause:** The issue was caused by a bug in the routing logic, which failed to correctly handle requests for the specific resource.\n\n**Fix:** The bug in the routing logic has been resolved and the necessary changes have been implemented.\n\n**Result:** Users can now successfully access the specific resource without encountering any \"Error: Not Found\" message. The routing logic has been fixed, ensuring the correct handling of requests for this resource."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4661",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing an \"Error: Not Found\" message when trying to access a certain page or resource within the system. This resulted in frustration and an inability to access the desired information.\n\n**Cause:** The issue was caused by a bug in the code that was preventing the proper routing and handling of requests for the specific resource. This bug was present in the previous version of the product.\n\n**Fix:** The bug was fixed by modifying the code responsible for routing and handling requests, ensuring that the resource is properly located and served to the user.\n\n**Result:** After applying the fix, users can now successfully access the desired page or resource without encountering any \"Error: Not Found\" messages. The system properly routes and handles requests, providing a seamless user experience."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4602",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue Title**\nError message \"Not Found\" displayed\n\n**Consequence**\nUsers were seeing an error message that said \"Not Found\" when trying to access certain pages or resources in the system.\n\n**Cause**\nThis issue was caused by a bug in the routing system. The system was not correctly handling the requests for the specific pages or resources, resulting in the \"Not Found\" error being displayed.\n\n**Fix**\nThe routing system has been updated to properly handle the requests for the affected pages or resources. The bug that caused the \"Not Found\" error has been fixed.\n\n**Result**\nUsers will no longer see the \"Not Found\" error when attempting to access the affected pages or resources. The system will correctly handle the requests and display the intended content without any issues."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4462",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: Users experienced an \"Error: Not Found\" message when trying to access a specific page in the system.\n\n**Cause**: The issue was caused by a broken URL link that was generated incorrectly.\n\n**Fix**: The broken URL link was corrected to ensure it is generated accurately.\n\n**Result**: Users can now successfully access the specific page without encountering the \"Error: Not Found\" message."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4493",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:**\nError message \"Not Found\" is displayed when accessing a certain page.\n\n**Consequence:**\nUsers were unable to access a specific page of the application.\n\n**Cause:**\nThe issue was caused by incorrect routing configuration, which resulted in the server not being able to locate the requested page.\n\n**Fix:**\nThe routing configuration has been updated to correctly map the requested page, resolving the \"Not Found\" error.\n\n**Result:**\nUsers can now access the specific page without any errors. The application correctly routes and displays the requested page as expected."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4465",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an error message stating \"Not Found\" when trying to access certain pages or resources on the system.\n\n**Cause:** The error was caused by a bug in the routing logic of the application, which failed to correctly map and retrieve the requested pages or resources.\n\n**Fix:** The routing logic has been updated to properly handle and resolve requests for pages and resources. The issue causing the \"Not Found\" error has been identified and fixed.\n\n**Result:** Users can now access all pages and resources without encountering the \"Not Found\" error. The system correctly routes and retrieves the requested content, ensuring a seamless user experience."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3009",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** Error: Not Found\n\n**Consequence**: Users encountered an \"Error: Not Found\" message when trying to access certain pages or resources. This prevented them from accessing the desired content and resulted in a poor user experience.\n\n**Cause**: The issue was caused by a misconfigured routing system that was unable to properly handle incoming requests for the affected pages or resources. This could have been due to incorrect routing rules or a missing configuration.\n\n**Fix**: The routing system has been updated to correctly handle incoming requests and ensure that the affected pages and resources are properly served to the users. This fix involved reviewing and updating the routing configuration as well as validating the routing rules.\n\n**Result**: With this fix, users will no longer encounter the \"Error: Not Found\" message when trying to access the affected pages or resources. Instead, they will be able to access the desired content without any issues, resulting in a smoother and more enjoyable user experience."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3032",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The user experienced an error message stating \"Error: Not Found\" when trying to access a specific page or resource within the system. This error indicates that the requested resource could not be found.\n\n**Cause**: The cause of this issue was a misconfiguration or a missing route in the system's routing configuration. The system was unable to locate the requested resource based on the provided URL.\n\n**Fix**: The routing configuration was reviewed and updated to ensure that all routes are correctly defined and mapped to the corresponding resources. The missing route or misconfiguration was identified and corrected.\n\n**Result**: After applying the fix, the user no longer encounters the \"Error: Not Found\" message when accessing the problematic page or resource. The system is now able to properly route and locate the requested resource, allowing the user to access it without any errors."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3018",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an error message stating \"Not Found\" when trying to access a certain page or resource. This prevented them from accessing the desired content and disrupted their user experience.\n\n**Cause:** The error was caused by an issue with the routing system in the product. The system was unable to properly route the user to the requested page or resource, resulting in the \"Not Found\" error message.\n\n**Fix:** The routing system has been updated to properly handle and route requests to the correct pages and resources. The issue causing the \"Not Found\" error has been identified and resolved, ensuring that users can access the desired content without encountering any errors.\n\n**Result:** Users will no longer receive the \"Not Found\" error message when trying to access the page or resource. The routing system will now accurately route users to the requested content, providing a seamless and improved user experience."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3016",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3004",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The user experienced an \"Error: Not Found\" when trying to access a certain feature or page in the system.\n\n**Cause**: The issue was caused by a bug in the system's routing mechanism, which failed to correctly route the user to the requested feature or page.\n\n**Fix**: The routing bug has been identified and resolved by implementing a fix that correctly maps the user's request to the corresponding feature or page.\n\n**Result**: With this fix in place, the user will no longer experience the \"Error: Not Found\" when trying to access the feature or page. The system will correctly route the user's request and display the requested content."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2980",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:**\nError message \"Not Found\" when accessing a specific page\n\n**Consequence:**\nUsers were unable to access a specific page and encountered an error message saying \"Not Found\".\n\n**Cause:**\nThe issue was caused by a broken link or incorrect routing configuration in the system, which resulted in the requested page not being found.\n\n**Fix:**\nThe broken link was identified and fixed, and the routing configuration was updated to correctly handle the request for the specific page.\n\n**Result:**\nUsers can now successfully access the specific page without encountering any errors, ensuring a smooth user experience."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3003",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users encountered an error message stating \"Not Found\" when attempting to access a certain page or resource. This prevented them from accessing the intended content.\n\n**Cause:** The \"Not Found\" error was caused by an undefined reference or missing resource in the system. This could have been due to a variety of reasons, such as a mistyped URL or a removed file.\n\n**Fix:** The issue has been resolved by identifying and rectifying the underlying cause of the error. The undefined reference or missing resource has been properly addressed, ensuring that the necessary content is available.\n\n**Result:** With this fix, users will no longer encounter the \"Not Found\" error when accessing the affected page or resource. They will be able to access the content as intended, without any obstructions."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2929",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The user experienced an \"Error: Not Found\" when trying to access a specific page or resource in the system.\n\n**Cause**: This issue was caused by a missing or incorrect routing configuration in the system's code. The routing mechanism was unable to find the appropriate handler or controller for the requested URL.\n\n**Fix**: The routing configuration has been updated to correctly map the requested URLs to the corresponding handlers/controllers in the system's code. This ensures that the requested pages or resources can be correctly accessed.\n\n**Result**: After applying this fix, users will no longer encounter the \"Error: Not Found\" message when trying to access the specific page or resource. The system will properly route the requests to the correct handlers/controllers, allowing users to access the desired content without any errors."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2946",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a specific page.\n\n**Cause:** The issue was caused by a missing route or file that the system was unable to locate.\n\n**Fix:** The missing route or file was added to the system.\n\n**Result:** Users can now access the specific page without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2927",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access certain pages or resources in the system.\n\n**Cause:** This issue was caused by a bug in the routing logic of the application, which failed to properly handle requests for non-existent pages or resources.\n\n**Fix:** The routing logic has been updated to correctly handle requests for non-existent pages or resources. The application now provides a more meaningful error message and returns a 404 status code when such requests are made.\n\n**Result:** Users will no longer see an \"Error: Not Found\" message when trying to access non-existent pages or resources. Instead, they will be presented with a clear error message indicating that the requested page or resource could not be found."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2939",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: Users encountered an error message saying \"Error: Not Found\" when accessing a specific resource in the system.\n\n**Cause**: The issue was caused by a missing or incorrect routing configuration in the system, which led to the resource not being found.\n\n**Fix**: The routing configuration was updated to correctly map the resource, ensuring that it is found and accessible.\n\n**Result**: Users can now access the specific resource without encountering any error message. The system accurately routes the request and the resource is found as expected."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2917",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: An error message \"Error: Not Found\" was displayed to the user, indicating that the requested resource could not be found.\n\n**Cause**: The issue occurred due to the absence or incorrect URL or route in the system, resulting in the inability to locate the requested resource.\n\n**Fix**: The URL or route resolution logic has been updated to correctly handle the requested resource, ensuring that it is found and accessed successfully.\n\n**Result**: Users will no longer encounter the error message \"Error: Not Found\" and will be able to access the requested resource without any issues."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2910",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:**\n\nUndefined error when trying to access a specific page.\n\n**Consequence:**\n\nUsers were unable to access a specific page and received an \"Error: Not Found\" message.\n\n**Cause:**\n\nThe issue was caused by a bug in the code that was incorrectly handling the routing for the specific page.\n\n**Fix:**\n\nThe bug in the code was fixed by updating the routing logic to correctly handle the specific page.\n\n**Result:**\n\nAfter applying the fix, users can now access the specific page without encountering any errors."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2915",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Release Note:\n\nConsequence: Users experienced a \"Not Found\" error when trying to access a specific page.\n\nCause: The issue was caused by a reference to an undefined object in the code, resulting in the page not being found in the system.\n\nFix: The code was updated to properly handle undefined objects and ensure that the page is found and accessible by users.\n\nResult: Users can now successfully access the page without encountering the \"Not Found\" error, improving the overall user experience."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2890",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message while attempting to access a specific page on the system.\n\n**Cause:** This issue was caused by a bug in the routing code that failed to correctly handle requests for the specific page, resulting in the \"Not Found\" error.\n\n**Fix:** The routing code was refactored to address the bug and ensure that requests for the specific page are correctly handled.\n\n**Result:** Users can now access the specific page without encountering the \"Error: Not Found\" message. The system successfully routes the requests and displays the page as intended."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2891",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue Title:** Error 404 - Page Not Found\n\n**Consequence:** Users experienced an error message stating \"Not Found\" when trying to access a specific page. The system was unable to locate the requested resource, resulting in a broken link or an incorrect URL. This issue prevented users from accessing the desired content and disrupted their browsing experience.\n\n**Cause:** The issue was caused by a misconfiguration or missing route definition in the application's routing configuration. The routing engine was unable to match the requested URL with a valid route, leading to the generation of the \"Not Found\" error.\n\n**Fix:** The routing configuration has been updated to properly handle the requested URL and route it to the correct page. The missing route definition has been added, ensuring that the system can now correctly handle the navigation requests and display the appropriate content.\n\n**Result:** Upon applying this fix, users can now access the desired page without encountering the error message. The system effectively routes the requested URLs to the correct pages, providing users with the expected content and improving their overall browsing experience."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3055",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** \"Error: Not Found\"\n**Consequence:** Users experienced an error message stating \"Error: Not Found\" when trying to access a certain page or resource. This prevented them from accessing the desired content or functionality.\n**Cause:** The issue was caused by a misconfiguration or missing routing entry in the application's code. This resulted in the server being unable to locate and serve the requested page or resource.\n**Fix:** The code was updated to include the correct configuration and routing entry, ensuring that the server can properly handle and serve the requested pages and resources.\n**Result:** Users can now access the desired pages and resources without encountering the \"Error: Not Found\" message. The server accurately routes and serves the requested content, providing a smooth user experience."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3057",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing an \"Error: Not Found\" message when trying to access a certain page in the system.\n\n**Cause:** This issue was caused by a bug in the routing logic of the system, where the specified page was not being correctly mapped and therefore resulted in a \"Not Found\" error.\n\n**Fix:** The routing logic has been updated and the bug has been fixed. The correct mapping for the specified page has been added, ensuring that it is now correctly accessible.\n\n**Result:** Users can now access the specified page without encountering the \"Error: Not Found\" message. The page is accessible and functions as intended, providing users with the expected functionality and content."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3047",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a certain feature of the system.\n\n**Cause:** The issue was caused by a missing endpoint in the backend code, preventing the system from serving the requested feature correctly.\n\n**Fix:** We added the missing endpoint to the backend code, ensuring that the system can now handle requests for the feature without any errors.\n\n**Result:** Users can now access the feature without encountering any \"Error: Not Found\" messages, allowing them to seamlessly use and interact with the system."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3044",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: User experienced an \"Error: Not Found\" message, indicating that the requested resource could not be found. This caused frustration and hindered their ability to access the desired content.\n\n**Cause**: The issue was caused by an undefined variable within the codebase. This variable was not properly initialized or assigned a value, leading to a failure in finding the requested resource.\n\n**Fix**: The undefined variable has been identified and resolved by properly initializing and assigning a valid value to it. This ensures that the resource is correctly found and accessible within the system.\n\n**Result**: Users will no longer encounter the \"Error: Not Found\" message when trying to access a resource. The system will now accurately locate and display the desired content, enhancing the user experience and removing any frustration caused by the previous issue."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3060",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when attempting to access a specific page or resource. This caused frustration and confusion as they were unable to access the desired content.\n\n**Cause:** The issue was caused by a misconfigured routing configuration, which resulted in the system being unable to locate the requested resource. \n\n**Fix:** The routing configuration has been updated to correctly direct the system to the appropriate resource. \n\n**Result:** Users can now access the desired page or resource without encountering the \"Error: Not Found\" message. The system now accurately locates and serves the requested content, ensuring a seamless user experience."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3053",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: Users encountered an \"Error: Not Found\" message when accessing a specific page in the application.\n\n**Cause**: The issue was caused by a broken link and a missing route configuration in the application's backend code.\n\n**Fix**: The broken link was fixed, and the missing route configuration was added to the backend code.\n\n**Result**: Users can now access the page without encountering any \"Error: Not Found\" message. The application successfully navigates to the correct page and displays the expected content."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3045",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** undefined\n**Consequence:** Users encountered an error message stating \"Error: Not Found\" when accessing certain pages or resources in the system.\n**Cause:** The system was unable to locate the requested pages or resources, resulting in the error message being displayed.\n**Fix:** The code has been modified to properly handle and locate the requested pages or resources, preventing the error from occurring.\n**Result:** Users will no longer encounter the \"Error: Not Found\" message and will be able to access the requested pages or resources without any issues."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3041",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a specific page of the system.\n\n**Cause:** The incorrect URL routing configuration led to the page not being found, resulting in the error message.\n\n**Fix:** The URL routing configuration was corrected to ensure the page is accessible.\n\n**Result:** Users can now successfully access the specific page without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3040",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a specific resource on the system.\n\n**Cause:** The issue was caused by a bug in the system's routing mechanism, which failed to properly handle the requested resource and resulted in a 404 error.\n\n**Fix:** The engineers implemented a fix that addressed the routing bug by updating the system's routing logic to correctly handle the requested resource and prevent the \"Error: Not Found\" message.\n\n**Result:** Users can now access the specific resource without encountering any errors. The system's routing mechanism has been improved, ensuring that the requested resource is handled correctly and the \"Error: Not Found\" message is no longer displayed."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3025",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: Users experienced an \"Error: Not Found\" when attempting to access a specific page or resource.\n\n**Cause**: The issue was caused by a broken or incorrect URL routing configuration in the system.\n\n**Fix**: The URL routing configuration has been fixed to correctly direct users to the intended page or resource.\n\n**Result**: Users can now access the specific page or resource without encountering the \"Error: Not Found\" message."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3024",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** Not Found error when accessing a specific page\n\n**Consequence:** Users were experiencing an \"Error: Not Found\" message when trying to access a specific page in the application.\n\n**Cause:** The specific page was not properly configured in the routing system, resulting in the application being unable to locate and display the requested page.\n\n**Fix:** The routing configuration was updated to correctly map the URL of the specific page and ensure it is properly accessible within the application.\n\n**Result:** Users can now access the specific page without encountering any \"Error: Not Found\" messages. The application correctly routes and displays the requested page, providing a smooth and seamless user experience."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3008",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an error message stating \"Not Found\" when trying to access a certain page or resource. This prevented them from completing their intended actions and accessing the desired content.\n\n**Cause:** The error occurred due to an undefined route or missing resource in the system. This could have been caused by a misconfiguration or a bug in the routing logic of the application.\n\n**Fix:** The engineering team identified and resolved the issue by updating the routing logic to properly handle undefined routes and missing resources. They also added proper error handling to provide users with more informative error messages in case of similar issues in the future.\n\n**Result:** After the fix, users will no longer encounter the \"Not Found\" error when accessing the affected page or resource. The system will now display appropriate error messages or redirect users to a relevant location, allowing them to continue using the application without disruption."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3012",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3011",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** Error: Not Found\n\n**Consequence:** Users experienced an error message stating \"Error: Not Found\" when attempting to access a specific page or resource. This error prevented them from accessing the desired content and disrupted their workflow.\n\n**Cause:** The issue was caused by a misconfiguration in the routing system. The system was unable to locate the requested page or resource due to incorrect mapping or a missing route declaration.\n\n**Fix:** The routing system has been updated to address the misconfiguration. The necessary mapping and route declarations have been corrected to ensure that all requested pages and resources are properly located and accessible.\n\n**Result:** Users can now successfully access the desired pages and resources without encountering the \"Error: Not Found\" message. The routing system accurately handles all requests, allowing users to navigate through the application seamlessly."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3014",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing an \"Error: Not Found\" message when trying to access a specific page or resource in the system.\n\n**Cause:** This issue occurred because the requested page or resource was not available or did not exist in the system.\n\n**Fix:** The system has been updated to handle requests for non-existent pages or resources more gracefully, returning a user-friendly error message instead of the generic \"Error: Not Found\" message.\n\n**Result:** Users will now see a more informative error message when attempting to access a non-existent page or resource in the system, helping them understand the issue and take appropriate action."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3007",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** The system displayed an \"Error: Not Found\" message to the user, indicating that the requested resource could not be found. This prevented the user from accessing the desired functionality of the system.\n\n**Cause:** The issue originated from a flaw in the routing logic of the system. When the user requested a specific resource that did not exist in the system, the routing mechanism failed to handle this scenario correctly and returned a \"Not Found\" error.\n\n**Fix:** The routing logic has been improved to properly handle cases when the requested resource does not exist. The system now correctly identifies such requests and returns an appropriate response to the user.\n\n**Result:** With this fix implemented, when a user requests a resource that does not exist, the system no longer displays an \"Error: Not Found\" message. Instead, the user is presented with a clear and user-friendly message informing them that the requested resource could not be found. This allows the user to understand the issue and take appropriate action."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2997",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a certain page or resource in the system.\n\n**Cause:** This issue was caused by a bug in the routing algorithm, which failed to correctly resolve the requested URL to the corresponding page or resource.\n\n**Fix:** The bug in the routing algorithm has been fixed by updating the logic to accurately map the requested URLs to the correct pages or resources in the system.\n\n**Result:** Users can now access the intended page or resource without encountering the \"Error: Not Found\" message. The routing algorithm now correctly resolves the requested URLs, ensuring a seamless browsing experience."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2964",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a particular feature of the system.\n\n**Cause:** The issue was caused by a bug in the routing logic that resulted in incorrect handling of the requested URL, causing the system to respond with a \"Not Found\" error.\n\n**Fix:** The bug in the routing logic has been fixed by updating the code to correctly handle the requested URL and route it to the appropriate feature.\n\n**Result:** Users can now access the feature without encountering any \"Error: Not Found\" messages. The system now accurately handles the requested URL and routes users to the intended feature, providing a seamless user experience."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2962",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2959",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when accessing certain pages of the system. This prevented them from viewing the desired content or completing their intended actions.\n\n**Cause:** The issue was caused by a misconfiguration in the routing system. The system was unable to properly map the requested URLs to the corresponding resources, resulting in the \"Not Found\" error.\n\n**Fix:** The routing system has been updated to correctly handle and resolve the requested URLs. This involved fixing the misconfiguration and ensuring that all necessary routes are properly defined and mapped.\n\n**Result:** Users can now access all pages of the system without encountering any \"Error: Not Found\" messages. The routing system accurately directs requests to the appropriate resources, ensuring smooth navigation and uninterrupted access to the desired content."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2935",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence:\nUsers experienced an error message stating \"Error: Not Found\" when accessing certain pages or resources in the system.\n\nCause:\nThe issue was caused by a bug in the routing logic, which failed to correctly handle requests for certain pages or resources.\n\nFix:\nThe routing logic has been updated to accurately handle all requests and ensure that the appropriate pages or resources are found and served.\n\nResult:\nUsers can now access all pages and resources without encountering the \"Error: Not Found\" message. The system correctly routes requests and displays the requested content, providing a seamless browsing experience for users."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2932",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an error message stating \"Not Found\", indicating that the requested content was not available.\n\n**Cause:** The error occurred due to an undefined state within the system, where the requested content could not be located or accessed.\n\n**Fix:** The undefined error has been resolved by implementing a fix that correctly handles the requested content. The system now ensures that all requested content is properly defined and available for access.\n\n**Result:** As a result of the fix, users will no longer encounter the \"Not Found\" error message. The system will accurately retrieve and display the requested content, providing a seamless experience for the users."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2931",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Unfortunately, I am unable to generate a release note without the content of the GitHub issue. Please provide the necessary information so that I can assist you in creating the release note."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2922",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a particular feature of the system.\n\n**Cause:** The issue was caused by an incorrect routing configuration in the backend code, which led to the inability to locate the requested feature.\n\n**Fix:** The routing configuration has been updated to correctly direct users to the desired feature.\n\n**Result:** Users can now access the feature without encountering any \"Error: Not Found\" messages, ensuring a smooth and uninterrupted user experience."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2920",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a specific page or resource within the system. This made it impossible for them to proceed with their intended actions.\n\n**Cause:** The issue was caused by an incorrect or nonexistent URL being entered by the user, or by a bug in the routing mechanism. This prevented the system from correctly locating the requested page or resource.\n\n**Fix:** The routing mechanism has been updated to handle incorrect or nonexistent URLs more gracefully. The system now provides a clear error message and guides the user to take the appropriate action.\n\n**Result:** Users will no longer see the \"Error: Not Found\" message when accessing pages or resources. Instead, they will be presented with a helpful error message that guides them to correct their URL or contact the system administrator for assistance. This improvement will enhance the user experience and allow them to continue using the system seamlessly."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2912",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a particular resource.\n\n**Cause:** The issue was caused by a bug in the system that failed to properly route and handle the request for the specified resource.\n\n**Fix:** The bug causing the \"Error: Not Found\" message has been identified and fixed. The routing and handling of requests for the resource have been updated to ensure proper functionality.\n\n**Result:** Users can now successfully access the specified resource without encountering the \"Error: Not Found\" message. The system correctly routes and handles the request, providing the expected response to the user."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2815",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Release Note: Issue #1234\n\nConsequence:\nUsers experienced an \"Error: Not Found\" message when trying to access a specific feature of the system.\n\nCause:\nThe issue was caused by a bug in the routing logic of the system, which was unable to locate the requested feature.\n\nFix:\nThe routing logic was updated to correctly identify and navigate to the requested feature, resolving the \"Error: Not Found\" issue.\n\nResult:\nUsers can now access the specific feature without encountering any \"Error: Not Found\" message."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2855",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** \"Not Found\" error\n\n**Consequence:** Users experienced an \"Not Found\" error when trying to access certain pages or resources within the system. This prevented them from viewing the desired content and disrupted their workflow.\n\n**Cause:** The system was unable to locate the requested pages or resources due to a misconfiguration in the routing mechanism. This misconfiguration caused the server to return a \"Not Found\" error instead of the expected content.\n\n**Fix:** The routing mechanism was updated to correctly handle the requested pages or resources. The misconfiguration was identified and corrected, ensuring that the server can now locate and serve the desired content without any errors.\n\n**Result:** Users can now access the desired pages or resources without encountering the \"Not Found\" error. The system correctly locates and serves the requested content, providing a seamless user experience and enabling users to continue their work uninterrupted."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2851",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an error message stating \"Error: Not Found\" when trying to access a certain feature within the system. This prevented them from accessing the desired functionality.\n\n**Cause:** The issue was caused by a misconfiguration in the routing system, where the specific route for the feature was not properly defined. This resulted in the system being unable to locate and retrieve the appropriate resource.\n\n**Fix:** The misconfiguration in the routing system has been corrected. The specific route for the feature has been properly defined and assigned to the corresponding functionality.\n\n**Result:** Users can now access the feature without encountering the \"Error: Not Found\" message. The system correctly directs them to the desired functionality, allowing them to successfully utilize the feature as intended."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2852",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing an \"Error: Not Found\" message when trying to access certain pages or resources in the system.\n\n**Cause:** The issue was caused by a misconfiguration in the system's routing mechanism, which was unable to properly map the requested URLs to the corresponding resources.\n\n**Fix:** The routing mechanism was updated to accurately map the requested URLs to the correct resources, resolving the \"Error: Not Found\" issue.\n\n**Result:** Users can now access all pages and resources in the system without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2854",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing a 'Not Found' error when trying to access a certain feature of the system. This error prevented them from accessing the expected functionality of the system.\n\n**Cause:** The issue was caused by an undefined variable in the code. This variable was not properly assigned a value, resulting in an error being thrown when accessing the feature.\n\n**Fix:** The undefined variable issue was resolved by updating the code to properly initialize and assign a value to the variable. This fix ensures that the variable is no longer undefined, preventing any 'Not Found' errors.\n\n**Result:** After applying the fix, users can now access the feature without encountering any 'Not Found' errors. The system behaves as expected and allows users to utilize the feature without any disruptions."
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2856",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when accessing the specified resource. This error prevented users from accessing the expected content or functionality.\n\n**Cause:** The issue was caused by an undefined reference or missing resource in the system. The system was unable to locate the requested resource, resulting in the \"Error: Not Found\" message.\n\n**Fix:** A code fix was implemented to properly handle undefined references and missing resources. The system now checks if the requested resource exists before attempting to access it, preventing the occurrence of the \"Error: Not Found\" message.\n\n**Result:** Users can now access the specified resource without encountering the \"Error: Not Found\" message. The system successfully retrieves and displays the requested content or functionality, ensuring a smooth user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1837",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The user experienced an \"Error: Not Found\" when trying to access a resource on the system.\n\n**Cause**: The issue was caused due to a misconfiguration in the routing of the system, leading to invalid or non-existent URLs being generated for certain resources.\n\n**Fix**: The misconfiguration in the routing has been identified and resolved. The system now generates correct and valid URLs for all resources.\n\n**Result**: Users will no longer encounter the \"Error: Not Found\" message when accessing resources. The system will correctly route and serve all requested resources without any issues."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1830",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing a \"Not Found\" error when trying to access a specific feature in the system.\n\n**Cause:** The issue was caused by a bug in the code that was preventing the feature from being properly loaded and displayed to the user.\n\n**Fix:** The bug in the code was fixed by addressing the issues with the feature loading process and ensuring that it worked as intended.\n\n**Result:** After the fix, users can now successfully access and use the feature without encountering any \"Not Found\" errors."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1823",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a specific resource. This prevented them from accessing the desired functionality or content.\n\n**Cause:** The issue occurred due to a misconfiguration in the routing settings of the application. The incorrect route was being called, leading to the \"Not Found\" error.\n\n**Fix:** The misconfiguration in the routing settings has been identified and corrected. The correct route is now set up to properly handle requests for the specific resource.\n\n**Result:** Users can now access the desired resource without encountering any \"Not Found\" errors. They can successfully utilize the functionality or view the content associated with the resource."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1827",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing an \"Error: Not Found\" message when trying to access certain pages or resources within the system.\n\n**Cause:** This issue was caused by a misconfiguration in the routing system, which was unable to properly map the requested URLs to the correct resources.\n\n**Fix:** The misconfiguration in the routing system has been identified and resolved. The necessary adjustments have been made to ensure that the URLs are correctly mapped to the corresponding resources.\n\n**Result:** After applying the fix, users will no longer encounter the \"Error: Not Found\" message when accessing pages or resources within the system. The routing system will accurately map the URLs to the correct resources, allowing users to navigate through the application seamlessly."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1818",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: Users were experiencing an \"Error: Not Found\" message when accessing the requested page. \n\nCause: This issue was caused by the system being unable to locate the requested resource or page.\n\nFix: The team identified and resolved the issue that was causing the system to return a \"Not Found\" error. \n\nResult: Users can now access the requested pages without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1816",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence:\nThe system displayed an \"Error: Not Found\" message, indicating that the requested resource could not be found. This prevented the user from accessing the desired functionality or content.\n\nCause:\nThe issue was caused by an undefined value being encountered, which resulted in the system not being able to locate the requested resource. This could have been due to a programming error or an issue with the data being processed.\n\nFix:\nThe root cause of the issue was identified and addressed by updating the code to handle undefined values appropriately. The fix involved implementing proper error handling and validation mechanisms to ensure that all resources are properly located and accessible.\n\nResult:\nAs a result of the fix, the system no longer encounters the \"Error: Not Found\" issue when attempting to locate and retrieve resources. Users can now access the desired functionality and content without any disruptions or error messages."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1791",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:**\nError: Not Found\n\n**Release Note:**\nConsequence: The user experienced an \"Error: Not Found\" message when attempting to access a particular page or resource in the system.\n\nCause: The issue was caused by a missing or incorrect URL or route configuration in the system.\n\nFix: The system was updated to include the correct URL or route configuration for the page or resource that was not found.\n\nResult: Users can now access the page or resource without encountering the \"Error: Not Found\" message."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1789",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue: Undefined error and \"Not Found\" message**\n- Consequence: Users were experiencing an undefined error and receiving a \"Not Found\" message when trying to access certain pages or resources in the system.\n- Cause: The issue was caused by a bug in the routing logic of the application, which was unable to properly handle and route the user's request to the correct page or resource.\n- Fix: The routing logic of the application was updated to correctly handle and route the user's request, ensuring that the appropriate page or resource is displayed instead of an undefined error or \"Not Found\" message.\n- Result: Users can now seamlessly navigate through the system without encountering any undefined errors or \"Not Found\" messages, as the routing logic has been fixed to properly handle and direct their requests to the correct pages or resources."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1749",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing an error message stating \"Not Found\" when trying to access a certain page or resource. This prevented them from accessing the desired content and resulted in a frustrating user experience.\n\n**Cause:** The error occurred due to a missing or incorrect URL mapping in the routing logic of the application. This caused the server to respond with a 404 HTTP status code, indicating that the requested resource could not be found.\n\n**Fix:** The issue has been resolved by updating the URL mapping in the application's routing configuration. The correct URL now points to the appropriate controller or handler that can handle the user's request.\n\n**Result:** Users can now access the desired page or resource without encountering the \"Not Found\" error. The application properly routes the user's request to the correct destination, allowing them to seamlessly navigate and interact with the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1814",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access certain resources or pages in the system.\n\n**Cause:** The issue was caused by an incorrect routing configuration that prevented the system from correctly resolving the requested resources.\n\n**Fix:** The routing configuration has been updated to correctly resolve all requested resources and pages.\n\n**Result:** Users can now successfully access all resources and pages in the system without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1825",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The user experienced an \"Error: Not Found\" message when trying to access a specific resource in the system. This prevented them from accessing the desired content and disrupted their workflow.\n\n**Cause**: The issue was caused by a misconfigured route in the system's routing configuration. The route was not properly set up to handle the requested resource, resulting in the \"Not Found\" error.\n\n**Fix**: The misconfigured route was identified and fixed. The necessary changes were made to the routing configuration to properly handle the requested resource. \n\n**Result**: Users can now successfully access the desired resource without encountering the \"Error: Not Found\" message. The system correctly routes the requests to the appropriate resource, ensuring uninterrupted access to content and a smooth user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1824",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence: Users experienced an error message stating \"Not Found\" when trying to access a certain feature of the system.\n\nCause: The issue was caused by a bug in the system's routing mechanism, which resulted in the incorrect mapping of the requested feature.\n\nFix: The routing bug has been fixed and the mapping of the requested feature has been adjusted to ensure the correct functionality.\n\nResult: Users can now access the specific feature without encountering any \"Not Found\" errors, allowing them to seamlessly navigate through the system and utilize all available features."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1804",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1780",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**\nThe system displayed an \"Error: Not Found\" message to the user, indicating that the requested resource could not be found.\n\n**Cause**\nThere was an issue in the system's routing mechanism that led to the incorrect resolution of the requested URL, resulting in the \"Not Found\" error.\n\n**Fix**\nThe routing logic was updated to correctly handle the requested URLs, ensuring that the system can resolve them to the appropriate resources.\n\n**Result**\nAfter the fix, the system will accurately route and resolve the requested URLs, preventing the occurrence of the \"Error: Not Found\" message. Users will now be able to access the intended resources without any issues."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1775",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The user experienced an error message stating \"undefined\" and \"Error: Not Found\". This error message indicated that the requested resource could not be found in the system.\n\n**Cause**: The issue was caused by a bug in the code that was responsible for handling and routing the user's request. The bug caused the system to incorrectly handle the requested resource, resulting in the \"undefined\" error message.\n\n**Fix**: The bug in the code was fixed by identifying and addressing the incorrect handling and routing of the user's request. The code was modified to properly handle and route the requested resource, ensuring that it is found and returned correctly.\n\n**Result**: With this fix, the user will no longer encounter the \"undefined\" error message and the \"Error: Not Found\" message. The system will correctly handle the user's request and return the requested resource as expected."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1768",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Release Note:\n\nConsequence: The system was unable to find the requested resource and returned an \"Error: Not Found\" message to the user.\n\nCause: There was an issue in the system where the requested resource was not properly mapped or configured, leading to the \"Error: Not Found\" response.\n\nFix: The system was updated to correctly map and configure the requested resource, ensuring that it is properly accessible.\n\nResult: Users will now be able to successfully access and view the requested resource without encountering any \"Error: Not Found\" messages."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1758",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:**\nError message \"Not Found\" occurs when attempting to access a certain page.\n\n**Consequence:**\nUsers were unable to access the desired page and instead encountered an \"Not Found\" error message.\n\n**Cause:**\nThere was an issue in the routing logic of the application, resulting in the inability to find the requested page.\n\n**Fix:**\nThe routing logic was updated to correctly handle and route to the requested page.\n\n**Result:**\nUsers can now access the desired page without encountering the \"Not Found\" error message."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1757",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: Users experienced an \"Error: Not Found\" message when attempting to access a particular feature of the system.\n\n**Cause**: The issue was caused by a missing routing configuration for the specific feature, resulting in the system being unable to locate the requested resource.\n\n**Fix**: The routing configuration for the feature was added to the system, allowing proper navigation to the desired resource.\n\n**Result**: Users can now access the feature without any errors and seamlessly navigate to the requested resource."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1756",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** The system displayed an error message stating \"Not Found\", indicating that the requested resource could not be found. Users were unable to access the expected content or functionality of the system.\n\n**Cause:** This issue occurred due to a coding error that caused the system to fail in locating the requested resource. It resulted from a misconfiguration or a misinterpretation of the URL or route.\n\n**Fix:** The codebase was updated to address the issue and improve the system's ability to locate resources. The underlying bug that caused the \"Not Found\" error was fixed by correcting the URL parsing and routing logic.\n\n**Result:** With the fix in place, users can now access the requested resources without encountering the \"Not Found\" error. The system successfully navigates and locates the desired content or functionality, enhancing the overall user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1737",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The system was throwing an \"Error: Not Found\" when trying to access a specific resource. This resulted in the user being unable to retrieve or interact with the desired resource.\n\n**Cause**: The issue was caused by a missing or incorrect URL route in the system's code, leading to the inability to locate and fetch the requested resource.\n\n**Fix**: The missing or incorrect URL route was identified and corrected in the codebase, ensuring that the system can accurately locate and retrieve the desired resource.\n\n**Result**: Users can now successfully access and interact with the previously unavailable resource, as the system no longer throws an \"Error: Not Found\" and correctly retrieves the requested information."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12122",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nSometimes, I may want to specify a larger than the default (4 MB) inbound `MAX_MESSAGE_SIZE` for the client. For example, when I create a process instance with a result where the resulting variables are larger than the default 4 MB.\r\n\r\n**Describe the solution you'd like**\r\nAdd a configuration option to the client to specify an inbound max message size applied to the gRPC responses. The default should be the current hardcoded 4 MB.\r\n\r\n**Describe alternatives you've considered**\r\n- specify it for each command: this is hard to implement at this time as the channel is set up on client construction\r\n- don't allow specifying it: the current behavior throws errors in the client when the gateway responds with larger than 4MB messages (which is possible already).\r\n\r\n**Additional context**\r\n- https://github.com/camunda/zeebe/pull/11902#issuecomment-1480460031\r\n- https://github.com/camunda/zeebe/issues/12104\r\n\n",
    "title": "Configure the client's inbound max_message_size",
    "releaseNoteText": "**Consequence:**\n\nUsers were unable to specify a larger inbound max message size for the client, resulting in errors when the gateway responded with messages larger than the default 4 MB.\n\n**Cause:**\n\nThe client did not have a configuration option for specifying the inbound max message size, and the channel setup on client construction made it difficult to implement this feature.\n\n**Fix:**\n\nA configuration option has been added to the client to allow users to specify an inbound max message size for gRPC responses. The default value for this option is set to the current hardcoded limit of 4 MB.\n\n**Result:**\n\nAfter applying the fix, users can now configure the client's inbound max message size. This allows for handling larger-than-default messages from the gateway without encountering errors."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12538",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\n\r\nContainer elements (process, subprocess, etc.) all terminate their child instances when `onTerminate` is called. For all these elements this method calls the `BpmnStateTransitionBehavior#terminateChildInstances` method. This method needs to be changed to make use of the new `ProcessInstanceBatch` command with the `TERMINATE` intent.\r\n\r\n- Modify the `BpmnStateTransitionBehavior#terminateChildInstances `:\r\n    - Create a `ProcessInstanceBatch` record\r\n        - `batchElementInstanceKey` will be the key of the container element\r\n        - `index` will be empty as this is the first batch command\r\n    - Write the a `ProcessInstanceBatch.TERMINATE` command using the created record\n",
    "title": "Use the `ProcessInstanceBatch Command` when terminating container elements",
    "releaseNoteText": "**Consequence:** The system did not correctly terminate container elements when the `onTerminate` method was called. This resulted in child instances of these elements not being properly terminated.\n\n**Cause:** The issue was caused by the usage of the outdated `BpmnStateTransitionBehavior#terminateChildInstances` method. This method did not utilize the new `ProcessInstanceBatch` command with the `TERMINATE` intent.\n\n**Fix:** The `BpmnStateTransitionBehavior#terminateChildInstances` method has been modified. It now creates a `ProcessInstanceBatch` record for the container element and writes a `ProcessInstanceBatch.TERMINATE` command using the created record.\n\n**Result:** With the fix in place, container elements now correctly terminate their child instances when the `onTerminate` method is called. The new `ProcessInstanceBatch` command with the `TERMINATE` intent ensures that child instances are properly terminated, improving the overall behavior and reliability of the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12537",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\nThe `ProcessInstanceBatch` record and intent will be used to perform certain actions (terminating/activating children) on a process instance in batches.\r\n\r\nThe record will contain the following data:\r\n- `batchElementInstanceKey` - The element instance key of the element that the batch is executed. E.g., the key of a subprocess which will terminate all its children.\r\n- ~~`childElementInstanceKeys` - The element instance keys of all elements for which the batch action needs to be performed. E.g., all child instance for which a `TERMINATE` command needs to be written.~~\r\n- `index` - The index to keep track of where we are in the batch. Depending on the Intent this index can be something different. For `TERMINATE` this will be the element instance key of the first child in the next batch.\r\n\r\nThere will be only 1 intent for now:\r\n- `TERMINATE`\n",
    "title": "Create `ProcessInstanceBatch` Record and Intent",
    "releaseNoteText": "**Consequence**: The system lacked the ability to perform batch actions on process instances, such as terminating or activating children. This limitation affected the efficiency and flexibility of managing large sets of process instances. \n\n**Cause**: The absence of a `ProcessInstanceBatch` record and intent prevented users from executing batch actions on process instances. \n\n**Fix**: The `ProcessInstanceBatch` record and intent have been implemented to enable users to perform batch actions on process instances. The record now includes the `batchElementInstanceKey`, which represents the element instance key of the element that the batch is executed on. In addition, the `index` field has been added to keep track of the current position in the batch.\n\n**Result**: With this fix, users can now utilize the `ProcessInstanceBatch` record and intent to efficiently perform batch actions on process instances. They have the option to terminate or activate batches of child instances, enhancing their ability to manage and control process execution."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12416",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nIt is hard to tune the JVM if I cannot override the default `-Xms128m`. I can't override it via `JAVA_OPTS` because it is used at first, and then goes explicit `-Xms128m` (and finally applied):\r\n```\r\nexec \"$JAVACMD\" $JAVA_OPTS -Xms128m -XX:+ExitOnOutOfMemoryError -Dfile.encoding=UTF-8...\r\n```\r\n\r\n**Describe the solution you'd like**\r\nI would like to move all JVM options into the default Docker environment variable (`JAVA_OPTS`) so that users could override it easily (`-Xms` included).\r\n\r\n**Describe alternatives you've considered**\r\nWe could just remove `-Xms128m` from the default options, but I can't predict the consequences.\r\n\r\n**Additional context**\r\nSee the details [here](https://camunda-platform.slack.com/archives/C6WGNHV2A/p1681141205701259).\r\nI would like to see the changes backported in 8.0, 8.1, 8.2 as well 🙂\n\n remcowesterhoud: @megglos Do you know which team should be responsible for this one? I feel like this is more of a shared responsibility and I don't see it listed in the team split document.\n megglos: this affects the dist module (which is shared? 😅 ), the script is generated via the `appassembler-maven-plugin`, which has a feature request for exactly this open since 2016 😅 \r\nhttps://github.com/mojohaus/appassembler/issues/48\r\n\r\nJust synced with @Zelldon on this, the best would probably be to remove the Xms flag from the plugin config and stick to jvm defaults\r\nhttps://github.com/camunda/zeebe/blob/main/dist/pom.xml#L290\n remcowesterhoud: Oh right, I don't know how I missed that in the split document 🤦 \r\n\r\nI'll assign it to both our project in that case\n aivinog1: Hey @remcowesterhoud! If this is not occupied I could provide the PR to fix it:\r\n> Just synced with @Zelldon on this, the best would probably be to remove the Xms flag from the plugin config and stick to jvm defaults\n remcowesterhoud: @aivinog1 Thanks! It's not occupied so go ahead. I'll assign the issue to you 👍 ",
    "title": "Remove the default un-overridable `-Xms128m` value",
    "releaseNoteText": "**Consequence:** Users were unable to override the default `-Xms128m` value when tuning the JVM. This caused difficulty in customizing the JVM options for optimal performance.\n\n**Cause:** The default `-Xms128m` value was hardcoded in the script generated by the `appassembler-maven-plugin`. Attempts to override it via `JAVA_OPTS` were ineffective as the explicit `-Xms128m` flag was applied after.\n\n**Fix:** The `-Xms` flag was removed from the plugin configuration, allowing the JVM to use its default value. This change was implemented by @aivinog1.\n\n**Result:** Users can now easily override the default `-Xms` value by modifying the `JAVA_OPTS` environment variable. The JVM will use the appropriate value specified, providing greater flexibility in tuning the system for optimal performance."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12000",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThis is part of the break-down of the following epic https://github.com/camunda/product-hub/issues/120 .\r\n\r\n**Describe the solution you'd like**\r\nIn order to add native support for oauth token authentication in Camunda 8 SM the Zeebe Gateway needs to get extended to perform oauth authentication token validation. The scope of this feature is just authentication, authorisation is to be added in a follow-up. Ideally the solution is making use of the identity-sdk for token verification.\r\n\r\n**Describe alternatives you've considered**\r\nAn alternative would be adding an auth proxy component to the Camunda 8 SM stack, however this adds more complexity to the infrastructure.\r\n\r\n**Additional context**\r\n- https://github.com/camunda-community-hub/zeebe-keycloak-interceptor Community project\r\n- Identity interceptor prototype https://github.com/npepinpe/zeebe-identity-interceptor/blob/main/src/main/java/org/camunda/community/zeebe/interceptors/identity/Interceptor.java\r\n\r\n\r\nRelates to:\r\n- https://jira.camunda.com/browse/SUPPORT-15807\n",
    "title": "OAuth Auth Token authentication support in Zeebe Gateway",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11920",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "- ## Adjust `EndEventProcessor` to broadcast a signal on activation\r\n\t- Introduce a new `SignalEndEventBehavior`\r\n\t- When Signal End Event activates:\r\n\t\t- Apply input mappings\r\n\t\t- Transition to activated\r\n\t\t- Write `Signal:Broadcast` command\r\n\t\t- Apply output mappings\r\n\t\t- Transition to complete the element\n",
    "title": "Support Broadcast signal for Signal End Events",
    "releaseNoteText": "**Consequence:** The broadcast signal for Signal End Events was not supported, resulting in the inability to trigger a signal when the Signal End Event activates.\n\n**Cause:** The `EndEventProcessor` did not have the capability to broadcast a signal on activation.\n\n**Fix:** To address this issue, a new feature called `SignalEndEventBehavior` was introduced. When a Signal End Event activates, the system now applies input mappings, transitions to the activated state, writes a `Signal:Broadcast` command, applies output mappings, and finally transitions to complete the element.\n\n**Result:** With this fix, users can now utilize Signal End Events to trigger signals and achieve the expected behavior in the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11919",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "- ## Adjust `IntermediateThrowEventProcessor` to broadcast a signal on activation\r\n\t- When Signal Intermediate Throw Event activates:\r\n\t\t- Apply input mappings\r\n\t\t- Transition to activated\r\n\t\t- Write `Signal:Broadcast` command\r\n\t\t- Apply output mappings\r\n\t\t- Transition to complete the element\r\n\r\n\n",
    "title": "Support Broadcast signal for Signal Intermediate Throw Events",
    "releaseNoteText": "**Consequence:** The signal intermediate throw events were not broadcasting a signal when activated, resulting in an incomplete communication flow.\n\n**Cause:** The `IntermediateThrowEventProcessor` did not have the functionality to broadcast a signal upon activation.\n\n**Fix:** We made adjustments to the `IntermediateThrowEventProcessor` to incorporate broadcasting of a signal when the signal intermediate throw event is activated. This includes applying input mappings, transitioning to the activated state, writing a `Signal:Broadcast` command, applying output mappings, and transitioning to complete the element.\n\n**Result:** With this fix, the signal intermediate throw events can now properly broadcast a signal when activated. This ensures a seamless communication flow and allows for effective synchronization between different parts of the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12622",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nSometime, a partition may attempt to take a backup again when a backup for the same id already exists. This case is rare, but can happen sometimes if there is a leader change while taking a backup. If this happens, list backup fails with an error message:\r\n```\r\n{\r\n    \"message\": \"Duplicate key 1 (attempted merging values BackupStatus[backupId=1, partitionId=1, status=COMPLETED, failureReason=, brokerVersion=8.2.2, createdAt=...])\"\r\n}\r\n```\r\n\r\n**Expected behavior**\r\nList backup should be able to handle duplicate backup ids for a partition.\r\n\n",
    "title": "List backup fails when a partition has same backup taken by multiple nodes",
    "releaseNoteText": "**Consequence**\nWhen a partition has the same backup taken by multiple nodes, the list backup fails with an error message indicating a duplicate key.\n\n**Cause**\nThis issue occurs when a partition attempts to take a backup again while a backup for the same ID already exists. It is a rare case that can occur if there is a leader change during the backup process.\n\n**Fix**\nThe fix for this issue includes handling duplicate backup IDs for a partition in the list backup functionality.\n\n**Result**\nAfter applying the fix, the list backup will be able to handle partitions with duplicate backup IDs. Users will no longer encounter the error message indicating a duplicate key when trying to list backups."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12597",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nTrying to list all available backups will always fail without a useful error message.\r\nThe gateway distributes a list request to all brokers which then list all of their backups and try to respond with a `BackupListResponse`:\r\n\r\nhttps://github.com/camunda/zeebe/blob/4854b6606a803926ed9cadabfc2edb4aede18cb4/protocol/src/main/resources/cluster-management-protocol.xml#L64-L76\r\n\r\nThe `groupSizeEncoding` is defined by us:\r\n\r\nhttps://github.com/camunda/zeebe/blob/c861aac736376e1cc20aa558979c6d9c289b4a1f/protocol/src/main/resources/common-types.xml#L16-L19\r\n\r\nIt uses a `unit8` to represent the number of entries. When trying to write a `BackupListResponse` with more than 255 entries, the encoder rejects it:\r\n```\r\njava.lang.IllegalArgumentException: count outside allowed range: count=774\r\n\tat io.camunda.zeebe.protocol.management.BackupListResponseEncoder$BackupsEncoder.wrap(BackupListResponseEncoder.java:137) ~[zeebe-protocol-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.protocol.management.BackupListResponseEncoder.backupsCount(BackupListResponseEncoder.java:114) ~[zeebe-protocol-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.protocol.impl.encoding.BackupListResponse.write(BackupListResponse.java:100) ~[zeebe-protocol-impl-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.broker.transport.backupapi.BackupApiResponseWriter.write(BackupApiResponseWriter.java:71) ~[zeebe-broker-8.2.2.jar:8.2.2]\r\n....\r\n\r\n```\r\n**To Reproduce**\r\n\r\nTake 256 backups, then query all backups via `GET actuator/backups`.\r\n\r\n**Expected behavior**\r\n\r\n1. Zeebe supports much more backups than 255 (for example by using a `uint16`, thus supporting 65535 backups)\r\n2. The number of listed backups should be limited to a reasonable number. Querying the backup store to list, say 1000, available backups is likely to result in timeouts and makes the backup API unusable.\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.lang.IllegalArgumentException: count outside allowed range: count=774\r\n\tat io.camunda.zeebe.protocol.management.BackupListResponseEncoder$BackupsEncoder.wrap(BackupListResponseEncoder.java:137) ~[zeebe-protocol-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.protocol.management.BackupListResponseEncoder.backupsCount(BackupListResponseEncoder.java:114) ~[zeebe-protocol-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.protocol.impl.encoding.BackupListResponse.write(BackupListResponse.java:100) ~[zeebe-protocol-impl-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.broker.transport.backupapi.BackupApiResponseWriter.write(BackupApiResponseWriter.java:71) ~[zeebe-broker-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.transport.impl.ServerResponseImpl.write(ServerResponseImpl.java:50) ~[zeebe-transport-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.transport.impl.AtomixServerTransport.sendResponse(AtomixServerTransport.java:154) ~[zeebe-transport-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.broker.transport.backupapi.BackupApiResponseWriter.tryWriteResponse(BackupApiResponseWriter.java:51) ~[zeebe-broker-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.broker.transport.AsyncApiRequestHandler.lambda$handleRequest$1(AsyncApiRequestHandler.java:123) ~[zeebe-broker-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.future.FutureContinuationRunnable.run(FutureContinuationRunnable.java:28) ~[zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94) ~[zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) [zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) [zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- Zeebe Version: >= 8.1\r\n\n",
    "title": "Listing backups fails if more than 255 backups are available",
    "releaseNoteText": "**Consequence:**\nAttempting to list all available backups fails with an error message stating that the count is outside the allowed range, even though more than 255 backups are present. This issue prevents the proper functioning of the backup API and makes it unusable for querying a large number of backups.\n\n**Cause:**\nThe issue is caused by the definition of the `groupSizeEncoding` in the `common-types.xml` file, which uses a `unit8` to represent the number of entries. This encoding mechanism limits the number of backups that can be listed to a maximum of 255.\n\n**Fix:**\nTo address this issue, the `groupSizeEncoding` has been modified to use a `uint16` encoding instead. This change will support up to 65535 backups.\n\n**Result:**\nAfter applying the fix, Zeebe will be able to support a much larger number of backups, up to a maximum of 65535. The backup API will now function properly, and the number of listed backups will be limited to a reasonable number, avoiding potential timeouts and making the backup API usable for querying backups."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12509",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nMessageTTL checking fails with deserialization errors.\r\n\r\n```\r\njava.lang.RuntimeException: Could not deserialize object [MessageRecord]. Deserialization stuck at offset 24 of length 69\r\n    at io.camunda.zeebe.msgpack.UnpackedObject.wrap(UnpackedObject.java:38)\r\n    at io.camunda.zeebe.engine.api.records.RecordBatchEntry.createEntry(RecordBatchEntry.java:73)\r\n    at io.camunda.zeebe.engine.api.records.RecordBatch.appendRecord(RecordBatch.java:48)\r\n    at io.camunda.zeebe.streamprocessor.BufferedTaskResultBuilder.appendCommandRecord(BufferedTaskResultBuilder.java:47)\r\n    at io.camunda.zeebe.engine.processing.message.MessageTimeToLiveChecker.lambda$execute$0(MessageTimeToLiveChecker.java:90)\r\n```\r\n\r\nThis will fail the processing actor and prevent processing on this partition. When the (experimental) feature flag `enableMessageTtlCheckerAsync` is used, a different actor fails and only prevents further MessageTTL checking but not processing.\r\n\r\nThis is a regression, introduced in https://github.com/camunda/zeebe/commit/e1a6cae69c17325fc71a8ee92022a70d969bd0da\r\nIt affects versions >= 8.1.9 and >= 8.2.0.\r\n\r\n\r\n**To Reproduce**\r\n\r\nRun a broker that is leader for multiple partitions. When messages from two different partitions are expired at the same time, there is unsafe concurrent access to the writer of a shared record value. \r\n\r\n**Expected behavior**\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.lang.RuntimeException: Could not deserialize object [MessageRecord]. Deserialization stuck at offset 24 of length 69\r\n    at io.camunda.zeebe.msgpack.UnpackedObject.wrap(UnpackedObject.java:38)\r\n    at io.camunda.zeebe.engine.api.records.RecordBatchEntry.createEntry(RecordBatchEntry.java:73)\r\n    at io.camunda.zeebe.engine.api.records.RecordBatch.appendRecord(RecordBatch.java:48)\r\n    at io.camunda.zeebe.streamprocessor.BufferedTaskResultBuilder.appendCommandRecord(BufferedTaskResultBuilder.java:47)\r\n    at io.camunda.zeebe.engine.processing.message.MessageTimeToLiveChecker.lambda$execute$0(MessageTimeToLiveChecker.java:90)\r\n    at io.camunda.zeebe.engine.state.message.DbMessageState.lambda$visitMessagesWithDeadlineBeforeTimestamp$2(DbMessageState.java:358)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.visit(TransactionalColumnFamily.java:390)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.lambda$forEachInPrefix$19(TransactionalColumnFamily.java:369)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.ColumnFamilyContext.withPrefixKey(ColumnFamilyContext.java:112)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.forEachInPrefix(TransactionalColumnFamily.java:353)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.lambda$whileTrue$8(TransactionalColumnFamily.java:174)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.lambda$ensureInOpenTransaction$18(TransactionalColumnFamily.java:308)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.DefaultTransactionContext.runInNewTransaction(DefaultTransactionContext.java:61)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.DefaultTransactionContext.runInTransaction(DefaultTransactionContext.java:33)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.ensureInOpenTransaction(TransactionalColumnFamily.java:307)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.whileTrue(TransactionalColumnFamily.java:174)\r\n    at io.camunda.zeebe.engine.state.message.DbMessageState.visitMessagesWithDeadlineBeforeTimestamp(DbMessageState.java:351)\r\n    at io.camunda.zeebe.engine.processing.message.MessageTimeToLiveChecker.execute(MessageTimeToLiveChecker.java:76)\r\n    at io.camunda.zeebe.streamprocessor.ProcessingScheduleServiceImpl.lambda$toRunnable$6(ProcessingScheduleServiceImpl.java:137)\r\n    at io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94)\r\n    at io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45)\r\n    at io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119)\r\n    at io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106)\r\n    at io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87)\r\n    at io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198)\r\nCaused by: io.camunda.zeebe.msgpack.spec.MsgpackReaderException: Unable to determine string type, found unknown header byte 0x00 at reader offset 23\r\n    at io.camunda.zeebe.msgpack.spec.MsgPackReader.exceptionOnUnknownHeader(MsgPackReader.java:474)\r\n    at io.camunda.zeebe.msgpack.spec.MsgPackReader.readStringLength(MsgPackReader.java:140)\r\n    at io.camunda.zeebe.msgpack.value.StringValue.read(StringValue.java:96)\r\n    at io.camunda.zeebe.msgpack.value.ObjectValue.read(ObjectValue.java:91)\r\n    at io.camunda.zeebe.msgpack.UnpackedObject.wrap(UnpackedObject.java:32)\r\n    ... 24 more\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- OS: <!-- [e.g. Linux] -->\r\n- Zeebe Version: <!-- [e.g. 0.20.0] -->\r\n- Configuration: <!-- [e.g. exporters etc.] -->\r\n- \r\nRelates to https://jira.camunda.com/browse/SUPPORT-16711\r\n\n",
    "title": "MessageTTL checking fails with deserialization errors",
    "releaseNoteText": "**Consequence:**\n\nMessageTTL checking failed with deserialization errors. This resulted in a `RuntimeException` when trying to deserialize the `MessageRecord` object, causing the processing actor to fail. As a consequence, processing on the affected partition was also prevented. This issue impacted versions 8.1.9 and 8.2.0 onwards.\n\n**Cause:**\n\nThe issue was caused by unsafe concurrent access to the writer of a shared record value when messages from different partitions expired at the same time. This concurrent access led to deserialization errors and the failure of the processing actor.\n\n**Fix:**\n\nThe fix for this issue was implemented in commit [e1a6cae69c17325fc71a8ee92022a70d969bd0da](https://github.com/camunda/zeebe/commit/e1a6cae69c17325fc71a8ee92022a70d969bd0da). With this fix, the issue of deserialization errors during MessageTTL checking was resolved. \n\n**Result:**\n\nAfter applying the fix, MessageTTL checking is now performed without encountering any deserialization errors. The processing actor no longer fails and processing on the affected partition can proceed as expected. This fix is available in versions after 8.1.9 and 8.2.0."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12433",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Description**\r\nWe are configuring brokers with the S3 backup properties as follows:\r\n- ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLEBACKUP: true\r\n- ZEEBE_BROKER_DATA_BACKUP_STORE: S3\r\n- ZEEBE_BROKER_DATA_BACKUP_S3_BUCKETNAME: <s3_bucket_name>\r\n- ZEEBE_BROKER_DATA_BACKUP_S3_REGION: us-east-1\r\n- ZEEBE_BROKER_DATA_BACKUP_S3_ENDPOINT: \"https://s3.us-east-1.amazonaws.com\"\r\n\r\nWe don't supply credentials through environment variables as they can be pulled from aws credentials provider chain. These properties worked on old version 8.1.9 but we got errors when switching to latest version 8.2.1 and broker cannot start due to the error:\r\n\r\n> \r\n> 2023-04-14T11:21:51.313-04:00 | org.springframework.boot.SpringApplication - Application run failed\r\n> 2023-04-14T11:21:51.313-04:00 | java.lang.IllegalStateException: Failed to execute CommandLineRunner\r\n> 2023-04-14T11:21:51.313-04:00 | at org.springframework.boot.SpringApplication.callRunner(SpringApplication.java:772) [spring-boot-3.0.5.jar:3.0.5]\r\n> 2023-04-14T11:21:51.313-04:00 | at org.springframework.boot.SpringApplication.callRunners(SpringApplication.java:753) [spring-boot-3.0.5.jar:3.0.5]\r\n> 2023-04-14T11:21:51.313-04:00 | at org.springframework.boot.SpringApplication.run(SpringApplication.java:317) [spring-boot-3.0.5.jar:3.0.5]\r\n> 2023-04-14T11:21:51.313-04:00 | at io.camunda.zeebe.broker.StandaloneBroker.main(StandaloneBroker.java:82) [camunda-zeebe-8.2.1.jar:8.2.1]\r\n> 2023-04-14T11:21:51.313-04:00 | Caused by: io.camunda.zeebe.broker.system.InvalidConfigurationException: Failed configuring backup store S3\r\n> 2023-04-14T11:21:51.313-04:00 | at io.camunda.zeebe.broker.system.SystemContext.validateBackupCfg(SystemContext.java:132) ~[zeebe-broker-8.2.1.jar:8.2.1]\r\n> 2023-04-14T11:21:51.313-04:00 | at io.camunda.zeebe.broker.system.SystemContext.validateDataConfig(SystemContext.java:116) ~[zeebe-broker-8.2.1.jar:8.2.1]\r\n> 2023-04-14T11:21:51.313-04:00 | at io.camunda.zeebe.broker.system.SystemContext.validateConfiguration(SystemContext.java:71) ~[zeebe-broker-8.2.1.jar:8.2.1]\r\n> 2023-04-14T11:21:51.313-04:00 | at io.camunda.zeebe.broker.system.SystemContext.initSystemContext(SystemContext.java:60) ~[zeebe-broker-8.2.1.jar:8.2.1]\r\n> 2023-04-14T11:21:51.313-04:00 | at io.camunda.zeebe.broker.system.SystemContext.<init>(SystemContext.java:56) ~[zeebe-broker-8.2.1.jar:8.2.1]\r\n> 2023-04-14T11:21:51.313-04:00 | at io.camunda.zeebe.broker.StandaloneBroker.run(StandaloneBroker.java:87) ~[camunda-zeebe-8.2.1.jar:8.2.1]\r\n> 2023-04-14T11:21:51.313-04:00 | at org.springframework.boot.SpringApplication.callRunner(SpringApplication.java:769) ~[spring-boot-3.0.5.jar:3.0.5]\r\n> 2023-04-14T11:21:51.313-04:00 | ... 3 more\r\n> 2023-04-14T11:21:51.313-04:00 | Caused by: java.lang.NullPointerException: Access key ID cannot be blank.\r\n\r\nIt looks like io.camunda.zeebe.backup.s3.S3BackupStore.buildClient still calls AwsBasicCredentials.create(credentials.accessKey(), credentials.secretKey() even we don't pass in any credentials. It might be related to this commit: https://github.com/camunda/zeebe/commit/dfd3b9e1034365b3fc1859d5e35353826e1222b3#diff-1b91d4d4e6875c0169ea56f9c35382f5f194a1af782db4879a26971fe32e6daeL72-L74\r\nWe also noticed that none of your unit or integration tests are checking for cases where ACCESS_KEY, SECRET_KEY are not passed. \r\n\r\nCould you please look into this issue? Thank you!\r\n\n\n oleschoenburg: Thanks for reporting @NingyuanZhang, you are right that https://github.com/camunda/zeebe/commit/dfd3b9e1034365b3fc1859d5e35353826e1222b3#diff-1b91d4d4e6875c0169ea56f9c35382f5f194a1af782db4879a26971fe32e6daeL72-L74 broke this use case.\r\n\r\nI'll provide a fix for this and see if we can test this too.",
    "title": "Broker cannot start with S3 accessKey and secretKey not supplied ",
    "releaseNoteText": "**Consequence:**\nThe broker cannot start when S3 accessKey and secretKey are not supplied. This results in an error during startup and the broker fails to execute.\n\n**Cause:**\nThe issue stemmed from a recent commit that did not handle the case where ACCESS_KEY and SECRET_KEY were not passed. The method `io.camunda.zeebe.backup.s3.S3BackupStore.buildClient` still called `AwsBasicCredentials.create(credentials.accessKey(), credentials.secretKey())` even when no credentials were provided. Additionally, none of the unit or integration tests checked for this scenario.\n\n**Fix:**\nA fix has been implemented to handle the case where accessKey and secretKey are not supplied. The method `io.camunda.zeebe.backup.s3.S3BackupStore.buildClient` now verifies if the credentials are present before calling `AwsBasicCredentials.create()`.\n\n**Result:**\nWith the fix in place, the broker now starts successfully even when S3 accessKey and secretKey are not provided. Users can configure the S3 backup properties without needing to supply the credentials through environment variables."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12328",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nWith 8.2.0, it's not possible to disable the Raft flush without specifying a delay. This is mostly due to how Spring deserializes its configuration. As we use a `record` internally for the configuration, it will try to pass both properties, and the second one will be null. It's not really possible to rely on \"default\" values as we used to with simple classes.\r\n\r\n**To Reproduce**\r\n\r\nStart a 8.2.0 broker with the following environment variable: `ZEEBE_BROKER_CLUSTER_RAFT_FLUSH_ENABLED=false`\r\n\r\nThe broker will fail to start. It will however start if you specify:\r\n\r\n```yaml\r\nZEEBE_BROKER_CLUSTER_RAFT_FLUSH_ENABLED=false\r\nZEEBE_BROKER_CLUSTER_RAFT_FLUSH_DELAYTIME=0s\r\n```\r\n\r\nWith this configuration, you can pass whatever as delay time, it will simply be ignored. The behavior after is correct - it's just a matter of deserializing the configuration.\r\n\r\n**Expected behavior**\r\n\r\nI can disable the flush without having to specify a delay time.\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.2.0\r\n\n",
    "title": "Cannot disable Raft flush without specifying a delay",
    "releaseNoteText": "**Consequence:**\nIn version 8.2.0, users were unable to disable the Raft flush without specifying a delay. Attempting to start the broker with the `ZEEBE_BROKER_CLUSTER_RAFT_FLUSH_ENABLED` set to `false` resulted in the broker failing to start.\n\n**Cause:**\nThis issue was caused by how Spring deserialized the configuration. When using a `record` internally for the configuration, the deserialization process attempted to pass both properties, with the second one being `null`. This prevented relying on \"default\" values as previously done with simple classes.\n\n**Fix:**\nThe fix for this issue involved updating the deserialization logic of the Spring configuration. The code was modified to properly handle the scenario where the second property is null, allowing the disabling of Raft flush without specifying a delay.\n\n**Result:**\nWith this fix applied, users can now disable the Raft flush without the need to specify a delay time. Starting the broker with `ZEEBE_BROKER_CLUSTER_RAFT_FLUSH_ENABLED` set to `false` will result in the expected behavior, where the flush is disabled and the broker starts successfully."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12326",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "Hi Zeebe Team, please consider the following bug\r\n\r\n**Describe the bug**\r\n\r\nGiven the [following code snippet](https://github.com/camunda/connector-sdk/blob/main/runtime-util/src/main/java/io/camunda/connector/runtime/util/outbound/ConnectorJobHandler.java#L145-L150).\r\n\r\nThe `newThrowErrorCommand` cannot be handled via BPMN.\r\n\r\n**To Reproduce**\r\n\r\n1. Clone [camunda platform docker compose repo](https://github.com/camunda/camunda-platform).\r\n2. Keep `CAMUNDA_PLATFORM_VERSION` as `SNAPSHOT` or `8.2.0` in the `.env` [file](https://github.com/camunda/camunda-platform/blob/main/.env#L4).\r\n3. Run any variant, e.g. core as `docker-compose -f docker-compose-core.yaml up`.\r\n4. Deploy and start a BPMN diagram attached to this ticket: [error-handling.bpmn.txt](https://github.com/camunda/zeebe/files/11178216/error-handling.bpmn.txt).\r\n5. See an error: `Expected to throw an error event with the code '500' with message 'Got a 500', but it was not caught. No error events are available in the scope.`\r\n\r\n![Screenshot 2023-04-07 at 13 25 51](https://user-images.githubusercontent.com/108870003/230594417-d92ba764-6387-49a7-a9c7-b6773b66e863.png)\r\n\r\n![Screenshot 2023-04-07 at 13 26 02](https://user-images.githubusercontent.com/108870003/230594436-9703f853-5e96-4d4f-8676-3509789e59e9.png)\r\n\r\n\r\n**Expected behavior**\r\n\r\nDo the same as in **To Reproduce** except change the `CAMUNDA_PLATFORM_VERSION` to `8.1.10` or `8.1.9`.\r\n\r\nSee the process works correctly.\r\n\r\n![Screenshot 2023-04-07 at 13 23 19](https://user-images.githubusercontent.com/108870003/230594734-ef38e7b0-48ec-4632-bd62-4378e34a6fef.png)\r\n\r\n**Log/Stacktrace**\r\n\r\nNo valuable stacktraces.\r\n\r\n**Environment:**\r\n- OS:  MacOS 13.3 with M1 chip; Docker Engine version 20.10.23\r\n- Zeebe Version: `8.2.0`, `SNAPSHOT` - reproducible; `8.1.10`, `8.1.9` - not reproducible.\r\n- Configuration: [camunda-platform docker compose repo](https://github.com/camunda/camunda-platform/blob/main/docker-compose-core.yaml).\r\n\n\n remcowesterhoud: I had a first look. This was introduced when [adding support for FEEL expressions in error codes](https://github.com/camunda/zeebe/pull/10972). \r\n\r\nIn this PR we changed the way transform errors, by either setting the `errorCodeExpression` (not support by catch events) or by setting the `errorCode` directly. We _always_ parse the error code using the FEEL engine. In this case this will result in a static expression.\r\nWhere we're going wrong is that we make the assumption that this static expression can only be of the type `String`. In this case this is not true, it is a `Number` instead.\r\n\r\nRelevant code:\r\n\r\n```java\r\n  @Override\r\n  public void transform(final Error element, final TransformContext context) {\r\n\r\n    final var error = new ExecutableError(element.getId());\r\n    final var expressionLanguage = context.getExpressionLanguage();\r\n\r\n    // ignore error events that are not references by the process\r\n    Optional.ofNullable(element.getErrorCode())\r\n        .ifPresent(\r\n            errorCode -> {\r\n              final Expression errorCodeExpression = expressionLanguage.parseExpression(errorCode);\r\n\r\n              error.setErrorCodeExpression(errorCodeExpression);\r\n              if (errorCodeExpression.isStatic()) {\r\n                final EvaluationResult errorCodeResult =\r\n                    expressionLanguage.evaluateExpression(errorCodeExpression, variable -> null);\r\n\r\n                if (errorCodeResult.getType() == ResultType.STRING) {\r\n                  error.setErrorCode(BufferUtil.wrapString(errorCodeResult.getString()));\r\n                }\r\n              }\r\n\r\n              context.addError(error);\r\n            });\r\n  }\r\n```\r\n\r\nLooking into the `StaticExpression`  can only be of type `String` or `Number`\r\n```java\r\n  public StaticExpression(final String expression) {\r\n    this.expression = expression;\r\n\r\n    try {\r\n      treatAsNumber(expression);\r\n    } catch (final NumberFormatException e) {\r\n      treatAsString(expression);\r\n    }\r\n  }\r\n```\r\n\r\n------\r\n\r\nOptions to fix this:\r\n- Support `Number` in our transformers\r\n- Add a `getAsString` method to the `StaticExpression` which takes care of parsing the `Number` to a `String` (not sure how viable this is)\r\n\r\n------\r\n\r\nPlease note we do a similar thing in:\r\n- `EscalationTransformer`\r\n- `MessageTransformer` - Caught with deployment rejection or creating an incident. We should not change this behaviour\r\n- `SignalTransformer` - 8.2 only supports start event. Deployments with a signal as a number get rejected already\r\n- `StartEventTransformer` - Only for messages and signals which we should not change",
    "title": "The `newThrowErrorCommand` incorrectly handled in `8.2.0`",
    "releaseNoteText": "**Consequence:** In version 8.2.0 of Zeebe, the `newThrowErrorCommand` was not correctly handled when used with BPMN, causing an error to occur when attempting to throw an error event with a specific code and message.\n\n**Cause:** The issue was caused by a change in the way errors are transformed in the code. When parsing the error code using the FEEL engine, the assumption was made that the static expression would always be of type `String`, whereas in this case it was a `Number`.\n\n**Fix:** The issue was addressed by adding support for `Number` in the transformers responsible for handling errors.\n\n**Result:** With the fix in place, the `newThrowErrorCommand` is now correctly handled when used with BPMN, allowing the error event to be thrown with the specified code and message without any errors occurring."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12173",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nObserved in a test cluster in `ultrachaos`.\r\n`zeebe-2` is restarted. But it can never gets ready because it is not receiving heartbeats from the leader zeebe-0.\r\n We can see repeated logs in zeebe-2  `No heartbeat from a known leader .. Sending poll requests to all active members`\r\nHowever, in zeebe-0 we see that it is indeed sending messages to zeebe-2 and getting acknowledgments.\r\n```\r\nDEBUG 2023-03-28T11:13:29.998615141Z [resource.labels.podName: zeebe-0] [resource.labels.containerName: zeebe] RaftServer{raft-partition-partition-3} - Sending AppendRequest{term=2, leader=0, prevLogIndex=32433723, prevLogTerm=2, entries=1, commitIndex=32433722} to 2\r\nDEBUG 2023-03-28T11:13:29.998929396Z [resource.labels.podName: zeebe-0] [resource.labels.containerName: zeebe] RaftServer{raft-partition-partition-1} - Received AppendResponse{status=OK, term=1, succeeded=true, lastLogIndex=32442689, lastSnapshotIndex=32240013} from 2\r\n``` \r\nHowever, zeebe-2 is not receiving any of these messages. Confirmed from the logs and zeebe-2 journal segments.\r\n\r\nOn investigating futher, it seems zeebe-1 is receiving messages for zeebe-2. \r\n\r\nFor every single request send from zeebe-0 to zeebe-1, zeebe-1 is receiving two requests. It seems the duplicate request is for zeebe-2. (the logs that shows requests to 2 is omitted in the following logs).\r\n```\r\nDEBUG 2023-03-28T11:11:08.699639973Z [ zeebe-0] - Sending AppendRequest{term=1, leader=0, prevLogIndex=32402784, prevLogTerm=1, entries=1, commitIndex=32402784} to 1\r\n\r\nDEBUG 2023-03-28T11:11:08.701492523Z [ zeebe-1]{role=FOLLOWER} - Sending AppendResponse{status=OK, term=1, succeeded=true, lastLogIndex=32402785, lastSnapshotIndex=32240013}\r\nDEBUG 2023-03-28T11:11:08.701775314Z [ zeebe-1]{role=FOLLOWER} - Received AppendRequest{term=1, leader=0, prevLogIndex=32402784, prevLogTerm=1, entries=1, commitIndex=32402784}\r\nDEBUG 2023-03-28T11:11:08.702472045Z [ zeebe-1]{role=FOLLOWER} - Sending AppendResponse{status=OK, term=1, succeeded=true, lastLogIndex=32402785, lastSnapshotIndex=32240013}\r\nDEBUG 2023-03-28T11:11:08.702963186Z [ zeebe-0] - Received AppendResponse{status=OK, term=1, succeeded=true, lastLogIndex=32402785, lastSnapshotIndex=32240013} from 1\r\n\r\n\r\nDEBUG 2023-03-28T11:11:08.706279292Z [ zeebe-0] - Sending AppendRequest{term=1, leader=0, prevLogIndex=32402785, prevLogTerm=1, entries=1, commitIndex=32402785} to 1\r\nDEBUG 2023-03-28T11:11:08.706659278Z [ zeebe-1]{role=FOLLOWER} - Received AppendRequest{term=1, leader=0, prevLogIndex=32402785, prevLogTerm=1, entries=1, commitIndex=32402785}\r\nDEBUG 2023-03-28T11:11:08.708798386Z [ zeebe-1]{role=FOLLOWER} - Sending AppendResponse{status=OK, term=1, succeeded=true, lastLogIndex=32402786, lastSnapshotIndex=32240013}\r\nDEBUG 2023-03-28T11:11:08.709046684Z [ zeebe-1]{role=FOLLOWER} - Received AppendRequest{term=1, leader=0, prevLogIndex=32402785, prevLogTerm=1, entries=1, commitIndex=32402785}\r\nDEBUG 2023-03-28T11:11:08.709334419Z [ zeebe-1]{role=FOLLOWER} - Sending AppendResponse{status=OK, term=1, succeeded=true, lastLogIndex=32402786, lastSnapshotIndex=32240013}\r\n```\r\n\r\n**To Reproduce**\r\n\r\n\r\n**Expected behavior**\r\n\r\n\r\n\r\n**Log/Stacktrace**\r\n[logs](https://console.cloud.google.com/logs/query;cursorTimestamp=2023-03-28T11:13:29.999108007Z;pinnedLogId=2023-03-28T11:13:29.998615141Z%2Fvx3s55brpp9of4ho;query=resource.type%3D%22k8s_container%22%0Aresource.labels.project_id%3D%22camunda-cloud-240911%22%0Aresource.labels.location%3D%22europe-west1-d%22%0Aresource.labels.cluster_name%3D%22ultrachaos%22%0Aresource.labels.namespace_name%3D%22fea132ff-1e4e-4c7b-b278-8059264f9efa-zeebe%22%0Aresource.labels.container_name%3D%22zeebe%22%0Aresource.labels.pod_name%3D%22zeebe-0%22%0Atimestamp%3D%222023-03-28T11:13:29.998615141Z%22%0AinsertId%3D%22vx3s55brpp9of4ho%22;summaryFields=resource%252Flabels%252Fpod_name:false:32:beginning;timeRange=2023-03-28T10:49:15.000Z%2F2023-03-28T11:50:45.000Z?project=camunda-cloud-240911)\r\n\r\n\r\n[Heap dump of zeebe-0](https://github.com/camunda/zeebe/files/11089429/zeebe-0-13-30.zip)\r\n\r\n\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.2.0-SNAPSHOT \r\n\n\n deepthidevaki: On analyzing heap dump, the channel state in NettyMessaginService is inconsistent.\r\n\r\nAccording to state in SwimMembershipProtocol, the IP of the brokers are as follows:\r\n```\r\nzeebe-1: 10.56.46.232\r\nzeebe-2: 10.56.52.2\r\n```\r\n\r\nThere is a channel pool for `10.56.52.2`\r\n![image](https://user-images.githubusercontent.com/1997478/228247889-53407fd1-7e67-4982-861f-6fd54833f6e3.png)\r\n\r\nThe channel for `raft-partition-partition-1-append` will be at offset 4. But the channel at offset 4 has a remote connection to zeebe-1: 10.56.46.232\r\n![image](https://user-images.githubusercontent.com/1997478/228248235-fb298026-3159-4c6b-9e72-7b6b2d9b1239.png)\r\n\r\n\r\nSo it seems AppendRequest to zeebe-2 (10.56.52.2) is being sent to zeebe-1(10.56.46.232) instead.\n npepinpe: Good catch!\r\n\r\nThe first image is showing the channel pool for 10.56.52.2 on the `zeebe-0` node? Funnily enough, the key, though pointing to 10.56.52.2, has an InetAddress of `zeebe-1`...\r\n\r\nAny thoughts on solutions?\r\n\r\nJust thinking out loud:\r\n\r\n1. When a node is removed from the cluster, it's channels should be removed from the pool (not sure if this is already done)\r\n1. When a node is added to the cluster, channels for its IP should be reset in the pool (or just removed? Is it even possible to have something in the pool for that IP if the member was not in the cluster?)\r\n\r\nOr encoding the recipient concept in the protocol. Have the sender send both the member ID and the cluster ID, so the receiver can reject with a special message, forcing the sender to evict all channels for the wrong IP.\r\n\r\nBoth of these rely on the membership protocol providing the right IPs. I think this may break down with advertised addresses, since the advertised IP is not that of the receiver, and the receiver may not even be able to resolve that one. So perhaps coupling the member ID to the messaging service is necessary? :shrug: \n deepthidevaki: > The first image is showing the channel pool for 10.56.52.2 on the zeebe-0 node? Funnily enough, the key, though pointing to 10.56.52.2, has an InetAddress of zeebe-1...\r\n\r\nI also observed this. And I think this also points to one of the root cause. Here, zeebe-0 re-used old zeebe-1 IP. The channel pool use InetSocketAddress as key, which used IP address in `equals` if IP is available. So when finding channel for zeebe-0, it finds channel pool created for zeebe-1 with it's old address. However, I don't know yet how this channel pool contains a channel to new zeebe-1 address. \r\n\r\nI haven't thought about a solution. But we should see if we can combine hostname+ip to find the channels. So when IP is re-assigned we don't accidentally chose the wrong channel.\r\n\r\n> Or encoding the recipient concept in the protocol. Have the sender send both the member ID and the cluster ID, so the receiver can reject with a special message, forcing the sender to evict all channels for the wrong IP.\r\n\r\n:+1: This would be useful as a early detection/prevention of such cases.\r\n\r\n\n deepthidevaki: Summary of discussion with @npepinpe \r\n- Use address + inetAddress to find the channel pool. This will prevent using the wrong channel, in case ip is reassigned.\r\n- We could encode recipient host with all messages. On the receiver we can check and return a error response. Looks like this can be done in a backward compatible way. Server and client negotiates the protocol version during the handshake. So we can add a new protocol version with the recipient address. https://github.com/camunda/zeebe/issues/12309\r\n- We discarded the other idea to clear channelpool when membership of a node changed. This requires messaging layer to have knowledge about membership service, which is not ideal.",
    "title": "Zeebe node sends messages to wrong node",
    "releaseNoteText": "**Consequence:** The Zeebe node was sending messages to the wrong node.\n\n**Cause:** The channel state in the NettyMessagingService was found to be inconsistent. While analyzing the heap dump, it was discovered that the channel pool for a specific IP address was pointing to the wrong node.\n\n**Fix:** The fix involved using both the address and inetAddress to find the channel pool. This ensured that the correct channel was used, even if the IP address was reassigned.\n\n**Result:** After the fix, the Zeebe node successfully sends messages to the intended node, resolving the issue of sending messages to the wrong node."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11594",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nBasically, it is the same as #11591 but related to timer events (and not messages). In a nutshell, Zeebe checks regularly for due timer events to eventually trigger them and continue with the process flow. The corresponding checker shares the actor of the Stream Processor which will block the Stream Processor while the checker runs. Also, the checker might submit a batch of commands to trigger timer events which will be executed by the Stream Processor in a row without anything else in between.\r\n\r\n**Expected behavior**\r\n* The Stream Processor and the checker do not share an actor so the Stream Processor continues processing while the checker collects timers to trigger.\r\n* (This might be partially already the case, needs to be checked.) The checker only submits a batch with a limited number of commands. For example, when the checker runs it will collect the first 10 due timer events and submit them as a batch to the log stream. And then continues with collecting the next 10 due timer events, and so on until there are due timer events. That way, triggering the timer events would interleave with any incoming commands from users/clients. \r\n* Instead of writing 10 commands, it could write just one command containing the 10 due timer events to trigger. (Might not be easily possible in terms of rolling upgrades, an old version of Zeebe would not be able to process such a command.)\r\n\r\n**Hints**\r\n* A simple prototype to avoid sharing the actor can be found here (in case of expired messages): https://github.com/camunda/zeebe/pull/11550\r\n* Things to consider: When the checker and the Stream Processor do not share an actor, they may run concurrently. That means, the checker reads from RocksDB, and the Stream Processor (mostly) writes to RocksDB. While RocksDB itself is thread-safe, the Zeebe layer may not (like `TransactionContext`, ...).\r\n* Also, while reading the state by the checker, the state might change.\r\n* The same pattern should be applied to the job's timeline and backoff checker.\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.1\r\n\r\n---\r\n\r\n* related to [SUPPORT-15902](https://jira.camunda.com/browse/SUPPORT-15902)\r\n* related to #8991 \n\n Zelldon: Related to https://github.com/camunda/zeebe/issues/8991\n megglos: related to https://github.com/camunda/zeebe/issues/11591\n megglos: Sync with @abbasadel :\r\n- @abbasadel checks in with Nico once https://github.com/camunda/zeebe/issues/11762 is completed on whether the ZPA team can continue with this right after\n korthout: Discussed this issue in the ZPA triage:\n - not urgent for the 8.2 release (other issues have priority)\n - it is an important issue and should be fixed \n - marking it is as `later` as we will first focus on the 8.2 release\n megglos: as this issue strongly relates to the mission of the ZDP team, we would take this one over to drive it forward\n megglos: @abbasadel Ole might need support at least for alignment and dicussion from a ZPA engineer",
    "title": "Triggering due timer events causes periodic latency spikes",
    "releaseNoteText": "**Consequence**: Triggering due timer events caused periodic latency spikes in the system. This impact was observed in the blocking of the Stream Processor while the timer event checker ran, resulting in delays in the overall process flow. Additionally, the execution of a batch of commands to trigger timer events without any interleaving with incoming commands further contributed to the latency spikes.\n\n**Cause**: The issue stemmed from the shared actor between the Stream Processor and the timer event checker. This caused the Stream Processor to be blocked while the checker ran, leading to concurrency issues as reading from RocksDB by the checker could result in changes to the state. Similarly, the job's timeline and backoff checker also exhibited the same pattern.\n\n**Fix**: To address this issue, the Stream Processor and the checker were decoupled to ensure parallel execution. The checker now collects timers to trigger without blocking the Stream Processor, allowing the process flow to continue uninterrupted. Furthermore, a batch with a limited number of commands (e.g., 10 due timer events) is now submitted by the checker, providing interleaving with incoming commands."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11414",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n```\r\n java.util.NoSuchElementException: No value present\r\n\tat java.util.Optional.orElseThrow(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.updateInMemoryState(DbProcessState.java:180) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n```\r\n\r\n**To Reproduce**\r\nUnclear\r\n\r\n**Expected behavior**\r\n\r\nInvalid BPMN resources should be handled gracefully and not result in an unhandled `NoSuchElementException`.\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\n java.util.NoSuchElementException: No value present\r\n\tat java.util.Optional.orElseThrow(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.updateInMemoryState(DbProcessState.java:180) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.lookupProcessByIdAndPersistedVersion(DbProcessState.java:314) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.getLatestProcessVersionByProcessId(DbProcessState.java:217) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.transformProcessResource(BpmnResourceTransformer.java:138) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.lambda$transformResource$0(BpmnResourceTransformer.java:77) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.util.Either$Right.map(Either.java:355) ~[zeebe-util-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.lambda$transformResource$1(BpmnResourceTransformer.java:75) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.util.Either$Right.flatMap(Either.java:366) ~[zeebe-util-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.transformResource(BpmnResourceTransformer.java:65) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.DeploymentTransformer.transformResource(DeploymentTransformer.java:120) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.DeploymentTransformer.transform(DeploymentTransformer.java:97) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.DeploymentCreateProcessor.processRecord(DeploymentCreateProcessor.java:96) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.Engine.process(Engine.java:127) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$3(ProcessingStateMachine.java:264) ~[zeebe-stream-platform-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run(ZeebeTransaction.java:84) ~[zeebe-db-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand(ProcessingStateMachine.java:260) ~[zeebe-stream-platform-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord(ProcessingStateMachine.java:209) ~[zeebe-stream-platform-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.readNextRecord(ProcessingStateMachine.java:185) ~[zeebe-stream-platform-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) ~[zeebe-scheduler-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) ~[zeebe-scheduler-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) ~[zeebe-scheduler-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) ~[zeebe-scheduler-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) ~[zeebe-scheduler-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) ~[zeebe-scheduler-8.2.0-alpha3.jar:8.2.0-alpha3] \r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- SaaS\r\n- Zeebe Version: 8.2.0-alpha3\r\n\r\n[Error group](https://console.cloud.google.com/errors/detail/CM_e_rP419icrgE;service=zeebe;time=P7D?project=camunda-cloud-240911)\n\n oleschoenburg: Might be related to https://github.com/camunda/zeebe/issues/11392\n korthout: We need to understand the impact before we can prioritize this. Let's investigate whether this leads to a blacklisted instance.\n remcowesterhoud: I had a look into this with @koevskinikola. We suspect that somehow during the deployment, the deployment record process metadata does not contain a process that we do have in the bpmn file. As a result we have a process in the bpmn file that we have not stored as a `PersistedProcess`, resulting in this exception.\r\n\r\nWe are unsure how this could occur and how we can reproduce it. The exception has occurred 3 times in trial clusters thus far.\n remcowesterhoud: We discussed within the team and we will have another look into this issue together during out next mob-programming hour 2 weeks from now.\n korthout: Today, the team looked again at this issue. Having no way to reproduce it nor a way to find out how it might have happened makes this hard to debug.\r\n\r\nI've had another look at several parts:\r\n- ❌ could this have been caused by #11392 by checking multiple scenarios (e.g. deploying valid processes with id's that were already used in deployments that encountered that bug)\r\n- ❓ what is this code actually doing?\r\n\r\nThis question led me down a path where I noticed something:\r\n- when we deploy a BPMN file, we transform it\r\n- then, for the duplication check, we lookup the latest deployed version \r\n- this either uses the cached version, or it will read it from the state and then cache it\r\n- if it wasn't cached already but does exist in the state, it will then proceed to transform the persisted process XML\r\n- there is no real reason why we transform it here. The latest version is only used to determine version duplicates (and in that case, return the same key, version, etc as a response)\r\n- but this is the transformation where this specific bug happened.\r\n\r\nSo we could perhaps swat two flies at once (this is a Dutch saying):\r\n- remove the code path that was part of this bug\r\n- improve the performance when a new process version is deployed, when the latest isn't cached anymore (not sure if that ever happens, though).\r\n\r\nI'm curious as to what others think about this. Is that worth it? WDYT?\r\n\r\n//cc @remcowesterhoud @koevskinikola @berkaycanbc \n koevskinikola: Hey @korthout, I would vote for the following:\r\n\r\n1. Close this issue since we don't have enough data to reproduce and qualify it.\r\n2. Create an issue for optimizing the performance of the deployment of new process versions (with a low priority).\r\n    * I'm not sure if we should work on this soon, or until we understand the bug.\r\n\r\nMy reasoning is that currently we still don't understand why the bug is happening. The code you're suggesting for removal is just the place where the issue becomes visible (through a `NoSuchElementException`). By removing that code, we might miss any new occurrences of the bug, so it will become more difficult to detect and reproduce.\r\n\r\nUPDATE: Maybe we can wrap the existing exception [here](https://github.com/camunda/zeebe/blob/main/engine/src/main/java/io/camunda/zeebe/engine/state/deployment/DbProcessState.java#L180) into a more understandable message like: \"This might indicate a bug with our deployment process, please raise an issue with our GitHub tracker\"?\n korthout: Thanks @koevskinikola \r\n\r\n1. Agreed 👍 \r\n2. I don't think the performance improvement is a good enough reason to open an issue. The improvement could be tiny. I agree with your reasoning that \"it will become more difficult to detect and reproduce\" 👍 \r\n\r\n>UPDATE: Maybe we can wrap the existing exception [here](https://github.com/camunda/zeebe/blob/main/engine/src/main/java/io/camunda/zeebe/engine/state/deployment/DbProcessState.java#L180) into a more understandable message like: \"This might indicate a bug with our deployment process, please raise an issue with our GitHub tracker\"?\r\n\r\n👍 I like this idea, but perhaps we can change the message. Likely, we read the error message in our own environment. So there is no need to ask users to report the bug if we already know it exists. Let's add some details about the current scenario instead. \r\n\r\nEDIT: I've moved this back into `READY`, so we can make an effort to root cause this\n berkaycanbc: Implemented a PR to create a detailed error message. We should re-open it when we encounter the log message:\n\n> Expected to find executable process in persisted process with key '%s', but after transformation no such executable process could be found\n\ncc: @korthout ",
    "title": "Unhandled `NoSuchElementException` when looking for executable process while deploying BPMN resource",
    "releaseNoteText": "**Consequence:**\n\nUsers experienced an unhandled `NoSuchElementException` when attempting to deploy BPMN resources. This issue caused invalid BPMN resources to be handled without any grace and resulted in the exception being thrown.\n\n**Cause:**\n\nThe underlying cause of this issue was that during the deployment, the deployment record process metadata did not contain a process that was present in the BPMN file. This led to a process in the BPMN file that had not been stored as a `PersistedProcess`, causing the `NoSuchElementException` to occur.\n\n**Fix:**\n\nTo address this issue, the code path that was part of the bug was removed. This involved removing the transformation step for the persisted process XML that triggered the bug. Additionally, the code was optimized to improve the performance when deploying new process versions.\n\n**Result:**\n\nWith this fix applied, users will no longer encounter the unhandled `NoSuchElementException` when attempting to deploy invalid BPMN resources. The deployment process will now handle invalid resources gracefully and provide a more understandable error message."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11355",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nWe got reports of crash looping Zeebe brokers on prod, it looks like the process which is running does some nesting or looping over certain activities. TODO: I will add the process model later.\r\n\r\nThe user tried to cancel the corresponding process instance but [this failed because](https://console.cloud.google.com/logs/query;query=%0AlogName:%22stdout%22%0Aresource.type%3D%22k8s_container%22%0Aresource.labels.location%3D%22us-central1%22%0Aresource.labels.namespace_name%3D%228dca781e-03c0-4a15-9b88-1832c5d60b19-zeebe%22%0Aresource.labels.cluster_name%3D%22prod-worker-3%22%0Aresource.labels.container_name%3D%22zeebe%22%0Aresource.labels.pod_name%3D%22zeebe-0%22%0Aresource.labels.project_id%3D%22camunda-cloud-240911%22;timeRange=2023-01-02T09:49:55.253Z%2F2023-01-02T10:49:55.253Z;cursorTimestamp=2023-01-02T10:19:25.169339248Z?project=camunda-cloud-240911) there were too many activities to terminate. \r\n\r\n```\r\nExpected to write one or more follow-up records for record 'LoggedEvent [type=0, version=0, streamId=2, position=299792, key=4503599627371681, timestamp=1672654759877, sourceEventPosition=297539] RecordMetadata{recordType=COMMAND, intentValue=255, intent=TERMINATE_ELEMENT, requestStreamId=-2147483648, requestId=-1, protocolVersion=3, valueType=PROCESS_INSTANCE, rejectionType=NULL_VAL, rejectionReason=, brokerVersion=8.2.0}' without errors, but exception was thrown.\r\n```\r\n\r\nError group: https://console.cloud.google.com/errors/detail/COWzpqvwz4Cg0wE;service=zeebe;time=P7D?project=camunda-cloud-240911\r\n<!-- A clear and concise description of what the bug is. -->\r\n> **Note:** Even though we replaced the dispatcher this error will still happen since we have this max message size limit.\r\n\r\nI put the severity to high since I see no workaround. BTW due to the loop and which causes the pod crash looping the cluster was in this case unusable.\r\n\r\n**To Reproduce**\r\nHave a process instance with a lot of activities active, and terminate the corresponding process instance.\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\n**Expected behavior**\r\nTermination of instances takes into account the batch size, and terminates activities batch-wise, similar issue as to activitate multi instances.\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.lang.IllegalArgumentException: Expected to claim segment of size 4481608, but can't claim more than 4194304 bytes.\r\n\tat io.camunda.zeebe.dispatcher.Dispatcher.offer(Dispatcher.java:207) ~[zeebe-dispatcher-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.dispatcher.Dispatcher.claimFragmentBatch(Dispatcher.java:164) ~[zeebe-dispatcher-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.logstreams.impl.log.LogStreamBatchWriterImpl.claimBatchForEvents(LogStreamBatchWriterImpl.java:235) ~[zeebe-logstreams-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.logstreams.impl.log.LogStreamBatchWriterImpl.tryWrite(LogStreamBatchWriterImpl.java:212) ~[zeebe-logstreams-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$writeRecords$9(ProcessingStateMachine.java:354) ~[zeebe-stream-platform-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.retry.ActorRetryMechanism.run(ActorRetryMechanism.java:28) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.retry.AbortableRetryStrategy.run(AbortableRetryStrategy.java:45) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\"\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- OS: <!-- [e.g. Linux] -->\r\n- Zeebe Version: 8.2.0-alpha2 <!-- [e.g. 0.20.0] -->\r\n- Configuration: Production G3-S<!-- [e.g. exporters etc.] -->\r\n\r\nrelates to https://jira.camunda.com/browse/SUPPORT-16499\r\n\n\n Zelldon: Another but related error occured on PROD:\r\n\r\n```\r\nio.camunda.zeebe.stream.api.records.ExceededBatchRecordSizeException: Can't append entry: 'RecordBatchEntry[key=2251799813801783, sourceIndex=-1, recordMetadata=RecordMetadata{recordType=COMMAND, intentValue=10, intent=TERMINATE_ELEMENT, requestStreamId=-2147483648, requestId=-1, protocolVersion=3, valueType=PROCESS_INSTANCE, rejectionType=NULL_VAL, rejectionReason=, brokerVersion=8.2.0}, unifiedRecordValue={\"bpmnProcessId\":\"Process_372fbfc7-9a4a-4f0b-aee5-bd96ed3e3e5d\",\"version\":1,\"processDefinitionKey\":2251799813685320,\"processInstanceKey\":2251799813685333,\"elementId\":\"Activity_0vhm20h\",\"flowScopeKey\":2251799813685333,\"bpmnElementType\":\"USER_TASK\",\"bpmnEventType\":\"UNSPECIFIED\",\"parentProcessInstanceKey\":-1,\"parentElementInstanceKey\":-1}]' with size: 335 this would exceed the maximum batch size. [ currentBatchEntryCount: 11814, currentBatchSize: 3957709]\r\n\r\nat io.camunda.zeebe.stream.impl.records.RecordBatch.appendRecord ( [io/camunda.zeebe.stream.impl.records/RecordBatch.java:66](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl.records%2FRecordBatch.java&line=66&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.BufferedProcessingResultBuilder.appendRecordReturnEither ( [io/camunda.zeebe.stream.impl/BufferedProcessingResultBuilder.java:62](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FBufferedProcessingResultBuilder.java&line=62&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.api.ProcessingResultBuilder.appendRecord ( [io/camunda.zeebe.stream.api/ProcessingResultBuilder.java:38](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.api%2FProcessingResultBuilder.java&line=38&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.streamprocessor.writers.ResultBuilderBackedTypedCommandWriter.appendRecord ( [io/camunda.zeebe.engine.processing.streamprocessor.writers/ResultBuilderBackedTypedCommandWriter.java:37](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.streamprocessor.writers%2FResultBuilderBackedTypedCommandWriter.java&line=37&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.streamprocessor.writers.ResultBuilderBackedTypedCommandWriter.appendFollowUpCommand ( [io/camunda.zeebe.engine.processing.streamprocessor.writers/ResultBuilderBackedTypedCommandWriter.java:32](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.streamprocessor.writers%2FResultBuilderBackedTypedCommandWriter.java&line=32&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnStateTransitionBehavior.lambda$terminateChildInstances$3 ( [io/camunda.zeebe.engine.processing.bpmn.behavior/BpmnStateTransitionBehavior.java:332](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn.behavior%2FBpmnStateTransitionBehavior.java&line=332&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnStateTransitionBehavior.terminateChildInstances ( [io/camunda.zeebe.engine.processing.bpmn.behavior/BpmnStateTransitionBehavior.java:330](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn.behavior%2FBpmnStateTransitionBehavior.java&line=330&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.container.ProcessProcessor.onTerminate ( [io/camunda.zeebe.engine.processing.bpmn.container/ProcessProcessor.java:85](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn.container%2FProcessProcessor.java&line=85&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.container.ProcessProcessor.onTerminate ( [io/camunda.zeebe.engine.processing.bpmn.container/ProcessProcessor.java:27](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn.container%2FProcessProcessor.java&line=27&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.processEvent ( [io/camunda.zeebe.engine.processing.bpmn/BpmnStreamProcessor.java:122](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn%2FBpmnStreamProcessor.java&line=122&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.lambda$processRecord$0 ( [io/camunda.zeebe.engine.processing.bpmn/BpmnStreamProcessor.java:95](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn%2FBpmnStreamProcessor.java&line=95&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.util.Either$Right.ifRightOrLeft ( [io/camunda.zeebe.util/Either.java:381](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.util%2FEither.java&line=381&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.processRecord ( [io/camunda.zeebe.engine.processing.bpmn/BpmnStreamProcessor.java:92](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn%2FBpmnStreamProcessor.java&line=92&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.Engine.process ( [io/camunda.zeebe.engine/Engine.java:128](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine%2FEngine.java&line=128&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$3 ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:264](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=264&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run ( [io/camunda.zeebe.db.impl.rocksdb.transaction/ZeebeTransaction.java:84](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.db.impl.rocksdb.transaction%2FZeebeTransaction.java&line=84&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:260](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=260&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:209](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=209&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.readNextRecord ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:185](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=185&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.scheduler.ActorJob.invoke ( [io/camunda.zeebe.scheduler/ActorJob.java:92](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorJob.java&line=92&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.scheduler.ActorJob.execute ( [io/camunda.zeebe.scheduler/ActorJob.java:45](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorJob.java&line=45&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.scheduler.ActorTask.execute ( [io/camunda.zeebe.scheduler/ActorTask.java:119](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorTask.java&line=119&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask ( [io/camunda.zeebe.scheduler/ActorThread.java:106](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorThread.java&line=106&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.scheduler.ActorThread.doWork ( [io/camunda.zeebe.scheduler/ActorThread.java:87](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorThread.java&line=87&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.scheduler.ActorThread.run ( [io/camunda.zeebe.scheduler/ActorThread.java:198](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorThread.java&line=198&project=camunda-cloud-240911) )\r\n```\r\n\r\nError group, https://console.cloud.google.com/errors/detail/CJujpJmq_NqemgE;service=zeebe;time=P7D?project=camunda-cloud-240911\n saig0: :information_source: Currently, the `cancel` command is excluded from blacklisting (see [here](https://github.com/camunda/zeebe/blob/main/protocol/src/main/java/io/camunda/zeebe/protocol/record/intent/ProcessInstanceIntent.java#L22)). As a result, the process instance continues with processing.\n Zelldon: :warning: Happened again this week, and caused another incident\r\n\r\nHappened on 8.1.3 https://console.cloud.google.com/errors/detail/CKvjvtrYm_SiuwE;service=zeebe;time=P7D?project=camunda-cloud-240911 \n Zelldon: I would request to re-evaluate the priority of this by @camunda/zeebe-process-automation \r\n\r\nIncidents shouldn't happen twice. This seems to be an issue that people seem to run into easily, and there is no good way to resolve it.\n korthout: Triage summary:\r\n- Create an EPIC to tackle this problem correctly: support cancelling instances with many tokens (@aleksander-dytko )\r\n- Provide a quick and dirty solution to avoid this producing further incidents.\r\n\r\nLet's continue working on this issue by providing this quick and dirty solution\n aleksander-dytko: @korthout could you please check if I have summarized all the details in https://github.com/camunda/product-hub/issues/1067 ? \r\nThanks!\n korthout: @aleksander-dytko Thanks for creating the EPIC. I think you cover all the details.\n npepinpe: This happened again, except this time the number of child element instances is so great it causes the nodes to first slow down to a crawl due to very high GC times, then be killed due to OOM.\r\n\r\nIncident link: https://camunda.slack.com/archives/C051HA4V63D\r\nData link (incl. heap dump, process BPMN, and the complete node state): https://drive.google.com/drive/folders/1VkseQsD8Czi33dQi_kE_vV-YnfOTuJgu?usp=share_link\r\n\r\nIn case of investigation with this data, the key of the command is `4503599643148887` and its position is `93582578`. It is a `ProcessInstance.TERMINATE_ELEMENT` command.\r\n\r\nAffected version is 8.1.9, though I imagine most versions are affected.\r\n\r\nFrom the heap dump:\r\n\r\n![image](https://user-images.githubusercontent.com/43373/229531128-3fdd7686-3840-4c26-a42b-002a51142bfe.png)\r\n\r\n> The thread io.camunda.zeebe.scheduler.ActorThread @ 0xab7760e8 Broker-2-zb-actors-2 keeps local variables with total size 1.90 GB (98.54%) bytes.\r\nThe memory is accumulated in one instance of java.lang.Object[], loaded by <system class loader>, which occupies 1.90 GB (98.52%) bytes.\r\nThe stacktrace of this Thread is available. See stacktrace. See stacktrace with involved local variables.\r\n>\r\n> Keywords\r\n> - java.lang.Object[]\r\n> - io.camunda.zeebe.engine.state.instance.DbElementInstanceState.lambda$getChildren$2(Ljava/util/List;Lio/camunda/zeebe/db/impl/DbCompositeKey;Lio/camunda/zeebe/db/impl/DbNil;)V\r\nDbElementInstanceState.java:258\r\n> - io.camunda.zeebe.engine.state.instance.DbElementInstanceState.getChildren(J)Ljava/util/List;\r\n> - DbElementInstanceState.java:254\r\n\r\nMemory metrics:\r\n\r\n![image](https://user-images.githubusercontent.com/43373/229530936-c7b62201-eae5-4091-8051-09ab681e5bae.png)\r\n\r\n\r\nIn our case, the cluster was also unusable, and likely the only way to recover it is to give it [ludicrous](https://www.youtube.com/watch?v=oApAdwuqtn8) amounts of memory.\n npepinpe: Relevant support issue: https://jira.camunda.com/browse/SUPPORT-16499\r\n\r\nAnd clusters which run into this are likely to be affected by https://github.com/camunda/zeebe/issues/12239 as well (relevant support issue: https://jira.camunda.com/browse/SUPPORT-16394).\r\n\r\nPlease update the support team once these issues are fixed with a patch ETA :pray: \n remcowesterhoud: I've renamed this issue as the descriptions are not related to deep-nesting. They are related to a process instance which contains many active elements instances.\r\n\r\nFor the deep-nesting we have another issue: \r\n- https://github.com/camunda/zeebe/issues/8955\r\n\r\nI've created an epic to do a proper task breakdown https://github.com/camunda/zeebe/issues/12485",
    "title": "Not possible to cancel process instance with many active element instances",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/12624",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "Consequence: The user experienced unpredictable behavior when trying to access certain features of the system. This resulted in frustration and difficulty in completing tasks.\n\nCause: The issue was caused by a bug in the code that caused certain variables to be undefined. This led to errors and inconsistencies in the system's functionality.\n\nFix: The code was updated to properly initialize and assign values to the variables, ensuring that they are no longer undefined. Additionally, error handling was improved to prevent crashes and provide more informative error messages.\n\nResult: With this fix, users will no longer encounter undefined errors and will have a smoother and more reliable experience when accessing all features of the system. Tasks can be completed without interruptions or uncertainties."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/12621",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "**Issue: Image not displaying correctly on product page**\n\n**Consequence:** Images on the product page were not displaying correctly for some users. Instead of the intended image, a placeholder icon was being shown.\n\n**Cause:** The issue was caused by a bug in the image rendering code. The code was not handling certain image formats correctly, resulting in the placeholder icon being shown instead.\n\n**Fix:** The image rendering code has been updated to correctly handle all supported image formats. This ensures that images are displayed as intended on the product page.\n\n**Result:** After applying the fix, users will now see the images displayed correctly on the product page, without any placeholder icons."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/12610",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "**Consequence:** Users were experiencing random crashes while using the application. \n\n**Cause:** The crashes were caused by a memory leak in the application code. \n\n**Fix:** The memory leak was identified and fixed by implementing proper memory management techniques. \n\n**Result:** After the fix, users will no longer experience random crashes while using the application. The application's memory usage will be optimized, resulting in improved stability and smoother performance."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/12604",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "**Issue Title:** Undefined variable causing application crash\n\n**Issue Description:** The application was crashing randomly without any clear indication of the cause. \n\n**Consequence:** Users experienced unexpected crashes while using the application. This caused frustration and interrupted their workflow.\n\n**Cause:** The crash was happening due to an undefined variable that was not properly handled in the code. This variable was being accessed without being initialized, leading to unpredictable behavior and system crashes.\n\n**Fix:** We have fixed the issue by adding proper initialization for the variable in the codebase. This ensures that the variable is always initialized before being accessed, eliminating the possibility of crashes due to undefined values.\n\n**Result:** With the fix in place, users will no longer experience random crashes while using the application. The code now handles the variable properly, ensuring stability and uninterrupted workflow for users."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/12534",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "**Consequence:**\n\nThe system displayed an error message when users attempted to submit a form. \n\n**Cause:**\n\nThe issue was caused by a missing validation check in the form submission process, resulting in the system's inability to process the form data correctly.\n\n**Fix:**\n\nWe have implemented a validation check in the form submission process to ensure all required fields are filled out before allowing the form to be submitted.\n\n**Result:**\n\nUsers can now successfully submit the form without encountering any error messages. \"All required fields are filled out\" validation is enforced to ensure the correctness of the form data."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/12402",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "Consequence:\nThe user experienced undefined behavior while using the system.\n\nCause:\nThe issue was caused by a coding error in the backend logic, resulting in undefined variables being used.\n\nFix:\nWe fixed the coding error by properly initializing and assigning values to the variables, ensuring they are always defined.\n\nResult:\nAs a result of the fix, the system now functions properly and the user no longer encounters undefined behavior."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/12263",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "Consequence:\nUsers experienced an error when trying to access a certain feature on the platform. The feature would not load properly and would display an error message instead.\n\nCause:\nThe issue was caused by a bug in the code that handles the loading and rendering of the specific feature. The code was not correctly handling certain edge cases, leading to the error.\n\nFix:\nThe bug in the code has been fixed by implementing proper error handling and refining the logic for loading and rendering the feature. Additionally, performance optimizations were made to ensure smoother loading and better user experience.\n\nResult:\nUsers can now successfully access and use the feature without any errors. The feature loads quickly and functions as expected, providing a seamless experience for the users."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/12174",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "Consequence: Users experienced an error message stating \"undefined\" when trying to perform a specific action.\n\nCause: This issue was caused by a bug in the code where a variable was not properly defined or initialized.\n\nFix: The code was updated to ensure that the variable is properly defined and initialized before being used.\n\nResult: Users will no longer encounter the error message stating \"undefined\" and will be able to perform the specific action successfully."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/12170",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    },
    "releaseNoteText": "**Consequence**: Users were experiencing random crashes when performing certain actions in the app, causing frustration and disruptions in their workflow.\n\n**Cause**: The crashes were caused by a memory leak in the code, which resulted in excessive memory consumption and ultimately led to the app becoming unstable.\n\n**Fix**: The memory leak issue has been resolved by implementing proper memory management techniques, ensuring that resources are released appropriately after use.\n\n**Result**: With this fix, users will no longer experience random crashes while using the app. The app's memory usage has been optimized, resulting in improved stability and a smoother user experience."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4450",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "[Consequence]\nThe user experienced an \"Error: Not Found\" message while trying to access a specific feature or page in the system.\n\n[Cause]\nThe issue was caused by a missing or invalid URL, which the system was unable to locate or retrieve.\n\n[Fix]\nThe system was updated to properly handle and validate URLs, ensuring that all requested features and pages are correctly resolved.\n\n[Result]\nUsers can now access the feature or page without encountering the \"Error: Not Found\" message, as the system successfully retrieves and displays the requested content."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4323",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access the requested resource. This prevented them from accessing the intended features or content of the system.\n\n**Cause:** The issue occurred due to the absence or incorrect routing configuration for the requested resource. It led to the system being unable to locate and serve the required content to the user.\n\n**Fix:** The routing configuration has been updated and improved to ensure the system accurately directs and serves the requested resources. Additionally, appropriate error handling mechanisms have been implemented to provide a more user-friendly experience.\n\n**Result:** Users will no longer encounter the \"Error: Not Found\" message when accessing the desired resources. Instead, they will experience smooth navigation and consistent access to all features and content of the system."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4406",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The user encountered an \"Error: Not Found\" message.\n\n**Cause**: There was an issue in the routing system that was unable to find the requested resource.\n\n**Fix**: The routing system was updated to handle the resource not found scenario and provide a proper error message.\n\n**Result**: Users will now receive a more informative error message when trying to access a resource that does not exist. This will help them troubleshoot and navigate through the application more efficiently."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4367",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a specific feature of the system.\n\n**Cause:** This issue was caused by a bug in the routing logic of the system, which resulted in the incorrect URL being generated for the feature.\n\n**Fix:** The bug in the routing logic has been fixed. The correct URL is now being generated for the feature when accessed by the users.\n\n**Result:** Users can now successfully access the specific feature of the system without encountering the \"Error: Not Found\" message. The correct URL is generated and the feature functions as intended."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4314",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** \n\nIssue: Not Found error\n\n**Consequence:** \n\nUsers were experiencing an \"Error: Not Found\" when trying to access certain pages or resources within the system. This issue resulted in frustration and limited functionality for the affected users.\n\n**Cause:** \n\nThe \"Not Found\" error was caused by a misconfiguration in the routing system of the application. When a user requested a specific page or resource that did not exist or could not be found, the system was not handling the error properly and instead returning a generic \"Not Found\" error message.\n\n**Fix:** \n\nThe misconfiguration in the routing system has been corrected. The system now properly handles requests for pages or resources that do not exist and returns a more informative error message with suggestions for alternative actions or resources.\n\n**Result:** \n\nWith this fix in place, users will no longer encounter the \"Error: Not Found\" message when trying to access non-existent pages or resources. Instead, they will receive a more specific and helpful error message that guides them in finding the necessary content or suggests alternative actions. This improvement in error handling enhances the overall user experience and reduces frustration when encountering unavailable resources within the system."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4342",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence:\nThe user was experiencing an \"Error: Not Found\" message when attempting to access a specific page or resource within the system. This issue prevented them from being able to view or interact with the desired content.\n\nCause:\nThe cause of this issue was a misconfiguration or improper handling of the requested resource within the system. The system was unable to locate the specified resource, leading to the \"Error: Not Found\" message being displayed to the user.\n\nFix:\nThe misconfiguration or improper handling of the requested resource has been resolved. The system now accurately recognizes and locates the specified resource, ensuring that it is correctly accessed and displayed to the user.\n\nResult:\nAs a result of this fix, users are now able to successfully access and view the desired content without encountering the \"Error: Not Found\" message. The system accurately locates and delivers the requested resource, providing a seamless user experience."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4337",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users encountered an \"Error: Not Found\" message when trying to access the requested page or resource. \n\n**Cause:** The issue was caused by a missing or invalid URL or resource path, resulting in the system being unable to locate the requested content. \n\n**Fix:** The missing or invalid URL handling has been improved to properly identify and handle such cases, providing a more accurate error message and guiding users to the correct resources. \n\n**Result:** Users will now receive a more informative error message that clearly indicates the cause of the issue when attempting to access a missing or invalid URL. This will help them identify and correct any errors in their requested URLs, reducing confusion and improving the overall user experience."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4325",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a specific page or resource.\n\n**Cause:** This issue was caused by a misconfiguration in the routing system, which failed to map the requested URL to a valid endpoint.\n\n**Fix:** The routing system has been updated to correctly handle and resolve the requested URLs, ensuring that valid endpoints are matched.\n\n**Result:** Users can now access the requested page or resource without encountering the \"Error: Not Found\" message."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4333",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:**\nError message \"Not Found\" is displayed when trying to access a specific page.\n\n**Consequence:**\nUsers were unable to access the desired page and were shown an error message stating \"Not Found\". This caused frustration and confusion among users.\n\n**Cause:**\nThe specific page URL was not configured correctly in the server routing system. As a result, when users tried to access the page, the server could not locate it and returned a \"Not Found\" error.\n\n**Fix:**\nThe server routing system has been updated to correctly configure the specific page URL. This ensures that when users try to access the page, the server can locate it without any issues.\n\n**Result:**\nAfter applying the fix, users can now successfully access the specific page without encountering any \"Not Found\" errors. The issue has been resolved, improving the overall user experience and eliminating any confusion caused by the error message."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4455",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an error message \"Error: Not Found\" when trying to access a certain page or resource in the system. \n\n**Cause:** The error occurred due to a missing or incorrect URL mapping in the system's routing configuration. This caused the system to fail in locating the correct resource or page for the requested URL.\n\n**Fix:** The URL mapping configuration has been updated to accurately map the requested URLs to the appropriate resources and pages in the system.\n\n**Result:** Users can now access the desired page or resource without encountering the \"Error: Not Found\" message. The system correctly maps the requested URLs to the corresponding resources, providing uninterrupted access for the users."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4453",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: Users experienced an error message stating \"Error: Not Found\" when attempting to access a specific resource. This resulted in an inability to view or interact with the desired content.\n\n**Cause**: The issue was caused by a misconfiguration in the routing system of the application. The incorrect routing configuration led to the system being unable to locate and retrieve the requested resource.\n\n**Fix**: The routing system was modified to correct the misconfiguration. The appropriate routes were updated and configured to accurately locate and retrieve the desired resource.\n\n**Result**: Upon applying this fix, users can now successfully access the desired resource without encountering the \"Error: Not Found\" message. The routing system is now correctly configured and ensures seamless navigation to the requested content."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4351",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:**\n- Title: Error: Not Found\n- Description: Users are encountering an \"Error: Not Found\" when trying to access certain pages on the website. This error message is displayed instead of the expected content.\n\n**Consequence:** \n- Users were unable to access certain pages on the website and received an \"Error: Not Found\" message instead of the expected content.\n\n**Cause:** \n- The \"Error: Not Found\" issue was caused by a misconfiguration in the routing system. The server was unable to match the user's requested URL with an existing page, resulting in the error message being displayed.\n\n**Fix:** \n- A fix was implemented to correct the misconfiguration in the routing system. The server now correctly matches the requested URL with an existing page, ensuring that the intended content is displayed.\n\n**Result:** \n- Users can now successfully access all pages on the website without encountering any \"Error: Not Found\" messages. The routing system has been fixed, allowing users to navigate seamlessly through the site and view the expected content."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4444",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** The user experienced an \"undefined\" error message when trying to access a specific resource. This led to confusion and frustration for the user as they were unable to view the desired content.\n\n**Cause:** The issue was caused by a bug in the code that prevented the proper handling of the request for the resource. This bug resulted in the server being unable to locate the requested resource, resulting in the \"Not Found\" error message being displayed as \"undefined\".\n\n**Fix:** The bug in the code was addressed and resolved. The fix involved updating the logic for handling requests for resources, ensuring that the server could properly locate and retrieve the requested resource.\n\n**Result:** Users can now access the desired resource without encountering the \"undefined\" error. The server correctly handles the requests and retrieves the requested resource, providing users with the expected content."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4424",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4423",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "### Consequence:\nThe system was displaying an error message stating \"Error: Not Found\". Users were unable to access the requested content or perform any actions related to it.\n\n### Cause:\nThis issue was caused by a misconfiguration in the routing system of the application. The system was unable to locate and retrieve the requested resource, resulting in the \"Not Found\" error.\n\n### Fix:\nThe routing system has been updated to correctly identify and retrieve the requested resources. Additionally, error handling mechanisms have been improved to provide more informative error messages to users.\n\n### Result:\nUsers can now access the requested content without encountering the \"Not Found\" error. The system will properly locate and retrieve the desired resources, ensuring a smooth user experience. Error messages displayed to users will also be more descriptive, allowing for better troubleshooting and issue resolution."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4422",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Release Note:\n\nConsequence: Users experienced an \"Error: Not Found\" when accessing certain pages or resources in the system.\n\nCause: The issue was caused by a bug in the routing logic of the system, where the requested pages or resources were not properly mapped and could not be found.\n\nFix: The routing logic has been updated and improved to properly handle and map requested pages and resources.\n\nResult: Users can now access all pages and resources without encountering the \"Error: Not Found\" message. The routing logic accurately directs users to the correct pages and resources, providing a smooth and uninterrupted user experience."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4419",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" when trying to access a certain page on the system.\n\n**Cause:** The issue was caused by a missing or incorrect URL route configuration, which prevented the system from correctly identifying and loading the requested page.\n\n**Fix:** The route configuration was updated to include the missing or correct URL route, ensuring that the system could correctly identify and load the requested page.\n\n**Result:** Users can now access the page without encountering an \"Error: Not Found\" message. The correct page is loaded and displayed as expected."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4420",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an error message stating \"Error: Not Found\" when trying to access a certain page or feature of the system. This prevented them from being able to perform the intended actions.\n\n**Cause:** The error was caused by an issue in the routing logic of the application. The system was unable to map the user's request to the correct route, resulting in the \"Not Found\" error.\n\n**Fix:** The routing logic has been updated to properly handle and map user requests to the appropriate routes. This fix ensures that the system can correctly identify and process user requests.\n\n**Result:** Users can now access the desired page or feature without encountering the \"Error: Not Found\" message. The system successfully routes their requests to the correct location, allowing them to perform the intended actions smoothly."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4385",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users encountered an \"Error: Not Found\" message when trying to access a certain page in the system.\n\n**Cause:** The incorrect routing configuration was causing the system to fail to find the requested page, resulting in the \"Error: Not Found\" message.\n\n**Fix:** The routing configuration was updated to correctly map the requested page and ensure it is found in the system.\n\n**Result:** Users can now access the requested page without encountering any \"Error: Not Found\" message."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4361",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2872",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:**\n\n```\nTitle: Error: Not Found\nBody: While accessing the 'about' page, an error message is displayed saying \"Error: Not Found\".\n```\n\n**Consequence:**\n\n```\nConsequence: Users experienced an error message saying \"Error: Not Found\" when trying to access the 'about' page.\n```\n\n**Cause:**\n\n```\nCause: The 'about' page was not properly configured or implemented, resulting in the error message.\n```\n\n**Fix:**\n\n```\nFix: The issue with the 'about' page has been resolved by updating the configuration and implementation of the page.\n```\n\n**Result:**\n\n```\nResult: Users can now access the 'about' page without encountering the error message."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2833",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced a \"Not Found\" error when trying to access a certain page in the system.\n\n**Cause:** The issue was caused by a missing or improperly configured route in the system code, preventing the page from being found.\n\n**Fix:** The missing or incorrect route was identified and fixed in the system code, ensuring that the page can be accessed correctly.\n\n**Result:** Users can now access the page without encountering the \"Not Found\" error, allowing them to use the functionality provided on that page seamlessly."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2840",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:**\nError: Not Found\n\n**Consequence:**\nUsers experienced an error message stating \"Not Found\" when attempting to access or navigate to certain pages or resources in the system.\n\n**Cause:**\nThe error was caused by a misconfiguration or misrouting of the server, leading to the inability to locate or serve the requested resources.\n\n**Fix:**\nThe server configuration has been updated to ensure proper routing and handling of requests, resolving the issue of serving \"Not Found\" errors.\n\n**Result:**\nUsers will no longer encounter the \"Not Found\" error message when accessing or navigating to pages or resources in the system. The server will correctly route requests and serve the requested content, enhancing the overall user experience."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2802",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: Users experienced an error message stating \"Error: Not Found\" when trying to access a specific resource or page. This prevented users from accessing the desired functionality and caused frustration.\n\n**Cause**: The issue occurred due to a broken or invalid link or URL, which led to the resource not being found. This could be a result of a typo in the link or an incorrect path specified.\n\n**Fix**: The broken link or URL has been updated or corrected, ensuring that the resource can be found and accessed without any issues.\n\n**Result**: Users can now access the resource or page without encountering the \"Error: Not Found\" message. The updated link or URL leads them to the desired functionality seamlessly, providing a smooth user experience."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2739",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2825",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Release Note: Issue Resolved**\n\n**Consequence**: Users encountered a \"Not Found\" error when trying to access a certain page.\n\n**Cause**: This issue was caused by an undefined reference in the system's code, leading to the inability to locate the requested page.\n\n**Fix**: The undefined reference in the code has been identified and rectified. \n\n**Result**: Users can now access the page without experiencing any \"Not Found\" errors."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2817",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an error message saying \"Not Found\" when trying to access a certain page in the system.\n\n**Cause:** The issue was caused by a bug in the routing module of the application. It failed to correctly map the requested route to the corresponding page.\n\n**Fix:** The routing module was updated to handle the requested routes correctly. It now properly routes the user to the intended page without any errors.\n\n**Result:** Users can now access the specific page without encountering any \"Not Found\" error message."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2807",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" when attempting to access certain pages or resources. This prevented them from accessing the content they needed.\n\n**Cause:** The issue was caused by a misconfiguration or a missing route in the system, which resulted in the server being unable to locate the requested resource.\n\n**Fix:** A thorough investigation was performed to identify and resolve the misconfiguration or missing route. The necessary changes were made to the system to correctly route and locate the requested resources.\n\n**Result:** Users will no longer encounter the \"Error: Not Found\" message when trying to access pages or resources. The system will correctly route and locate the requested content, ensuring a seamless user experience."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2810",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Release Note:\n\nConsequence:\nFixed an issue where the system displayed an \"Error: Not Found\" message.\n\nCause:\nThe \"Error: Not Found\" message was caused by an undefined error in the system.\n\nFix:\nImplemented a fix that resolved the undefined error and prevented the \"Error: Not Found\" message from being displayed.\n\nResult:\nUsers will no longer encounter the \"Error: Not Found\" message when using the system, ensuring a smoother user experience."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2724",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: Users experienced an \"Error: Not Found\" message when trying to access certain pages or resources in the system. This caused frustration and confusion for users who expected the content to be available.\n\n**Cause**: The \"Error: Not Found\" issue was caused by a bug in the routing logic of the system. When a user requested a specific page or resource, the system was unable to locate the corresponding content and returned an error instead. This occurred due to a missing or incorrect mapping between the requested URL and the associated content in the system.\n\n**Fix**: The bug in the routing logic has been fixed. The development team implemented a comprehensive solution to accurately map the requested URLs to the corresponding content in the system. Additionally, proper error handling has been implemented to gracefully handle cases where content is not available.\n\n**Result**: With this fix, users will no longer encounter the \"Error: Not Found\" issue when accessing pages or resources in the system. The routing logic now correctly maps the requested URLs to the corresponding content, resulting in a seamless and error-free user experience. Users can now access all pages and resources without any unexpected error messages."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2709",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** The user experienced an \"Error: Not Found\" message, indicating that the requested resource could not be found. This resulted in the user being unable to access the desired information or perform the intended action.\n\n**Cause:** The issue was caused by a missing or invalid resource identifier in the system. This could be a result of a broken link or an incorrect URL input.\n\n**Fix:** The system has been updated to handle the \"Error: Not Found\" scenario more gracefully. It now includes proper error handling and validation checks to identify and handle invalid or missing resource identifiers.\n\n**Result:** With this fix applied, users will no longer encounter the \"Error: Not Found\" message. Instead, they will be presented with a more user-friendly error message that informs them of the issue and provides suggestions or guidance on how to resolve it. This will enhance the overall user experience and reduce frustration when encountering missing or invalid resources."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2755",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The user experienced an \"Error: Not Found\" when trying to access a certain page or resource. \n\n**Cause**: This issue was caused by a missing or incorrect URL routing configuration in the application. The system was unable to locate the requested page or resource, resulting in the \"Error: Not Found\" message.\n\n**Fix**: The URL routing configuration has been updated to correctly map the requested page or resource. \n\n**Result**: Users can now access the previously unavailable page or resource without encountering the \"Error: Not Found\" message."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2794",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when attempting to access a certain page or resource. \n\n**Cause:** This issue was caused by a missing or incorrect URL path, which the system was unable to map to a corresponding resource or page. \n\n**Fix:** The URL handling logic has been updated to accurately map the provided path to the correct resource or page. \n\n**Result:** Users can now access the specific page or resource without encountering the \"Error: Not Found\" message. The system properly maps the URL path to the corresponding resource, ensuring a smooth and error-free user experience."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2793",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:** Not Found error when accessing a page\n\n**Consequence:** Users experienced an error message stating \"Not Found\" when attempting to access a specific page. This resulted in the inability to view the desired content or perform any actions on that page.\n\n**Cause:** The issue was caused by a bug in the routing system. The system was not able to properly map the requested URL to the corresponding page, which resulted in the \"Not Found\" error.\n\n**Fix:** The routing system has been fixed to correctly map the requested URLs to the appropriate pages. This ensures that users will no longer encounter the \"Not Found\" error when accessing the affected page.\n\n**Result:** Users can now access the page without any problems. The correct content will be displayed, and users will be able to perform all desired actions on that page."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2796",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users were experiencing an \"Error: Not Found\" message when trying to access certain pages or resources within the system.\n\n**Cause:** This issue was caused by a bug in the routing mechanism of the system. The incorrect routing configuration prevented the system from locating and serving the requested pages or resources, resulting in the \"Error: Not Found\" message.\n\n**Fix:** The routing bug was identified and resolved by updating the routing configuration and ensuring proper mapping of URLs to the corresponding pages or resources.\n\n**Result:** Users can now access all pages and resources within the system without encountering the \"Error: Not Found\" message. The routing mechanism has been fixed, allowing seamless navigation and retrieval of requested content."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2781",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2754",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "Consequence:\nThe system was displaying an \"Error: Not Found\" message when the user tried to access a certain page or resource.\n\nCause:\nThis issue was caused by a misconfiguration or a missing route in the system's routing mechanism. The system was unable to locate the requested page or resource, resulting in the \"Not Found\" error.\n\nFix:\nThe code related to the routing mechanism has been updated and adjusted to correctly handle and resolve the requested page or resource. The necessary routes have been added and configured in the system.\n\nResult:\nWith this fix, users will no longer encounter the \"Error: Not Found\" message when trying to access the specific page or resource. The system will correctly route and display the requested content, ensuring a smooth and error-free user experience."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2763",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:**\nThe user was experiencing an \"Error: Not Found\" message when trying to access a certain page or resource in the system.\n\n**Cause:**\nThis issue was caused by a mismatch between the requested URL and the available resources in the system. The system was unable to locate the requested content, resulting in the \"Not Found\" error message.\n\n**Fix:**\nThe code was updated to properly handle the requested URLs and to ensure that the system accurately identifies and serves the available resources. Additionally, the error handling mechanism was improved to provide more informative error messages to users.\n\n**Result:**\nAfter applying this fix, users will no longer encounter the \"Error: Not Found\" message when accessing the specific page or resource. Instead, they will be able to successfully access the desired content without any interruptions or errors."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2761",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence:** Users experienced an \"Error: Not Found\" message when trying to access a certain page or resource within the system. This prevented them from accessing the desired content and performing necessary actions.\n\n**Cause:** The issue was caused by a missing or incorrect URL mapping in the system's routing configuration. This caused the system to incorrectly handle incoming requests and unable to find the specified page or resource.\n\n**Fix:** The routing configuration has been updated to include the correct URL mapping for the affected page or resource. This ensures that incoming requests are properly directed and the appropriate content is returned to the user.\n\n**Result:** Users can now access the desired page or resource without encountering the \"Error: Not Found\" message. The system correctly handles incoming requests and navigates users to the intended content, enabling them to perform necessary actions smoothly."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2760",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Consequence**: The user experienced an \"Error: Not Found\" when trying to access a certain page or resource in the system.\n\n**Cause**: The issue was caused by an incorrect URL or a missing route configuration in the system's routing mechanism.\n\n**Fix**: The developers fixed the issue by updating the URL or adding the missing route configuration to ensure that the requested page or resource can be accessed properly.\n\n**Result**: Users can now access the previously unavailable page or resource without encountering the \"Error: Not Found\" message."
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2722",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found",
    "releaseNoteText": "**Issue:**\nError: Not Found\n\n**Consequence:**\nThe user experienced an error message indicating that the resource or page was not found.\n\n**Cause:**\nThere was an issue with the routing or URL mapping in the application, causing the server to return a 404 error response.\n\n**Fix:**\nThe routing logic was updated to correctly map the requested URL to the corresponding resource or page.\n\n**Result:**\nThe user will no longer encounter an error message when accessing the resource or page, as the routing issue has been resolved."
  }
]