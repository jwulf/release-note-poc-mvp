[
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/13203",
    "component": "Zeebe",
    "subcomponent": "Broker",
    "context": "Enhancements",
    "gitHubText": "**Is your feature request related to a problem? Please describe.**\r\nIn our Elasticsearch Exporter we support management of index lifecycles with ILM. OpenSearch provides a similar mechanic called ISM. In other to keep the exporters aligned and fully support OpenSearch we should add support for ISM to our OpenSearch Exporter.\r\n\r\n**Describe the solution you'd like**\r\nUsers should be able to configure ISM similar to how ILM works. The solution must support both OpenSearch 1.3.x and 2.8.x\r\n\r\nThis also should be documented in the [OpenSearch Exporter docs](https://docs.camunda.io/docs/next/self-managed/zeebe-deployment/exporters/opensearch-exporter/)!\r\n\r\n**Describe alternatives you've considered**\r\nNot implementing this. Users could do it through the OpenSearch UI. It is less convenient and an extra burden on users. It would also mean we'd provide more feature in our Elasticsearch Exporter than our OpenSearch Exporter.\r\n\r\n**Additional context**\r\nN/A\r\n\n",
    "title": "Support Index State Management in OpenSearch exporter"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14277",
    "component": "Zeebe",
    "subcomponent": "Gateway",
    "context": "Enhancements",
    "gitHubText": "### Description\r\n\r\nThe Zeebe Gateway supports handling tenant-aware `BroadcastSignal` RPC calls.\r\n\r\n\r\n```[tasklist]\r\n### AC\r\n- [x] `BroadcastSignal` RPC requests and responses contain a `tenantId` property.\r\n- [x] Signal-related records contain a `tenantId` property.\r\n- [x] `BroadcastSignal` requests are validated for correct a `tenantId` .\r\n```\r\n\n",
    "title": "Gateway supports multi-tenancy in signal broadcast RPCs"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15161",
    "component": "Zeebe",
    "subcomponent": "Java Client",
    "context": "Enhancements",
    "gitHubText": "We want to make it possible for users of the Java client to migrate process instances through the API.\r\n\r\nTo unblock Operate early, we want to provide the interfaces of the Java client without the full implementation. This allows them to start development against these interfaces.\n",
    "title": "Provide the Java client's interfaces for migrating a process instance"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14921",
    "component": "Zeebe",
    "subcomponent": "Java Client",
    "context": "Enhancements",
    "gitHubText": "We want to make it possible for users of the Java client to migrate process instances through the API.\r\n\r\nThe Java client should make it easy to reuse a migration plan for migrating several process instances. This issue will contain the implementation of the Java client's interfaces for migrating a process instance.\r\n\r\nBlocked by\r\n- https://github.com/camunda/zeebe/issues/15161\r\n- https://github.com/camunda/zeebe/issues/15119\n",
    "title": "Support migrating a process instance from the Java client"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/13558",
    "component": "Zeebe",
    "subcomponent": "Java Client",
    "context": "Enhancements",
    "gitHubText": "### Description\r\n\r\nThe Java client provides a `BroadcastSignalCommand` for broadcasting tenant-aware signals in Zeebe. These commands should support multi-tenancy by exposing an optional `tenantId` property/method.\r\n\r\nThe following error codes may be returned:\r\n* PERMISSION_DENIED (code: 7) \r\n   * when a user attempts to broadcast a signal of a tenant they are not authorized for, when multi-tenancy is enabled.\r\n* INVALID_ARGUMENT (code: 3)\r\n   * For a provided tenant id, when multi-tenancy is disabled\r\n   * For a missing tenant id, when multi-tenancy is enabled\r\n   * For an invalid tenant id (i.e. doesn't match the pre-defined format), when multi-tenancy is enabled.\r\n\n",
    "title": "Java client supports multi-tenancy for BroadcastSignal RPC"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15153",
    "component": "Zeebe",
    "subcomponent": "Go Client",
    "context": "Enhancements",
    "gitHubText": "### Description\r\n\r\nThe Zeebe Go client should support multi-tenancy.\r\n\r\nThe following commands should provide a method to set a `tenantId`:\r\n* `DeployResource`\r\n* `CreateProcessInstance`\r\n* `PublishMessage`\r\n* `BroadcastSignal`\r\n* `EvaluateDecision`\r\n\r\nThe following commands should provide a method to set `tenantIds`:\r\n* `ActivateJobs`\n",
    "title": "Go client commands allow specifying a `tenantId` / `tenantIds` property"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14477",
    "component": "Zeebe",
    "subcomponent": "Go Client",
    "context": "Enhancements",
    "gitHubText": "### Description\r\n\r\nThe Zeebe Go client should support multi-tenancy.\r\n\r\n```[tasklist]\r\n### Tasks\r\n- [ ] https://github.com/camunda/zeebe/issues/15153\r\n- [ ] https://github.com/camunda/zeebe/issues/15095\r\n- [ ] https://github.com/camunda/zeebe/issues/15096\r\n- [ ] https://github.com/camunda/zeebe/issues/15097\r\n- [ ] https://github.com/camunda/camunda-docs/issues/2893\r\n```\r\n\n\n Sijoma: Picked this up and created: https://github.com/camunda/zeebe/pull/14946\n koevskinikola: I've converted this issue to an umbrella issue that covers all the sub-tasks needed to support multi-tenancy for the Go client. I moved the previous issue description to #15153 and re-assigned [the related PR ](https://github.com/camunda/zeebe/pull/14946)there.\n koevskinikola: ZPA team:\r\nMoving this issue back to the `Backlog` column of the ZPA project board, as this feature was deprioritized until `8.5`.\r\n\r\nI also removed the `target:8.4` label.\n koevskinikola: Closing as this topic has been de-prioritized and it is currently not clear if/when it will be picked up again.",
    "title": "Add multi-tenancy support in the Zeebe Go client"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/6150",
    "component": "Zeebe",
    "subcomponent": "Go Client",
    "context": "Enhancements",
    "gitHubText": "**Description**\r\n\r\n#6093 provides a backoff mechanism for the java client's job worker. When an error is received from the gateway by the job poller, the worker delays its next poll for available jobs. The go client should support the same functionality.\r\n\r\nThis functionality should be the same as the java client's. Including:\r\n- a default exponential backoff mechanism with the same defaults: {maxDelay:5s, minDelay:50ms, backoffFactor:1.6, jitterFactor:0.1}\r\n- the exponential backoff should be configurable\r\n- users should be able to provide their own implementation of a backoff strategy\r\n- backoff should only occur on errors received on the poll for available jobs\n\n Sijoma: Hey yall, I did a implementation for this for the go client. I'm a bit unsure in which cases the backoff should be applied though. \r\n\r\nIs it every time there is a ResourceExhausted? Or also when it fails to reopen the job polling stream? Is here: `clients/go/pkg/worker/jobPoller.go:95` the correct place?\r\n\r\n\n korthout: The backoff should be applied when the job poller fails to ask for jobs from the gateway. For example, in Java, [it backs off when any exception is thrown during the polling for jobs](https://github.com/camunda/zeebe/blob/069c17a74398d291b95d1c761357a457c62257ae/clients/java/src/main/java/io/zeebe/client/impl/worker/JobWorkerImpl.java#L136-L147). So that it actually controlled from the JobWorker, the poller is just used once in a while by the job worker.\r\n\r\nThis seems to happen differently in the go client, but the details are a bit unclear to me. Somewhere [around here](https://github.com/camunda/zeebe/blob/main/clients/go/pkg/worker/jobPoller.go#L77-L114) would be my guess. \r\n\r\nI think that this part is the [main loop](https://github.com/camunda/zeebe/blob/main/clients/go/pkg/worker/jobPoller.go#L47-L71) for the poller. It seems to:\r\n- poll for jobs, and add them to the job queue\r\n- determine if it if needs to poll again\r\n\r\nSo, what is needed is some way to backoff from polling when an error occurs during polling. There are [other errors](https://docs.camunda.io/docs/apis-clients/grpc/#technical-error-handling) than `RESOURCE_EXHAUSTED` that could occur during polling.\n Sijoma: Thanks for the info @korthout, indeed it looks a bit differently in the go client. I will put in a draft-pr :). ",
    "title": "go-client: Job worker polling backoff mechanism"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15675",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "gitHubText": "**Is your feature request related to a problem? Please describe.**\r\nThe process instance migration command should support multi-tenancy.\r\n\r\n**Describe the solution you'd like**\r\nIf a command isn't authorized to work with the tenant that owns the process instance the command wants to migrate, then it should be rejected.\r\n\r\n**Describe alternatives you've considered**\r\n/\r\n\r\n**Additional context**\r\n/\r\n\n",
    "title": "Process instance migration supports multi-tenancy"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15659",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "gitHubText": "Recreating event subscriptions during the process instance migrations implemented with #15407. Afterwards, there was a bug reported related to it #15570.\r\n\r\nAs agreed with @saig0, it is not possible to implement a quick fix for the bug until the minor release. Also, engineering and product management (@aleksander-dytko) agreed on handling event subscription migration in a broader scope. (e.g. we do not reset timers)\r\n\r\nThis issue will revert the changes introduced by #15407 according to decisions mentioned above.\r\n\r\nCc: @abbasadel \n",
    "title": "Revert: Migrate task instances with recreated event subscriptions"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15568",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "gitHubText": "### Description\r\n\r\nResponse codes and their explanations are out of sync with what we documented on our Camunda 8 gRPC API docs. The issue is created to fix that problem and have the same error code explanations in both places:\r\n\r\n`gateway.proto`: https://github.com/camunda/zeebe/blob/main/gateway-protocol/src/main/proto/gateway.proto\r\ngRPC doc: https://github.com/camunda/camunda-docs/pull/2955/files/b7d1ef9a7803044e815d5bd93bc849815b74bdc0#diff-d8972d11490d2d55a8153807a50d23665ce723dce9c70e7b52ba27ba0627f53cR821\r\n\r\nRelated change request: https://github.com/camunda/camunda-docs/pull/2955#discussion_r1420217502\n",
    "title": "Sync Process Instance Migration gRPC documentation with gateway.proto"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15407",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "gitHubText": "When migrating a process instance:\r\n- there may exist event subscriptions in the process instance\r\n- there may be events that must be subscribed to in the target process\r\n\r\nMigrating the existing event subscriptions is difficult, because they can exist on other partitions than the process instance, e.g. message subscription.\r\n\r\nWe can deal with this by unsubscribing each active element instance in the process instance **and** subscribing to new events in the target process for each migrated element instance.\n\n abbasadel: This issue changes is reverted as mentioned in https://github.com/camunda/zeebe/issues/15659 \n berkaycanbc: @korthout should we keep this one in the backlog?",
    "title": "Migrate task instances with recreated event subscriptions"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15391",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "gitHubText": "**Is your feature request related to a problem? Please describe.**\r\nTo add support for authentication via [Microsoft Entra ID (Azure AD) and generic OpenID Connect](https://github.com/camunda/product-hub/issues/739) it is required to set the `scope` parameter in the token request, see [Microsoft Entra ID docs](https://learn.microsoft.com/en-us/entra/identity-platform/v2-oauth2-client-creds-grant-flow#first-case-access-token-request-with-a-shared-secret). Currently the Zeebe java client does not support setting the `scope` parameter.\r\n\r\n**Describe the solution you'd like**\r\nThe Zeebe client supports setting the `scope` parameter in the token request (for example in https://github.com/camunda/zeebe/blob/main/clients/java/src/main/java/io/camunda/zeebe/client/impl/oauth/OAuthCredentialsProvider.java#L125)\r\n\r\n**Describe alternatives you've considered**\r\nUse Identity-sdk which supports creating auth tokens.\r\n\r\n**Additional context**\r\n- When using Microsoft Entra ID, the scope parameter has to contain the client api in a specific format, for example `scope=4393964b-2273-4b23-94fa-98c3d2396b1e/.default`\r\n- The additional parameter has to also be integrated into [`spring-zeebe`](https://github.com/camunda-community-hub/spring-zeebe) since it is used by Connectors\n\n megglos: > The additional parameter has to also be integrated into [spring-zeebe](https://github.com/camunda-community-hub/spring-zeebe) since it is used by Connectors\r\n\r\n@dlavrenuek from my understanding, having the env var support in the client which you added with https://github.com/camunda/zeebe/pull/15404 will be enough to make use of it in spring-zeebe, right?\n 1nb0und: 1. Are we also considering adding the scope soon to other components as well (operate, optimize, etc) ?\r\n2. If yes on above, will all components use the same scope, or different components will use its own scope?\r\n\n megglos: @1nb0und best reach out on the product-hub issue https://github.com/camunda/product-hub/issues/739 to the PM and engineering DRI",
    "title": "Allow setting `scope` parameter for OAuth2 authentication"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15389",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "gitHubText": "**Is your feature request related to a problem? Please describe.**\r\nCamunda Identity now provides an `identity-spring-boot-starter` which, in turn, provides an `IdentityConfiguration` Spring bean.\r\n\r\nThe goal is to decouple the Identity configuration properties from the Zeebe configuration classes. The `identity-spring-boot-starter` should create an `IdentityConfiguration` Spring bean, which in turn can be used to configure the `identity-sdk` inside Zeebe. \r\n\r\nThe outcome will be that Zeebe doesn't need to add new Identity configuration properties every time Identity adds them.\r\n\r\n**Describe the solution you'd like**\r\n\r\n- [x] The `IdentityInterceptor` class should create an `Identity` instance using an `IdentityConfiguration` instance ([src](https://github.com/camunda/zeebe/blob/303b0d7bbc60440fee8baaf943ab020d49783d38/gateway/src/main/java/io/camunda/zeebe/gateway/interceptors/impl/IdentityInterceptor.java#L37-L39)).\r\n- [x] The `Gateway` class should pass an `IdentityConfiguration` instance to the `IdentityInterceptor` constructor ([src](https://github.com/camunda/zeebe/blob/b3c3397f10b8e96d12af053ff72d616f80264dca/gateway/src/main/java/io/camunda/zeebe/gateway/Gateway.java#L323-L328)).\r\n- [x] The `StandaloneGateway` class should `Autowire` an `IdentityConfiguration` bean and pass it along to the `Gateway` constructor ([src](https://github.com/camunda/zeebe/blob/44769129dbfbdee7b44889780215d20e395453f2/dist/src/main/java/io/camunda/zeebe/gateway/StandaloneGateway.java#L101-L103)).\r\n- [x] The `EmbeddedGatewayService` class should pass an `IdentityConfiguration` instance to the `Gateway` constructor ([src](https://github.com/camunda/zeebe/blob/de669e90aa74a0330f6e17f07de0b14aa430a2cc/broker/src/main/java/io/camunda/zeebe/broker/system/EmbeddedGatewayService.java#L36-L37)).\r\n   - The `IdentityConfiguration` bean should be passed through the `StandaloneBroker` class ([src](https://github.com/camunda/zeebe/blob/4c4e7d2404f00045e5ac3d033e007e30b063b14c/dist/src/main/java/io/camunda/zeebe/broker/StandaloneBroker.java#L82)).\r\n- [x] The `MultiTenancyOverIdentityIT` test should configure Identity using an `IdentityConfiguration` bean ([src](https://github.com/camunda/zeebe/blob/d98d04cbe6ab675c981aea808566264f8552d647/qa/integration-tests/src/test/java/io/camunda/zeebe/it/multitenancy/MultiTenancyOverIdentityIT.java#L210-L213)).\r\n\r\n\r\n**Describe alternatives you've considered**\r\n- None. Zeebe needs to integrate the `identity-spring-boot-starter` in order to decouple the Identity configuration properties from Zeebe.\r\n\r\n**Additional context**\r\n* https://github.com/camunda/product-hub/issues/1849\r\n* https://github.com/camunda-cloud/identity/issues/2162\r\n\n",
    "title": "Configure `identity-sdk` through `identity-spring-boot-starter` in the Gateway"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15382",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "gitHubText": "Add IT tests in the `MultiTenancyOverIdentityIT` test. Some [similar test cases can be found in the same class](https://github.com/camunda/zeebe/blob/d6eaafadfb411f1b30a87022658691ec82c4630c/qa/integration-tests/src/test/java/io/camunda/zeebe/it/multitenancy/MultiTenancyOverIdentityIT.java#L759-L842).\r\n`shouldCompleteJobForTenant` can serve as an example.\n",
    "title": "Add integration test to `MultiTenancyOverIdentityIT` test"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15366",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "gitHubText": "Add Job Update Timeout command to the Go Client.\r\nThis command is very similar to the Job Update Retries command.\n",
    "title": "Support `UpdateJobTimeout` RPC in Go Client"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15306",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "gitHubText": "**Is your feature request related to a problem? Please describe.**\r\nElasticSearch provides frequent minor updates. Many bug-fixes are provided via minors (and not via patches).\r\nDue to bugs and CVEs we are upgrading our ElasticSearch instances to 8.9+.\r\n\r\n**Describe the solution you'd like**\r\n\r\n- [x] Update Zeebe's ElasticSearch dependencies to 8.9+\r\n- [ ] Update zeebe-analytics dependency \r\n- [x] Make sure tests run successfully\r\n\r\n\r\n**Additional context**\r\nPDP Epic: https://github.com/camunda/product-hub/issues/1902 \n\n korthout: @abbasadel The linked [supported environments specification](https://confluence.camunda.com/display/HAN/Camunda+8+Supported+Environments) mentions ES 8.8+ for the 8.4 instead of 8.9. Are we sure we want to upgrade to 8.9? This should be aligned across the components.\n korthout: ZPA triage:\r\n- If the rest of the platform is upgrading, we should as well\r\n- We should update the dependencies by hand (I believe auto-updates are disabled for ES client)\r\n- We should also update the dependency in zeebe-analytics\r\n- Marking it as upcoming\n abbasadel: @korthout yes, all components are aligned to upgrade to 8.9, as mentioned in https://github.com/camunda/product-hub/issues/1902 ",
    "title": "Support ElasticSearch 8.9+"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15193",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "gitHubText": "null\n",
    "title": "Update `broker.yaml.template` and `broker.standalone.yaml.template` files in order to allow user to disable form exporting"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15140",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "gitHubText": "## Description\r\n\r\nWe introduced a new compensation record for BPMN compensation events. We should export these events to Elasticsearch and OpenSearch. \r\n\r\nWe should follow the development guide on how to add the record for the exporters [here](https://github.com/camunda/zeebe/blob/main/docs/developer_handbook.md#support-a-recordvalue-in-the-elasticsearch-exporter).\r\n\r\nrequires #15063 \n\n saig0: Done by https://github.com/camunda/zeebe/pull/15392. :rocket: ",
    "title": "Export compensation subscription events"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15114",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "gitHubText": "Introduce a new `VariableIntent` `MIGRATED`.\r\n\r\nAdd an event applier that updates the following properties of a variable:\r\n\r\n- `bpmnProcessId`: the BPMN ID of the target process definition\r\n- `processDefinitionVersion`: the version of the target process definition\r\n- `processDefinitionKey`: the key of the target process definition\r\n\r\nIn the `ProcessInstanceMigration` command processor, add:\r\n- for each variable of a process instance:\r\n  - append `Variable:MIGRATED` event\r\n- for each child element instance of the process:\r\n  - append `Variable:MIGRATED` event for each variable at that scope\r\n\r\nMake sure to clear the variable's `value` when appending these events to avoid exceeding the result records batch size.\r\n\r\nAdd a test case that show that we can migrate a process instance, and use the variables in activated jobs and can update the variables with the Set Variables RPC.\r\n\r\n**Alternatively**\r\nFind a way to migrate all variables for a process instance in a single pass. I'm not sure this is possible.\r\n\r\n**Out of scope**\r\nMigrate all variables with a single event.\r\n\r\nBlocked by:\r\n- #15113 \n\n sdorokhova: Hi @korthout , I don't think that variable has `elementId` field. Is this a copy-paste error?\r\nBut variable has `scopeKey` field. Can it change?\n korthout: Good point @sdorokhova, I've removed the `elementId` from the list in the description.\r\n\r\nThe `scopeKey` won't change. Variables will stay existing at the same scope. We're not planning to move variables to different scopes. That might be needed when we make Sub Process hierarchy changes possible, but even then I doubt we'll move variables. If we change our mind on this for future iterations, we'll let you know in time. ",
    "title": "Migrate task instances with variables"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15113",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "gitHubText": "Introduce a new `JobIntent` `MIGRATED`.\r\n\r\nAdd an event applier that updates the following properties of a job:\r\n\r\n- `bpmnProcessId`: the BPMN ID of the target process definition\r\n- `processDefinitionVersion`: the version of the target process definition\r\n- `processDefinitionKey`: the key of the target process definition\r\n- `elementId`: changes to the mapped element ID of the associated service task (note that the mapping may define the sourceElementId and the targetElementId as equivalent, in which case it stays the same)\r\n\r\nIn the `ProcessInstanceMigration` command processor, add:\r\n- for each child instance of the process instance:\r\n  - append `ProcessInstance:ELEMENT_MIGRATED` event\r\n  - if this child is of type `SERVICE_TASK`, append `Job:MIGRATED` event\r\n\r\nAdd a test case that show that we can migrate a process instance, and can still use all job commands, e.g. activate, fail, throwError, yield, complete, etc.\r\n\r\nBlocked by:\r\n- https://github.com/camunda/zeebe/issues/15111\n",
    "title": "Migrate service task instances (including jobs)"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15063",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "gitHubText": "## Description\r\n\r\nWe need a new record for compensation events. It should be used to:\r\n- Find completed activities with compensation handlers\r\n- Keep track of the compensation handler state: ready for invocation, invoked  \r\n- Find the compensation throw event that triggered a compensation handler\r\n\r\nThe compensation record may contain the following data:\r\n- tenant id\r\n- process instance key\r\n- process definition key\r\n- element id of activity with compensation handler\r\n- flow scope element id of activity with compensation handler\r\n- element id of compensation throw event\r\n- element instance key of compensation throw event\r\n- local variables of activity with compensation handler (i.e. snapshot data for compensation event subprocess)\r\n \r\nThe lifecycle of a compensation record may contain the following states:\r\n\r\n```\r\nCREATED -> TRIGGERED -> COMPLETED | DELETED\r\n```\r\n\r\n- CREATED: \r\n  - when an activity with a compensation boundary event is completed, or\r\n  - when an embedded subprocess with a compensation event subprocess is completed\r\n  - then add the subscription to the state \r\n- TRIGGERED: \r\n  - when the compensation throw event is activated\r\n  - then set the element instance key and element id of the compensation throw event\r\n  - and update the subscription in the state\r\n- COMPLETED: \r\n  - when the compensation handler is completed\r\n  - then remove the subscription from the state\r\n- DELETED: \r\n  - when the process instance is completed or terminated\r\n  - then remove the subscription from the state  \r\n\r\nWe should follow the development guide on how to create a new record [here](https://github.com/camunda/zeebe/blob/main/docs/developer_handbook.md#how-to-create-a-new-record).\n\n saig0: I created a separate issue for exporting these events: #15140. If possible, we can work on the exporters as a follow-up. ",
    "title": "Introduce new compensation subscription record"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15034",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "gitHubText": "Add new Intent to the `JobIntent` class\r\n\r\n- `UPDATE_TIMEOUT`\r\nThis command will look for the Job in the state and for it's deadline. If it can't find one of those it will return a `NOT_FOUND` rejection. Otherwise it will write the `TIMEOUT_UPDATED` event which will modify the deadline in the state.\r\n- `TIMEOUT_UPDATED`\r\nThis event will update the `JOB_DEADLINES` ColumnFamily. Since the key of this ColumnFamily is deadline-jockey we can't just update the existing one. Instead we will need to remove the existing entry for this Job, and insert a new value. For this we must have access to the old deadline. This is stored in state and should available through the JobRecord.\n",
    "title": "Add Job Update Timeout intents"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14967",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "gitHubText": "## Description \r\n\r\nAs a user, I can deploy a process with a compensation end event. Now, I want to execute the process. In the first step, we want to ignore the invocation of compensation handlers. \r\n\r\n- I can complete a process instance that contains a compensation end event.\r\n- The compensation end event is activated and completed.\r\n- The records for the compensation end event have the event type `COMPENSATION`.\r\n\r\n![Screenshot from 2023-11-02 14-01-15](https://github.com/camunda/zeebe/assets/4305769/97ea69e5-3f3c-4393-82bf-c58bfe11e353)\r\n\r\nrequires #14942 \r\nrequires #14944 \n",
    "title": "I can execute a process with a compensation end event"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14965",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "gitHubText": "## Description \r\n\r\nAs a user, I can deploy a process with a compensation intermediate throw event. Now, I want to execute the process. In the first step, we want to ignore the invocation of compensation handlers. \r\n\r\n- I can complete a process instance that contains a compensation intermediate throw event.\r\n- The compensation intermediate throw event is activated and completed.\r\n- The records for the compensation intermediate throw event have the event type `COMPENSATION`.\r\n\r\n![Screenshot from 2023-11-02 13-47-31](https://github.com/camunda/zeebe/assets/4305769/6c13ca34-ab13-4ad1-bcfe-a74cee62e237)\r\n\r\nrequires #14942 \r\nrequires #14944 \n",
    "title": "I can execute a process with an intermediate compensation throw event"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14944",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "gitHubText": "## Description \r\n\r\nWhen we execute a BPMN process, we write events of the value type `ProcessInstance`. These events have a field `bpmnEventType`. The field is set for BPMN events with their respective type. \r\n\r\nTo reflect the execution of compensation events, we need to introduce a new [event type](https://github.com/camunda/zeebe/blob/main/protocol/src/main/java/io/camunda/zeebe/protocol/record/value/BpmnEventType.java) `COMPENSATION`. All events related to a BPMN compensation event should have the event type `COMPENSATION` (i.e. compensation intermediate throwing events, compensation end events, compensation boundary events, compensation start events).   \n",
    "title": "The record for a compensation event has the correct event type"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14943",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "gitHubText": "## Description\r\n\r\nIn order to execute a process with compensation events, we need to deploy the process first. Currently, it is impossible to deploy a process with compensation event subprocesses because of validation rules.  \r\n\r\nAdjust the validation rules to allow compensation start events within event subprocesses.\r\n\r\nAdditionally, we ensure the following restrictions:\r\n- A compensation start event must be within an event subprocess.\r\n- A compensation event subprocess must be within an embedded subprocess. The event subprocess is not allowed on the process level. \r\n\r\n![Screenshot from 2023-11-01 13-15-57](https://github.com/camunda/zeebe/assets/4305769/4ff1a5e7-600e-42f5-9918-b29d82f3c0e8)\r\n\r\nrequires #14939 \r\nrequires #14942 \n",
    "title": "I can deploy a process with BPMN compensation event subprocesses"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14942",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "gitHubText": "## Description\r\n\r\nIn order to execute a process with compensation events, we need to deploy the process first. Currently, it is impossible to deploy a process with compensation events because of validation rules.  \r\n\r\nAdjust the validation rules to allow the following BPMN elements:\r\n- Compensation intermediate throwing event\r\n- Compensation end event\r\n- Compensation boundary event\r\n- Activity with compensation marker (i.e. a compensation handler)\r\n\r\nAdditionally, we ensure the following restrictions:\r\n- A compensation boundary event should have no outgoing sequence flows. The compensation handler is connected by an association. \r\n- A compensation handler should have no incoming and outgoing sequence flows.\r\n- A compensation handler should have no boundary events.\r\n- Only the following activities can have a compensation marker:\r\n  - Service Tasks\r\n  - User Tasks\r\n  - Send Tasks\r\n  - Script Tasks\r\n  - Manual Tasks\r\n  - Undefined Tasks\r\n- A compensation intermediate throwing or end event should have no attribute `waitForCompletion` or `waitForCompletion` must be `true`. By default, the attribute is `true` (i.e. the throwing event waits until the compensation is complete).  \r\n\r\n![Screenshot from 2023-11-01 12-43-10](https://github.com/camunda/zeebe/assets/4305769/1f69fed7-0dcd-48cd-a3ee-a441647c4886)\r\n\r\n\r\nrequires #14939 \n",
    "title": "I can deploy a process with BPMN compensation events"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14939",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "gitHubText": "## Description\r\n\r\nThe BPMN model API supports compensation events already. But, creating a process via builder with compensation events is verbose and feels clunky. \r\n\r\nSee the example here:\r\n\r\n```java\r\nBpmn.createExecutableProcess(\"compensation-process\")\r\n            .startEvent()\r\n            .userTask(\"A\")\r\n            .boundaryEvent()\r\n            .compensateEventDefinition()\r\n            .compensateEventDefinitionDone()\r\n            .compensationStart()\r\n            .userTask(\"undo-A\")\r\n            .compensationDone()\r\n            .endEvent()\r\n            .done();\r\n```\r\n\r\n![Screenshot from 2023-11-01 11-10-35](https://github.com/camunda/zeebe/assets/4305769/94e1f994-a3cd-4c4b-9045-01acd38e6ac5)\r\n\r\nWe will need the builder to create test cases for compensation events. Let's make the builder smoother for better test cases.\r\n\r\nSee the proposal based on other events. The final solution may look different. \r\n\r\n```java\r\nBpmn.createExecutableProcess(\"compensation-process\")\r\n        .startEvent()\r\n        .userTask(\"A\")\r\n        .boundaryEvent()\r\n        .compensation(c -> c.userTask(\"undo-A\"))\r\n        .moveToActivity(\"A\")\r\n        .endEvent()\r\n        .done();\r\n\r\n// or via nested lambda definition \r\nBpmn.createExecutableProcess(\"compensation-process\")\r\n        .startEvent()\r\n        .userTask(\"A\")\r\n        .boundaryEvent(\r\n            \"compensate\",\r\n            boundaryEvent -> boundaryEvent.compensation().userTask(\"undo-A\"))\r\n        .endEvent()\r\n        .done();\r\n``` \r\n\r\n\n",
    "title": "I can model a process with compensation events via BPMN model API"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14858",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "gitHubText": "**Description**\r\n\r\nBlocked by #14272 \n",
    "title": "Add support to Java client for Form deletion"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14701",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "gitHubText": "**Is your feature request related to a problem? Please describe.**\r\n\r\nCurrently we have Dependabot and Renovate in our repo, makes sense to only use one, and Renovate seems to cover all our use cases, unlike Dependabot (see https://github.com/camunda/zeebe/issues/13409).\r\n\r\n**Describe the solution you'd like**\r\n\r\nChange renovate.json to also cover the maven and docker updates. Remove the Dependabot tool for these (do we have other things being updated by Dependabot?).\r\n\r\nThe following command can be used to test renovate locally:\r\n```\r\nnpx renovate --token=$GITHUB_TOKEN --schedule=\"\" --require-config=ignored --dry-run=full camunda/zeebe \r\n\r\n```\r\n\r\nAlso helps to set the env var LOG_LEVEL=debug, so that the previous command produces more information.\n\n npepinpe: Triage:\n\nLet's discuss it in planning later and see if we have time for it, but we shouldn't postpone it too much to avoid missing out on dependency updates and losing momentum (since Rodrigo has worked on it recently).\n megglos: One hint: we also need to check how we can update go dependencies, we could keep dependabot for this for a first scope as well.\n rodrigo-lourenco-lopes: https://github.com/camunda/zeebe/pull/14909 is merged. The requirements from the dependabot were added to renovate and dependabot.yml removed.  \r\nNow we are going to be validating the next PRs that get created.\r\nIf all is correct then we still need to remove the \"Dependabot auto-merge updates\" workflow.\n oleschoenburg: @rodrigo-lourenco-lopes We are now getting maven dependency PRs from Renovate, which is great :rocket: \r\nBut the PR title and commit message is a bit wrong. https://github.com/camunda/zeebe/pull/14962 for example uses \"deps(deps)\" as the topic, it should be \"deps(maven)\". Could you fix this as a follow up? :pray: \n remcowesterhoud: @rodrigo-lourenco-lopes Thanks for doing this ðŸš€ As the PR is merged, are you okay with closing this issue?\r\n\r\nEither way I will remove it from the ZPA board cause we don't need to take any action here.\n rodrigo-lourenco-lopes: > @rodrigo-lourenco-lopes Thanks for doing this ðŸš€ As the PR is merged, are you okay with closing this issue?\r\n> \r\n> Either way I will remove it from the ZPA board cause we don't need to take any action here.\r\n\r\n@remcowesterhoud we still need to remove the dependabot github related actions, I will do this, and then close it.\n megglos: we might also remove this as part of the dependabot cleanup\r\nhttps://github.com/camunda/zeebe/blob/main/commitlint.config.js#L6C5-L7C71",
    "title": "Replace Dependabot with Renovate"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14562",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "gitHubText": "**Is your feature request related to a problem? Please describe.**\r\nCurrently Forms are received from the state without using any caching mechanism. Adding a caching for recently retrieved Forms will increase the performance of retrieval. We do a similar implementation for Decision and Process resources in their `DbState` classes as well.\r\n\r\nSince deploying forms to Zeebe is a feature that is introduced in 8.3, there is no such requirement or problems with performance at the moment. Therefore, this issue is just an performance improvement at the moment.\r\n\r\n**Describe the solution you'd like**\r\nAdd caching mechanism to DbFormState as already done for Decision and Process states.\r\n\r\n**Describe alternatives you've considered**\r\nThe alternative is basically not using caching at all and that is already implemented.\r\n\r\n**Additional context**\r\nEPIC link: https://github.com/camunda/zeebe/issues/12874\n",
    "title": "Use caching for Forms"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14285",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "gitHubText": "Zeebe 8.3.0 only supports multi-tenancy over Identity:\r\n\r\nhttps://github.com/camunda/zeebe/blob/5b23af53e0c3837abc11325a8a496ac37f793e4e/dist/src/main/config/broker.standalone.yaml.template#L108-L119\r\n\r\nUsers may want to use their own tenant provider. We want to make this available using [custom interceptors](https://docs.camunda.io/docs/next/self-managed/zeebe-deployment/zeebe-gateway/interceptors/).\r\n\r\nAccording to [the decision made below](https://github.com/camunda/zeebe/issues/14285#issuecomment-1851578839), the following tasks need to be completed:\r\n\r\n### Tasks\r\n- [x] Expand the `InterceptorUtil` class to provide the `AUTHORIZED_TENANTS_KEY` gRPC context key.\r\n- [x] ~Extract the tenant-providing logic from the `IdentityInterceptor` into a new `IdentityTenantProvidingInterceptor`~\r\n\t1. ~This new interceptor is added only if `identity` is set as the `authentication.mode`, and `multiTenancy` is enabled (set to `true`).~\r\n\t2. Won't be done since the tenant-providing logic for Identity is closely tied to the Identity token, which is validated within the `IdentityInterceptor`. We would have to extract the token twice if we extract the tenant-providing logic.\r\n- [x] Handling `tenantId` data:\r\n\t1. Only use the data provided by the `AUTHORIZED_TENANTS_KEY` gRPC Context key if multi-tenancy is enabled.\r\n- [x] Tests\r\n\t1. Ensure that the existing tests that cover multi-tenancy over Identity pass\r\n\t2. Add IT tests to ensure that the Gateway uses the data provided by a custom tenant-providing interceptor when multi-tenancy is enabled.\r\n\t4. Add IT tests to ensure that the Gateway doesn't use the data provided by a custom tenant-providing interceptor when multi-tenancy is disabled.\r\n\t5. Add IT tests to ensure that a custom tenant-providing interceptor overrides the Identity tenant-providing interceptor when multi-tenancy is enabled.\r\n- [x] https://github.com/camunda/camunda-docs/issues/3029:\r\n       - Extend the [Interceptor docs](https://docs.camunda.io/docs/self-managed/zeebe-deployment/zeebe-gateway/interceptors/) with a section on *Implementing a tenant-providing interceptor*\r\n       - Update the MultiTenancy docs for Zeebe to state that:\r\n                - Multi-tenancy in Zeebe is independent from Identity, if Zeebe is used as a standalone product\r\n                - If Zeebe is used as a standalone product, and multi-tenancy is enabled, users can implement a custom tenant-providing interceptor. A link to the Interceptor docs should be provided.\r\n        - If the full Camunda 8 stack is used, then a custom tenant-providing interceptor can't be used, and Identity must be configured since other Camunda 8 components must use Identity.\n\n koevskinikola: Solution proposals:\r\n1. Zeebe provides an `AbstractTenantProvidingInterceptor` class. \r\n    1. The class will implement the logic for adding the `tenantId` list to the gRPC context.\r\n    2. Users will need to extend this class and implement the tenant-providing logic.\r\n2. Zeebe provides a `TenantProvider` interface\r\n    1. Zeebe uses a `TenantProvidingInterceptor` class in the Zeebe Gateway module. Implementations of the  `TenantProvider` interface are passed to this interceptor.\r\n    2. Users will need to implement the `TenantProvider` interface\r\n3. Zeebe extends the `InterceptorUtil` class to provide the `AUTHORIZED_TENANTS_KEY`, and documents (in a similar manner to the `QueryApi`) how to use it with the gRPC Context.\r\n   1. Users may implement a \"tenant-providing\" interceptor as they see fit.\r\n   2. We document this in the Zeebe Interceptors documentation.\n koevskinikola: Decision:\r\nWe use solution proposal [3] from [the list above](https://github.com/camunda/zeebe/issues/14285#issuecomment-1847356998). Why:\r\n* PRO: It provides the most flexibility for users. Users can implement their tenant-providing logic in any of their gRPC interceptors.\r\n* PRO: It uses an existing Zeebe Interceptor mechanism (the `InterceptorUtil` class) to pass and extract data to/from interceptors.\r\n* CON: Users may still provide a `tenantId` list using this mechanism when multi-tenancy is disabled, as we have no mechanism to prevent this within an interceptor.\r\n   * However, we can control if the `tenantId` list is used or not inside the Gateway, depending if multi-tenancy is disabled or not.\r\n   * The alternative to avoid this CON is to use approach [1] or [2]. However, we would need to re-implement a mechanism for instantiating externally provided classes (we have this with the `IdentityRepository`. Furthermore, these approaches are less flexible, as we would require users to either extend an abstract class or implement an interface inside their code.",
    "title": "Support general-usage tenant-providing interceptor for SM"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14279",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "gitHubText": "### Description\r\n\r\nResource deletion is tenant-aware.\r\n\r\n\r\n```[tasklist]\r\n### AC\r\n- [x] The `ResourceDeletionProcessor` is able to lookup resources using authorized tenant IDs.\r\n- [x] Unit tests are added for tenant-aware resource deletion.\r\n- [x] QA tests are added for tenant-aware resource deletion.\r\n- [x] Re-enable the tests in the engine's `TenantAwareTimerStartEventTest`\r\n- [ ] https://github.com/camunda/camunda-docs/issues/2845\r\n```\r\n\r\n\r\n### Additional context\r\n\r\n* Please see [comment](https://github.com/camunda/zeebe/issues/13279#issuecomment-1721349923) on `ElementInstanceState` before implementing.\r\n* We must also re-enable the tests in the `TenantAwareTimerStartEventTest`\n",
    "title": "Support multi-tenancy for Resource Deletion"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14272",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "gitHubText": "**Description**\r\n\r\nWith the epic #12874, deploying Forms to Zeebe will be possible. We now support deleting resources through DeleteResource gRPC and it needs to be updated for Form resource deletion. Details of the implementation will be provided later.\n",
    "title": "Support deletion of the Forms through DeleteResource gRPC"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/13433",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "gitHubText": "**Is your feature request related to a problem? Please describe.**\r\nAs described in the issue #13410, we want to keep track of event types that triggered elements in Grafana. This is not possible without actually adding them to the job metric in the Engine.\r\n\r\n**Describe the solution you'd like**\r\nUpdate `ProcessEngineMetrics` class to event types for `element_instance_events_total` metric. Then, update their usages for activated/completed/terminated element instances.\r\n\r\n**Describe alternatives you've considered**\r\nAs long as we want to show these metrics in Grafana, there is no a real alternative than keeping track of this metric in the Engine.\n\n korthout: Alternative solution, we can add another label to the [`element_instance_events_total` metric](https://github.com/camunda/zeebe/blob/main/engine/src/main/java/io/camunda/zeebe/engine/metrics/ProcessEngineMetrics.java#L55-L61) to track `eventType`. (size: x-small)\n korthout: I'd propose to pick this up soon, so we can unblock #13410, which can help us troubleshoot issues faster and more easily.",
    "title": "Keep track of event types in the Engine"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/13410",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "gitHubText": "**Is your feature request related to a problem? Please describe.**\r\nIn the past weeks, there was a support case where it reports that the Engine is lagging behind while process new records. The incident was resolved afterwards. The incident was caused by a timer catch event that keeps generating multiple new records, so that the thread pool was busy with generating new records.\r\n\r\nWhile investigating in Grafana, the team were able to observe a catch event was taking the CPU time but cannot see which type of catch event was causing the issue. Therefore, cannot root cause the issue without looking at other log resources.\r\n\r\n**Describe the solution you'd like**\r\nThe type of events should be added to Grafana `General Overview -> Current Events` view. Every element instance should contains its own type\r\n\r\n**Describe alternatives you've considered**\r\nAlternative way to achieve the same goal is to look at export logs which takes more time and less accessible than Grafana.\r\n\r\n**Additional context**\r\nIt is more precise if we have activation, completion and termination information of catch events in the view.\r\n\n\n korthout: Marking this as `upcoming` as it can help us troubleshoot problems faster and more easily.\r\n\r\nIt is blocked by:\r\n- #13433 ",
    "title": "Indicate the type of activated/completed/terminated catch events in Grafana"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/13336",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "gitHubText": "### Description\r\n\r\nThis issue covers multi-tenancy support for signal broadcasting in Zeebe. \r\n\r\n```[tasklist]\r\n### Task breakdown\r\n- [ ] https://github.com/camunda/zeebe/issues/14277\r\n- [ ] https://github.com/camunda/zeebe/issues/13558\r\n- [x] The `SignalSubscriptionState` can perist the `tenantId`\r\n- [x] Engine can process `SignalRecord` commands with tenant id\r\n- [x] Multi-tenancy data migration is provided for ProcessingStates used by signal broadcasting\r\n- [ ] https://github.com/camunda/camunda-docs/issues/2820\r\n- [x] ElasticSearch/OpenSearch signal templates include tenantId\r\n- [x] Notify the Operate team that multi-tenancy is supported for signal broadcast in Zeebe\r\n```\r\n\r\n### Blocked by\r\n- https://github.com/camunda/zeebe/issues/13238\n\n koevskinikola: Update: All (impacted) Elasticsearch/Opensearch record templates have been updated to include a `tenantId` property with https://github.com/camunda/zeebe/issues/13520.",
    "title": "Zeebe supports multi-tenancy for BPMN signal events"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15220",
    "component": "Zeebe",
    "subcomponent": "Broker",
    "context": "Bug Fixes",
    "gitHubText": "In the `ClusterTopologyManager`, the `onGoingTopologyChangeOperation`flag  is only set back to `false` when an operation is applied.\r\nIt's not set to `false` if a change is cancelled or if an operation fails to initialize.\r\nIf there are new operations later, the manager will not start them because `onGoingTopologyChangeOperation` is still set to `true`.\r\n\r\nRelates to #13642 \n",
    "title": "Failed or cancelled topology change operations block further operations"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15219",
    "component": "Zeebe",
    "subcomponent": "Broker",
    "context": "Bug Fixes",
    "gitHubText": "A broker can receive a new topology over gossip before the `TopologyInitializer` finished. At that point, the local topology is uninitialized and the received topology is persisted without merging.\r\nThis can result in a broker forgetting about already applied operations, something that we usually avoid by always persisting the local topology when changes are applied.\r\n\r\nNormally, topology updates received via gossip should not overwrite an uninitialized topology. The only exception to this would be the `GossipInitializer`.\r\n\r\nRelates to #13642 \n",
    "title": "Topology received via gossip can bypass merge with locally persisted topology"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14509",
    "component": "Zeebe",
    "subcomponent": "Broker",
    "context": "Bug Fixes",
    "gitHubText": "**Describe the bug**\r\n\r\nAfter restoring from backup, some partitions encountered [following error](https://console.cloud.google.com/logs/query;cursorTimestamp=2023-09-26T11:26:03.072149905Z;endTime=2023-09-26T11:56:36.088Z;pinnedLogId=2023-09-26T11:26:01.585896295Z%2Fh1uykcs15coafgsq;query=logName:%22stdout%22%0Aresource.type%3D%22k8s_container%22%0Aresource.labels.container_name%3D%22zeebe%22%0Aresource.labels.namespace_name%3D%22eab43606-a898-467f-967f-21a757fc69b7-zeebe%22%0Aresource.labels.location%3D%22europe-west1%22%0Aresource.labels.project_id%3D%22camunda-saas-int-chaos%22%0Aresource.labels.cluster_name%3D%22worker-chaos-1%22%0Aresource.labels.pod_name%3D%22zeebe-1%22%0AjsonPayload.context.partitionId%3D%2226%22%0Atimestamp%3D%222023-09-26T11:26:01.585896295Z%22%0AinsertId%3D%22h1uykcs15coafgsq%22;startTime=2023-09-26T10:56:36.088Z;summaryFields=jsonPayload%252Fcontext%252FpartitionId,resource%252Flabels%252Fpod_name:false:32:beginning?project=camunda-saas-int-chaos).\r\n\r\n```\r\njava.lang.IllegalStateException: Expected to delete index after 3982, but it is lower than the commit index 3983. Deleting committed entries can lead to inconsistencies and is prohibited.\r\n\tat io.atomix.raft.storage.log.RaftLog.deleteAfter(RaftLog.java:168) ~[zeebe-atomix-cluster-8.2.13.jar:8.2.13]\r\n\tat io.atomix.raft.roles.PassiveRole.tryToAppend(PassiveRole.java:565) ~[zeebe-atomix-cluster-8.2.13.jar:8.2.13]\r\n\tat io.atomix.raft.roles.PassiveRole.appendEntries(PassiveRole.java:511) ~[zeebe-atomix-cluster-8.2.13.jar:8.2.13]\r\n\tat io.atomix.raft.roles.PassiveRole.handleAppend(PassiveRole.java:367) ~[zeebe-atomix-cluster-8.2.13.jar:8.2.13]\r\n```\r\n\r\nWhat happened:\r\n\r\nFirst Zeebe-1 votes for Zeebe-2 and Zeebe-2 becomes leader.\r\n```\r\nINFO 2023-09-26T11:25:57.870482436Z [jsonPayload.context.partitionId: 26] [resource.labels.podName: zeebe-1] RaftServer{raft-partition-partition-26}{role=FOLLOWER} - Accepted PollRequest{term=0, candidate=2, lastLogIndex=3982, lastLogTerm=2}: candidate's log is up-to-date\r\nINFO 2023-09-26T11:25:57.887139637Z [jsonPayload.context.partitionId: 26] [resource.labels.podName: zeebe-1] RaftServer{raft-partition-partition-26}{role=FOLLOWER} - Accepted VoteRequest{term=1, candidate=2, lastLogIndex=3982, lastLogTerm=2}: candidate's log is up-to-date\r\nINFO 2023-09-26T11:25:57.892834402Z [jsonPayload.context.partitionId: 26] [resource.labels.podName: zeebe-1] RaftServer{raft-partition-partition-26} - Found leader 2\r\n```\r\nSee above. `term` is 1, but the lastLogTerm is 2. This is because after restore, raft metastore is empty. So it restarts the term from 1. This is ok so far. But after Zeebe-2 becomes the leader and commits its InitialEntry at index `3983`, Zeebe-0 starts election probably because it did not receive any heartbeat from the leader `Zeebe-2`.\r\n\r\n```\r\nINFO 2023-09-26T11:26:01.569385207Z [jsonPayload.context.partitionId: 26] [resource.labels.podName: zeebe-1] RaftServer{raft-partition-partition-26}{role=FOLLOWER} - Accepted PollRequest{term=1, candidate=0, lastLogIndex=3982, lastLogTerm=2}: candidate's log is up-to-date\r\nINFO 2023-09-26T11:26:01.577404370Z [jsonPayload.context.partitionId: 26] [resource.labels.podName: zeebe-1] RaftServer{raft-partition-partition-26}{role=FOLLOWER} - Accepted VoteRequest{term=2, candidate=0, lastLogIndex=3982, lastLogTerm=2}: candidate's log is up-to-date\r\nINFO 2023-09-26T11:26:01.582069421Z [jsonPayload.context.partitionId: 26] [resource.labels.podName: zeebe-1] RaftServer{raft-partition-partition-26} - Found leader 0\r\n```\r\nZeebe-1 accepts poll request from Zeebe-0 because Zeebe-0's lastLogTerm > Zeebe-1's current term 1. This leads to inconsistency. \r\n\r\n**Impact**\r\nIf it happens immediately after restore before writing any new data, it should be safe to restart the brokers. Due to the above error, the new leader Zeebe-0 cannot commit anything. So, as a result there won't be any actual data inconsistency. If we wait until all nodes are healthy before restarting the traffic to the cluster, this will not lead to any actual data inconsistency because no user requests has been processed yet. However, if there are new user data that has been processed, then there is a possibility for partial data loss. **Note that the data recovered from the backup won't be lost if this happens. Only new data will be affected.**\r\n\r\n**To Reproduce**\r\n\r\nFollowing scenario can lead to this error:\r\n1. A, B, C restored from the same backup, where the term of the last entry in the log is > 1\r\n2. A, B forms the quorum and A became the leader\r\n3. A and B committed new entries \r\n4. C does not know about the new leader. So it starts new election.\r\n\r\n**Expected behavior**\r\n\r\nAfter, restoring from a backup raft can continue working in all scenarios.\r\n\r\n**Environment:**\r\n- Zeebe Version: Observed in 8.2.13\r\n\r\nRelates to [https://jira.camunda.com/browse/SUPPORT-19174](https://jira.camunda.com/browse/SUPPORT-19174)\r\n\n\n nicpuppa: This affect also a SM customer ([support case](https://jira.camunda.com/browse/SUPPORT-19174)). In this case the problem was with `term`. Since it is reset, no new snapshots can be taken until the term increases to a higher value than the one before the backup. As a result, logs are not compacted and disk usage grows high.\r\n\r\nFixed by this [PR](https://github.com/camunda/zeebe/pull/15272)",
    "title": "Potential inconsistency in raft after restoring from a backup"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14465",
    "component": "Zeebe",
    "subcomponent": "Broker",
    "context": "Bug Fixes",
    "gitHubText": "## Description\r\n\r\nWhen we delete a process definition we have a check that looks for any running process instances that belong to the definition. If there is any, the command gets rejected and we don't allow the deletion to happen.\r\n\r\nThe check doesn't keep in mind that there could be banned instances. As a result, a definition with banned instances is always rejected. What's worse is that the check only happens for the partition which receives the command. As a result the command could be accepted on the receiving partition, but keeps getting rejected on other partition where there may be a banned instance.\r\n\r\n## Impact\r\n**Scenario 1**\r\nA user is unable to delete a process definition because there is a banned instance which causes the command to be rejected.\r\n\r\n**Scenario 2**\r\nA user deletes a process definition. The command is accepted and distributed. The partition that it gets distributed to has a banned instance. As a result it will never send an acknowledgment. The distributing partition will retry the command distribution indefinitely. This will happen every 5 minutes.\r\nIn the meantime users will still be able to start new instances for the process definition. However, this will be flaky as the definition is deleted on some partitions, but not on others. There is a state inconsistency until this bug is resolved.\n",
    "title": "Deleting a process definition only checks running instances and doesn't ignore banned instances"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14109",
    "component": "Zeebe",
    "subcomponent": "Broker",
    "context": "Bug Fixes",
    "gitHubText": "**Describe the bug**\r\n\r\nWhen the user task assignee is set to a static number value (not a FEEL expression), the activation of the user task leads to an exception.\r\n\r\n<img width=\"633\" alt=\"image\" src=\"https://github.com/camunda/zeebe/assets/62740686/2c846f19-9129-47a0-a1a9-0d312df25c66\">\r\n\r\nException:\r\n```\r\nExpected result of the expression '1547044236679495680' to be 'STRING', but was 'NUMBER'.\r\n```\r\n\r\n**To Reproduce**\r\n\r\n1. Create a model with an assignee of a user task set to a static number value, e.g. `12345567`.\r\n2. Deploy the model.\r\n3. Start an instance of the model.\r\n4. Let the instance reach the user task.\r\n\r\n**Expected behavior**\r\n\r\nThe user task is executed and the task's assignee is set to `12345567` (string value).\r\n\r\n**Log/Stacktrace**\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\n08:01:33.484 [] INFO  io.camunda.zeebe.test - Test failed, following records were exported:\r\n08:01:33.610 [] INFO  io.camunda.zeebe.test - Compact log representation:\r\n--------\r\n\t['C'ommand/'E'event/'R'ejection] [valueType] [intent] - #[position]->#[source record position] K[key] - [summary of value]\r\n\tP9K999 - key; #999 - record position; \"ID\" element/process id; @\"elementid\"/[P9K999] - element with ID and key\r\n\tKeys are decomposed into partition id and per partition key (e.g. 2251799813685253 -> P1K005). If single partition, the partition is omitted.\r\n\tLong IDs are shortened (e.g. 'startEvent_5d56488e-0570-416c-ba2d-36d2a3acea78' -> 'star..acea78'\r\n--------\r\nC DPLY     CREATE         - #01-> -1  -1 - \r\nE PROC     CREATED        - #02->#01 K01 - process.xml -> \"process\" (version:1)\r\nE DPLY     CREATED        - #03->#01 K02 - process.xml\r\nC CREA     CREATE         - #04-> -1  -1 - new <process \"process\"> (default start)  (no vars)\r\nC PI       ACTIVATE       - #05->#04 K03 - PROCESS \"process\" in <process \"process\"[K03]>\r\nE CREA     CREATED        - #06->#04 K04 - new <process \"process\"> (default start)  (no vars)\r\nE PI       ACTIVATING     - #07->#04 K03 - PROCESS \"process\" in <process \"process\"[K03]>\r\nE PI       ACTIVATED      - #08->#04 K03 - PROCESS \"process\" in <process \"process\"[K03]>\r\nC PI       ACTIVATE       - #09->#04  -1 - START_EVENT \"startEv..da0e8bd\" in <process \"process\"[K03]>\r\nE PI       ACTIVATING     - #10->#04 K05 - START_EVENT \"startEv..da0e8bd\" in <process \"process\"[K03]>\r\nE PI       ACTIVATED      - #11->#04 K05 - START_EVENT \"startEv..da0e8bd\" in <process \"process\"[K03]>\r\nC PI       COMPLETE       - #12->#04 K05 - START_EVENT \"startEv..da0e8bd\" in <process \"process\"[K03]>\r\nE PI       COMPLETING     - #13->#04 K05 - START_EVENT \"startEv..da0e8bd\" in <process \"process\"[K03]>\r\nE PI       COMPLETED      - #14->#04 K05 - START_EVENT \"startEv..da0e8bd\" in <process \"process\"[K03]>\r\nE PI       SEQ_FLOW_TAKEN - #15->#04 K06 - SEQUENCE_FLOW \"sequenc..22d1677\" in <process \"process\"[K03]>\r\nC PI       ACTIVATE       - #16->#04 K07 - USER_TASK \"task\" in <process \"process\"[K03]>\r\nE PI       ACTIVATING     - #17->#04 K07 - USER_TASK \"task\" in <process \"process\"[K03]>\r\nE INCIDENT CREATED        - #18->#04 K08 - EXTRACT_VALUE_ERROR Expected result of the expression '1234567891011121314' to be 'STRING', but was 'NUMBER'.,  @\"task\"[K07] in <process \"process\"[K03]>\r\n\r\n-------------- Deployed Processes ----------------------\r\nprocess.xml -> \"process\" (version:1)[K01] ------\r\n<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n<definitions xmlns:bpmndi=\"http://www.omg.org/spec/BPMN/20100524/DI\" xmlns:dc=\"http://www.omg.org/spec/DD/20100524/DC\" xmlns:di=\"http://www.omg.org/spec/DD/20100524/DI\" xmlns:ns0=\"http://camunda.org/schema/zeebe/1.0\" exporter=\"Zeebe BPMN Model\" exporterVersion=\"${project.version}\" id=\"definitions_75c21f8c-968a-4322-a009-827562ef7cd8\" xmlns:modeler=\"http://camunda.org/schema/modeler/1.0\" modeler:executionPlatform=\"Camunda Cloud\" modeler:executionPlatformVersion=\"${project.version}\" targetNamespace=\"http://www.omg.org/spec/BPMN/20100524/MODEL\" xmlns=\"http://www.omg.org/spec/BPMN/20100524/MODEL\">\r\n  <process id=\"process\" isExecutable=\"true\">\r\n    <startEvent id=\"startEvent_61a36e5e-4c70-4f51-879b-7e626da0e8bd\">\r\n      <outgoing>sequenceFlow_acf570d9-e7c2-4b21-bbb0-abfe122d1677</outgoing>\r\n    </startEvent>\r\n    <userTask id=\"task\" name=\"task\">\r\n      <extensionElements>\r\n        <ns0:assignmentDefinition assignee=\"1234567891011121314\"/>\r\n      </extensionElements>\r\n      <incoming>sequenceFlow_acf570d9-e7c2-4b21-bbb0-abfe122d1677</incoming>\r\n      <outgoing>sequenceFlow_af07740c-3c8f-43ba-9dc4-fc17cd763919</outgoing>\r\n    </userTask>\r\n    <sequenceFlow id=\"sequenceFlow_acf570d9-e7c2-4b21-bbb0-abfe122d1677\" sourceRef=\"startEvent_61a36e5e-4c70-4f51-879b-7e626da0e8bd\" targetRef=\"task\"/>\r\n    <endEvent id=\"endEvent_534843ee-82a6-4517-8f36-227ccc58fb59\">\r\n      <incoming>sequenceFlow_af07740c-3c8f-43ba-9dc4-fc17cd763919</incoming>\r\n    </endEvent>\r\n    <sequenceFlow id=\"sequenceFlow_af07740c-3c8f-43ba-9dc4-fc17cd763919\" sourceRef=\"task\" targetRef=\"endEvent_534843ee-82a6-4517-8f36-227ccc58fb59\"/>\r\n  </process>\r\n  <bpmndi:BPMNDiagram id=\"BPMNDiagram_2efdf830-44de-4ca3-bc61-fe5370e0e2ae\">\r\n    <bpmndi:BPMNPlane bpmnElement=\"process\" id=\"BPMNPlane_d580e40c-3af7-4444-9a9c-b3b39ab750ff\">\r\n      <bpmndi:BPMNShape bpmnElement=\"startEvent_61a36e5e-4c70-4f51-879b-7e626da0e8bd\" id=\"BPMNShape_7f7e5e4a-d4c3-4654-acac-0c086e6008b5\">\r\n        <dc:Bounds height=\"36.0\" width=\"36.0\" x=\"100.0\" y=\"100.0\"/>\r\n      </bpmndi:BPMNShape>\r\n      <bpmndi:BPMNShape bpmnElement=\"task\" id=\"BPMNShape_8854f338-096c-4ef7-aaaf-18f865708d8b\">\r\n        <dc:Bounds height=\"80.0\" width=\"100.0\" x=\"186.0\" y=\"78.0\"/>\r\n      </bpmndi:BPMNShape>\r\n      <bpmndi:BPMNEdge bpmnElement=\"sequenceFlow_acf570d9-e7c2-4b21-bbb0-abfe122d1677\" id=\"BPMNEdge_061e7ea7-c454-4b68-a8b9-a68d5189a850\">\r\n        <di:waypoint x=\"136.0\" y=\"118.0\"/>\r\n        <di:waypoint x=\"186.0\" y=\"118.0\"/>\r\n      </bpmndi:BPMNEdge>\r\n      <bpmndi:BPMNShape bpmnElement=\"endEvent_534843ee-82a6-4517-8f36-227ccc58fb59\" id=\"BPMNShape_d3ae1f10-bb2c-492c-9616-61628bdddd14\">\r\n        <dc:Bounds height=\"36.0\" width=\"36.0\" x=\"336.0\" y=\"100.0\"/>\r\n      </bpmndi:BPMNShape>\r\n      <bpmndi:BPMNEdge bpmnElement=\"sequenceFlow_af07740c-3c8f-43ba-9dc4-fc17cd763919\" id=\"BPMNEdge_2edec7f1-95a7-4f99-9724-097f787280db\">\r\n        <di:waypoint x=\"286.0\" y=\"118.0\"/>\r\n        <di:waypoint x=\"336.0\" y=\"118.0\"/>\r\n      </bpmndi:BPMNEdge>\r\n    </bpmndi:BPMNPlane>\r\n  </bpmndi:BPMNDiagram>\r\n</definitions>\r\n\r\n\r\n--------------- Decomposed keys (for debugging) -----------------\r\n -1 <-> -1\r\nK01 <-> 2251799813685249\r\nK02 <-> 2251799813685250\r\nK03 <-> 2251799813685251\r\nK04 <-> 2251799813685252\r\nK05 <-> 2251799813685253\r\nK06 <-> 2251799813685254\r\nK07 <-> 2251799813685255\r\nK08 <-> 2251799813685256\r\n\r\n\r\nio.camunda.zeebe.test.util.stream.StreamWrapperException: No event found matching the criteria\r\n\r\n\tat java.base/java.util.Optional.orElseThrow(Optional.java:403)\r\n\tat io.camunda.zeebe.test.util.stream.StreamWrapper.getFirst(StreamWrapper.java:91)\r\n\tat io.camunda.zeebe.engine.processing.bpmn.activity.UserTaskTest.shouldCreateJobWithAssigneeNumberAsStringHeader(UserTaskTest.java:322)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)\r\n\tat org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)\r\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)\r\n\tat org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)\r\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)\r\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)\r\n\tat org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)\r\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)\r\n\tat org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)\r\n\tat org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)\r\n\tat org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)\r\n\tat org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)\r\n\tat org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)\r\n\tat org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)\r\n\tat org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)\r\n\tat org.junit.rules.RunRules.evaluate(RunRules.java:20)\r\n\tat org.junit.rules.RunRules.evaluate(RunRules.java:20)\r\n\tat org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)\r\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:413)\r\n\tat org.junit.runner.JUnitCore.run(JUnitCore.java:137)\r\n\tat com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)\r\n\tat com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)\r\n\tat com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)\r\n\tat com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)\r\n\tat com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:232)\r\n\tat com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:55)\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- OS: <!-- [e.g. Linux] --> linux\r\n- Zeebe Version: <!-- [e.g. 0.20.0] --> 8.2.12\r\n- Configuration: <!-- [e.g. exporters etc.] --> n/a\r\n\n\n korthout: Triage:\n- Valid concern, the static value should be considered to be a string even when it is a number\n- Workaround is to define it as a string literal expression `= \"1547044236679495680\"`\n- Low hanging fruit, we think it's straightforward to resolve\n- If anyone is interested in working on this, please let us know so we can help out\n zxuanhong: @korthout That shouldn't be a problem. So I will close\n korthout: Happy to hear that @zxuanhong \r\n\r\nI'd still like this bug to be fixed at some point, so I'll reopen it. It would be a great issue for any new onboarding team members to get familiar with the FEEL expression vs static expressions concepts.\n tmetzke: Scope clarification:\r\n\r\n* This only happens for number values (every other static value is treated as a string already).\r\n* This only happens for the assignee attribute (the other user task fields `candidateGroups`, `candidateUsers`, `dueDate`, and `followUpDate` require specific input that is also validated and don't allow plain static number values).",
    "title": "Static user task assignee value treated as number (leads to an exception)"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15677",
    "component": "Zeebe",
    "subcomponent": "Gateway",
    "context": "Bug Fixes",
    "gitHubText": "**Describe the bug**\r\n\r\nOur [docs state](https://docs.camunda.io/docs/self-managed/zeebe-deployment/zeebe-gateway/interceptors/#loading-an-interceptor-into-a-gateway) that gRPC interceptors are called in the order they are added in the configuration. This is not the case, as the configuration order is not always respected.\r\n\r\n**To Reproduce**\r\n\r\n1. Replace the `containsExactlyInAnyOrder` with `containsExactly` in [this test case](https://github.com/camunda/zeebe/blob/main/gateway/src/test/java/io/camunda/zeebe/gateway/interceptors/impl/InterceptorRepositoryTest.java#L196).\r\n2. Run the test.\r\n\r\n**Expected behavior**\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nThe test passes, as the interceptors are instantiated in the same order as they were added.\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\n<STACKTRACE>\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- OS: <!-- [e.g. Linux] -->\r\n- Zeebe Version: <!-- [e.g. 0.20.0] --> `8.4.0-SNAPSHOT`\r\n- Configuration: <!-- [e.g. exporters etc.] -->\r\n\n\n koevskinikola: Triage:\r\n* Severity is `mid`, as the behavior doesn't impact the correct behavior of Zeebe, but users may experience unexpected behavior if they have interdependent interceptors.\r\n* Size is small, as it requires a refactoring of the `IdentityRepository`.\n koevskinikola: Fixed by https://github.com/camunda/zeebe/pull/15566/commits/95ac625e0102e7c85db590970fb820f6de800c98",
    "title": "Interceptor order is not respected"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14176",
    "component": "Zeebe",
    "subcomponent": "Java Client",
    "context": "Bug Fixes",
    "gitHubText": "**Describe the bug**\r\n\r\nThe constructor of the `ActivateJobsCommandImpl` is trying to set the `workerName` during construction and expects the `config.getDefaultJobWorkerName())` to be non null. This leads to a `NullPointerException` as the underlying builder doesnt support a null value. This is an issue as clients are able to set the `workerName` using the builder pattern after constructing the command.\r\n\r\n```\r\npublic ActivateJobsCommandImpl(\r\n      final GatewayStub asyncStub,\r\n      final ZeebeClientConfiguration config,\r\n      final JsonMapper jsonMapper,\r\n      final Predicate<Throwable> retryPredicate) {\r\n    this.asyncStub = asyncStub;\r\n    this.jsonMapper = jsonMapper;\r\n    this.retryPredicate = retryPredicate;\r\n    builder = ActivateJobsRequest.newBuilder();\r\n    requestTimeout(config.getDefaultRequestTimeout());\r\n    timeout(config.getDefaultJobTimeout());\r\n    workerName(config.getDefaultJobWorkerName());\r\n  }\r\n```\r\n\r\n**To Reproduce**\r\n\r\nCreate an implementation of the `ZeebeClientProperties` that returns `null` for the `getDefaultJobWorkerName`\r\n\r\n**Expected behavior**\r\n\r\nThe builder should fallback to the default worker name only if it wasnt set by a client when constructing the final command.\r\n\r\n**Log/Stacktrace**\r\n\r\n```\r\njava.lang.NullPointerException: null\r\n\tat io.camunda.zeebe.gateway.protocol.GatewayOuterClass$ActivateJobsRequest$Builder.setWorker(GatewayOuterClass.java:2169)\r\n\tat io.camunda.zeebe.client.impl.command.ActivateJobsCommandImpl.workerName(ActivateJobsCommandImpl.java:83)\r\n\tat io.camunda.zeebe.client.impl.command.ActivateJobsCommandImpl.<init>(ActivateJobsCommandImpl.java:60)\r\n```\r\n\r\n**Environment:**\r\n- Zeebe Version:8.3.0-alpha4\r\n\r\nLooks like it happens as the command builder is used instead of the previously used request builder since: https://github.com/camunda/zeebe/commit/359a402b5bbee7247749385a458c5b3f3aba7e78\n\n korthout: @npepinpe This sounds like a regression introduced with #12888. Could you please have a look?\n npepinpe: We had a look together, and AFAIK this is the same behavior as ever. We were always calling `workerName` in the constructor of the command, and it makes sense to me to follow Simon's suggestion and only apply defaults in the build method, not in the constructor.\r\n\r\nEDIT: here's the commit which introduced setting the worker name as the default configuration: https://github.com/camunda/zeebe/blob/06f4e463efd3ca1be67056660f12abd4ad7867d8/clients/java/src/main/java/io/zeebe/client/impl/job/ActivateJobsCommandImpl.java#L46\r\n\r\nThat's from 2018, so it looks to me like this was always the behavior. The only thing I can imagine would be that _maybe_ the gRPC behavior changed, where they simply don't allow null values anymore, but I doubt it. I suspect if we check, it was always like this.\n npepinpe: > Looks like it happens as the command builder is used instead of the previously used request builder since: https://github.com/camunda/zeebe/commit/359a402b5bbee7247749385a458c5b3f3aba7e78\r\n\r\n@sbuettner - can you point out to me where the change from request to command builder happened? From the commit, the code is the same - we extracted an interface out of a class, but the implementation was not changed.\r\n\r\nFor example, in the \"old\" implementation:\r\n\r\n```java\r\n    final ActivateJobsCommandStep3 activateCommand =\r\n        jobClient\r\n            .newActivateJobsCommand()\r\n            .jobType(jobType)\r\n            .maxJobsToActivate(maxJobsToActivate)\r\n            .timeout(timeout)\r\n            .workerName(workerName);\r\n```\r\n\r\nAnd it was simply moved in the \"new\" one, but remains the same:\r\n\r\n```java\r\n    final ActivateJobsCommandStep3 activateCommand =\r\n        jobClient\r\n            .newActivateJobsCommand()\r\n            .jobType(jobType)\r\n            .maxJobsToActivate(maxJobsToActivate)\r\n            .timeout(timeout)\r\n            .workerName(workerName);\r\n```\r\n\r\nI'm also not sure what you mean by request vs command builder :thinking: \n sbuettner: @npepinpe It looks like it was changed here: https://github.com/camunda/zeebe/commit/11c548dd4d46f62119a600da0dc1abad7231c417#diff-7101968ae21254c317a1606e4a1969ec3864b538403541ecc7161351b2bd38ddL86\n npepinpe: Good catch! Seems unrelated to job push, but it is indeed a regression in that sense for 8.3.0. I'll defer to the ZPA team on what the priority for it is.\n korthout: Thanks @npepinpe and @sbuettner for the extra input.\r\n\r\n- We'll need to fix this before 8.3 as it regressed recently.\r\n- We consider this mid severity, as a workaround is available (don't set null as the default job worker name)\r\n- We wonder how quickly a user would run into this, do you have to set `null` explicitly, or does it also occur when the client doesn't define any default?\r\n- This would change the impact dramatically, and so our priority\r\n- We want to timebox whether that is the case\r\n- It might be worth it to fix immediately then (but please timebox it)",
    "title": "ActivateJobsCommandImpl throws NullPointerException when ZeebeClientProperties.getDefaultJobWorkerName is null"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15726",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "gitHubText": "null\n",
    "title": "After cancellation, a completed change operation should not overwrite updated topology"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15447",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "gitHubText": "**Describe the bug**\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nAs visible in [this comment](https://github.com/camunda/feel-scala/pull/750#pullrequestreview-1731283017) and in the description of [this issue](https://github.com/camunda/zeebe/issues/9859) special characters in static input are escaped.\r\n\r\nThe provided static input\r\n\r\n| name | value |\r\n|:---|:---|\r\n| static_input | `Hello\\n\\nYOU!` |\r\n\r\nbecomes:\r\n\r\n| name | value |\r\n|:---|:---|\r\n| static_input | `\"Hello\\\\\\\\n\\\\\\\\nYOU!\"` |\r\n\r\n**To Reproduce**\r\n\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\nFollow the step in the issue linked above\r\n\r\n**Expected behavior**\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\nCharacters should not be escaped\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\nnone\r\n\r\n\n",
    "title": "Special characters in static inputs should not be escaped"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15445",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "gitHubText": "**Description**\r\n\r\nFEEL 1.17.3 resolves the issue related to [escaping special characters](https://github.com/camunda/feel-scala/issues/701). On Zeebe side we should verify that special characters inside input mappings are not escaped anymore.\r\n\r\n\r\n\r\n\n",
    "title": "Verify correct behaviour of input mappings after bump to FEEL 1.17.3"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15381",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "gitHubText": "While testing the impact of pod restarts during dynamic scaling, we observed the following error and partition join was stuck. \r\n```\r\njava.lang.NullPointerException: Cannot invoke \"io.atomix.raft.storage.system.Configuration.index()\" because the return value of \"io.atomix.raft.cluster.impl.RaftClusterContext.getConfiguration()\" is null\r\n\tat io.atomix.raft.impl.RaftContext.setCommitIndex(RaftContext.java:503) ~[zeebe-atomix-cluster-8.4.0-SNAPSHOT.jar:8.4.0-SNAPSHOT]\r\n\tat io.atomix.raft.roles.PassiveRole.appendEntries(PassiveRole.java:588) ~[zeebe-atomix-cluster-8.4.0-SNAPSHOT.jar:8.4.0-SNAPSHOT]\r\n\tat io.atomix.raft.roles.PassiveRole.handleAppend(PassiveRole.java:426) ~[zeebe-atomix-cluster-8.4.0-SNAPSHOT.jar:8.4.0-SNAPSHOT]\r\n\tat io.atomix.raft.roles.PassiveRole.onAppend(PassiveRole.java:349) ~[zeebe-atomix-cluster-8.4.0-SNAPSHOT.jar:8.4.0-SNAPSHOT]\r\n\tat io.atomix.raft.impl.RaftContext.lambda$registerHandlers$19(RaftContext.java:323) ~[zeebe-atomix-cluster-8.4.0-SNAPSHOT.jar:8.4.0-SNAPSHOT]\r\n\tat io.atomix.raft.impl.RaftContext.lambda$runOnContext$26(RaftContext.java:334) ~[zeebe-atomix-cluster-8.4.0-SNAPSHOT.jar:8.4.0-SNAPSHOT]\r\n\tat io.atomix.utils.concurrent.SingleThreadContext$WrappedRunnable.run(SingleThreadContext.java:178) ~[zeebe-atomix-utils-8.4.0-SNAPSHOT.jar:8.4.0-SNAPSHOT]\r\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) ~[?:?]\r\n\tat java.base/java.util.concurrent.FutureTask.run(Unknown Source) ~[?:?]\r\n\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source) ~[?:?]\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]\r\n\tat java.base/java.lang.Thread.run(Unknown Source) [?:?]\r\n```\r\nWorkaround: restart both the leader and the restarted pod.\r\n\r\nInitial root cause analysis\r\n\r\n- Server B sends join request to the leader\r\n- Leader commits the configuration in the existing members\r\n- Server B receives the configuration, but did not receive the commit information => configuration is kept in memory\r\n- Server B restarts (pod was restarted)\r\n- Server B retries join request => noop because previous join has already completed\r\n- Leader does not sent the configuration again, because Leader assumes Server B already has it. When it sends the Append request, it results in null pointer exception.\r\n\r\nThis was not a problem before because all servers always started with the static configuration.\r\n\r\nProposed fixes\r\n1. Quick fix: Leader should sent the configuration again when it receives a duplicate join request. Handle null pointer exception in the follower, by simply rejecting the AppendRequest. Eventually the follower should receive the new configuration.\r\n2. Probably a better fix: Include the `configurationIndex` in the `AppendResponse` so that the leader can re-send the configuration if required. \n",
    "title": "NullPointerException when join is retried after a restart"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15343",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "gitHubText": "**Summary**\r\n\r\n> Try to answer the following as best as possible\r\n\r\n- How often does the test fail? several times \r\n- Does it block your work? It stopped me from merging some changes\r\n- Do we suspect that it is a real failure? ðŸ¤· \r\n\r\n**Failures**\r\n\r\n> Outline known failure cases, e.g. a failed assertion and its stacktrace obtained from Jenkins\r\n\r\n<details><summary>GitHub Error</summary>\r\n<pre>\r\nThe operation was canceled.\r\n</pre>\r\n</details>\r\n\r\n**Hypotheses**\r\n\r\n> List any hypotheses if you have one; can be ommitted\r\n\r\n**Logs**\r\n\r\n> If possible, provide more context here, e.g. standard output logs, link to build, etc.\r\n\r\n[io.camunda.zeebe.it.clustering.dynamic.ScaleResiliencyTest$ScaleDown-1-output.txt](https://github.com/camunda/zeebe/files/13451838/io.camunda.zeebe.it.clustering.dynamic.ScaleResiliencyTest.ScaleDown-1-output.txt)\r\n\r\n**Additional**\r\n\r\nI was able to find out which test was still running thanks to @deepthidevaki's awesome script:\r\n\r\n```sh\r\ngrep \"Running io.\" test.out > running.out\r\n\r\ngrep \"Tests run:.*\" test.out | grep \"io.\" > testsRan.out\r\ncut -d' ' -f3 running.out > testsRunning.out\r\ncut -d'-' -f3 testsRan.out | cut -d' ' -f3 > testsRanClasses.out\r\n\r\ngrep -vf  testsRanClasses.out testsRunning.out \r\n```\r\n\n\n deepthidevaki: Observations from the log:\r\n\r\nBroker-2 has attempted to leave partition 2, but could not succeed because Broker-1 is not available. But it has already committed that first part of the joint consensus. When Broker-1 is restarted, in some cases partition 2 in Broker-2 is not yet started. In this case, Broker-0 which is the leader might commit the second part of joint consensus before Broker-2 is ready. When this happens, Broker-2 waits forever to become partition-2 to be ready. \r\n\r\n```\r\n15:02:25.031 [Broker-0] [raft-server-2] [raft-server-0-2] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{raft-partition-partition-2}{role=CANDIDATE} - Cancelling election\r\n15:02:25.032 [Broker-0] [raft-server-2] [raft-server-0-2] INFO  io.atomix.raft.impl.RaftContext - RaftServer{raft-partition-partition-2} - Found leader 0\r\n15:02:25.032 [Broker-0] [raft-server-2] [raft-server-0-2] DEBUG io.atomix.raft.roles.LeaderAppender - RaftServer{raft-partition-partition-2} - Configuring 2\r\n15:02:25.032 [Broker-0] [raft-server-2] [raft-server-0-2] DEBUG io.atomix.raft.roles.LeaderAppender - RaftServer{raft-partition-partition-2} - Configuring 1\r\n15:02:25.034 [Broker-2] [raft-server-2] [raft-server-2-2] INFO  io.atomix.raft.impl.RaftContext - RaftServer{raft-partition-partition-2} - Found leader 0\r\n```\r\nHere leader is configuring both 1 and 2 with the first part of joint consensus which was already committed before the restart of broker-2.\r\n```\r\n15:02:25.035 [Broker-1] [raft-server-2] [raft-server-1-2] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{raft-partition-partition-2}{role=FOLLOWER} - Rejected InternalAppendRequest[term=3, leader=0, prevLogIndex=5, prevLogTerm=2, commitIndex=4, entries=[]]: Previous index (5) is greater than the local log's last index (3)\r\n15:02:25.035 [Broker-0] [raft-server-2] [raft-server-0-2] DEBUG io.atomix.raft.roles.LeaderAppender - RaftServer{raft-partition-partition-2} - Configuring 1\r\n```\r\nHere, Broker-0 configures Broker 1 with second part of joint consensus where Broker-2 is not part of the configuration anymore.\r\n```\r\n15:02:25.037 [Broker-1] [raft-server-2] [raft-server-1-2] INFO  io.atomix.raft.impl.RaftContext - RaftServer{raft-partition-partition-2} - Setting firstCommitIndex to 4. RaftServer is ready only after it has committed events upto this index\r\n15:02:25.037 [Broker-1] [raft-server-2] [raft-server-1-2] INFO  io.atomix.raft.impl.RaftContext - RaftServer{raft-partition-partition-2} - Commit index is 4. RaftServer is ready\r\n15:02:25.037 [Broker-1] [raft-server-2] [raft-server-1-2] DEBUG io.atomix.raft.impl.DefaultRaftServer - RaftServer{raft-partition-partition-2} - Server started successfully!\r\n15:02:25.037 [Broker-1] [raft-server-2] [raft-server-1-2] INFO  io.atomix.raft.partition.impl.RaftPartitionServer - RaftPartitionServer{raft-partition-partition-2} - Server successfully bootstrapped partition PartitionId{id=2, group=raft-partition} in 4096ms\r\n```\r\nBroker-2 has already left the configuration, So it never becomes READY. As a result `startFuture` in Partition's StartupProcess is never completed.\r\n```\r\n15:02:32.091 [Broker-2] [Startup] [zb-actors-0] INFO  io.camunda.zeebe.broker.partitioning.PartitionManagerImpl - Leaving partition 2\r\n15:02:32.094 [Broker-2] [Startup] [zb-actors-1] DEBUG io.camunda.zeebe.broker.partitioning.Partition - Shutdown was called with context: PartitionStartupContext{partition=2}\r\n```\r\nLeave completes successfully, as it has already left the configuration. So it tries to shutdown. But [shutdown is blocked because it is waiting on the `startupFuture`](https://github.com/camunda/zeebe/blob/b10310596bc38b644d062786efca505d8c51c8fd/scheduler/src/main/java/io/camunda/zeebe/scheduler/startup/StartupProcess.java#L198). \r\n```\r\n15:04:20.968 [Broker-2] [Startup] [zb-actors-0] DEBUG io.camunda.zeebe.broker.partitioning.Partition - Shutdown was called with context: PartitionStartupContext{partition=2}\r\n15:04:20.968 [Broker-2] [Startup] [zb-actors-0] INFO  io.camunda.zeebe.broker.partitioning.Partition - Shutdown already in progress\r\n```\r\nAfter the test fails, when it tries to shutdown the broker, we see that the shutdown of partition 2 is already in progress. Test is stuck waiting in the shutdown leading to the timeout of CI job.\r\n\r\nThe test passes when Broker-2 becomes ready before Broker-1 is up. ",
    "title": "IT timeout due to `ScaleResiliencyTest.ScaleDown`"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15194",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "gitHubText": "This could cause problem in `replay` mode, Form events will be missed and not added to the state.\n\n korthout: I've quickly verified the issue, and this can lead to the loss of the deployed Forms on 8.3 and 8.4.0-alpha1.\r\n\r\nIn that case, existing jobs of User Tasks may still be referring to the key of that form, but it no longer exists in Zeebe. \r\n\r\nWhen instances activate User Task with form id set after the replay missed the Form events, an Incident is raised at the User Task:\r\n```\r\nE PI       ACTIVATING     - #34->#21 K14 - USER_TASK \"task\" in <process \"PROCESS\"[K10]>\r\nE INCIDENT CREATED        - #35->#21 K15 - FORM_NOT_FOUND Expected to find a form with id 'Form_0w7r08e', but no form with this id is found, at least a form with this id should be available. To resolve the Incident please deploy a form with the same id,  @\"task\"[K14] in <process \"PROCESS\"[K10]>\r\n```\n korthout: In short: **The impact is a potential loss of deployed form**. \r\n\r\nWhen that happens, the user will notice because newly activated User Tasks will raise an Incident that the Form cannot be found.\r\n\r\nUser must redeploy the form to fix the problem, but the bug can re-occur afterward until we have a fix.\n korthout: Marking it as sev high because users can recover easily. Loss of data is very limited. And chance of occurrence is low. A fix is on the way though.",
    "title": "Form events are not replayed"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15188",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "gitHubText": "**Describe the bug**\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nA leader update on a partition wasn't successful.\r\n\r\nNo further details could be seen from the log.\r\n\r\n**To Reproduce**\r\n\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\n**Expected behavior**\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\nA partition leader is successfully updated.\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n* [GCP log](https://console.cloud.google.com/logs/query;query=logName:%22stdout%22%0Aresource.type%3D%22k8s_container%22%0Aresource.labels.cluster_name%3D%22worker-chaos-1%22%0Aresource.labels.location%3D%22europe-west1%22%0Aresource.labels.container_name%3D%22zeebe%22%0Aresource.labels.project_id%3D%22camunda-saas-int-chaos%22%0Aresource.labels.namespace_name%3D%2207ed94ef-cfa9-445f-b755-37d1e5c8f2ea-zeebe%22%0Aresource.labels.pod_name%3D%22zeebe-2%22;pinnedLogId=2023-11-13T17:12:02.726497982Z%2Fuwqxje0gquvak55g;cursorTimestamp=2023-11-13T17:12:35.079051138Z;startTime=2023-11-13T16:42:32.726Z;endTime=2023-11-13T17:42:32.726Z?project=camunda-saas-int-chaos) / [downloaded logs](https://drive.google.com/file/d/1SUX9Nb4LMKVs0CV3akvbcxH3hG-j0vxw/view?usp=sharing)\r\n\r\n```\r\nCould not update new leader for partition X at term Y. Expected to have a non-null value for current leader term, but found null\r\n```\r\n\r\n**Environment:**\r\n- OS: <!-- [e.g. Linux] --> Linux (SaaS Int - chaos)\r\n- Zeebe Version: <!-- [e.g. 0.20.0] --> `8.4.0-alpha1`\r\n- Configuration: <!-- [e.g. exporters etc.] -->\r\n\n\n npepinpe: Triage:\n\nWe're having some difficulty assigning a severity and priority to this without more details.\n\n- What was the impact on the cluster? \n- Did it cause any processing delays? \n- Was the cluster unavailable for a period of time? \n- Did it eventually recover? If so, after how long?\n\n deepthidevaki: I will look into it, as I observed it in a qa test.",
    "title": "Could not update new leader for partition"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15129",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "gitHubText": "If we allow the migration of process instances not entirely in a wait state, Zeebe would wrongly execute the process instance after the migration.\r\n\r\nIn short, unprocessed commands referring to the source process definition may exist on the log stream with a position higher than the process instance migration command. When we process these commands, the engine wrongly attempts to continue the processing on the source process definition as if the migration didn't occur. For a complete discussion, please read 'How should we deal with process instances in non-wait states?' in the epic's 'Discussions and Decisions' section. \r\n\r\nWe can either:\r\n1. Reject process instance migration if we know the process instance is not entirely in a wait state.\r\n2. Don't trust the process definition key in such commands, but rather look up the process definition from the state when processing commands.\r\n\r\nA downside to option 1 is: We don't know what commands exist on the log with a higher position than the command being processed. It may be that this solution is not water tight, requiring us to investigate option 2.\r\n\r\nA downside to option 2 is: Some commands may refer to elements that no longer exist in the target process. These will be rejected.\n\n korthout: Labeling this as a bug because it is if we don't resolve this.\n korthout: Estimated as medium, because it's not entirely clear how to resolve this yet. Estimate might be lower when this becomes clear.\n korthout: TL;DR: **I want to focus on option 2 for two reasons**:\r\n- **it should be a design requirement for any command processor in the first place**. By applying this requirement to all command processors, we simplify reasoning about the system and bring consistency to implementation. This requirement should be documented in our developer documentation.\r\n- **it is less restrictive than option 1**, allowing for migrations of process instances in active execution rather than only those in a complete wait state.\r\n\r\n---\r\n\r\nLet's consider the different options.\r\n\r\n1. Reject process instance migration if we know the process instance is not entirely in a wait state.\r\n\r\nThis raises the question: Can we detect that the process instance is currently not entirely in a wait state? \r\n\r\nLet's start with regular process instance execution over sequence flows. Each flow scope element instance keeps track of the number of sequence flows that are currently taken inside it for which the target element has not yet been activated. So, we can use this. \r\n\r\nIf the sequence flow has not yet been taken, an element instance is always active (i.e. activating, activated, completing, or terminating). \r\n\r\nThis is the case irrespective of unprocessed `ProcessInstance` and `ProcessInstanceBatch` commands on the log.\r\n\r\nHowever, other unprocessed commands referring to the process definition on the log may continue the process execution. This means that option 1 is restrictive, and that option 1 is not water tight. This leads us to consider option 2: \r\n\r\n2. Don't trust the process definition key in such commands, but rather look up the process definition from the state when processing commands.\r\n\r\nOption 2 is actually a requirement for any command in our system because of the async nature of Zeebe's engine. Typically, the state is different when the command is appended to the log than when it is processed because other commands may been executed in between. Therefore, we can never trust all data in commands. This rule follows naturally from our stream processing design, and is why we've build things like [ProcessInstanceStateTransitionGuard](https://github.com/camunda/zeebe/blob/main/engine/src/main/java/io/camunda/zeebe/engine/processing/bpmn/ProcessInstanceStateTransitionGuard.java).\r\n\r\nCommands that refer to data that may have changed due to migration are:\r\n- `Timer:TRIGGER` refers to `targetElementId` and `processDefinitionKey` \r\n- `ProcessMessageSubscription:CORRELATE` refers to `bpmnProcessId` and `elementId`\r\n- ~~`Signal:BROADCAST`~~ does not refer to properties that may have changed due to migration\r\n- `Incident:Resolve` refers to `bpmnProcessId`, `processDefinitionKey`, and `elementId`\r\n- `Job:*` (multiple intents) refers to `bpmnProcessId`, `processDefinitionVersion`, `processDefinitionKey`, and `elementId`\r\n- `UserTask:*` (multiple intents) refers to `bpmnProcessId`, `processDefinitionVersion`, `processDefinitionKey`, and `elementId`\r\n\r\nIn addition, the following commands also continue the process execution but could be covered by rejection of the command (option 1) if we follow that approach:\r\n- `ProcessInstance:*` (multiple intents) refers to `bpmnProcessId`, `elementId`, `version`, and `processDefinitionKey`\r\n- `ProcessInstanceBatch:*` (multiple intents)\r\n\r\nLastly, some commands don't directly continue the process execution but still refer to data that may have changed:\r\n- `MessageSubscription:*` (multiple intents) refers to `bpmnProcessId`\r\n\r\nSo, option\n korthout: The number of affected commands is strongly reduced for the first iteration due to the supported scope of instance migration MVP.\r\n\r\nHowever, in later iterations. These problems may surface when more is supported.\n korthout: Quick dump of my latest spike: https://github.com/camunda/zeebe/compare/korthout-15129-unprocessed-commands-with-outdated-data-due-to-migration\n romansmirnov: @korthout and @saig0 ,\r\n\r\na potential option: Would it be possible to bookmark that a current \"active path\" (aka element instance or similar) has an outstanding follow-up command (to bring that in a valid - or wait - state)? While migrating an active path, the processor could check for this bookmark/flag being set or not. And if there is an outstanding command, it stops migrating, rolls back, and rejects the migration command.\r\n\r\nOr the instance itself could bookkeeping all outstanding commands? So they are all in one place stored, and once a command is being executed it is removed from that.\r\n\r\nDetails would need to be defined, but would this resolve the issue? If not, what is missing?\n korthout: @romansmirnov Sorry I did not find the time to answer anymore before my FTO. I believe @saig0 will consider your proposal.\r\n\r\n@saig0 If you manage to resolve this bug, please notify @berkaycanbc that we can remove the `@ExperimentalAPI` annotation from the java-client **and** the danger banner in the [concept docs](https://github.com/camunda/camunda-docs/pull/3000#issuecomment-1847526591)\n saig0: @romansmirnov I will follow your idea. :+1:  \r\n\r\nThe required data about the active \"outstanding\" command should be available already. I will check the data in the migration processor to reject the migration command.",
    "title": "Avoid executing process according to unprocessed commands referring to source process definition"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15047",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "gitHubText": "**Describe the bug**\r\n\r\nThis error happened on Sat 03/11:\r\n\r\n```\r\n\"StreamProcessor-1 failed, marking it as dead: StreamProcessor-1{status=DEAD, issue=HealthIssue[message=null, throwable=io.camunda.zeebe.util.exception.UnrecoverableException: Authorization data unavailable: The Token can't be used before 2023-11-03T14:46:52Z., cause=null]}\"\r\n```\r\n\r\nYou can find the logs [here](https://drive.google.com/drive/folders/1wJqLwgla0px44joF49Ve0bnW0WzZvtQC)\r\n\n\n megglos: ZDP-Triage:\r\n- JWT token validation must relates to multi-tenancy, would delegate this to ZPA\n korthout: ZPA triage:\r\n- marking it as high severity because it caused a DEAD partition\r\n- we should investigate and fix it soon\r\n- might make sense to work on this while multi-tenancy is fresh\r\n\r\n@koevskinikola I heard that you may already have some thoughts about this issue. Could you share them in a comment?\n koevskinikola: Related to a [SUPPORT case](https://jira.camunda.com/browse/SUPPORT-19112) as well.\n koevskinikola: My hypothesis is that this is a time zone issue.\r\n\r\n1. We're using `Instance.now()` to set the `JWT#issuedAt` (`iat`) claim [here](https://github.com/camunda/zeebe/blob/6a730a7bf57f0491cb0b29e5b2c90533330b6c1b/auth/src/main/java/io/camunda/zeebe/auth/impl/JwtAuthorizationEncoder.java#L66).\r\n2. The `Instance.now()` method provides an `Instant` from the system UTC clock.\r\n3. If the cluster is set on a different time zone, the JWT validation might throw an exception that `The Token can't be used before xxx`.\r\n\r\nI will try to confirm this with a test.\r\n\r\nProposed solutions:\r\n1. We use the `ActorClock` to set the time instead.\r\n2. We remove the `JWT.issuedAt` (`iat`) claim\r\n3. We remove the JWT validation when decoding the JWT token.\r\n\r\nDecision:\r\nWe will implement proposals 2 and 3 from above because:\r\n* We're only using the JWT token for data transport. The JWT token is not signed, and not used outside of Zeebe, so any data other than the authorization data isn't relevant. That means that the `issuedAt` claim can be removed.\r\n* Since the token isn't signed, validation isn't required. Coupled with the bad `issuedAt` implementation, it is causing bugs. It is better to disable the validation since existing Zeebe installations may already face the problem, and we want to disable the validation so that the issue doesn't pop up for existing JWT tokens.\r\n   * In that case, we will need to do our own validation that the `authorized_tenants` JWT claim is present. This can be done by doing a null-check when fetching the claim data since if the claim is not present, a `null` is returned.\r\n\r\ncc @korthout \r\n\n romansmirnov: @koevskinikola, just trying to understand the root cause of the issue, have you been able to validate your hypothesis? Isn't this \"just\" a clock syncing problem between the gateway node and broker node? By setting a leeway when verifying the token, would it help mitigate the issue?\r\n\r\nHowever, please consider this just a question to get a better understanding of the root cause. Meaning, I agree with the decision above to solve the problem anyway ðŸ˜„ \n koevskinikola: Hey @romansmirnov ,\r\n\r\nI think you're right. It doesn't seem to be time-zone related, but just a matter of leeway.\r\n\r\nThat's because:\r\n1. The JWT library that we use (`java-jwt`) also uses `Instant.now()` inside the `Verifier` to get the current time to compare against.\r\n    * Since we're also using `Instant.now()` to set the `issuedAt` claim, this means the Verifier is always comparing UTC times.\r\n2. The `defaultLeeway` value in `java-jwt` is set to `0`. This means that verifying this token even half a second earlier will cause an issue (this seems to be the case in one of the occurences).\r\n\r\nSo we can definitely adjust the `leeway` to compensate. However, as I pointed out, we're not really signing the JWT token, so the `issuedAt` claim is irrelevant here.\r\n\r\nI would feel more confident in not verifying the `issuedAt` claim at all right now. If we ever decide to expose the JWT data to exported events, we can:\r\n1. Sign it first.\r\n2. Set and adjust the verification for the best-practice claims (like the `issuedAt`)\r\n\r\nUPDATE:\r\nI confirmed my hypothesis by:\r\n1. Providing a fixed clock with which I set the `issuedAt` claim on a JWT token.\r\n2. Create a JWT Verifier on which I set a fixed clock 1 second behing.\r\n\r\nThe result is that the JWT verification fails with the same message `The Token can't be used before...`/\n Zelldon: Happened in 8.4.0-alpha1 https://console.cloud.google.com/errors/detail/CI_Y0MHF3fmnggE;service=zeebe;time=P7D?project=zeebe-io but I suspect this fix was not part of the release.\n korthout: Nope 0d600f736e6c1c6cc69cda38d40c78dc4d5abfea is not part of `8.4.0-alpha1`.",
    "title": "8.3.x release benchmark - Authorization data unavailable: The Token can't be used before xxx"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14931",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "gitHubText": "**Describe the bug**\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nWhen I deploy a form with the java client, the output of the `DeploymentEvent` doesn't print the form.\r\n\r\n**To Reproduce**\r\n\r\nrun\r\n```\r\nDeploymentEvent deployment = zeebeClient.newDeployResourceCommand().addResourceFromClasspath(\"input_1.form\").send().join();\r\nLOG.info(\"Deployed: {}\", deployment);\r\n```\r\nit prints\r\n```\r\nDeployed: DeploymentEventImpl{key=2251799813804733, processes=[], decisions=[], decisionRequirements=[], tenantId='<default>'}\r\n```\r\n\r\n\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\n**Expected behavior**\r\n\r\nThe output should contain the form as well.\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n**Environment:**\r\n- OS: Windows\r\n- Zeebe Version: 8.3.0\r\n- Configuration: <!-- [e.g. exporters etc.] -->\r\n\n\n remcowesterhoud: ZPA Triage:\r\n- Related Slack thread: https://camunda.slack.com/archives/CSQ2E3BT4/p1698767548209869\r\n- @berkaycanbc please have a look ðŸ‘€ and do an individual triage\n nicpuppa: @berkaycanbc as seems a very easy fix, this should be completed before the next patch release in my opinion. What do you think ? ",
    "title": "A deployed form is not mentioned in the `DeploymentEventImpl.toString()`"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14924",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "gitHubText": "**Describe the bug**\r\n\r\nWhile this is not always the case, in CW44 (and new benchmarks), I've seen that some nodes sometimes end up doing up to 8000 IOPS, instead of the ~4000 from the previous benchmarks. \r\n\r\nNote that some nodes still do 4000, and this issue is intermittent. But it's unclear in our benchmarks why one node would do so much more IOPS. Leadership is well balanced, each partition is doing ~50 PI/s, the journal append rate is similar, the leader commit rate is similar, etc.\r\n\r\nYet somehow, one or more nodes is doing _way_ more I/O than the others.\r\n\r\n**Log/Stacktrace**\r\n\r\nHere's CW43 when one node is leader for 2 partitions:\r\n\r\n![image](https://github.com/camunda/zeebe/assets/43373/0e2450e1-747d-44f2-89fd-fbe9232c943a)\r\n\r\nHere's CW43 when leadership is well balanced:\r\n\r\n![image](https://github.com/camunda/zeebe/assets/43373/446f502a-9dc8-4afc-90ce-8c0127d6d04d)\r\n\r\nNow, it's still weird that one node is doing way less I/O than the others, but we're still at the 4000 mark.\r\n\r\nHere's CW44 with leadership well balanced: \r\n\r\n![image](https://github.com/camunda/zeebe/assets/43373/1f6d1c99-cf5c-4279-afd1-1317b6466ac3)\r\n\n\n Zelldon: Triage:\n\nWe should take a look asap and asses the current state/issue.\n npepinpe: Timebox root causing in the next 2 weeks. First check CW45, if doesn't happen then we can consider it a fluke and close. Otherwise root cause, and put it back into planning to discuss priority on the fix.\n oleschoenburg: I just stumbled over this again and I think it's likely that we are counting some metrics twice.\r\n\r\nJust querying for the raw data shows two almost identical entries for zeebe-0 writing to /dev/sdb.\r\n\r\n```promql\r\nrate(container_fs_writes_bytes_total{cluster=~\"$cluster\", namespace=~\"$namespace\", pod=~\"$pod\"}[$__rate_interval])\r\n```\r\n\r\n```js\r\n{container=\"zeebe\", device=\"/dev/sdb\", endpoint=\"https-metrics\", id=\"/kubepods/burstable/podb8b5b883-fb5f-4da2-933a-7cb778cd3a91/d2ef05d2d8f8442992842479db4d7b2558106a1c53d44c1fac69b76865be8a67\", image=\"gcr.io/zeebe-io/zeebe:medic-y-2023-cw-45-322236a-benchmark-322236a\", instance=\"10.132.0.158:10250\", job=\"kubelet\", metrics_path=\"/metrics/cadvisor\", name=\"d2ef05d2d8f8442992842479db4d7b2558106a1c53d44c1fac69b76865be8a67\", namespace=\"medic-y-2023-cw-45-322236a-benchmark\", node=\"gke-zeebe-cluster-n2-standard-2-d2e6c783-rw8j\", pod=\"medic-y-2023-cw-45-322236a-benchmark-zeebe-0\", service=\"monitoring-kube-prometheus-kubelet\"}\r\n{device=\"/dev/sdb\", endpoint=\"https-metrics\", id=\"/kubepods/burstable/podb8b5b883-fb5f-4da2-933a-7cb778cd3a91\", instance=\"10.132.0.158:10250\", job=\"kubelet\", metrics_path=\"/metrics/cadvisor\", namespace=\"medic-y-2023-cw-45-322236a-benchmark\", node=\"gke-zeebe-cluster-n2-standard-2-d2e6c783-rw8j\", pod=\"medic-y-2023-cw-45-322236a-benchmark-zeebe-0\", service=\"monitoring-kube-prometheus-kubelet\"}\r\n```\n Zelldon: ![iops](https://github.com/camunda/zeebe/assets/2758593/79d970ab-7757-47e5-a127-b3c5c0f56c8b)\r\n\r\nIt is the same what we had with the CPU a while ago\r\n\r\n\r\nhttps://github.com/camunda/zeebe/pull/13386\n Zelldon: It seems to be even quadrupled. \r\n\r\nWe have two different containers empty and zeebe AND two services: monitoring and metrics.\r\n![iops](https://github.com/camunda/zeebe/assets/2758593/ea5fad0a-2ba8-495b-b16d-283ac2aaf588)\r\n\r\n\r\n",
    "title": "IOPS doubled in CW44"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14884",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "gitHubText": "**Describe the bug**\r\n\r\nThe brokers currently naively request that all members restart their streams whenever a new member is added. This causes some noise since obviously the other brokers aren't handling such messages. \r\n\r\nFurther, we don't retry failures, so we could still have streams that are not known on the brokers.\r\n\r\n**Expected behavior**\r\n\r\n- We only request stream restart from gateways.\r\n- We retry (with back off) until the request is successful.\r\n\n",
    "title": "Improve resilience of remote stream restart"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14837",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "gitHubText": "**Describe the bug**\r\n\r\nIn CW42 and CW43, we have a massive UDP resource leak which causes UDP listening connections to eventually max out, such that no new ones can be created. This causes widespread DNS failures, eventually leading to out of disk space as we cannot export data anymore.\r\n\r\nI can confirm this is not happening in 8.3.1 or prior versions.\r\n\r\n**To Reproduce**\r\n\r\nIn any of the pods in CW42 and CW43, run `watch -n1 -- 'wc -l /proc/net/udp'`, and you will see the amount of open UDP connections is constantly increasing, all coming from the Java process.\r\n\r\nA quick `netstat -ulpne` gives little more information.\r\n\r\n**Expected behavior**\r\n\r\nThe only open UDP connections are our Netty unicast service and temporary ones, meaning we should have a baseline of 2, and possibly transient ones that do not remain.\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.4.0-SNAPSHOT CW42 and CW43\r\n\n",
    "title": "UDP resource leaks"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14814",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "gitHubText": "On a Zeebe 8.2.12 Cluster we observed the following load. This does not resolve itself and the load remains constant.\r\n\r\n```\r\nCOMMAND PROCESS_MESSAGE_SUBSCRIPTION CORRELATE 22517998138010331 \r\nCOMMAND PROCESS_MESSAGE_SUBSCRIPTION CORRELATE 22517998139003162 \r\nCOMMAND PROCESS_MESSAGE_SUBSCRIPTION CORRELATE 22517998140261067 \r\nCOMMAND PROCESS_MESSAGE_SUBSCRIPTION CORRELATE 22517998142789239 \r\n...\r\nCOMMAND_REJECTION PROCESS_MESSAGE_SUBSCRIPTION CORRELATE 22517998138010331 \r\nCOMMAND MESSAGE_SUBSCRIPTION REJECT 22517998138010331 \r\nCOMMAND_REJECTION MESSAGE_SUBSCRIPTION REJECT 22517998138010331 \r\nCOMMAND_REJECTION PROCESS_MESSAGE_SUBSCRIPTION CORRELATE 22517998139003162 \r\nCOMMAND MESSAGE_SUBSCRIPTION REJECT 22517998139003162 \r\nCOMMAND_REJECTION MESSAGE_SUBSCRIPTION REJECT 22517998139003162 \r\nCOMMAND_REJECTION PROCESS_MESSAGE_SUBSCRIPTION CORRELATE 22517998140261067 \r\nCOMMAND MESSAGE_SUBSCRIPTION REJECT 22517998140261067 \r\nCOMMAND_REJECTION MESSAGE_SUBSCRIPTION REJECT 22517998140261067 \r\nCOMMAND_REJECTION PROCESS_MESSAGE_SUBSCRIPTION CORRELATE 22517998142789239 \r\nCOMMAND MESSAGE_SUBSCRIPTION REJECT 22517998142789239 \r\nCOMMAND_REJECTION MESSAGE_SUBSCRIPTION REJECT 22517998142789239 \r\n```\r\n\r\nThe `PendingMessageSubscriptionChecker` writes `PROCESS_MESSAGE_SUBSCRIPTION/CORRELATE` commands for entries in `MutablePendingMessageSubscriptionState`. While processing these commands, the `ProcessMessageSubscriptionCorrelateProcessor` doesn't find a matching subscription and writes a `COMMAND_REJECTION` and a `MESSAGE_SUBSCRIPTION/REJECT` command. The `MessageSubscriptionRejectProcessor` _also_ rejects the `MESSAGE_SUBSCRIPTION/REJECT` command by writing a `COMMAND_REJECTION` because it does not find a matching correlation in the message state.\r\n\r\nThe `PROCESS_MESSAGE_SUBSCRIPTION/CORRELATE` is rejected with cause `NOT_FOUND` and a matching message: \"Expected to correlate process message subscription with element key '22517998206768899' and message name 'MY_MESSAGE_NAME', but no such subscription was found\".\r\nThe `MESSAGE_SUBSCRIPTION/REJECT` is rejected with cause `INVALID_STATE` and message \"Expected message '112589990754201722' to be correlated for process with BPMN process id 'MY_PROCESS_ID' but no correlation was found\"\r\n\r\n## Cause\r\n\r\nOn one partition we have the subscription column family containing a subscription with `correlating=true` so the `PROCESS_MESSAGE_SUBSCRIPTION/CORRELATE` is sent again and again. On the other partition, the subscription column family does not contain such an entry.\r\nHypothesis: A `MESSAGE_SUBSCRIPTION/CORRELATE` or  `MESSAGE_SUBSCRIPTION/DELETE`command was lost, leaking the subscription entry. `ProcessMessageSubscriptionCorrelateProcessor` handles non interrupting subscriptions by removing them from the state immediately and then sending `MESSAGE_SUBSCRIPTION/CORRELATE` just once. If that command is lost, nothing will resend it. The other partition keeps the partition in the state, tries to correlate again and again but only gets rejections because the receiving partition doesn't know about the subscription anymore.\r\n \r\n## Related Notes\r\n\r\n- It probably doesn't make sense to reject a `MESSAGE_SUBSCRIPTION/REJECT` command, can't we just clean up the subscription when the receiving partition rejected it?\r\n- Why write `COMMAND_REJECTION`s for internal commands not sent by a user anyway? Especially since the rejection is already described with a `MESSAGE_SUBSCRIPTION/REJECT` command.\r\n- To fully fix this, we need to find a way to recover from this broken state, not just prevent it from happening again.\r\n\r\nrelates to https://jira.camunda.com/browse/SUPPORT-19189\n\n oleschoenburg: Actually a duplicate of https://github.com/camunda/zeebe/issues/5750 (thanks to @saig0 for finding it!) but let's keep this one open because it contains some more information on how a partition can get into this state\n remcowesterhoud: ZPA Triage:\r\n- As there is already a PR open for this I've put it in our iteration and in review so it's visible.\r\n- The PR is in draft, I've asked @oleschoenburg for clarification on the status of the PR.",
    "title": "Leaked message subscriptions cause constant processing load"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14773",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "gitHubText": "**Describe the bug**\r\n\r\nIt would seem aggregate of job streams is failing on the gateway due to our usage of `ArrayProperty` (and underlying `ArrayValue`). The implementation of `equals` and `hashCode` for these is not consistent since it relies on the last modification being reflected in their internal buffer, which is not always the case.\r\n\r\n**To Reproduce**\r\n\r\nStart a cluster of one gateway, one broker. Create two job streams/workers with the exact same parameters. You will see two client streams on the gateway - which is correct - but each will have a different server stream ID, which is incorrect.\r\n\r\n**Expected behavior**\r\n\r\nLogically equivalent client streams are aggregated on the gateway.\n",
    "title": "Job streams not aggregated correctly on gateway"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14771",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "gitHubText": "**Describe the bug**\r\n\r\nIn one benchmark, I ran into an issue where two brokers out of three had no job streams registered, and only one broker was aware of them.\r\n\r\nHowever, the gateway actuator shows that they think all streams are connected to all brokers, which is incorrect.\r\n\r\nOne interesting thing is the gateway did not aggregate the streams at all, treating each client as a different server stream.\r\n\r\n**To Reproduce**\r\n\r\nNot sure why this happened. Could be related to #14624 I suppose, but likely not.\r\n\r\n**Expected behavior**\r\n\r\nGateway and broker stream states are consistent.\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.3.0+\r\n\n\n npepinpe: It seems the member was not restarted, but it was locally removed/re-added, yet the streams were not recreated. This is an issue we thought was fixed, but apparently isn't.\n npepinpe: Yeah, we didn't send a restart streams request to the gateway when it rejoined, so this is clearly broken.",
    "title": "Job streams not registered on broker after some time"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14699",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "gitHubText": "**Describe the bug**\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nDuring startup of the broker Zeebe encounters snapshot 0. Likely an indication of some corruption.\r\n\r\nWe encountered this before on prod. The solution was to delete the partition data and restart the broker. After this the broker recovers. When we first found it https://github.com/camunda/zeebe/issues/14486 was created. This was released in 8.3.0. However, today it occurred again on 8.3.0. \r\n\r\n**To Reproduce**\r\n\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\nUnclear, but it's probably hard to do as you'd need to introduce some corruption.\r\n\r\n**Expected behavior**\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nIt is handled more gracefully and doesn't prevent the broker from reaching a bootloop.\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.lang.IllegalStateException: In partition 1 expected to find a snapshot at index >= log's first index 30443521, but found snapshot 0. A previous snapshot is most likely corrupted.\r\nat io.atomix.raft.utils.StateUtil.verifySnapshotLogConsistent ( io/atomix.raft.utils/StateUtil.java:33 )\r\nat io.atomix.raft.impl.RaftContext.<init> ( io/atomix.raft.impl/RaftContext.java:223 )\r\nat io.atomix.raft.impl.DefaultRaftServer$Builder.build ( io/atomix.raft.impl/DefaultRaftServer.java:242 )\r\nat io.atomix.raft.impl.DefaultRaftServer$Builder.build ( io/atomix.raft.impl/DefaultRaftServer.java:216 )\r\nat io.atomix.raft.partition.impl.RaftPartitionServer.buildServer ( io/atomix.raft.partition.impl/RaftPartitionServer.java:159 )\r\nat io.atomix.raft.partition.impl.RaftPartitionServer.<init> ( io/atomix.raft.partition.impl/RaftPartitionServer.java:91 )\r\nat io.atomix.raft.partition.RaftPartition.createServer ( io/atomix.raft.partition/RaftPartition.java:127 )\r\nat io.atomix.raft.partition.RaftPartition.initServer ( io/atomix.raft.partition/RaftPartition.java:110 )\r\nat io.atomix.raft.partition.RaftPartition.bootstrap ( io/atomix.raft.partition/RaftPartition.java:94 )\r\nat io.camunda.zeebe.broker.partitioning.startup.steps.RaftBootstrapStep.startup ( io/camunda.zeebe.broker.partitioning.startup.steps/RaftBootstrapStep.java:31 )\r\nat io.camunda.zeebe.broker.partitioning.startup.steps.RaftBootstrapStep.startup ( io/camunda.zeebe.broker.partitioning.startup.steps/RaftBootstrapStep.java:14 )\r\nat io.camunda.zeebe.scheduler.startup.StartupProcess.proceedWithStartupSynchronized ( io/camunda.zeebe.scheduler.startup/StartupProcess.java:161 )\r\nat io.camunda.zeebe.scheduler.startup.StartupProcess.lambda$proceedWithStartupSynchronized$3 ( io/camunda.zeebe.scheduler.startup/StartupProcess.java:169 )\r\nat io.camunda.zeebe.scheduler.future.FutureContinuationRunnable.run ( io/camunda.zeebe.scheduler.future/FutureContinuationRunnable.java:28 )\r\nat io.camunda.zeebe.scheduler.ActorJob.invoke ( io/camunda.zeebe.scheduler/ActorJob.java:94 )\r\nat io.camunda.zeebe.scheduler.ActorJob.execute ( io/camunda.zeebe.scheduler/ActorJob.java:45 )\r\nat io.camunda.zeebe.scheduler.ActorTask.execute ( io/camunda.zeebe.scheduler/ActorTask.java:119 )\r\nat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask ( io/camunda.zeebe.scheduler/ActorThread.java:130 )\r\nat io.camunda.zeebe.scheduler.ActorThread.doWork ( io/camunda.zeebe.scheduler/ActorThread.java:108 )\r\nat io.camunda.zeebe.scheduler.ActorThread.run ( io/camunda.zeebe.scheduler/ActorThread.java:227 )\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- OS: SaaS\r\n- Zeebe Version: 8.3.0\r\n\r\nhttps://console.cloud.google.com/logs/query;query=logName:%22stdout%22%0Aresource.type%3D%22k8s_container%22%0Aresource.labels.project_id%3D%22camunda-saas-int-chaos%22%0Aresource.labels.namespace_name%3D%22f2275ddc-b3a0-44b4-bc8a-cdfc242514f1-zeebe%22%0Aresource.labels.location%3D%22europe-west1%22%0Aresource.labels.cluster_name%3D%22worker-chaos-1%22%0Aresource.labels.container_name%3D%22zeebe%22%0Aresource.labels.pod_name%3D%22zeebe-0%22;pinnedLogId=2023-10-12T12:31:31.413159098Z%2Fa7opurgfpm3zpxtj;cursorTimestamp=2023-10-12T12:53:35.349727194Z;duration=PT3H?project=camunda-saas-int-chaos\r\n\n\n oleschoenburg: Since this is not fixed by https://github.com/camunda/zeebe/issues/14486 we need to investigate this!\n npepinpe: Did anyone check what was on disk? Did we save the \"corrupted\" snapshot? Or is 0 assumed to mean there is no snapshot?\n deepthidevaki: related #14594 \n rodrigo-lourenco-lopes: This issue also happened again in a trial cluster also with the 8.3.0 version. [Alert](https://camunda.slack.com/archives/C04D5HLFP24/p1697503341553379), [Logs](https://console.cloud.google.com/logs/query;query=resource.type%3D%22k8s_container%22%0Aresource.labels.project_id%3D%22camunda-cloud-240911%22%0Aresource.labels.location%3D%22us-central1%22%0Aresource.labels.cluster_name%3D%22prod-worker-3%22%0Aresource.labels.namespace_name%3D%2236891658-b31b-4efa-bbbc-7dbef6acec10-zeebe%22%0Aresource.labels.pod_name%3D%22zeebe-2%22%0Aresource.labels.container_name%3D%22zeebe%22%0A;pinnedLogId=2023-10-17T06:45:28.130308499Z%2Fldkhiotcmp0x0y7s;cursorTimestamp=2023-10-17T06:45:28.213227478Z;duration=PT12H?project=camunda-cloud-240911).\n Zelldon: Happened again.\r\n\r\nWas not able to connect to the failing pod (with debug container I had not access to the filesystem) had to delete the PVC and pod:\r\n\r\n```\r\n  k delete pvc data-zeebe-2 & k delete pod zeebe-2\r\n```\n npepinpe: Thinking out loud...\r\n\r\nImmediate hypothesis: the broker was shutdown right before this happened, without apparent error (so graceful shutdown).\r\n\r\nWe already have a regression where we delete pending snapshots concurrently, outside of the snapshot store actor. What if we had been persisting the snapshot (on the actor) while the Raft partition was shutting down (thus deleting pending snapshots on the Raft thread)?\r\n\r\nHm, I don't think that's possible actually, since the Raft thread waits for the persist operation to finish before doing anything else...\r\n\r\nSo the changes we did to snapshot:\r\n\r\n- No integrity check after the move - if the move is not done completely, then the snapshot may be corrupted.\r\n- However, we only write the checksum file once the move is done, so if the node crashed during the move we would have no checksum file and would not consider the snapshot persisted.\r\n\r\n\n npepinpe: It's currently happening on a cluster, so I had a chance to look at the data. I've uploaded the partition's snapshot directory here:\r\n\r\nhttps://drive.google.com/file/d/10cMGw7QPepM5euTNl3tWfuaLHVnuQ5YI/view?usp=share_link\r\n\r\nThe snapshot itself looks fine. But we can see the checksum file is completely empty :scream: \n npepinpe: Hm, this happened right after this bug: https://github.com/camunda/zeebe/issues/14776\r\n\r\nI wonder if it's not related. We open the checksum file, but crash before we can write. The file now exists, but it's empty.\r\n\r\nOops.\n npepinpe: And also `FileOutputStream#flush` does nothing :tada: ",
    "title": "IllegalStateException: found snapshot 0. A previous snapshot is most likely corrupted"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14663",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "gitHubText": "**Describe the bug**\r\n\r\nWhen replicating a snapshot >= 4GB, leaders will regularly go out of memory, with one of the biggest culprit being the `NettyMessagingService`'s timeout executor. \r\n\r\n![image](https://github.com/camunda/zeebe/assets/43373/5e7db99e-34fa-4ebf-915b-7dcba5db2305)\r\n\r\nLooking at the heap dump produced, we can see that one of the threads `netty-messaging-timeout` is the biggest culprit:\r\n\r\n![image](https://github.com/camunda/zeebe/assets/43373/e36268d2-6bd8-4096-8f8c-9e462512e359)\r\n\r\nLooking at the code, it's quite obvious what the issue is:\r\n\r\n```java\r\n    final ProtocolRequest message =\r\n        new ProtocolRequest(messageId, advertisedAddress, type, payload);\r\n    final CompletableFuture<byte[]> responseFuture;\r\n    if (keepAlive) {\r\n      responseFuture =\r\n          executeOnPooledConnection(address, type, c -> c.sendAndReceive(message), executor);\r\n    } else {\r\n      responseFuture =\r\n          executeOnTransientConnection(address, c -> c.sendAndReceive(message), executor);\r\n    }\r\n    final var timeoutFuture =\r\n        timeoutExecutor.schedule(\r\n            () -> {\r\n              responseFuture.completeExceptionally(\r\n                  new TimeoutException(\r\n                      String.format(\r\n                          \"Request %s to %s timed out in %s\", message, address, timeout)));\r\n              openFutures.remove(responseFuture);\r\n            },\r\n            timeout.toNanos(),\r\n            TimeUnit.NANOSECONDS);\r\n    responseFuture.whenComplete((ignored, error) -> timeoutFuture.cancel(true));\r\n```\r\n\r\nAs you can see, the scheduled lambda closes over the complete message. Since replicating snapshot means your messages risk being in the double (or at worst, triple) digit megabytes, you can easily run out of memory very quickly.\r\n\r\n**To Reproduce**\r\n\r\nReplicate a large snapshot, with large files (at least a bunch of 50MB files).\r\n\r\n**Expected behavior**\r\n\r\nNo out of memory occurs when we can avoid it - here, for example, the message is only used for logging, so it's not worth closing over it!\r\n\r\n**Log/Stacktrace**\r\n\r\n[Here's a link to the heap dump for future reference](https://drive.google.com/file/d/1IkKyOP1gR6CDe9lSlNrbD6jkxuQfryAR/view?usp=drive_link)\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.3.0 and prior\r\n\n",
    "title": "Regular OOM when replicating large state due to messaging timeouts"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14624",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "gitHubText": "**Describe the bug**\r\n\r\nObserved following error in release-8-3-0 benchmark.\r\n\r\n```\r\nExpected to process record 'TypedRecordImpl{metadata=RecordMetadata{recordType=COMMAND, valueType=PROCESS_INSTANCE_CREATION, intent=CREATE, authorization=JWT.xxxxxxxx.}, value={\"bpmnProcessId\":\"benchmark\",\"processDefinitionKey\":2251799813685253,\"processInstanceKey\":2251799855211562,\"version\":1,\"variables\":\".....}' without errors, but exception occurred with message 'Unable to determine string type, found unknown header byte 0x3e at reader offset 0'.\r\n\r\nio.camunda.zeebe.msgpack.spec.MsgpackReaderException: Unable to determine string type, found unknown header byte 0x74 at reader offset 0\r\n\r\nat io.camunda.zeebe.msgpack.spec.MsgPackReader.exceptionOnUnknownHeader ( io/camunda.zeebe.msgpack.spec/MsgPackReader.java:474 )\r\nat io.camunda.zeebe.msgpack.spec.MsgPackReader.readStringLength ( io/camunda.zeebe.msgpack.spec/MsgPackReader.java:140 )\r\nat io.camunda.zeebe.msgpack.value.StringValue.read ( io/camunda.zeebe.msgpack.value/StringValue.java:96 )\r\nat io.camunda.zeebe.msgpack.value.ArrayValue.readInnerValue ( io/camunda.zeebe.msgpack.value/ArrayValue.java:234 )\r\nat io.camunda.zeebe.msgpack.value.ArrayValue.next ( io/camunda.zeebe.msgpack.value/ArrayValue.java:160 )\r\nat io.camunda.zeebe.msgpack.value.ArrayValue.next ( io/camunda.zeebe.msgpack.value/ArrayValue.java:17 )\r\nat io.camunda.zeebe.protocol.impl.stream.job.JobActivationPropertiesImpl.getTenantIds ( io/camunda.zeebe.protocol.impl.stream.job/JobActivationPropertiesImpl.java:79 )\r\nat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnJobActivationBehavior.lambda$publishWork$0 ( io/camunda.zeebe.engine.processing.bpmn.behavior/BpmnJobActivationBehavior.java:70 )\r\nat io.camunda.zeebe.transport.stream.impl.RemoteStreamerImpl.lambda$streamFor$0 ( io/camunda.zeebe.transport.stream.impl/RemoteStreamerImpl.java:69 )\r\nat io.camunda.zeebe.transport.stream.impl.RemoteStreamerImpl.streamFor ( io/camunda.zeebe.transport.stream.impl/RemoteStreamerImpl.java:70 )\r\nat io.camunda.zeebe.broker.jobstream.RemoteJobStreamer.streamFor ( io/camunda.zeebe.broker.jobstream/RemoteJobStreamer.java:40 )\r\nat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnJobActivationBehavior.publishWork ( io/camunda.zeebe.engine.processing.bpmn.behavior/BpmnJobActivationBehavior.java:68 )\r\nat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnJobBehavior.writeJobCreatedEvent ( io/camunda.zeebe.engine.processing.bpmn.behavior/BpmnJobBehavior.java:217 )\r\nat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnJobBehavior.createNewJob ( io/camunda.zeebe.engine.processing.bpmn.behavior/BpmnJobBehavior.java:117 )\r\nat io.camunda.zeebe.engine.processing.bpmn.task.JobWorkerTaskProcessor.lambda$onActivate$3 ( io/camunda.zeebe.engine.processing.bpmn.task/JobWorkerTaskProcessor.java:57 )\r\nat io.camunda.zeebe.util.Either$Right.ifRightOrLeft ( io/camunda.zeebe.util/Either.java:381 )\r\nat io.camunda.zeebe.engine.processing.bpmn.task.JobWorkerTaskProcessor.onActivate ( io/camunda.zeebe.engine.processing.bpmn.task/JobWorkerTaskProcessor.java:55 )\r\nat io.camunda.zeebe.engine.processing.bpmn.task.JobWorkerTaskProcessor.onActivate ( io/camunda.zeebe.engine.processing.bpmn.task/JobWorkerTaskProcessor.java:25 )\r\nat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.lambda$processEvent$2 ( io/camunda.zeebe.engine.processing.bpmn/BpmnStreamProcessor.java:144 )\r\nat io.camunda.zeebe.util.Either$Right.ifRightOrLeft ( io/camunda.zeebe.util/Either.java:381 )\r\nat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.processEvent ( io/camunda.zeebe.engine.processing.bpmn/BpmnStreamProcessor.java:143 )\r\nat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.lambda$processRecord$0 ( io/camunda.zeebe.engine.processing.bpmn/BpmnStreamProcessor.java:92 )\r\nat io.camunda.zeebe.util.Either$Right.ifRightOrLeft ( io/camunda.zeebe.util/Either.java:381 )\r\nat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.processRecord ( io/camunda.zeebe.engine.processing.bpmn/BpmnStreamProcessor.java:89 )\r\nat io.camunda.zeebe.engine.Engine.process ( io/camunda.zeebe.engine/Engine.java:127 )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.batchProcessing ( io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:352 )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$2 ( io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:268 )\r\n```\r\n\r\nThis error was logged for repeatedly for a couple of seconds. After that it didn't occur again. Occured in broker on two partitions around the same time.\r\n\r\n\r\n**Log/Stacktrace**\r\n\r\n[link to logs](https://console.cloud.google.com/errors/detail/CM_M__nUjKW63wE;service=zeebe;time=P7D?project=zeebe-io)\r\n[logs.zip](https://github.com/camunda/zeebe/files/12827812/downloaded-logs-20231006-08.zip)\r\n\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.3.0\r\n\n\n deepthidevaki: There are more errors similar to this https://console.cloud.google.com/errors;service=zeebe;version=release-8-3-0;time=PT6H;resolution=OPEN?project=zeebe-io \n remcowesterhoud: I also see a lot of `failed to push job` and `failed to push payload` warnings. Looking at the dashboard there's 70+ banned instances. We must find out where this is coming from and resolve it asap.\n deepthidevaki: The pods also encountered SIGSEGV. Looks like it is also caused when reading msgpack.\r\n[error.log](https://github.com/camunda/zeebe/files/12828385/error.log)\r\n\n remcowesterhoud: After root-causing I've lowered the severity from `critical` to `high`. This is because this issue only occurs when job push is enabled, which is an experimental api. Because of this we can fix this in a patch release.\r\n\r\n**Root-cause**\r\nThe problem lies in the job streamer. This is an object that only exists once per broker. This instance is shared between the partition on the broker. If a broker is leader for more than 1 partition, iterating through the tenants in the `JobActivationPropertiesImpl` will fail, as the ArrayProperty is not thread-safe.\r\n\r\n**Quick-fix**\r\nWe can clone [the job activation properties in the `BpmnJobActivationBehavior`](https://github.com/camunda/zeebe/blob/main/engine/src/main/java/io/camunda/zeebe/engine/processing/bpmn/behavior/BpmnJobActivationBehavior.java#L67-L70), before we get the list of tenants. Because we have a copy we'll no longer have multiple partitions iterating over the same `ArrayProperty`.\r\n\r\n**Nicer fix**\r\nThe engine always assumes everything is single-threaded. This makes it very easy to make changes in the engine. The job streamer is now the exception. If the broker can register streams per partition, instead of reusing the streamer we'll no longer have multiple partitions interacting with the same instance. This resolves the problem.\n npepinpe: Isn't the real issue that iterating over a supposedly immutable `ArrayProperty` actually mutates the object? :sweat_smile: The stream registry expects all its information to be immutable, so when `JobActivationPropertiesImpl` is, in fact, mutable, it's what causes the design dissonance.\r\n\r\nI think almost all of us got bitten by the fact that iterating over an `ArrayValue` mutates the underlying object, which is (much like the Spanish inquisition) never what you expect. You can see so many of these objects where we clone the array property to a list or something before iterating for this specific reason. For example, `JobBatchRecord#getJobKeys`.\r\n\r\nAt any rate, I wouldn't propose registering streams per partition - it's just redundant work. Instead, you can wrap the given `JobStreamer` in a partition aware facade which can copy info when you need it.\n npepinpe: This came up in our planning - should I work on this? Will you work on this? If so, should I assume that, since it's in `ready`, it will be in the next 2 weeks?\n remcowesterhoud: Sorry, I put it in ready because we thought it was a critical issue and release blocker. We may have time for the quick-fix this week, but we have to prioritise documentation for the release. We want this to be part of 8.3.1 release.\r\n\r\nWe haven't decided yet on which approach we should take. If we have a separate job streamer for each partition I'd say it falls under ZDP responsibility. The quick-fix we can definitely do ourselves. Iterating over the ArrayProperty without mutating it sounds great too and would also be our responsibility.\n Zelldon: [Run into this issue as well when using cloud benchmark with SaaS cluster with v8.3.0](https://console.cloud.google.com/logs/query;query=resource.type%3D%22k8s_container%22%0Aresource.labels.project_id%3D%22camunda-saas-prod%22%0Aresource.labels.location%3D%22northamerica-northeast2%22%0Aresource.labels.cluster_name%3D%22worker-5%22%0Aresource.labels.namespace_name%3D%222b3d50c8-e590-4eba-97d4-1da76e0d8fb4-zeebe%22%0Aresource.labels.pod_name:%22zeebe-%22%0A-resource.labels.container_name%3D%22zeebe-analytics%22;pinnedLogId=2023-10-10T14:22:20.739210406Z%2Fbvjumarl3k64dbew;cursorTimestamp=2023-10-10T14:22:28.849604018Z;startTime=2023-10-10T13:24:40.544024Z;endTime=2023-10-10T15:24:40.544Z?project=camunda-saas-prod)\r\n\r\njust want to highlight that this makes job push right now totally unusable.\n megglos: > We haven't decided yet on which approach we should take. If we have a separate job streamer for each partition I'd say it falls under ZDP responsibility. \r\n\r\nImho, this sounds like a general change that would resolve the issue as a side-effect but the issue shouldn't be the reason for us to do it? Wdyt @npepinpe ?\r\n\r\n> The quick-fix we can definitely do ourselves. Iterating over the ArrayProperty without mutating it sounds great too and would also be our responsibility.\r\n\r\nCould you still do that this week so we have it ready for an upcoming 8.3.1 patch? Or do you need help with that? @remcowesterhoud \n remcowesterhoud: > Could you still do that this week so we have it ready for an upcoming 8.3.1 patch? Or do you need help with that? @remcowesterhoud\r\n\r\nIf medic week stays as quiet as today I'm sure I can find the time. If I get pulled in support cases I might not ðŸ¤· \n Zelldon: âš ï¸ \r\n\r\nOur releases benchmarks are also failing with SIGSEGV\r\n\r\n[error.log](https://github.com/camunda/zeebe/files/12893335/error.log)\r\n",
    "title": "MsgpackReaderException when reading TenantId"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14588",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "gitHubText": "**Describe the bug**\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nThere appears to be a race condition in the `PendingMessageSubscriptionChecker`. It can run concurrently to the stream processor, where the two access a shared in-memory state (aka transient state). Access to this state is threadsafe, but not spread over multiple calls.\r\n\r\nThe race condition appears to exist here: The entries are first retrieved from the transient state, before we `get` the subscription from the persisted state. The transient state may have been modified before, leading to both states not being in-sync.\r\n\r\nhttps://github.com/camunda/zeebe/blob/d928ba2ed6321b3e613320e9c7fcdd0bc0decc44/engine/src/main/java/io/camunda/zeebe/engine/state/message/DbMessageSubscriptionState.java#L229-L236\r\n\r\n**To Reproduce**\r\n\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\nThis may be hard to reproduce, as it's a race condition. It may be possible to set up by calling the associated method in a specific order where the order can be controlled.\r\n\r\n**Expected behavior**\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\nThe checker should be able to run concurrently to the stream processor without issues.\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.lang.NullPointerException: Cannot invoke \"io.camunda.zeebe.engine.state.message.MessageSubscription.getRecord()\" because \"subscription\" is null\r\n    at io.camunda.zeebe.engine.processing.message.PendingMessageSubscriptionChecker.sendCommand(PendingMessageSubscriptionChecker.java:36) ~[zeebe-workflow-engine-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n    at io.camunda.zeebe.engine.state.message.DbMessageSubscriptionState.visitPending(DbMessageSubscriptionState.java:253) ~[zeebe-workflow-engine-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n    at io.camunda.zeebe.engine.processing.message.PendingMessageSubscriptionChecker.run(PendingMessageSubscriptionChecker.java:32) ~[zeebe-workflow-engine-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n    at io.camunda.zeebe.stream.api.scheduling.SimpleProcessingScheduleService.lambda$runAtFixedRate$0(SimpleProcessingScheduleService.java:35) ~[zeebe-stream-platform-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n[Link to log](https://console.cloud.google.com/logs/query;cursorTimestamp=2023-10-02T13:01:28.455354092Z;endTime=2023-10-02T14:54:41.744537Z;query=error_group%2528%22CKOmuvKMwLOtvAE%22%2529%0AlogName:%22stdout%22%0Aresource.type%3D%22k8s_container%22%0Aresource.labels.project_id%3D%22camunda-saas-int-chaos%22%0Aresource.labels.location%3D%22europe-west1%22%0Aresource.labels.container_name%3D%22zeebe%22%0Aresource.labels.cluster_name%3D%22worker-chaos-1%22%0Aresource.labels.namespace_name%3D%2228dc004a-c9eb-4ee4-bd43-122d19470705-zeebe%22%0Aresource.labels.pod_name%3D%22zeebe-0%22%0Atimestamp%3D%222023-10-02T13:01:28.455354092Z%22%0AinsertId%3D%22t5z8oevltfufcaqu%22;startTime=2023-10-02T12:54:41.744Z?project=camunda-saas-int-chaos)\r\n\n\n korthout: We don't consider this to be a blocker for the 8.3.0 release.\r\n\r\n- The cluster recovered after the leader, that encountered the NPE, stepped down. The new leader did not run into the same race condition.\r\n- The race condition has already been around since 8.3.0-alpha3, and did not occur in our extensive testing. It's a rare occurrence.\r\n- Lastly, users that do run into this too often can disable the checker from running concurrently to stream processor by setting `zeebe.broker.processing.enableAsyncScheduledTasks: false`. This will completely mitigate this issue.\r\n\r\nHaving said that, this issue is high severity as it disrupts regular processing, causing a drop in availability.\n korthout: It's possible that this is similar to this issue for timers:\r\n- https://github.com/camunda/zeebe/issues/13598\n berkaycanbc: @korthout do you think we can move this one to ready so that it can be picked up? I can take a look at it.\n berkaycanbc: We can add a null check and log that subscription removed from the state if it is not found. Since message subscription record either correlated or deleted from the state, it should be safe to skip that record. WDYT? @korthout \n korthout: @berkaycanbc Sounds like a good plan. Feel free to pick it up. ðŸ‘  Please make sure to verify whether this race condition exists on previous versions (i.e. 8.2)",
    "title": "NPE in `PendingMessageSubscriptionChecker`"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14458",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "gitHubText": "**Describe the bug**\r\n\r\nNPE while the healthcheck service was trying to query the health of a Zeebe partition during shutdown.\r\n\r\n**To Reproduce**\r\n\r\nOccurs sometimes during shutdown.\r\n\r\n**Expected behavior**\r\n\r\nNo NPE.\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\n java.lang.NullPointerException: Cannot invoke \"io.camunda.zeebe.broker.system.partitions.PartitionContext.getComponentHealthMonitor()\" because \"this.context\" is null\r\n\tat io.camunda.zeebe.broker.system.partitions.ZeebePartition.getHealthReport(ZeebePartition.java:410) ~[zeebe-broker-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.health.CriticalComponentsHealthMonitor.getHealth(CriticalComponentsHealthMonitor.java:152) ~[zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.health.CriticalComponentsHealthMonitor.lambda$updateHealth$5(CriticalComponentsHealthMonitor.java:105) ~[zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat java.util.HashMap$KeySet.forEach(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.scheduler.health.CriticalComponentsHealthMonitor.updateHealth(CriticalComponentsHealthMonitor.java:105) ~[zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94) ~[zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:130) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:108) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:227) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT] \r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- Zeebe Version: zeebe:8.3.0-SNAPSHOT-cw-39-706f1b0\r\n- Error group: https://console.cloud.google.com/errors/detail/CKyTiNSSr_HjHw;service=zeebe;time=P7D?project=camunda-cloud-240911\r\n\n\n megglos: ZDP-Triage:\n- noise during shutdown\n- easy fix though - e.g. check for null and return unhealthy in this case\n Aniket-kumar404: I would like to work on this issue.\n rodrigo-lourenco-lopes: This issue happened again in the medic benchmark: [zeebe:medic-y-2023-cw-38-be24882-benchmark](https://console.cloud.google.com/errors/detail/CKyTiNSSr_HjHw;service=zeebe;version=medic-y-2023-cw-38-be24882-benchmark;time=P7D?project=zeebe-io)\r\n\r\nError group: https://console.cloud.google.com/errors/detail/CKyTiNSSr_HjHw%3Bservice=zeebe%3Btime=P7D?project=zeebe-io\r\n\r\n",
    "title": " NullPointerException: Cannot invoke `PartitionContext.getComponentHealthMonitor()` because `this.context` is null "
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14329",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "gitHubText": "**Describe the bug**\r\n\r\n[E2E tests](https://camunda.slack.com/archives/C013MEVQ4M9/p1694702959002779) failed because the send-ping job is not completed. All the instances that did not complete are stuck in `send-message-ping` or `send-message-pong` tasks.\r\n\r\n![image](https://github.com/camunda/zeebe/assets/1997478/2b210594-5e5e-47cf-8bae-15a5ba3c0006)\r\n\r\nLooking at the logs, publishing message failed due to resource exhausted (there were broker restarts and leader change around this time). So the job was failed with a retry backoff of 30 seconds. However, the worker logs and the exported records show that these jobs were not re-activated. \r\n\r\nHere is screenshot showing exported records in elastic search for some of these jobs.\r\n\r\n![tmp-78371-iIZGT5mhN4lx-image](https://github.com/camunda/zeebe/assets/1997478/4dbb026d-31e2-42f2-8d0b-a98e872cc481)\r\n\r\n![tmp-78371-p0xwE2yFVwKD-image](https://github.com/camunda/zeebe/assets/1997478/5a38c06e-ff69-4830-a4fd-51cb7d7f5621)\r\n\r\n![tmp-78371-23tXS7l4N8jV-image](https://github.com/camunda/zeebe/assets/1997478/512e83ae-291b-45ef-8d78-29e1c1bc9887)\r\n\r\nFor a job that retried after backoff, you can see those records in ES. But we don't see them for the above jobs. Here is a an example of such a job were retrying after backoff worked successfully (from the same cluster and for same job type). \r\n![tmp-78371-cK9yR3YYFP5d-image](https://github.com/camunda/zeebe/assets/1997478/dec2c231-cb4a-4f03-ac87-7f94c54fb7b4)\r\n\r\nAlso ran `zbctl activate jobs \"send-message-ping\"` locally against this cluster. But it returned  no jobs, even though operate shows these jobs as not completed.\r\n\r\n**To Reproduce**\r\n\r\nNot sure. But I think restarts and leader change may have something to do with this.\r\n\r\n**Expected behavior**\r\n\r\nFailed jobs should be re-activatable after backoff timeout.\r\n\r\n**Log/Stacktrace**\r\n\r\nThere were no relevant error in Zeebe. But here are the logs around 14.09.2023 16:33 CEST. All uncompleted tasks were created around 2023-09-14 16:32 - 16:33\r\n[downloaded-logs-20230915-105930.zip](https://github.com/camunda/zeebe/files/12618554/downloaded-logs-20230915-105930.zip)\r\n\r\nHere is the snapshot of partition 3 on which these jobs exist. \r\n[snapshot-partition-3.zip](https://github.com/camunda/zeebe/files/12617504/snapshot-partition-3.zip)\r\n\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.1.16-SNAPSHOT (8.1.16-SNAPSHOT-stable-8.1-8522cb63)\r\n\n\n korthout: ZPA triage:\n- when this happens it's quite bad, those jobs will not picked up again\n- we have tests for this, so it must have been a special situation\n- may be hard to reproduce\n- marking this as `later` unless it's reported again\n deepthidevaki: Happened again twice:\r\n1. Stable/8.0 https://camunda.slack.com/archives/C013MEVQ4M9/p1696265270192779 \r\n2. Stable/8.2 https://camunda.slack.com/archives/C013MEVQ4M9/p1696265619453009 \r\n\r\nIn both cases, there were broker restarts when the job failed.\n abbasadel: ZPA triage:\n- We see this issue happening more often now (today's morning and lastweek)\n- We don't think this is related to the latest changes since it also happens on 8.0 and 8.3\n- We think we should start investigating this and give it a high priority as 'upcoming' \n\n deepthidevaki: All E2E tests are failing every week due to this bug. It would be good to fix this asap. If the fix is not planned, could you please disable this `ping-pong` process in e2e tests? It doesn't make sense to run the tests if it keeps failing.\n megglos: reached out on slack on this, see https://camunda.slack.com/archives/C037W9NMATG/p1700578334906099\n nicpuppa: From the snapshot provided me and @tmetzke saw that the `JOB_BACKOFF` column family is empty. Possible scenarios:\r\n\r\n- Job Failed event is not correctly replayed, after the restart new leader doesn't know about the failed job that needs to be retried\r\n- JOB_BACKOFF column family might be [cleaned up ](https://github.com/camunda/zeebe/blob/1c6c3cc2ec75bdb298c1e644131a586edbf0f501/engine/src/main/java/io/camunda/zeebe/engine/state/instance/DbJobState.java#L241)\n Zelldon: @nicpuppa and I had a closer look together.\r\n\r\nWe have seen in the state that in deed there is no entries for JOB_BACKOFF, but still there are jobs in state FAILED.\r\n\r\n\r\nAfter this we check the code in more depth, and tried to understand how this could happen. Based on the previous comment we check the Migration code, etc. Based on the code we would expect that the migration is not executed if there is still an job existing, but where not sure about the second condition https://github.com/camunda/zeebe/blob/1c6c3cc2ec75bdb298c1e644131a586edbf0f501/engine/src/main/java/io/camunda/zeebe/engine/state/instance/DbJobState.java#L246\r\n\r\n\r\nWe wrote an Engine test, with snapshot and replay. Here everything worked fine. This means without migration there is no issue.\r\n\r\nLater we came up with an integration test which involves restarting the node and transitioning (this will cause migration to run).\r\n\r\n\r\n```java\r\n\r\n  @Test\r\n  public void test() {\r\n    // given\r\n    final String jobType = \"test\";\r\n    clientRule.createSingleJob(jobType);\r\n\r\n    final var activateResponse =\r\n        clientRule\r\n            .getClient()\r\n            .newActivateJobsCommand()\r\n            .jobType(jobType)\r\n            .maxJobsToActivate(1)\r\n            .send()\r\n            .join();\r\n    final var jobKey = activateResponse.getJobs().get(0).getKey();\r\n\r\n    final Duration backoffTimeout = Duration.ofDays(30);\r\n    clientRule\r\n        .getClient()\r\n        .newFailCommand(jobKey)\r\n        .retries(1)\r\n        .retryBackoff(backoffTimeout)\r\n        .send()\r\n        .join();\r\n\r\n    // when\r\n    clusteringRule.triggerAndWaitForSnapshots();\r\n    clusteringRule.restartBroker(clusteringRule.getLeaderForPartition(1).getNodeId());\r\n    clusteringRule.getClock().addTime(backoffTimeout.plus(backoffTimeout));\r\n\r\n    // then\r\n    Awaitility.await()\r\n        .until(\r\n            () ->\r\n                clientRule\r\n                    .getClient()\r\n                    .newActivateJobsCommand()\r\n                    .jobType(jobType)\r\n                    .maxJobsToActivate(1)\r\n                    .send()\r\n                    .join(),\r\n            r -> !activateResponse.getJobs().isEmpty());\r\n  }\r\n```\r\n\r\n\r\nIt turns out that this test fails. After restart and increasing the time, the job will never become active again.\r\n\r\n\r\nAdding some logs to the `DbJobState`:\r\n\r\n```diff\r\n--- a/engine/src/main/java/io/camunda/zeebe/engine/state/instance/DbJobState.java\r\n+++ b/engine/src/main/java/io/camunda/zeebe/engine/state/instance/DbJobState.java\r\n@@ -244,6 +244,14 @@ public final class DbJobState implements JobState, MutableJobState {\r\n           final var backoff = key.first().getValue();\r\n           final var job = jobsColumnFamily.get(jobKey);\r\n           if (job == null || job.getRecord().getRetryBackoff() != backoff) {\r\n+            //\r\n+            LOG.error(\r\n+                \"DeleteExisting job {} from Backoff CF (backoff= {}, recordBackoff={})\",\r\n+                key,\r\n+                backoff,\r\n+                job.getRecord().getRetryBackoff());\r\n+            final State state = getState(jobKey.getValue());\r\n+            LOG.error(\"State of job is {}\", state);\r\n             backoffColumnFamily.deleteExisting(key);\r\n           }\r\n\r\n```\r\n\r\nWe can see when running the test that if the leader goes down:\r\n```\r\n13:36:14.604 [Broker-0] [Startup] [zb-actors-1] DEBUG io.camunda.zeebe.broker.system - Shutdown was called with context: BrokerStartupContextImpl{broker=0}\r\n13:36:14.604 [Broker-0] [Startup] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Shutdown Broker Admin Interface\r\n13:36:14.606 [Broker-0] [Startup] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Shutdown Partition Manager\r\n```\r\n\r\nA small election loop starts where both nodes became first candidate and follower again. In both cases they run the migration, when become follower again and delete the job from the Backoff column family:\r\n\r\n```\r\n## Broker 1\r\n13:36:17.458 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.engine.state.migration - Starting JobBackoffCleanupMigration migration (8/16)\r\n13:36:17.460 [Broker-1] [ZeebePartition-1] [zb-actors-1] ERROR io.camunda.zeebe.broker.process - DeleteExisting job DbCompositeKey{first=DbLong{1703853368362}, second=DbForeignKey[inner=DbLong{2251799813685256}, columnFamily=JOBS, match=Full, skip=io.camunda.zeebe.db.impl.DbForeignKey$$Lambda/0x00007f34746a94a0@29b35c5f]} from Backoff CF (backoff= 1703853368362, recordBackoff=2592000000)\r\n13:36:17.460 [Broker-1] [ZeebePartition-1] [zb-actors-1] ERROR io.camunda.zeebe.broker.process - State of job is FAILED\r\n13:36:17.460 [Broker-1] [ZeebePartition-1] [zb-actors-1] DEBUG io.camunda.zeebe.engine.state.migration - JobBackoffCleanupMigration migration completed in 2 ms.\r\n\r\n## Broker 2\r\n13:36:17.458 [Broker-2] [ZeebePartition-1] [zb-actors-0] INFO  io.camunda.zeebe.engine.state.migration - Starting JobBackoffCleanupMigration migration (8/16)\r\n13:36:17.460 [Broker-2] [ZeebePartition-1] [zb-actors-0] ERROR io.camunda.zeebe.broker.process - DeleteExisting job DbCompositeKey{first=DbLong{1703853368362}, second=DbForeignKey[inner=DbLong{2251799813685256}, columnFamily=JOBS, match=Full, skip=io.camunda.zeebe.db.impl.DbForeignKey$$Lambda/0x00007f34746a94a0@29b35c5f]} from Backoff CF (backoff= 1703853368362, recordBackoff=2592000000)\r\n13:36:17.460 [Broker-2] [ZeebePartition-1] [zb-actors-0] ERROR io.camunda.zeebe.broker.process - State of job is FAILED\r\n13:36:17.460 [Broker-2] [ZeebePartition-1] [zb-actors-0] DEBUG io.camunda.zeebe.engine.state.migration - JobBackoffCleanupMigration migration completed in 2 ms.\r\n```\r\n\r\nLater Broker-1 becomes leader (on the next migration there is nothing to do):\r\n\r\n```\r\n13:36:22.451 [Broker-1] [raft-server-1] [raft-server-1-1] INFO  io.atomix.raft.impl.RaftContext - RaftServer{raft-partition-partition-1} - Transitioning to LEADER\r\n\r\n....\r\n13:36:22.456 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.engine.state.migration - Starting JobBackoffCleanupMigration migration (8/16)\r\n13:36:22.456 [Broker-1] [ZeebePartition-1] [zb-actors-1] DEBUG io.camunda.zeebe.engine.state.migration - JobBackoffCleanupMigration migration completed in 0 ms.\r\n13:36:22.456 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.engine.state.migration - Finished JobBackoffCleanupMigration migration (8/16)\r\n```\r\n\r\n\r\n\r\nBased on the output of the error log we added we can see that  the condition doesn't work as expected \r\n\r\nhttps://github.com/camunda/zeebe/blob/1c6c3cc2ec75bdb298c1e644131a586edbf0f501/engine/src/main/java/io/camunda/zeebe/engine/state/instance/DbJobState.java#L246\r\n\r\n```\r\n(backoff= 1703853368362, recordBackoff=2592000000)\r\n```\r\n\r\n\r\n\r\n\r\n\r\n**Side note:**\r\n\r\nI feel there is something going on with the role changes/transitions. It transitioned to follower in term two, then it should transition to follower on term 3 but it says later it does on term 2 ðŸ¤” Completes this again and then starts with term three. Maybe @deepthidevaki @oleschoenburg wants to look into.\r\n\r\n<details>\r\n<pre>\r\n13:36:17.471 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Transition to FOLLOWER on term 2 completed\r\n13:36:17.471 [Broker-1] [Exporter-1] [zb-fs-workers-0] DEBUG io.camunda.zeebe.broker.exporter - Recovered exporter 'Exporter-1' from snapshot at lastExportedPosition 23\r\n13:36:17.471 [Broker-1] [Exporter-1] [zb-fs-workers-0] DEBUG io.camunda.zeebe.broker.exporter - Configure exporter with id 'test-recorder'\r\n13:36:17.471 [Broker-1] [Exporter-1] [zb-fs-workers-0] DEBUG io.camunda.zeebe.broker.exporter - Set event filter for exporters: ExporterEventFilter{acceptRecordTypes={EVENT=true, COMMAND_REJECTION=true, SBE_UNKNOWN=true, COMMAND=true, NULL_VAL=true}, acceptValueTypes={DECISION=true, JOB=true, DEPLOYMENT=true, VARIABLE_DOCUMENT=true, ESCALATION=true, FORM=true, PROCESS_INSTANCE_MIGRATION=true, SBE_UNKNOWN=true, VARIABLE=true, SIGNAL=true, MESSAGE_BATCH=true, ERROR=true, TIMER=true, MESSAGE=true, MESSAGE_SUBSCRIPTION=true, PROCESS_INSTANCE=true, SIGNAL_SUBSCRIPTION=true, USER_TASK=true, PROCESS_MESSAGE_SUBSCRIPTION=true, DECISION_EVALUATION=true, PROCESS_INSTANCE_MODIFICATION=true, DECISION_REQUIREMENTS=true, MESSAGE_START_EVENT_SUBSCRIPTION=true, PROCESS_EVENT=true, DEPLOYMENT_DISTRIBUTION=true, PROCESS_INSTANCE_RESULT=true, PROCESS_INSTANCE_CREATION=true, PROCESS_INSTANCE_BATCH=true, RESOURCE_DELETION=true, CHECKPOINT=true, NULL_VAL=true, INCIDENT=true, PROCESS=true, JOB_BATCH=true, COMMAND_DISTRIBUTION=true}}\r\n13:36:18.523 [Broker-1] [ClusterTopologyManagerService] [zb-actors-0] WARN  io.camunda.zeebe.topology.gossip.ClusterTopologyGossiper - Failed to sync with gateway\r\n13:36:18.811 [Broker-1] [ClusterTopologyManagerService] [zb-actors-1] WARN  io.camunda.zeebe.topology.gossip.ClusterTopologyGossiper - Failed to sync with gateway\r\n13:36:19.949 [Broker-1] [raft-server-1] [raft-server-1-1] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{raft-partition-partition-1}{role=FOLLOWER} - Node priority 2 < target priority 3. Not triggering election.\r\n13:36:22.449 [Broker-1] [raft-server-1] [raft-server-1-1] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{raft-partition-partition-1}{role=FOLLOWER} - No heartbeat from a known leader since 8018ms\r\n13:36:22.449 [Broker-1] [raft-server-1] [raft-server-1-1] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{raft-partition-partition-1}{role=FOLLOWER} - Sending poll requests to all active members: [DefaultRaftMember{id=2, type=ACTIVE, updated=2023-11-29T12:36:04.116Z}, DefaultRaftMember{id=0, type=ACTIVE, updated=2023-11-29T12:36:04.116Z}]\r\n13:36:22.449 [Broker-1] [raft-server-1] [raft-server-1-1] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{raft-partition-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=2, type=ACTIVE, updated=2023-11-29T12:36:04.116Z} for next term 3\r\n13:36:22.449 [Broker-1] [raft-server-1] [raft-server-1-1] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{raft-partition-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=0, type=ACTIVE, updated=2023-11-29T12:36:04.116Z} for next term 3\r\n13:36:22.450 [Broker-1] [raft-server-1] [raft-server-1-1] WARN  io.atomix.raft.roles.FollowerRole - RaftServer{raft-partition-partition-1}{role=FOLLOWER} - Poll request to 0 failed: io.atomix.cluster.messaging.MessagingException$NoSuchMemberException: Expected to send a message with subject 'raft-partition-partition-1-poll' to member '0', but member is not known. Known members are '[Member{id=1, address=0.0.0.0:1032, properties={brokerInfo=EADJAAAABAABAAAAAQAAAAMAAAADAAAAAAABCgAAAGNvbW1hbmRBcGkMAAAAMC4wLjAuMDoxMDMxBQABAQAAAAEMAAAOAAAAOC40LjAtU05BUFNIT1QFAAEBAAAAAQ==}}, Member{id=2, address=0.0.0.0:1036, properties={brokerInfo=EADJAAAABAACAAAAAQAAAAMAAAADAAAAAAABCgAAAGNvbW1hbmRBcGkMAAAAMC4wLjAuMDoxMDM1BQABAQAAAAEMAAAOAAAAOC40LjAtU05BUFNIT1QFAAEBAAAAAQ==}}, Member{id=gateway, address=0.0.0.0:1038, properties={event-service-topics-subscribed=KIIDAGpvYnNBdmFpbGFibOU=}}]'.\r\n13:36:22.450 [Broker-1] [raft-server-1] [raft-server-1-1] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{raft-partition-partition-1}{role=FOLLOWER} - Received accepted poll from DefaultRaftMember{id=2, type=ACTIVE, updated=2023-11-29T12:36:04.116Z}\r\n13:36:22.450 [Broker-1] [raft-server-1] [raft-server-1-1] INFO  io.atomix.raft.impl.RaftContext - RaftServer{raft-partition-partition-1} - Transitioning to CANDIDATE\r\n13:36:22.450 [Broker-1] [raft-server-1] [raft-server-1-1] INFO  io.atomix.raft.roles.CandidateRole - RaftServer{raft-partition-partition-1}{role=CANDIDATE} - Starting election\r\n13:36:22.450 [Broker-1] [raft-server-1] [raft-server-1-1] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{raft-partition-partition-1} - Set term 3\r\n13:36:22.450 [Broker-1] [raft-server-1] [raft-server-1-1] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{raft-partition-partition-1} - Voted for 1\r\n13:36:22.450 [Broker-1] [raft-server-1] [raft-server-1-1] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{raft-partition-partition-1}{role=CANDIDATE} - Requesting votes for term 3\r\n13:36:22.450 [Broker-1] [raft-server-1] [raft-server-1-1] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{raft-partition-partition-1}{role=CANDIDATE} - Requesting vote from DefaultRaftMember{id=2, type=ACTIVE, updated=2023-11-29T12:36:04.116Z} for term 3\r\n13:36:22.450 [Broker-1] [raft-server-1] [raft-server-1-1] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{raft-partition-partition-1}{role=CANDIDATE} - Requesting vote from DefaultRaftMember{id=0, type=ACTIVE, updated=2023-11-29T12:36:04.116Z} for term 3\r\n13:36:22.451 [Broker-1] [raft-server-1] [raft-server-1-1] WARN  io.atomix.raft.roles.CandidateRole - RaftServer{raft-partition-partition-1}{role=CANDIDATE} - io.atomix.cluster.messaging.MessagingException$NoSuchMemberException: Expected to send a message with subject 'raft-partition-partition-1-vote' to member '0', but member is not known. Known members are '[Member{id=1, address=0.0.0.0:1032, properties={brokerInfo=EADJAAAABAABAAAAAQAAAAMAAAADAAAAAAABCgAAAGNvbW1hbmRBcGkMAAAAMC4wLjAuMDoxMDMxBQABAQAAAAEMAAAOAAAAOC40LjAtU05BUFNIT1QFAAEBAAAAAQ==}}, Member{id=2, address=0.0.0.0:1036, properties={brokerInfo=EADJAAAABAACAAAAAQAAAAMAAAADAAAAAAABCgAAAGNvbW1hbmRBcGkMAAAAMC4wLjAuMDoxMDM1BQABAQAAAAEMAAAOAAAAOC40LjAtU05BUFNIT1QFAAEBAAAAAQ==}}, Member{id=gateway, address=0.0.0.0:1038, properties={event-service-topics-subscribed=KIIDAGpvYnNBdmFpbGFibOU=}}]'.\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Transition to FOLLOWER on term 3 requested.\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] DEBUG io.camunda.zeebe.broker.system - Partition role transitioning from FOLLOWER to CANDIDATE in term 3\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Prepare transition from FOLLOWER on term 2 to FOLLOWER\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Prepare transition from FOLLOWER on term 2 to FOLLOWER - preparing Admin API\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Prepare transition from FOLLOWER on term 2 to FOLLOWER - preparing BackupApiRequestHandler\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Prepare transition from FOLLOWER on term 2 to FOLLOWER - preparing ExporterDirector\r\n13:36:22.451 [Broker-1] [StreamProcessor-1] [zb-actors-1] DEBUG io.camunda.zeebe.logstreams - Paused replay for partition 1\r\n13:36:22.451 [Broker-1] [Exporter-1] [zb-fs-workers-1] DEBUG io.camunda.zeebe.broker.exporter - Closed exporter director 'Exporter-1'.\r\n13:36:22.451 [Broker-1] [raft-server-1] [raft-server-1-1] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{raft-partition-partition-1}{role=CANDIDATE} - Received successful vote from DefaultRaftMember{id=2, type=ACTIVE, updated=2023-11-29T12:36:04.116Z}\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Prepare transition from FOLLOWER on term 2 to FOLLOWER - preparing SnapshotDirector\r\n13:36:22.451 [Broker-1] [raft-server-1] [raft-server-1-1] INFO  io.atomix.raft.impl.RaftContext - RaftServer{raft-partition-partition-1} - Transitioning to LEADER\r\n13:36:22.451 [Broker-1] [SnapshotDirector-1] [zb-actors-1] DEBUG io.camunda.zeebe.scheduler.ActorTask - Discard job io.camunda.zeebe.broker.system.partitions.impl.AsyncSnapshotDirector$$Lambda/0x00007f34749a5230 QUEUED from fastLane of Actor SnapshotDirector-1.\r\n13:36:22.451 [Broker-1] [raft-server-1] [raft-server-1-1] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{raft-partition-partition-1}{role=CANDIDATE} - Cancelling election\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Prepare transition from FOLLOWER on term 2 to FOLLOWER - preparing StreamProcessor\r\n13:36:22.451 [Broker-1] [StreamProcessor-1] [zb-actors-1] DEBUG io.camunda.zeebe.logstreams - Closed stream processor controller StreamProcessor-1.\r\n13:36:22.451 [Broker-1] [raft-server-1] [raft-server-1-1] INFO  io.atomix.raft.impl.RaftContext - RaftServer{raft-partition-partition-1} - Found leader 1\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Prepare transition from FOLLOWER on term 2 to FOLLOWER - preparing InterPartitionCommandService\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Prepare transition from FOLLOWER on term 2 to FOLLOWER - preparing BackupManager\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Prepare transition from FOLLOWER on term 2 to FOLLOWER - preparing BackupStore\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Prepare transition from FOLLOWER on term 2 to FOLLOWER - preparing QueryService\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Prepare transition from FOLLOWER on term 2 to FOLLOWER - preparing Migration\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Prepare transition from FOLLOWER on term 2 to FOLLOWER - preparing ZeebeDb\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Prepare transition from FOLLOWER on term 2 to FOLLOWER - preparing LogStream\r\n13:36:22.451 [Broker-1] [LogStream-1] [zb-actors-1] INFO  io.camunda.zeebe.logstreams - Close appender for log stream logstream-raft-partition-partition-1\r\n13:36:22.451 [Broker-1] [LogStream-1] [zb-actors-1] INFO  io.camunda.zeebe.logstreams - On closing logstream logstream-raft-partition-partition-1 close 1 readers\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Prepare transition from FOLLOWER on term 2 to FOLLOWER - preparing LogStorage\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Preparing transition from FOLLOWER on term 2 completed\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Transition to FOLLOWER on term 3 starting\r\n\r\n</pre>\r\n</details>\n deepthidevaki: > It transitioned to follower in term two, then it should transition to follower on term 3 but it says later it does on term 2\r\n\r\n`Transition to FOLLOWER on term 2 completed`\r\nIt transitioned to Follower on term 2. \r\n\r\n```\r\nTransition to FOLLOWER on term 3 requested.\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] DEBUG io.camunda.zeebe.broker.system - Partition role transitioning from FOLLOWER to CANDIDATE in term 3\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Prepare transition from FOLLOWER on term 2 to FOLLOWER\r\n```\r\nThen it is \"Preparing\" transition from Follower on term 2. \r\n\r\n```\r\nTransition to FOLLOWER on term 3 starting\r\n```\r\nThen it is transitioning to term 3.\r\n\r\nSo everything looks good here.\n oleschoenburg: Hm, I guess this was my bad then :disappointed: https://github.com/camunda/zeebe/commit/1d82e6efc279043b9ff582cf5f66c04afbf53146\r\nI even raised it as a question here: https://github.com/camunda/zeebe/pull/13886#discussion_r1294250574\r\n\r\nI guess the cause is that the record backoff is not supposed to be the point in time where the backoff \"expires\", i.e. what's in the backoff CF.\n Zelldon: On fail we use the `getRecurringTime` to store the job in the backoff column\r\n\r\n```java\r\n  @Override\r\n  public void fail(final long key, final JobRecord updatedValue) {\r\n    if (updatedValue.getRetries() > 0) {\r\n      if (updatedValue.getRetryBackoff() > 0) {\r\n->        addJobBackoff(key, updatedValue.getRecurringTime());\r\n        updateJob(key, updatedValue, State.FAILED);\r\n      } else {\r\n        updateJob(key, updatedValue, State.ACTIVATABLE);\r\n      }\r\n    } else {\r\n      updateJob(key, updatedValue, State.FAILED);\r\n      makeJobNotActivatable(updatedValue.getTypeBuffer(), updatedValue.getTenantId());\r\n    }\r\n  }\r\n```\r\n\r\nLater we check against the backoff of the record. I guess this is the issue?\r\n\r\nhttps://github.com/camunda/zeebe/blob/1c6c3cc2ec75bdb298c1e644131a586edbf0f501/engine/src/main/java/io/camunda/zeebe/engine/state/instance/DbJobState.java#L246\n Zelldon: Thanks @deepthidevaki I'm always (still) confused by these logs.\r\n\r\nI guess what I miss is the to term which it is transitioning to like instead of\r\n\r\n```\r\nPrepare transition from FOLLOWER on term 2 to FOLLOWER\r\n```\r\n\r\n```\r\nPrepare transition from FOLLOWER on term 2 to FOLLOWER on term 3\r\n```\n oleschoenburg: > On fail we use the getRecurringTime to store the job in the backoff column\r\n\r\nYeah I think that's it :facepalm: \n Zelldon: I created https://github.com/camunda/zeebe/pull/15419 to fix the general issue.\r\n\r\n@camunda/zeebe-process-automation please take a look regarding how to handle already stuck jobs, I would like to hand this over to you from here on. \n abbasadel: Thanks @koevskinikola for mentioning this, I removed the label \"version\". it seems they were put mistakely by the change-log command.\n abbasadel: As discussed with @nicpuppa , we also need to backport this fix to 8.1, 8.2 plus 8.3",
    "title": "Failed jobs are not reactivate after backoff"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14047",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "gitHubText": "**Describe the bug**\r\n\r\nIt looks like our current benchmark starter and worker don't work anymore with SaaS clusters. \r\n\r\n@hisImminence tried to use them for benchmarking cluster plans and ran into several issues. I was able to reproduce the same as well. We were only able to overcome this by using an old version.\r\n\r\n```\r\n          image: gcr.io/zeebe-io/starter:8.1.8\r\n```\r\n\r\nWould be great if we could fix this since we also need these applications for upcoming game days etc.\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n**To Reproduce**\r\n\r\n1. Create a cluster in SaaS\r\n2. Setup [a cloud benchmark ](https://github.com/camunda/zeebe/blob/main/benchmarks/setup/newCloudBenchmark.sh)\r\n3. Create credentials\r\n4. Update the credentials file\r\n5. Deploy benchmark via `make secret starter worker` \r\n6. Observe\r\n\r\nAsk @hisImminence for more input\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\n**Expected behavior**\r\n\r\nThe starter and worker can connect without issues.\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\nSee example [log](https://console.cloud.google.com/logs/query;query=resource.type%3D%22k8s_container%22%0Aresource.labels.project_id%3D%22zeebe-io%22%0Aresource.labels.location%3D%22europe-west1-b%22%0Aresource.labels.cluster_name%3D%22zeebe-cluster%22%0Aresource.labels.namespace_name%3D%22ck-immi-test%22%0Alabels.k8s-pod%2Fapp%3D%22starter%22;pinnedLogId=2023-08-29T13:10:00.033238529Z%2F103mjwp1vy2i8imu;cursorTimestamp=2023-08-29T13:10:00.033238529Z;startTime=2023-08-29T12:58:16.638597Z;endTime=2023-08-29T13:29:19.104Z?project=zeebe-io) \r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\n\r\nio.camunda.zeebe.client.api.command.ClientStatusException: http2 exception\r\n\tat io.camunda.zeebe.client.impl.ZeebeClientFutureImpl.transformExecutionException(ZeebeClientFutureImpl.java:116) ~[zeebe-client-java-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:54) ~[zeebe-client-java-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.App.printTopology(App.java:87) [classes/:?]\r\n\tat io.camunda.zeebe.Starter.run(Starter.java:57) [classes/:?]\r\n\tat io.camunda.zeebe.App.createApp(App.java:55) [classes/:?]\r\n\tat io.camunda.zeebe.Starter.main(Starter.java:227) [classes/:?]\r\nCaused by: java.util.concurrent.ExecutionException: io.grpc.StatusRuntimeException: INTERNAL: http2 exception\r\n\tat java.util.concurrent.CompletableFuture.reportGet(Unknown Source) ~[?:?]\r\n\tat java.util.concurrent.CompletableFuture.get(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:52) ~[zeebe-client-java-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\t... 4 more\r\nCaused by: io.grpc.StatusRuntimeException: INTERNAL: http2 exception\r\n\tat io.grpc.Status.asRuntimeException(Status.java:537) ~[grpc-api-1.57.2.jar:1.57.2]\r\n\tat io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:491) ~[grpc-stub-1.57.2.jar:1.57.2]\r\n\tat io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39) ~[grpc-api-1.57.2.jar:1.57.2]\r\n\tat io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23) ~[grpc-api-1.57.2.jar:1.57.2]\r\n\tat io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40) ~[grpc-api-1.57.2.jar:1.57.2]\r\n\tat io.micrometer.core.instrument.binder.grpc.MetricCollectingClientCallListener.onClose(MetricCollectingClientCallListener.java:57) ~[micrometer-core-1.11.3.jar:1.11.3]\r\n\tat io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:567) ~[grpc-core-1.57.2.jar:1.57.2]\r\n\tat io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:71) ~[grpc-core-1.57.2.jar:1.57.2]\r\n\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:735) ~[grpc-core-1.57.2.jar:1.57.2]\r\n\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:716) ~[grpc-core-1.57.2.jar:1.57.2]\r\n\tat io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37) ~[grpc-core-1.57.2.jar:1.57.2]\r\n\tat io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133) ~[grpc-core-1.57.2.jar:1.57.2]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]\r\n\tat java.lang.Thread.run(Unknown Source) ~[?:?]\r\nCaused by: io.netty.handler.codec.http2.Http2Exception: First received frame was not SETTINGS. Hex dump for first 5 bytes: 485454502f\r\n\tat io.netty.handler.codec.http2.Http2Exception.connectionError(Http2Exception.java:109) ~[netty-codec-http2-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.handler.codec.http2.Http2ConnectionHandler$PrefaceDecoder.verifyFirstFrameIsSettings(Http2ConnectionHandler.java:353) ~[netty-codec-http2-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.handler.codec.http2.Http2ConnectionHandler$PrefaceDecoder.decode(Http2ConnectionHandler.java:247) ~[netty-codec-http2-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.handler.codec.http2.Http2ConnectionHandler.decode(Http2ConnectionHandler.java:453) ~[netty-codec-http2-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:529) ~[netty-codec-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:468) ~[netty-codec-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290) ~[netty-codec-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]\r\n\tat java.lang.Thread.run(Unknown Source) ~[?:?]\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n\r\n- Zeebe Version: SNAPSHOT?\r\n- Configuration: SaaS + benchmark apps\r\n\n\n Zelldon: BTW searching for that error it looks like we use HTTP instead of HTTPS? Did we changed anything in the clients in this regard?\r\n\r\n\r\nMight be an issue for the upcoming version, which we should check rather soon.\r\n\r\nRelated findings: \r\n\r\n* https://github.com/grpc/grpc-java/issues/2905#issuecomment-293625616\r\n* https://stackoverflow.com/a/73515627/2165134\r\n\r\n\n npepinpe: It looks like the client builder is fixed to use plaintext instead of properly connecting to SaaS. This you configure the worker/starter to use TLS or not?\n Zelldon: Before it just worked when setting the credentials, so this changed?\n npepinpe: From what I can tell, the benchmark apps haven't changed since 8.1.8, so maybe it's the client? :shrug: \r\n\r\nDid we use to enable TLS automatically if you had credentials maybe? I'd have to check.\n npepinpe: I'd be curious if you just tried setting the credentials/env vars, and then also the TLS application config if it works.\n Zelldon: > Did we use to enable TLS automatically if you had credentials maybe? I'd have to check.\r\n\r\nThis is how it worked before afaik.\n npepinpe: While this may be a regression if that previous behavior has changed, I'm setting this as low severity since it's just a matter of properly configuring it. Could you confirm that with proper configuration it works @Zelldon ? If not, then we can treat it as higher severity.\r\n\r\nSo for now I'm putting this in the backlog until then.\n Zelldon: TBH that is quite problematic, since it is totally unclear to the user what is the problem it took me quite a long time to understand and find the root cause.\n megglos: there is actually a config [for tls which is set to enabled](https://github.com/camunda/zeebe/blob/main/benchmarks/setup/cloud-default/starter.yaml#L26) I will check why it isn't used properly.\r\n\r\nThis impacted QA when they wanted to make use of the benchmark for generating datasets.",
    "title": "Snapshot version of  benchmark application (starter/worker) doesn't work with SaaS"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/13870",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "gitHubText": "<!--\r\nIn case you have questions about our software we encourage everyone to participate in our community via the\r\n- Camunda Platform community forum https://forum.camunda.io/ or \r\n- Slack https://camunda-cloud.slack.com/ (For invite: https://camunda-slack-invite.herokuapp.com/)\r\n\r\nThere you can exchange ideas with other Zeebe and Camunda Platform 8 users, as well as the product developers, and use the search to find answer to similar questions.\r\n\r\nThis issue template is used by the Zeebe engineers to create general tasks.\r\n-->\r\n\r\n**Description**\r\n\r\nThe workflow engine employs the help of several scheduled tasks that periodically append commands to the log stream. This helps offload time-consuming work that could otherwise block the actual workflow processing. \r\n\r\nFor example, job deadlines are checked for expiration by the `JobTimeoutTrigger`, which will append commands for each job to time them out. The stream processor will then process this command asynchronously. Eventually, the job changes state to `TIMED_OUT` when the command is processed. \r\n\r\nIn the meantime, the `JobTimeoutTrigger` continues to check for expired job deadlines periodically. If the stream processor is blocked, it may happen that the `JobTimeoutTrigger` continues to append the same `Job:TIME_OUT` command. There is nothing that stops it from overloading the stream processor.\r\n\r\nIn fact, many of the scheduled tasks suffer from a similar problem.\r\n\r\n**Proposed solution**\r\n\r\nScheduled tasks need to keep a cache of recently appended commands (or something similar) that can be used to avoid appending the same command multiple times.\r\n\r\nThis should be available in a generic way, so all scheduled tasks can easily add support for this.\r\n\r\n**Incident**\r\n\r\nFollow up issue from incident: [INC-274: Streamprocessor lagging behind](https://docs.google.com/document/d/1l8CPBThWtPUxEpmwcoxeb_9pn1eRB2CPpHPiwEVoa3Q/edit#heading=h.hyf892uforsw)\r\n\r\nSupport: \r\n- https://jira.camunda.com/browse/SUPPORT-18291\r\n- https://jira.camunda.com/browse/SUPPORT-18997 \r\n\n\n korthout: Marking this as high impact, as it is a recurring pain. Resolving this would lower the load on clusters experiencing problems where timers, buffered messages, or job timeouts play a role. This would help in the investigation as well as ease recovery.\n abbasadel: I'm increasing the priority of this issue/feature to 'upcoming' since there is a support case waiting for it. \r\nStill, it is highly unlikely to be able to work on this before 8.4. However, we will update the issue once we plan to work on it.\n megglos: ZDP-Planning:\r\n- pushing into the inbox of ZPA to get an update on this as it relates to #14003\n korthout: The ZPA-team discussed this issue again and now considers this a bug. Originally, I felt it was a feature, as we have never supported many timers triggering at once (i.e., there's always a limit somewhere). However, the team feels that outages should not occur from existing features, irrespective of how they are used. Add to this that we don't have documentation about some artibrary limit to the number of timers. Lastly, users feel that this is a bug, and so should we.\r\n\r\nAs a bug, it may be easier to prioritize and release a fix for this as we can backport and patch previous versions.\n dddpaul: @korthout Hi!\r\n\r\nIs there any updates for this bug? I would be nice to have a due date )\n korthout: Hi @dddpaul, the team plans to spend time on this in our current iteration. There's a chance that we can fit a solution in the upcoming patch releases, but we cannot promise this.\n\nWe'll need to consider different solutions. For example, the proposed cache reduces the load on the system in most cases but will not remove the problem completely. Alternatively, ideas like #12560 could help but come with other downsides. The developer(s) working on this issue should consider whether caching is enough or whether another solution might be more fitting. As this is not yet clear, it's possible that a solution will not be ready before the upcoming patch releases. However, we plan to spend time on this issue now.\n Zelldon: There was an L1 blocker again because of this https://jira.camunda.com/plugins/servlet/mobile#issue/SUPPORT-18997 \n\nHandled by @npepinpe \n npepinpe: Users are heavily impacted by this, so I would like to bump this to critical, with a focus on fixing timers. Timers are different from message correlation has they don't involve inter-partition communication, which may help narrow the scope of the fix for now (i.e. writes are guaranteed to succeed).\r\n\r\nThat said, I noticed some possible concurrency issues with the `DueDateTimerChecker`, which could lead to more timers than required being scheduled under high load.\r\n\r\nIf enabled, the `DueDateTimerChecker` runs in the asynchronous scheduler actor, separately from the stream processor.\r\n\r\nHowever, both actors will access the `DueDateTimerChecker`:\r\n\r\n`DueDateTimerChecker#scheduleTimer` is called from the engine/side-effects, i.e. processing actor. It then:\r\n\r\n- Accesses the following state: `shouldRescheduleChecker`, `checkerRunning`, `nextDueDate` (rw)\r\n- Checks if the checker is currently running. This is a non-volatile boolean check.\r\n- If it's not running, it schedules a task.\r\n\r\nAt the same time, scheduling is done on the async scheduler actor. This is the `TriggerEntitiesTask`. It:\r\n\r\n- Accesses the following state: `shouldRescheduleChecker`, `checkerRunning` (rw), `nextDueDate` (rw)\r\n\r\nIs this correct? If so we have non-volatile state being accessed by two different actors, and possibly some race conditions on when and if to schedule the next timer.\r\n\r\nMoving on the the possible solution. In order to keep it light of course, we'd need to to have a way to identify a timer trigger uniquely. Since every time instance can only be scheduled for trigger once in the state, it should be enough to distinguish them with the timer key, which is a single long. \r\n\r\nThe general idea, then, is to cache locally in the `DueDateTimerChecker` state (or some state) which commands have been written, so as to avoid writing them again. A perfect cache would be unbounded - you'd have to possibly store as many timers in there as could possibly exist (worst case scenario).  But a lossy cache is probably acceptable. Take `LongHashSet` from Agrona - an empty, pre-initialized set instance of 1 million keys takes 8MB of memory. Filled, it takes about twice, ~16MB. It's a bit hard to add LRU behavior to such a set however. Eviction could be random to keep things simple/fast, or we can use a `SortedSet`. However, a `TreeSet` takes about ~60MB for 1 million longs, so it's already substantially heavier - but it's pretty easy/fast to add LRU eviction to it. So on the off chance the key was evicted _before_ it was removed explicitly from the cache, you'd potentially still write a second trigger command. But the bigger the cache, the less likely this is to happen.\r\n\r\nI sync'd with @Zelldon today about this, and we decided to move forward with the local cache, only for checkers which perform no inter-partition communication.\r\n\r\nFor timers, we'll use the timer key as the unique cache ID. When iterating of timers, if the timer key is in the cache, then we skip it. Once the task result is built, we can populate the cache with all the timer keys we will be writing. Since the scheduler (even async) is sequenced, we're guaranteed that this write will succeed, or the whole thing will fail (and the cache would be \"restarted\"). We won't be touching things like leader election and so on. On recovery, it's still possible that the same timers would be triggered twice, but the overload load will be greatly reduced.\r\n\r\nThis solution is likely adaptable to other checkers which run locally, such as job timeout and message expiry. For messages we can easily use the key. For jobs, we can't use the job key, since you can have multiple activations/time outs. We could use the job key if we can guarantee that the cache is properly cleaned up whenever the job is activated.\r\n\r\nAnyway, long winded comment to say we'll get started on a bounded lossy cache-based prototype.\r\n\r\n\n npepinpe: So in the end, we'll build the cache on the stream processor side. I describe the approach we ended up with in the description of this PR: https://github.com/camunda/zeebe/pull/15073\r\n\r\nLet me know if you have questions. For now it's entirely on the stream platform/ZDP side, but it's still useful to know how it works. Plus, if you want to cache future scheduled commands in the future, you'll need to know where to add them :upside_down_face: \n npepinpe: See some benchmark results courtesy of @Zelldon - https://github.com/camunda/zeebe/pull/15073#issuecomment-1803887603 :tada: \n korthout: We took it off the ZPA board, as ZDP has found a solution on their side ðŸ™‡  Thanks again @Zelldon and @npepinpe \n dddpaul: Hi!\r\n\r\nThis looks like a wonderful job.  But this new timer cache doesn't solve my problem with bunch of UNIQUE timers from DIFFERENT process instances. I've illustrated my problem here https://github.com/camunda/zeebe/issues/5134#issuecomment-1752006281. My comment was stated as duplicate of this issue which was likely fixed with timer cache solution in PR #15136 (for 8.3).\r\n\r\nI could not understand how this cache solution will help me but had some hope and belief. Well, there no miracle happened.\r\nI've just tested on 8.3-stable\r\n![image](https://github.com/camunda/zeebe/assets/628226/19ab7e3f-7708-465b-8bce-215e55fd6278)\r\n\r\nThousands or millions of unique timer events still overload brokers:\r\n![image](https://github.com/camunda/zeebe/assets/628226/97951508-56da-4723-8910-3ee8149461eb)\r\n\r\nThis is my process with timer:\r\n![image](https://github.com/camunda/zeebe/assets/628226/471fd3e9-c5c5-4c0b-b4c8-e0a9263259b7)\r\n\r\n\r\n\r\n\r\n\n npepinpe: We're working on another solution for that in parallel. https://github.com/camunda/zeebe/issues/10087\n\nI know the issue specifies IPC, but the proposed solution will enforce rate limits between the different categories, user submitted commands, internal/scheduled commands, and remote ones.",
    "title": "Scheduled tasks should avoid overloading the streamprocessor"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/13551",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "gitHubText": "**Describe the bug**\r\n\r\nWhen evaluating a decision with the EvaluateDecisionCommand we can verify the result with the decisionOutput.\r\n\r\nSee doc: https://docs.camunda.io/docs/apis-tools/java-client-examples/decision-evaluate/\r\n\r\nThe weird part is that if the output is a string it is double quoted, which is totally unexpected.\r\n\r\n----\r\n\r\nContext: \r\n\r\nFor implementing [that feature in the C# client](https://github.com/camunda-community-hub/zeebe-client-csharp/pull/548) I used the DMN table from the C7 docs https://docs.camunda.org/get-started/dmn/model/ and wondered during some IT why the result is double-quoted. I was able to reproduce this issue in Java again. I suspect something weird going on in the DMN engine.\r\n\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n**To Reproduce**\r\n\r\nDMN \r\n[dinnerDecisions.dmn.txt](https://github.com/camunda/zeebe/files/12100358/dinnerDecisions.dmn.txt)\r\n\r\n\r\n```java\r\nimport static org.assertj.core.api.AssertionsForClassTypes.assertThat;\r\n\r\nimport io.camunda.zeebe.client.api.response.EvaluateDecisionResponse;\r\nimport org.junit.jupiter.api.Test;\r\n\r\npublic class DmnTest {\r\n\r\n  @Test\r\n  public void test() {\r\n    // given\r\n    final ZeebeClient zeebeClient = ZeebeClient.newClientBuilder()\r\n        .usePlaintext()\r\n        .build();\r\n    zeebeClient\r\n        .newDeployResourceCommand()\r\n        .addResourceFile(\r\n            \"dinnerDecisions.dmn\")\r\n        .send()\r\n        .join();\r\n\r\n    // when\r\n    final EvaluateDecisionResponse response =\r\n        zeebeClient\r\n            .newEvaluateDecisionCommand()\r\n            .decisionId(\"dish\")\r\n            .variables(\"{\\\"season\\\":\\\"Fall\\\", \\\"guestCount\\\":12}\")\r\n            .send()\r\n            .join();\r\n\r\n    // then\r\n    assertThat(response.getDecisionOutput()).isEqualTo(\"Stew\");\r\n  }\r\n}\r\n```\r\n\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\n**Expected behavior**\r\n\r\nThe string is returned without additional quotes. \r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\norg.opentest4j.AssertionFailedError: \r\nexpected: \"Stew\"\r\n but was: \"\"Stew\"\"\r\nExpected :\"Stew\"\r\nActual   :\"\"Stew\"\"\r\n<Click to see difference>\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- OS: <!-- [e.g. Linux] -->\r\n- Zeebe Version: <!-- [e.g. 0.20.0] -->\r\n- Configuration: <!-- [e.g. exporters etc.] -->\r\n\n\n korthout: This might be related to:\r\n- https://github.com/camunda/zeebe/issues/9859\n korthout: ZPA triage:\n- Workaround: parse the returned JSON string\n- We think the problem exists in how Zeebe transforms the returned value into an internal Zeebe value\n- This might be resolved by #9859\n- If not, priority: `later` because a workaround is available and no users/customers have mentioned this problem so far",
    "title": "EvaluateDecision request returns double quoted string"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/9859",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "gitHubText": "**Describe the bug**\n\nAs a user I want to pass special chars in strings ([FEEL ref](https://docs.camunda.io/docs/components/modeler/feel/language-guide/feel-data-types/#string)) in a way that I can consume them in a job worker.\n\nI've defined the following input mappings:\n\n| name | value |\n|:---|:---|\n| dynamic_input | `= \"Hello\\n\\nYOU!\"` |\n| static_input | `Hello\\n\\nYOU!` |\n| static_input_explicit_newlines | `HELLO<newline><newline>YOU!` |\n\nExecuting my process I receive the following input variable values in operate (but also in my process worker):\n\n\n| name | value |\n|:---|:---|\n| dynamic_input | `\"Hello\\\\n\\\\nYOU!\"` |\n| static_input | `\"Hello\\\\\\\\n\\\\\\\\nYOU!\"` |\n| static_input_explicit_newlines | `\"Hello\\\\n\\\\nYOU!\"` |\n\nAs you can see inputs are escaped differently depending on whether I use `\\n` encoded newlines or actual newline characters. Escaping happening in the first place is fine I believe as this is a JSON string and `\\\\n` is the standard way to encode newlines there.\n\n**To Reproduce**\n\n* Deploy + run [test process](https://github.com/camunda/zeebe/files/9159826/input-test.bpmn.txt)\n* See double escaping happening in string inputs\n\n**Expected behavior**\n\nWe do the unescaping magic ourselves so that in user land no one needs to worry:\n\n  * I define an input mapping `foo = \"a\\nb\"`\n  * In my job worker I see `foo = \"a\\nb\"`\n\nAs a more user un-friendly alternative we could consider: String inputs are escaped like expression inputs (no double escaping), we clearly document how escaped strings shall be worked with (and how un-escaping has to be done in user land). This is closer to the existing behavior.\n\n\n**Log/Stacktrace**\n\nnone.\n\n**Environment:**\n- OS: Linux\n- Zeebe Version: 8.0.2\n- Configuration: nothing special\n\nSupport: \n[SUPPORT-15887](https://jira.camunda.com/browse/SUPPORT-15887)\n[SUPPORT-17621](https://jira.camunda.com/browse/SUPPORT-17621)\n\n\n```[tasklist]\n### Tasks\n- [ ] https://github.com/camunda/zeebe/issues/15445\n- [ ] https://github.com/camunda/zeebe/issues/15447\n- [ ] https://github.com/camunda/zeebe/issues/15446\n- [x] Highlight the new behavior change in the release notes \n- [x] Inform the support team\n- [x] Update the [input mappings docs](https://github.com/camunda/camunda-docs/pull/3020).\n```\n\n\n nikku: To add to this I see zeebe escaping unicode characters (i.e. `ðŸ¤©`) despite JSON having full unicode support.\r\n\r\n```\r\nHello\\n\\nYOU! ðŸ¤©\r\n```\r\n\r\nturns into \r\n\r\n```\r\n\"Hello\\\\n\\\\nYOU! \\uD83E\\uDD29\"\r\n```\r\n\r\n\r\n----\r\n\r\n`\"Hello\\\\n\\\\nYOU! ðŸ¤©\"` is a valid JSON string.\n nikku: Is there a chance this can be fixed at some point? Or do we regard this as a nasty-bug-turned-public-api?\n menski: @felix-mueller this is something which mostly effects connectors but it is very unexpected behavior. We would like to understand if you have any input on priority for this.\n felix-mueller: It should be fixed at some point. Since we still need to root-cause and better understand the issue it is probably not S. We won't priotize it for the next few iterations (first we need to finish instance modification, DMN evaluation and delete definitions).\n menski: Thanks Felix, I put the priority on \"Later\" for now\n darox: I don't know whether I'm hitting the same issue. I'm using the Go client to ingest JSON formatted data into Zeebe. Jobs are failing due to escaped double quotes. If I set vars manually via simple-monitor everything works as expected. \n igpetrov: Hi team, is there any ETA to fix this? It is affecting currently 3 connectors.\n felix-mueller: @igpetrov currently we have planned to look into it after instance modification, DMN evaluation and Delete Definitions. these topics are scheduled to be included in our April minor release, so if we don't change priorities we probably will look at it in a few months.\r\n\r\nfrom connectors perspective can we live with this for a few months?\n igpetrov: Hi @felix-mueller, appreciate your update. Absolutely. Just wanted to understand the ETA. Thank you!\n tmetzke: For reference, in Connectors, we currently do the following to work around this with data we receive as variables after Zeebe has processed the user input:\r\n```\r\nStringEscapeUtils.unescapeJson(jsonStringVariable)\r\n```\r\nThis uses `StringEscapeUtils` from `org.apache.commons:commons-text`.\n nikku: @felix-mueller The longer we keep such behavior in the core, the harder it will be for people to unbuild their hacks (trying to work with zeebe data). My :two: :coin:.\n nikku: A similar issue is reported by a customer through [SUPPORT-15887](https://jira.camunda.com/browse/SUPPORT-15887). \n nikku: I've updated the issue with a clear expected behavior, reflecting https://jira.camunda.com/browse/SUPPORT-15887, but also https://github.com/camunda/zeebe/issues/9859#issuecomment-1318121197 have reported.\n menski: Discussed in the planning meeting on 2023-02-17. Right now we don't have capacity to work on this, and deprioritize it as the customer has a workaround available.\n korthout: When we work on this, we need to make sure not to break user space when backporting it to patches. We can do this by adding a configuration setting that allows switching between the current (broken) and new (fixed) behaviors. The default should stay the same in patch releases, but in a minor release we can change the default. This should be clearly communicated in the release notes (and blog) so users understand the change.\n abbasadel: This issue was mentioned again in [SUPPORT-17621](https://jira.camunda.com/browse/SUPPORT-17621) and blocking connectors team's [here](https://github.com/camunda/connectors-bundle/issues/239)\n korthout: When we resolve this issue, please verify whether the fix also resolves:\r\n- https://github.com/camunda/zeebe/issues/13551\n Benjoyo: Also came across this, the `StringEscapeUtils.unescapeJson` workaround works for now but would be nice to have this fixed soon :)\n mschoe: I run into this issue when using the GH outbound connector and create a new issue with a multi line description field.  As this is a quite common use case I'm wondering when we are planning to provide a fix. @felix-mueller is this still scheduled for the 8.5 release in April 2024? \n christian-konrad: Would https://github.com/camunda/product-hub/issues/1040 help here or is it just a quick fix on string behavior?\n saig0: @nicpuppa started to work on this issue by fixing the escaping in the FEEL engine [here](https://github.com/camunda/feel-scala/pull/750). We plan to deliver a fix in one of the next patch releases. :rocket: \r\n\r\n@christian-konrad the FEEL templating is a bigger topic. Let's fix the issue with the escaping first. :+1: \n christian-konrad: Perfect @saig0 ! Thanks\n MaxTru: Hi @abbasadel ,\r\n\r\naccording to the [previous assessment](https://github.com/camunda/zeebe/issues/9859#issuecomment-1805096523) this should have been fixed with this [version bump](https://github.com/camunda/zeebe/commit/4a90c77137dcad51106ee1e2fa8d1172c12640c2). Can you verify from Zeebe perspective please and update this issue? Thanks\n megglos: We need to clarify whether this change is breaking, especially in regards to present workaround in other components such as https://github.com/camunda/connector-slack/issues/48 &\r\nhttps://github.com/camunda/connector-google-drive/issues/20\r\nWhile for the REST connector no workaround seems to be in place and there a patch by zeebe would resolve the issue.\r\n\r\nIf it's breaking we may prefer to only release it in the next minor or at least coordinate with the affected components to remove the workaround applied for a patch. But then we have the risk that we have a dependency between patch levels of connectors and zeebe.\r\n\r\n@abbasadel will sync with @saig0 on this tomorrow and ping back.\n abbasadel: Hi everyone, this is Zeebe's assessment: \r\n- The recent [bug fix](https://github.com/camunda/feel-scala/pull/750) on the FEEL engine is not backward compatible from Zeebe's point of view. However, we believe, based on @saig0â€™s feedback and our tests, the impact on our users is low since the recommended workaround we provided was to use [unescapeJson API](https://commons.apache.org/proper/commons-lang/apidocs/org/apache/commons/lang3/StringEscapeUtils.html#unescapeJson-java.lang.String-) from Apache Commons which handles breaking change.\r\n- As the previous behavior was wrong, mentioning the new behavior change in the release notes and as an announcement would be needed\r\n- This FEEL fix should resolve [bug](https://github.com/camunda/connectors/issues/239), to be validated by Connectors/Operate teams. \r\n- The support team should also be aware of this FEEL behavior change once release\r\n\r\n\r\n\n MaxTru: > Hi everyone, this is Zeebe's assessment:\r\n> \r\n> * The recent [bug fix](https://github.com/camunda/feel-scala/pull/750) on the FEEL engine is not backward compatible from Zeebe's point of view. However, we believe, based on @saig0â€™s feedback and our tests, the impact on our users is low since the recommended workaround we provided was to use [unescapeJson API](https://commons.apache.org/proper/commons-lang/apidocs/org/apache/commons/lang3/StringEscapeUtils.html#unescapeJson-java.lang.String-) from Apache Commons which handles breaking change.\r\n> * As the previous behavior was wrong, mentioning the new behavior change in the release notes and as an announcement would be needed\r\n> * This FEEL fix should resolve [bug](https://github.com/camunda/connectors/issues/239), to be validated by Connectors/Operate teams.\r\n> * The support team should also be aware of this FEEL behavior change once release\r\n\r\nThanks @abbasadel . Will you and the ZPA team please ensure that all these things happen? If so, can we please track this via a breakdown and DRI assignments? Why is the issue not in `in progress` and has no assignee?\n abbasadel: @MaxTru, we will take care of this issue\n nicpuppa: Hi everyone, as follow up I created 3 separate issues to reduce the scope and make easier to follow the progress:\r\n\r\n- [Verify correct behaviour of input mappings after bump to FEEL 1.17.3](https://github.com/camunda/zeebe/issues/15445) -> this is a testing purpose issue, to verify that the new FEEL version has the correct behaviour\r\n- [Special characters in static inputs should not be escaped](https://github.com/camunda/zeebe/issues/15447) -> as mentioned also by @saig0 in [this comment](https://github.com/camunda/feel-scala/pull/750#pullrequestreview-1731283017), characters in static input are still escaped. Static input are handled directly by Zeebe\r\n- [Verify unicode characters are not escaped in input mappings](https://github.com/camunda/zeebe/issues/15446) -> after resolving the [feel issue related to unicode](https://github.com/camunda/feel-scala/issues/558), on Zeebe we should verify that the behaviour is as expected (unicode characters should not be escaped)\r\n\r\n~~In accordance with @abbasadel and @saig0 I will close this issue~~ This issue is still open only for tracking purpose, I mistakenly closed the issue\n abbasadel: Thanks, @nicpuppa, for the updates. \r\n\r\nI added the sub-issues into the main issue description for clarity and tracking purpose\n abbasadel: Hi everyone, \n- The [dynamic_input unnecessary escaping bug](https://github.com/camunda/feel-scala/pull/750) is currently resolved in [Zeebe 8.3.4 release](https://github.com/camunda/zeebe/releases/tag/8.3.4)\n- The bug fix is https://github.com/camunda/zeebe/issues/15445 resolves the [connectors team bug](https://github.com/camunda/connectors/issues/239). Thanks @saig0 and @nicpuppa \n- The corrected behavior change is highlighted in the [release notes](https://github.com/camunda/zeebe/releases/tag/8.3.4). Thanks @saig0 for the wording.\n- The support team is informed about the [corrected behavior change](https://camunda.slack.com/archives/CHAC0L80M/p1702029188512579)\n- Next action: Update [input mappings docs](https://docs.camunda.io/docs/components/concepts/variables/#input-mappings) as suggested by @akeller  ",
    "title": "Inconsistent/unnecessary escaping of characters in inputs"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/8938",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "gitHubText": "**Describe the bug**\r\n\r\n\r\nI run into this issue now several time. This time again I created a feel expression and deployed the model successfully. \r\n\r\nExpression:\r\n\r\n```\r\n=date an time (\"2022-03-21T11:15:00@Europe/Berlin\") - now()\r\n```\r\n\r\nCreating the instance works, but cause an incident. From the incident message it is totally unclear what is going on or what is wrong.\r\n\r\n```\r\nExpected result of the expression 'date an time (\"2022-03-21T11:15:00@Europe/Berlin\") - now()' to be one of '[DURATION, PERIOD, STRING]', but was 'NULL'\r\n```\r\n\r\nIf we take a look in the google console log we can see the real issue, [which is simply ignored](https://console.cloud.google.com/logs/query;query=resource.type%3D%22k8s_container%22%0Aresource.labels.project_id%3D%22camunda-cloud-240911%22%0Aresource.labels.location%3D%22europe-west1%22%0Aresource.labels.cluster_name%3D%22integration-worker-1%22%0Aresource.labels.namespace_name%3D%22061e827f-c1f1-486e-b9f7-263a1efdaedb-zeebe%22%0Aresource.labels.pod_name:%22zeebe-%22;cursorTimestamp=2022-03-21T10:06:05.543376569Z?project=camunda-cloud-240911) :imp: \r\n\r\n```\r\n \"Suppressed failure: no function found with name 'date an time' and 1 parameters\"\r\n```\r\n\r\nHere it might be easy to spot, but if users have bigger feel expression or more complex ones it might be not that simple. It is frustrating to find that issue, especially if an user has no access to the logs. \r\n\r\nThis is also more problematic if this values is later only consumes by some jobs and cause more issues, and is not visible as incident.\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n**To Reproduce**\r\n\r\nCreate process model with an invalid feel expression.\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\n**Expected behavior**\r\n\r\nWe create incidents for failed feel expression evaluation with a clear message, similar to the log message.\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n**Environment:**\r\n- Zeebe Version: 1.3.5<!-- [e.g. 0.20.0] -->\r\n- Configuration: <!-- [e.g. exporters etc.] -->\r\n\n\n Zelldon: This caused for example also this https://camunda.slack.com/archives/C034N0E5YCX/p1647857272889399\n Zelldon: Probably duplicate of https://github.com/camunda-cloud/zeebe/issues/5880 but tbh it is just annoying and I wanted to express my feeling here :sweat_smile: The other also still awaits stakeholder input, would be good to get this.\n saig0: Related feature request in the FEEL engine: https://github.com/camunda/feel-scala/issues/260\n saig0: With the new version of the FEEL engine (1.17.0), we could improve the UX. The new API doesn't return only the evaluation result but also potential warnings. We could append these warnings in the incident message to help the user to solve the problem. \r\n\r\nFor example, instead of creating an incident with the message:\r\n\r\n```\r\nExpected result of the expression 'date an time (\"2022-03-21T11:15:00@Europe/Berlin\") - now()' to be one of '[DURATION, PERIOD, STRING]', but was 'NULL'\r\n```\r\n\r\nWe could create an incident with the following message:\r\n\r\n```\r\nExpected result of the expression 'date an time (\"2022-03-21T11:15:00@Europe/Berlin\") - now()' to be one of '[DURATION, PERIOD, STRING]', but was 'NULL'. \r\nThe evaluation reported the following warnings:  [NO_FUNCTION_FOUND] No function found with name 'date an time' and 1 parameters; [INVALID_TYPE] Can't subtract '2023-09-25T12:00:43.591630872@Etc/UTC' from 'null'\r\n```\r\n\r\n---\r\n\r\ncc: @aleksander-dytko ",
    "title": "Feel error are causing bad user experiences"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15287",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Documentation",
    "gitHubText": "**Description**\r\n\r\nWe have identified the following steps to be missing from the developer handbook for adding new record types:\r\n\r\n* Inclusion of a new value type in the engine's supported value types to ensure events are replayed\r\n* Support for a new value type in Zeebe Process Test (ZPT)\r\n\r\nMissing those steps has led to bug reports and failing CI in the past.\r\n\n",
    "title": "Add missing steps to developer handbook"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/pull/15744",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Merged Pull Requests",
    "gitHubText": "## Description\r\n\r\nAdd maven plugin to prevent backward incompatible changes to protobuf message in topology module.\r\n\r\n## Related issues\r\n\r\ncloses #15743 ",
    "title": "build(topology): add backward compatibility check for protobuf messages"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/pull/15727",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Merged Pull Requests",
    "gitHubText": "## Description\r\n\r\nThe change must be cancelled only the operation cannot make progress. But it might still happen that after it was cancelled, operation was applied successfully and incorrectly apply the changes on the cancelled topology. To prevent this, we now compare the expected topology with the current topology before applying the change.\r\n\r\n## Related issues\r\n\r\ncloses #15726 \r\n\r\n",
    "title": "fix(topology): do not update the topology if the operation was cancelled"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/pull/15709",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Merged Pull Requests",
    "gitHubText": "Closes #15231",
    "title": "fix(topology): maintain gossip state update ordering"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/pull/15629",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Merged Pull Requests",
    "gitHubText": "[![Mend Renovate](https://app.renovatebot.com/images/banner.svg)](https://renovatebot.com)\n\nThis PR contains the following updates:\n\n| Package | Type | Update | Change |\n|---|---|---|---|\n| [actions/upload-artifact](https://togithub.com/actions/upload-artifact) | action | major | `v3` -> `v4` |\n\n---\n\n> [!WARNING]\n> Some dependencies could not be looked up. Check the Dependency Dashboard for more information.\n\n---\n\n### Release Notes\n\n<details>\n<summary>actions/upload-artifact (actions/upload-artifact)</summary>\n\n### [`v4`](https://togithub.com/actions/upload-artifact/compare/v3...v4)\n\n[Compare Source](https://togithub.com/actions/upload-artifact/compare/v3...v4)\n\n</details>\n\n---\n\n### Configuration\n\nðŸ“… **Schedule**: Branch creation - \"after 8pm every weekday,before 6am every weekday\" (UTC), Automerge - At any time (no schedule defined).\n\nðŸš¦ **Automerge**: Disabled by config. Please merge this manually once you are satisfied.\n\nâ™» **Rebasing**: Whenever PR becomes conflicted, or you tick the rebase/retry checkbox.\n\nðŸ”• **Ignore**: Close this PR and you won't be reminded about this update again.\n\n---\n\n - [ ] <!-- rebase-check -->If you want to rebase/retry this PR, check this box\n\n---\n\nThis PR has been generated by [Mend Renovate](https://www.mend.io/free-developer-tools/renovate/). View repository job log [here](https://developer.mend.io/github/camunda/zeebe).\n<!--renovate-debug:eyJjcmVhdGVkSW5WZXIiOiIzNy45My4xIiwidXBkYXRlZEluVmVyIjoiMzcuOTMuMSIsInRhcmdldEJyYW5jaCI6Im1haW4ifQ==-->\n",
    "title": "deps(github-tags): Update actions/upload-artifact action to v4 (main)"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/pull/15619",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Merged Pull Requests",
    "gitHubText": "null",
    "title": "fix(github): new project automation token for add-to-projects"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/pull/15618",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Merged Pull Requests",
    "gitHubText": "This reverts commit 6bcd02082e5e6d5a6087560c113c4e650e7f61f7.\r\n\r\n## Description\r\n\r\n<!-- Please explain the changes you made here. -->\r\n\r\n## Related issues\r\n\r\n<!-- Which issues are closed by this PR or are related -->\r\n\r\ncloses #\r\n\r\n<!-- Cut-off marker\r\n_All lines under and including the cut-off marker will be removed from the merge commit message_\r\n\r\n## Definition of Ready\r\n\r\nPlease check the items that apply, before requesting a review.\r\n\r\nYou can find more details about these items in our wiki page about [Pull Requests and Code Reviews](https://github.com/camunda/zeebe/wiki/Pull-Requests-and-Code-Reviews).\r\n\r\n* [ ] I've reviewed my own code\r\n* [ ] I've written a clear changelist description\r\n* [ ] I've narrowly scoped my changes\r\n* [ ] I've separated structural from behavioural changes\r\n-->\r\n\r\n## Definition of Done\r\n\r\n<!-- Please check the items that apply, before merging or (if possible) before requesting a review. -->\r\n\r\n_Not all items need to be done depending on the issue and the pull request._\r\n\r\nCode changes:\r\n* [ ] The changes are backwards compatibility with previous versions\r\n* [ ] If it fixes a bug then PRs are created to [backport](https://github.com/camunda/zeebe/compare/stable/0.24...main?expand=1&template=backport_template.md&title=[Backport%200.24]) the fix to the last two minor versions. You can trigger a backport by assigning labels (e.g. `backport stable/1.3`) to the PR, in case that fails you need to create backports manually.\r\n\r\nTesting:\r\n* [ ] There are unit/integration tests that verify all acceptance criterias of the issue\r\n* [ ] New tests are written to ensure backwards compatibility with further versions\r\n* [ ] The behavior is tested manually\r\n* [ ] The change has been verified by a QA run\r\n* [ ] The impact of the changes is verified by a benchmark\r\n\r\nDocumentation:\r\n* [ ] The documentation is updated (e.g. BPMN reference, configuration, examples, get-started guides, etc.)\r\n* [ ] If the PR changes how BPMN processes are validated (e.g. support new BPMN element) then the Camunda modeling team should be informed to adjust the BPMN linting.\r\n\r\nOther teams:\r\nIf the change impacts another team an issue has been created for this team, explaining what they need to do to support this change.\r\n- [ ] [Operate](https://github.com/camunda/operate/issues)\r\n- [ ] [Tasklist](https://github.com/camunda/tasklist/issues)\r\n- [ ] [Web Modeler](https://github.com/camunda/web-modeler/issues)\r\n- [ ] [Desktop Modeler](https://github.com/camunda/camunda-modeler/issues)\r\n- [ ] [Optimize](https://github.com/camunda/camunda-optimize/issues)\r\n\r\nPlease refer to our [review guidelines](https://github.com/camunda/zeebe/wiki/Pull-Requests-and-Code-Reviews#code-review-guidelines).\r\n",
    "title": "Revert \"fix(github): use project admin token for project query\""
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/pull/15617",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Merged Pull Requests",
    "gitHubText": "## Description\r\n\r\nSee https://github.com/camunda/zeebe/actions/runs/7208315699/job/19637188599\r\n\r\n* resolves a warning on how variables were set\r\n* fixed if conditions, which didn't work as the result of the has-project step is a string not a boolean\r\n* usage of project admin token for the project query, with the github repo token no results were returned as no access to projects\r\n\r\n\r\n## Related issues\r\n\r\nrelated https://github.com/camunda/zeebe/pull/15545",
    "title": "fix(github): fixed for add to projects workflow"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/pull/15508",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Merged Pull Requests",
    "gitHubText": "## Description\r\n\r\nThis PR downgrades job worker back pressure, which is observed as job yielding on the broker side, from an error state to an expected state. This means downgrading logs to trace level in most cases, and instead relying on metrics to track if we're blocked too often. Trace logging can then be turned on on-demand to narrow down issues.\r\n\r\nAdditionally, I took the opportunity to refactor the gateway push-to-client logic out of the `AggregatedClientStream` and `ClientStreamImpl` classes into its own class, `ClientStreamPusher`. These classes are now just data classes.\r\n\r\nWhen back pressure occurs, the previous state was the following:\r\n\r\n| Node type | Image |\r\n| --------- | ----- |\r\n| Gateway | ![image](https://github.com/camunda/zeebe/assets/43373/42e45e95-6450-430d-b800-eacac299995a) |\r\n| Broker | ![image](https://github.com/camunda/zeebe/assets/43373/3d62181b-fd04-4d93-afcc-a2b6c6bcb2a5) |\r\n\r\nIt now looks like this without trace logging:\r\n\r\n| Node type | Image |\r\n| --------- | ----- |\r\n| Gateway | ![image](https://github.com/camunda/zeebe/assets/43373/4049f382-a009-41b1-afb9-fc99bbda5d2e) |\r\n| Broker | ![image](https://github.com/camunda/zeebe/assets/43373/73d80908-7826-4667-a428-7a27ddc375ba) |\r\n\r\nAnd with trace logging enabled:\r\n\r\n| Node type | Image |\r\n| --------- | ----- |\r\n| Gateway | ![image](https://github.com/camunda/zeebe/assets/43373/624712f3-21f7-4ac0-bbc9-621b758d9084) |\r\n| Broker | ![image](https://github.com/camunda/zeebe/assets/43373/6318956f-7eb0-4f7c-9910-b0960a6155bb) |\r\n\r\n> [!Note]\r\n> I did not throttle the trace logging. My reasoning is if you're tracing something, you actually want to see everything, not just some of it.\r\n\r\nAnd a quick preview of the new metric visualization:\r\n\r\n> [!Note] \r\n> I will add the new metric visualization in a separate PR since Grafana changes tend to be quite big and difficult to review.\r\n\r\n![image](https://github.com/camunda/zeebe/assets/43373/c0289c06-0527-4cc5-b0c3-50c649d6d194)\r\n\r\nAnd here we can see when only one worker is slower than the others in the same deployment:\r\n\r\n![image](https://github.com/camunda/zeebe/assets/43373/7228acab-3682-495d-ab76-7edcefa79899)\r\n\r\nSo we see that one gateway reports some blocked errors, but the other push metrics are \"successful\". This could help diagnose issues with specific stream, as you can then look at that gateway's actuator/trace logs to narrow things down.\r\n\r\n## Related issues\r\n\r\ncloses #15455\r\n\r\n<!-- Cut-off marker\r\n_All lines under and including the cut-off marker will be removed from the merge commit message_\r\n\r\n## Definition of Ready\r\n\r\nPlease check the items that apply, before requesting a review.\r\n\r\nYou can find more details about these items in our wiki page about [Pull Requests and Code Reviews](https://github.com/camunda/zeebe/wiki/Pull-Requests-and-Code-Reviews).\r\n\r\n* [ ] I've reviewed my own code\r\n* [ ] I've written a clear changelist description\r\n* [ ] I've narrowly scoped my changes\r\n* [ ] I've separated structural from behavioural changes\r\n-->\r\n\r\n## Definition of Done\r\n\r\n<!-- Please check the items that apply, before merging or (if possible) before requesting a review. -->\r\n\r\n_Not all items need to be done depending on the issue and the pull request._\r\n\r\nCode changes:\r\n* [ ] The changes are backwards compatibility with previous versions\r\n* [ ] If it fixes a bug then PRs are created to [backport](https://github.com/camunda/zeebe/compare/stable/0.24...main?expand=1&template=backport_template.md&title=[Backport%200.24]) the fix to the last two minor versions. You can trigger a backport by assigning labels (e.g. `backport stable/1.3`) to the PR, in case that fails you need to create backports manually.\r\n\r\nTesting:\r\n* [ ] There are unit/integration tests that verify all acceptance criterias of the issue\r\n* [ ] New tests are written to ensure backwards compatibility with further versions\r\n* [ ] The behavior is tested manually\r\n* [ ] The change has been verified by a QA run\r\n* [ ] The impact of the changes is verified by a benchmark\r\n\r\nDocumentation:\r\n* [ ] The documentation is updated (e.g. BPMN reference, configuration, examples, get-started guides, etc.)\r\n* [ ] If the PR changes how BPMN processes are validated (e.g. support new BPMN element) then the Camunda modeling team should be informed to adjust the BPMN linting.\r\n\r\nOther teams:\r\nIf the change impacts another team an issue has been created for this team, explaining what they need to do to support this change.\r\n- [ ] [Operate](https://github.com/camunda/operate/issues)\r\n- [ ] [Tasklist](https://github.com/camunda/tasklist/issues)\r\n- [ ] [Web Modeler](https://github.com/camunda/web-modeler/issues)\r\n- [ ] [Desktop Modeler](https://github.com/camunda/camunda-modeler/issues)\r\n- [ ] [Optimize](https://github.com/camunda/camunda-optimize/issues)\r\n\r\nPlease refer to our [review guidelines](https://github.com/camunda/zeebe/wiki/Pull-Requests-and-Code-Reviews#code-review-guidelines).\r\n",
    "title": "Handle job worker back pressure/yield as an expected case"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/pull/15390",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Merged Pull Requests",
    "gitHubText": "## Description\r\n\r\nThe ES dashboard is currently completely broken due to recent changes https://github.com/camunda/zeebe/pull/15350\r\n\r\nIt looks like that ES dashboard don't like to be exported via sharing with externally, somehow this messed up everything.\r\n\r\nIn SaaS I had to fix this as well... I used this dashboard to restore the version and exported it again https://grafana.dev.zeebe.io/d/elasticsearch-saas/elasticsearch-saas?orgId=1&var-datasource=prometheus&var-cluster=All&var-namespace=medic-y-2023-cw-48-ce0b3b8-benchmark-mixed&var-index=All\r\n\r\n**Broken:**\r\n\r\n![bropkenfix](https://github.com/camunda/zeebe/assets/2758593/cd62746b-12cd-48ae-929e-ea5e183c6671)\r\n\r\n**Fixed:** \r\n![fix](https://github.com/camunda/zeebe/assets/2758593/9a1d96f2-592c-4e53-929c-da1ec3694e18)\r\n\r\n\r\n\r\n \r\n\r\n<!-- Please explain the changes you made here. -->\r\n\r\n## Related issues\r\n\r\n<!-- Which issues are closed by this PR or are related -->\r\n\r\ncloses #\r\n\r\n<!-- Cut-off marker\r\n_All lines under and including the cut-off marker will be removed from the merge commit message_\r\n\r\n## Definition of Ready\r\n\r\nPlease check the items that apply, before requesting a review.\r\n\r\nYou can find more details about these items in our wiki page about [Pull Requests and Code Reviews](https://github.com/camunda/zeebe/wiki/Pull-Requests-and-Code-Reviews).\r\n\r\n* [ ] I've reviewed my own code\r\n* [ ] I've written a clear changelist description\r\n* [ ] I've narrowly scoped my changes\r\n* [ ] I've separated structural from behavioural changes\r\n-->\r\n\r\n## Definition of Done\r\n\r\n<!-- Please check the items that apply, before merging or (if possible) before requesting a review. -->\r\n\r\n_Not all items need to be done depending on the issue and the pull request._\r\n\r\nCode changes:\r\n* [ ] The changes are backwards compatibility with previous versions\r\n* [ ] If it fixes a bug then PRs are created to [backport](https://github.com/camunda/zeebe/compare/stable/0.24...main?expand=1&template=backport_template.md&title=[Backport%200.24]) the fix to the last two minor versions. You can trigger a backport by assigning labels (e.g. `backport stable/1.3`) to the PR, in case that fails you need to create backports manually.\r\n\r\nTesting:\r\n* [ ] There are unit/integration tests that verify all acceptance criterias of the issue\r\n* [ ] New tests are written to ensure backwards compatibility with further versions\r\n* [ ] The behavior is tested manually\r\n* [ ] The change has been verified by a QA run\r\n* [ ] The impact of the changes is verified by a benchmark\r\n\r\nDocumentation:\r\n* [ ] The documentation is updated (e.g. BPMN reference, configuration, examples, get-started guides, etc.)\r\n* [ ] If the PR changes how BPMN processes are validated (e.g. support new BPMN element) then the Camunda modeling team should be informed to adjust the BPMN linting.\r\n\r\nOther teams:\r\nIf the change impacts another team an issue has been created for this team, explaining what they need to do to support this change.\r\n- [ ] [Operate](https://github.com/camunda/operate/issues)\r\n- [ ] [Tasklist](https://github.com/camunda/tasklist/issues)\r\n- [ ] [Web Modeler](https://github.com/camunda/web-modeler/issues)\r\n- [ ] [Desktop Modeler](https://github.com/camunda/camunda-modeler/issues)\r\n- [ ] [Optimize](https://github.com/camunda/camunda-optimize/issues)\r\n\r\nPlease refer to our [review guidelines](https://github.com/camunda/zeebe/wiki/Pull-Requests-and-Code-Reviews#code-review-guidelines).\r\n",
    "title": "Fix the current ES dashboard"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/pull/15341",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Merged Pull Requests",
    "gitHubText": "## Description\r\nThe generations were not correctly set, because the wrong variables were referenced\r\n\r\n![versions4](https://github.com/camunda/zeebe/assets/2758593/4b297724-d64c-47ad-a75f-091ed7be9d87)\r\n![versions3](https://github.com/camunda/zeebe/assets/2758593/0f9b9af8-1e49-4911-b925-998444e69ff0)\r\n![versions2](https://github.com/camunda/zeebe/assets/2758593/2caed5e7-0730-4265-87c8-595363255e42)\r\n![versions](https://github.com/camunda/zeebe/assets/2758593/c6defee7-cc6c-4528-850a-386d5c5bacc3)\r\n\r\n\r\n\r\nThe long names make it really hard to track... I was thinking whether we should maybe shorten them.\r\n<!-- Please explain the changes you made here. -->\r\n\r\n## Related issues\r\n\r\n<!-- Which issues are closed by this PR or are related -->\r\n\r\ncloses #\r\n\r\n<!-- Cut-off marker\r\n_All lines under and including the cut-off marker will be removed from the merge commit message_\r\n\r\n## Definition of Ready\r\n\r\nPlease check the items that apply, before requesting a review.\r\n\r\nYou can find more details about these items in our wiki page about [Pull Requests and Code Reviews](https://github.com/camunda/zeebe/wiki/Pull-Requests-and-Code-Reviews).\r\n\r\n* [ ] I've reviewed my own code\r\n* [ ] I've written a clear changelist description\r\n* [ ] I've narrowly scoped my changes\r\n* [ ] I've separated structural from behavioural changes\r\n-->\r\n\r\n## Definition of Done\r\n\r\n<!-- Please check the items that apply, before merging or (if possible) before requesting a review. -->\r\n\r\n_Not all items need to be done depending on the issue and the pull request._\r\n\r\nCode changes:\r\n* [ ] The changes are backwards compatibility with previous versions\r\n* [ ] If it fixes a bug then PRs are created to [backport](https://github.com/camunda/zeebe/compare/stable/0.24...main?expand=1&template=backport_template.md&title=[Backport%200.24]) the fix to the last two minor versions. You can trigger a backport by assigning labels (e.g. `backport stable/1.3`) to the PR, in case that fails you need to create backports manually.\r\n\r\nTesting:\r\n* [ ] There are unit/integration tests that verify all acceptance criterias of the issue\r\n* [ ] New tests are written to ensure backwards compatibility with further versions\r\n* [ ] The behavior is tested manually\r\n* [ ] The change has been verified by a QA run\r\n* [ ] The impact of the changes is verified by a benchmark\r\n\r\nDocumentation:\r\n* [ ] The documentation is updated (e.g. BPMN reference, configuration, examples, get-started guides, etc.)\r\n* [ ] If the PR changes how BPMN processes are validated (e.g. support new BPMN element) then the Camunda modeling team should be informed to adjust the BPMN linting.\r\n\r\nOther teams:\r\nIf the change impacts another team an issue has been created for this team, explaining what they need to do to support this change.\r\n- [ ] [Operate](https://github.com/camunda/operate/issues)\r\n- [ ] [Tasklist](https://github.com/camunda/tasklist/issues)\r\n- [ ] [Web Modeler](https://github.com/camunda/web-modeler/issues)\r\n- [ ] [Desktop Modeler](https://github.com/camunda/camunda-modeler/issues)\r\n- [ ] [Optimize](https://github.com/camunda/camunda-optimize/issues)\r\n\r\nPlease refer to our [review guidelines](https://github.com/camunda/zeebe/wiki/Pull-Requests-and-Code-Reviews#code-review-guidelines).\r\n",
    "title": "fix: set correct generation"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/pull/15340",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Merged Pull Requests",
    "gitHubText": "## Description\r\n\r\nBring back the version calculation, which was broken without the patch versions.\r\n\r\nFor me it was easier to bring the part back, and not refactor it too much.\r\n\r\nhttps://github.com/camunda/zeebe/commit/7039bf942fe31b9029ec23ada53a18f85b093355#diff-6a936ccc311a8f5af0da4203c51fec756b63a03f1a9b7dbd9c7e1b7b80531c1d\r\n<!-- Please explain the changes you made here. -->\r\n",
    "title": "Calculate the right versions"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/pull/15317",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Merged Pull Requests",
    "gitHubText": "## Description\r\n\r\nAllow easy disabling of TCC if required by simply removing the secret.\r\n\r\n<!-- Cut-off marker\r\n_All lines under and including the cut-off marker will be removed from the merge commit message_\r\n\r\n## Definition of Ready\r\n\r\nPlease check the items that apply, before requesting a review.\r\n\r\nYou can find more details about these items in our wiki page about [Pull Requests and Code Reviews](https://github.com/camunda/zeebe/wiki/Pull-Requests-and-Code-Reviews).\r\n\r\n* [ ] I've reviewed my own code\r\n* [ ] I've written a clear changelist description\r\n* [ ] I've narrowly scoped my changes\r\n* [ ] I've separated structural from behavioural changes\r\n-->\r\n\r\n## Definition of Done\r\n\r\n<!-- Please check the items that apply, before merging or (if possible) before requesting a review. -->\r\n\r\n_Not all items need to be done depending on the issue and the pull request._\r\n\r\nCode changes:\r\n* [ ] The changes are backwards compatibility with previous versions\r\n* [ ] If it fixes a bug then PRs are created to [backport](https://github.com/camunda/zeebe/compare/stable/0.24...main?expand=1&template=backport_template.md&title=[Backport%200.24]) the fix to the last two minor versions. You can trigger a backport by assigning labels (e.g. `backport stable/1.3`) to the PR, in case that fails you need to create backports manually.\r\n\r\nTesting:\r\n* [ ] There are unit/integration tests that verify all acceptance criterias of the issue\r\n* [ ] New tests are written to ensure backwards compatibility with further versions\r\n* [ ] The behavior is tested manually\r\n* [ ] The change has been verified by a QA run\r\n* [ ] The impact of the changes is verified by a benchmark\r\n\r\nDocumentation:\r\n* [ ] The documentation is updated (e.g. BPMN reference, configuration, examples, get-started guides, etc.)\r\n* [ ] If the PR changes how BPMN processes are validated (e.g. support new BPMN element) then the Camunda modeling team should be informed to adjust the BPMN linting.\r\n\r\nOther teams:\r\nIf the change impacts another team an issue has been created for this team, explaining what they need to do to support this change.\r\n- [ ] [Operate](https://github.com/camunda/operate/issues)\r\n- [ ] [Tasklist](https://github.com/camunda/tasklist/issues)\r\n- [ ] [Web Modeler](https://github.com/camunda/web-modeler/issues)\r\n- [ ] [Desktop Modeler](https://github.com/camunda/camunda-modeler/issues)\r\n- [ ] [Optimize](https://github.com/camunda/camunda-optimize/issues)\r\n\r\nPlease refer to our [review guidelines](https://github.com/camunda/zeebe/wiki/Pull-Requests-and-Code-Reviews#code-review-guidelines).\r\n",
    "title": "refactor: allow easy disabling via secret removal"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/pull/15201",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Merged Pull Requests",
    "gitHubText": "## Description\r\n\r\n<!-- Please explain the changes you made here. -->\r\n\r\nIncludes `Form` records in the supported Engine records so they are replayed.\r\n\r\nA test case is added as a regression test.\r\n\r\n## Related issues\r\n\r\n<!-- Which issues are closed by this PR or are related -->\r\n\r\ncloses #15194\r\n\r\n<!-- Cut-off marker\r\n_All lines under and including the cut-off marker will be removed from the merge commit message_\r\n\r\n## Definition of Ready\r\n\r\nPlease check the items that apply, before requesting a review.\r\n\r\nYou can find more details about these items in our wiki page about [Pull Requests and Code Reviews](https://github.com/camunda/zeebe/wiki/Pull-Requests-and-Code-Reviews).\r\n\r\n* [ ] I've reviewed my own code\r\n* [ ] I've written a clear changelist description\r\n* [ ] I've narrowly scoped my changes\r\n* [ ] I've separated structural from behavioural changes\r\n-->\r\n\r\n## Definition of Done\r\n\r\n<!-- Please check the items that apply, before merging or (if possible) before requesting a review. -->\r\n\r\n_Not all items need to be done depending on the issue and the pull request._\r\n\r\nCode changes:\r\n* [x] The changes are backwards compatibility with previous versions\r\n* [x] If it fixes a bug then PRs are created to [backport](https://github.com/camunda/zeebe/compare/stable/0.24...main?expand=1&template=backport_template.md&title=[Backport%200.24]) the fix to the last two minor versions. You can trigger a backport by assigning labels (e.g. `backport stable/1.3`) to the PR, in case that fails you need to create backports manually.\r\n\r\nTesting:\r\n* [ ] There are unit/integration tests that verify all acceptance criterias of the issue\r\n* [x] New tests are written to ensure backwards compatibility with further versions\r\n* [ ] The behavior is tested manually\r\n* [ ] The change has been verified by a QA run\r\n* [ ] The impact of the changes is verified by a benchmark\r\n\r\nDocumentation:\r\n* [ ] The documentation is updated (e.g. BPMN reference, configuration, examples, get-started guides, etc.)\r\n* [ ] If the PR changes how BPMN processes are validated (e.g. support new BPMN element) then the Camunda modeling team should be informed to adjust the BPMN linting.\r\n\r\nOther teams:\r\nIf the change impacts another team an issue has been created for this team, explaining what they need to do to support this change.\r\n- [ ] [Operate](https://github.com/camunda/operate/issues)\r\n- [ ] [Tasklist](https://github.com/camunda/tasklist/issues)\r\n- [ ] [Web Modeler](https://github.com/camunda/web-modeler/issues)\r\n- [ ] [Desktop Modeler](https://github.com/camunda/camunda-modeler/issues)\r\n- [ ] [Optimize](https://github.com/camunda/camunda-optimize/issues)\r\n\r\nPlease refer to our [review guidelines](https://github.com/camunda/zeebe/wiki/Pull-Requests-and-Code-Reviews#code-review-guidelines).\r\n",
    "title": "Replay deployed forms"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/pull/14631",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Merged Pull Requests",
    "gitHubText": "## Description\r\nUpdating CI after 8.3 minor release\r\n\r\n<!-- Please explain the changes you made here. -->\r\n\r\n## Related issues\r\n\r\n<!-- Which issues are closed by this PR or are related -->\r\n\r\ncloses #\r\n\r\n<!-- Cut-off marker\r\n_All lines under and including the cut-off marker will be removed from the merge commit message_\r\n\r\n## Definition of Ready\r\n\r\nPlease check the items that apply, before requesting a review.\r\n\r\nYou can find more details about these items in our wiki page about [Pull Requests and Code Reviews](https://github.com/camunda/zeebe/wiki/Pull-Requests-and-Code-Reviews).\r\n\r\n* [ ] I've reviewed my own code\r\n* [ ] I've written a clear changelist description\r\n* [ ] I've narrowly scoped my changes\r\n* [ ] I've separated structural from behavioural changes\r\n-->\r\n\r\n## Definition of Done\r\n\r\n<!-- Please check the items that apply, before merging or (if possible) before requesting a review. -->\r\n\r\n_Not all items need to be done depending on the issue and the pull request._\r\n\r\nCode changes:\r\n* [ ] The changes are backwards compatibility with previous versions\r\n* [ ] If it fixes a bug then PRs are created to [backport](https://github.com/camunda/zeebe/compare/stable/0.24...main?expand=1&template=backport_template.md&title=[Backport%200.24]) the fix to the last two minor versions. You can trigger a backport by assigning labels (e.g. `backport stable/1.3`) to the PR, in case that fails you need to create backports manually.\r\n\r\nTesting:\r\n* [ ] There are unit/integration tests that verify all acceptance criterias of the issue\r\n* [ ] New tests are written to ensure backwards compatibility with further versions\r\n* [x] The behavior is tested manually\r\n* [ ] The change has been verified by a QA run\r\n* [ ] The impact of the changes is verified by a benchmark\r\n\r\nDocumentation:\r\n* [ ] The documentation is updated (e.g. BPMN reference, configuration, examples, get-started guides, etc.)\r\n* [ ] If the PR changes how BPMN processes are validated (e.g. support new BPMN element) then the Camunda modeling team should be informed to adjust the BPMN linting.\r\n\r\nOther teams:\r\nIf the change impacts another team an issue has been created for this team, explaining what they need to do to support this change.\r\n- [ ] [Operate](https://github.com/camunda/operate/issues)\r\n- [ ] [Tasklist](https://github.com/camunda/tasklist/issues)\r\n- [ ] [Web Modeler](https://github.com/camunda/web-modeler/issues)\r\n- [ ] [Desktop Modeler](https://github.com/camunda/camunda-modeler/issues)\r\n- [ ] [Optimize](https://github.com/camunda/camunda-optimize/issues)\r\n\r\nPlease refer to our [review guidelines](https://github.com/camunda/zeebe/wiki/Pull-Requests-and-Code-Reviews#code-review-guidelines).\r\n",
    "title": "Aa update ci after 8.3 minor release"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/pull/14217",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Merged Pull Requests",
    "gitHubText": "## Description\r\n\r\nEnsure commands stop on permanent errors, whatever the behavior of the credential provider.\r\n- permanent errors are considered non-retry-able\r\n- otherwise default to the retry predicate\r\n\r\n## Related issues\r\n\r\n#6168 \r\n\r\ncloses #6168 \r\n\r\n<!-- Cut-off marker\r\n_All lines under and including the cut-off marker will be removed from the merge commit message_\r\n\r\n## Definition of Ready\r\n\r\nPlease check the items that apply, before requesting a review.\r\n\r\nYou can find more details about these items in our wiki page about [Pull Requests and Code Reviews](https://github.com/camunda/zeebe/wiki/Pull-Requests-and-Code-Reviews).\r\n\r\n* [ ] I've reviewed my own code\r\n* [ ] I've written a clear changelist description\r\n* [ ] I've narrowly scoped my changes\r\n* [ ] I've separated structural from behavioural changes\r\n-->\r\n\r\n## Definition of Done\r\n\r\n<!-- Please check the items that apply, before merging or (if possible) before requesting a review. -->\r\n\r\n_Not all items need to be done depending on the issue and the pull request._\r\n\r\nCode changes:\r\n* [ ] The changes are backwards compatibility with previous versions\r\n* [ ] If it fixes a bug then PRs are created to [backport](https://github.com/camunda/zeebe/compare/stable/0.24...main?expand=1&template=backport_template.md&title=[Backport%200.24]) the fix to the last two minor versions. You can trigger a backport by assigning labels (e.g. `backport stable/1.3`) to the PR, in case that fails you need to create backports manually.\r\n\r\nTesting:\r\n* [ ] There are unit/integration tests that verify all acceptance criterias of the issue\r\n* [ ] New tests are written to ensure backwards compatibility with further versions\r\n* [ ] The behavior is tested manually\r\n* [ ] The change has been verified by a QA run\r\n* [ ] The impact of the changes is verified by a benchmark\r\n\r\nDocumentation:\r\n* [ ] The documentation is updated (e.g. BPMN reference, configuration, examples, get-started guides, etc.)\r\n* [ ] If the PR changes how BPMN processes are validated (e.g. support new BPMN element) then the Camunda modeling team should be informed to adjust the BPMN linting.\r\n\r\nOther teams:\r\nIf the change impacts another team an issue has been created for this team, explaining what they need to do to support this change.\r\n- [ ] [Operate](https://github.com/camunda/operate/issues)\r\n- [ ] [Tasklist](https://github.com/camunda/tasklist/issues)\r\n- [ ] [Web Modeler](https://github.com/camunda/web-modeler/issues)\r\n- [ ] [Desktop Modeler](https://github.com/camunda/camunda-modeler/issues)\r\n- [ ] [Optimize](https://github.com/camunda/camunda-optimize/issues)\r\n\r\nPlease refer to our [review guidelines](https://github.com/camunda/zeebe/wiki/Pull-Requests-and-Code-Reviews#code-review-guidelines).\r\n",
    "title": "fix(clients/go): client does not retry on permanent errors"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/pull/8203",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Merged Pull Requests",
    "gitHubText": "# Description\nBackport of #8194 to `stable/1.2`.\n\nrelates to ",
    "title": "[Backport stable/1.2] Fixes flakiness with the ElasticsearchExporter integration tests"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/operate/issues/6083",
    "component": "Operate",
    "subcomponent": "Misc",
    "context": "ðŸ’Š Bugfixes",
    "gitHubText": "\r\n## Description\r\n\r\nExtract IndexLifecycleManagementIT. Get rid of ArchiverITRepository\r\n\r\n## Related issues\r\n\r\n<!-- Which issues are closed by this PR or are related -->\r\n\r\ncloses #\r\n\r\n<!-- Cut-off marker\r\n_All lines under and including the cut-off marker will be removed from the merge commit message_\r\n\r\n## Definition of Ready\r\n\r\nPlease check the items that apply, before requesting a review.\r\n\r\nYou can find more details in our [wiki page] (https://github.com/camunda/zeebe/wiki/Pull-Requests-and-Code-Reviews).\r\n\r\n* [ ] I've reviewed my own code\r\n* [ ] I've written a clear changelist description\r\n* [ ] I've narrowly scoped my changes\r\n* [ ] I've separated structural from behavioural changes\r\n-->\r\n\r\n## Definition of Done\r\n\r\n<!-- As author please check the items that apply before requesting a review. -->\r\n\r\n_Not all items need to be done depending on the issue and the pull request._\r\n\r\nCode changes:\r\n* [ ] All acceptance criteria described in the issue are met\r\n* [ ] The changes are backwards compatibility with previous versions\r\n* [ ] If it fixes a bug then PRs are created to backport (https://github.com/korthout/backport-action#how-it-works) the fix to the last two minor versions. You can trigger a backport by assigning labels (e.g. `backport stable/8.2`) to the PR, in case that fails you need to create backports manually.\r\n\r\nTesting:\r\n* [ ] There are unit/integration tests that verify all acceptance criterias of the issue\r\n* [ ] New tests are written to ensure backwards compatibility with further versions\r\n* [ ] The behavior is tested manually\r\n* [ ] The change has been verified by a QA run\r\n* [ ] The impact of the changes is verified by a benchmark\r\n\r\nDocumentation:\r\n- [ ] If documentation needs to be updated, an issue is created in the [camunda-platform-docs](https://github.com/camunda/camunda-platform-docs) repo, and the issue is added to our Operate/Tasklist project board.\r\n\r\nOther teams:\r\nIf the change impacts another team an issue has been created for this team, explaining what they need to do to support this change.\r\n- [ ] [Zeebe](https://github.com/camunda/zeebe/issues)\r\n- [ ] [Tasklist](https://github.com/camunda/tasklist/issues)\r\n- [ ] [Optimize](https://github.com/camunda/camunda-optimize/issues)\r\n- [ ] Zeebe Play\r\n      \r\n## Definition of Reviewed\r\n\r\n<!-- As a reviewer please check the items that apply before approving this PR -->\r\n\r\n- [ ] All acceptance criteria described in the issue are met\r\n- [ ] Unit/integration tests are written, that verify all acceptance criteria of the issue\r\n- [ ] E2E tests are written, if the acceptance criteria can't be covered in unit/integration tests\r\n- [ ] The fix/feature is tested manually by the reviewer\r\n\r\nSome additional [review guidelines](https://github.com/camunda/zeebe/wiki/Pull-Requests-and-Code-Reviews#code-review-guidelines).\r\n\n",
    "title": "fix(backend): Extract IndexLifecycleManagementIT. Get rid of ArchiverITRepository"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/operate/issues/6078",
    "component": "Operate",
    "subcomponent": "Misc",
    "context": "ðŸ’Š Bugfixes",
    "gitHubText": "## Description\r\n\r\nRemove unused items from tests\r\n\r\n## Related issues\r\n\r\n<!-- Which issues are closed by this PR or are related -->\r\n\r\ncloses #\r\n\r\n<!-- Cut-off marker\r\n_All lines under and including the cut-off marker will be removed from the merge commit message_\r\n\r\n## Definition of Ready\r\n\r\nPlease check the items that apply, before requesting a review.\r\n\r\nYou can find more details in our [wiki page] (https://github.com/camunda/zeebe/wiki/Pull-Requests-and-Code-Reviews).\r\n\r\n* [ ] I've reviewed my own code\r\n* [ ] I've written a clear changelist description\r\n* [ ] I've narrowly scoped my changes\r\n* [ ] I've separated structural from behavioural changes\r\n-->\r\n\r\n## Definition of Done\r\n\r\n<!-- As author please check the items that apply before requesting a review. -->\r\n\r\n_Not all items need to be done depending on the issue and the pull request._\r\n\r\nCode changes:\r\n* [ ] All acceptance criteria described in the issue are met\r\n* [ ] The changes are backwards compatibility with previous versions\r\n* [ ] If it fixes a bug then PRs are created to backport (https://github.com/korthout/backport-action#how-it-works) the fix to the last two minor versions. You can trigger a backport by assigning labels (e.g. `backport stable/8.2`) to the PR, in case that fails you need to create backports manually.\r\n\r\nTesting:\r\n* [ ] There are unit/integration tests that verify all acceptance criterias of the issue\r\n* [ ] New tests are written to ensure backwards compatibility with further versions\r\n* [ ] The behavior is tested manually\r\n* [ ] The change has been verified by a QA run\r\n* [ ] The impact of the changes is verified by a benchmark\r\n\r\nDocumentation:\r\n- [ ] If documentation needs to be updated, an issue is created in the [camunda-platform-docs](https://github.com/camunda/camunda-platform-docs) repo, and the issue is added to our Operate/Tasklist project board.\r\n\r\nOther teams:\r\nIf the change impacts another team an issue has been created for this team, explaining what they need to do to support this change.\r\n- [ ] [Zeebe](https://github.com/camunda/zeebe/issues)\r\n- [ ] [Tasklist](https://github.com/camunda/tasklist/issues)\r\n- [ ] [Optimize](https://github.com/camunda/camunda-optimize/issues)\r\n- [ ] Zeebe Play\r\n      \r\n## Definition of Reviewed\r\n\r\n<!-- As a reviewer please check the items that apply before approving this PR -->\r\n\r\n- [ ] All acceptance criteria described in the issue are met\r\n- [ ] Unit/integration tests are written, that verify all acceptance criteria of the issue\r\n- [ ] E2E tests are written, if the acceptance criteria can't be covered in unit/integration tests\r\n- [ ] The fix/feature is tested manually by the reviewer\r\n\r\nSome additional [review guidelines](https://github.com/camunda/zeebe/wiki/Pull-Requests-and-Code-Reviews#code-review-guidelines).\r\n\n",
    "title": "fix(backend): Remove unused items"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/operate/issues/6065",
    "component": "Operate",
    "subcomponent": "Misc",
    "context": "ðŸ’Š Bugfixes",
    "gitHubText": "## Description\r\n\r\nAdd support for per index shards and replicas configuration.\r\n\r\n## Related issues\r\n\r\n<!-- Which issues are closed by this PR or are related -->\r\n\r\ncloses #\r\n\r\n<!-- Cut-off marker\r\n_All lines under and including the cut-off marker will be removed from the merge commit message_\r\n\r\n## Definition of Ready\r\n\r\nPlease check the items that apply, before requesting a review.\r\n\r\nYou can find more details in our [wiki page] (https://github.com/camunda/zeebe/wiki/Pull-Requests-and-Code-Reviews).\r\n\r\n* [ ] I've reviewed my own code\r\n* [ ] I've written a clear changelist description\r\n* [ ] I've narrowly scoped my changes\r\n* [ ] I've separated structural from behavioural changes\r\n-->\r\n\r\n## Definition of Done\r\n\r\n<!-- As author please check the items that apply before requesting a review. -->\r\n\r\n_Not all items need to be done depending on the issue and the pull request._\r\n\r\nCode changes:\r\n* [ ] All acceptance criteria described in the issue are met\r\n* [ ] The changes are backwards compatibility with previous versions\r\n* [ ] If it fixes a bug then PRs are created to backport (https://github.com/korthout/backport-action#how-it-works) the fix to the last two minor versions. You can trigger a backport by assigning labels (e.g. `backport stable/8.2`) to the PR, in case that fails you need to create backports manually.\r\n\r\nTesting:\r\n* [ ] There are unit/integration tests that verify all acceptance criterias of the issue\r\n* [ ] New tests are written to ensure backwards compatibility with further versions\r\n* [ ] The behavior is tested manually\r\n* [ ] The change has been verified by a QA run\r\n* [ ] The impact of the changes is verified by a benchmark\r\n\r\nDocumentation:\r\n- [ ] If documentation needs to be updated, an issue is created in the [camunda-platform-docs](https://github.com/camunda/camunda-platform-docs) repo, and the issue is added to our Operate/Tasklist project board.\r\n\r\nOther teams:\r\nIf the change impacts another team an issue has been created for this team, explaining what they need to do to support this change.\r\n- [ ] [Zeebe](https://github.com/camunda/zeebe/issues)\r\n- [ ] [Tasklist](https://github.com/camunda/tasklist/issues)\r\n- [ ] [Optimize](https://github.com/camunda/camunda-optimize/issues)\r\n- [ ] Zeebe Play\r\n      \r\n## Definition of Reviewed\r\n\r\n<!-- As a reviewer please check the items that apply before approving this PR -->\r\n\r\n- [ ] All acceptance criteria described in the issue are met\r\n- [ ] Unit/integration tests are written, that verify all acceptance criteria of the issue\r\n- [ ] E2E tests are written, if the acceptance criteria can't be covered in unit/integration tests\r\n- [ ] The fix/feature is tested manually by the reviewer\r\n\r\nSome additional [review guidelines](https://github.com/camunda/zeebe/wiki/Pull-Requests-and-Code-Reviews#code-review-guidelines).\r\n\n",
    "title": "fix(backend): Configure number of shards per index #5920"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/operate/issues/6030",
    "component": "Operate",
    "subcomponent": "Misc",
    "context": "ðŸ’Š Bugfixes",
    "gitHubText": "\r\n\r\n## Description\r\n\r\nGet rid of CamundaPatchedSearchRequest and replace it with a Map-based model.\r\n\r\n## Related issues\r\n\r\n<!-- Which issues are closed by this PR or are related -->\r\n\r\ncloses #\r\n\r\n<!-- Cut-off marker\r\n_All lines under and including the cut-off marker will be removed from the merge commit message_\r\n\r\n## Definition of Ready\r\n\r\nPlease check the items that apply, before requesting a review.\r\n\r\nYou can find more details in our [wiki page] (https://github.com/camunda/zeebe/wiki/Pull-Requests-and-Code-Reviews).\r\n\r\n* [ ] I've reviewed my own code\r\n* [ ] I've written a clear changelist description\r\n* [ ] I've narrowly scoped my changes\r\n* [ ] I've separated structural from behavioural changes\r\n-->\r\n\r\n## Definition of Done\r\n\r\n<!-- As author please check the items that apply before requesting a review. -->\r\n\r\n_Not all items need to be done depending on the issue and the pull request._\r\n\r\nCode changes:\r\n* [ ] All acceptance criteria described in the issue are met\r\n* [ ] The changes are backwards compatibility with previous versions\r\n* [ ] If it fixes a bug then PRs are created to backport (https://github.com/korthout/backport-action#how-it-works) the fix to the last two minor versions. You can trigger a backport by assigning labels (e.g. `backport stable/8.2`) to the PR, in case that fails you need to create backports manually.\r\n\r\nTesting:\r\n* [ ] There are unit/integration tests that verify all acceptance criterias of the issue\r\n* [ ] New tests are written to ensure backwards compatibility with further versions\r\n* [ ] The behavior is tested manually\r\n* [ ] The change has been verified by a QA run\r\n* [ ] The impact of the changes is verified by a benchmark\r\n\r\nDocumentation:\r\n- [ ] If documentation needs to be updated, an issue is created in the [camunda-platform-docs](https://github.com/camunda/camunda-platform-docs) repo, and the issue is added to our Operate/Tasklist project board.\r\n\r\nOther teams:\r\nIf the change impacts another team an issue has been created for this team, explaining what they need to do to support this change.\r\n- [ ] [Zeebe](https://github.com/camunda/zeebe/issues)\r\n- [ ] [Tasklist](https://github.com/camunda/tasklist/issues)\r\n- [ ] [Optimize](https://github.com/camunda/camunda-optimize/issues)\r\n- [ ] Zeebe Play\r\n      \r\n## Definition of Reviewed\r\n\r\n<!-- As a reviewer please check the items that apply before approving this PR -->\r\n\r\n- [ ] All acceptance criteria described in the issue are met\r\n- [ ] Unit/integration tests are written, that verify all acceptance criteria of the issue\r\n- [ ] E2E tests are written, if the acceptance criteria can't be covered in unit/integration tests\r\n- [ ] The fix/feature is tested manually by the reviewer\r\n\r\nSome additional [review guidelines](https://github.com/camunda/zeebe/wiki/Pull-Requests-and-Code-Reviews#code-review-guidelines).\r\n\n",
    "title": "fix(backend): Workaround for search_after issue in OS java client (#5788)"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2412",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "ðŸš€ New Features",
    "gitHubText": "### Related issue(s)\r\n<!-- Changes should have a linked issue for traceability, use closing words such as \"Closes\" or \"Fixes\" to automatically handle related issue status. -->\r\nCloses #2409 \r\n\r\n### Description\r\n<!-- Consider suggesting why this change is being made, for example has an issue been spotted? Have we found a more performant way to solve the problem? -->\r\nThis PR introduces support for making the root url field on our KeycloakClient object able to be treated as a comma separated value field.\r\n\r\n### Is there anything else to consider?\r\n<!-- As a suggestion, were any alternatives considered? Is there any particular decision you'd like to highlight? -->\r\n\r\nWe could implement a new field for this but after discussing with @dlavrenuek this is the simplest approach that gets us where we need to be for 8.4 .\r\n\r\n### Acceptance criteria\r\n- [x] Code changes are covered by tests\r\n- [x] Design review requested or not required\r\n\n",
    "title": "feat: allow multiple root urls"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2406",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "ðŸš€ New Features",
    "gitHubText": "### Related issue(s)\r\nFixes #2329 \r\n\r\n### Description\r\nIdentity includes predefined client id definitions for all Camunda applications. There are use cases where users might want to use different client ids. The solution is to add the variable mapping the same way secrets are handled.\r\n\r\n### Acceptance criteria\r\n- [x] Code changes are covered by tests\r\n- [x] I can define a custom client ID through env variables for\r\n   - [x] Operate\r\n   - [x] Optimize\r\n   - [x] Tasklist\r\n   - [x] Connectors\r\n   - [x] Console\r\n   - [x] Web-Modeler\r\n   - [ ] New configuration variables are documented\r\n\n",
    "title": "feat: support custom client ids for camunda application"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2380",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "ðŸš€ New Features",
    "gitHubText": "### Related issue(s)\r\n<!-- Changes should have a linked issue for traceability, use closing words such as \"Closes\" or \"Fixes\" to automatically handle related issue status. -->\r\nFixes #2323 \r\n\r\n### Description\r\nThis PR is adding a new /ping endpoint that returns 2xx status and is accessible without auth for the sole purpose of checking the uptime. \r\n\r\n### Acceptance criteria\r\n- [x] Code changes are covered by tests\r\n- [x] Design review requested or not required\r\n\n\n maryarm: > Thank you for adding this, I assume that the built in Spring endpoint introduced undesirable changes?\r\n\r\nI've tried to configure health details and components to \"when_authorized\", so health provides enough details when user is authenticated and otherwise just return a status UP or DOWN, but it has two drawbacks: first one is we needed some changes in JwtFilters to handle, but my biggest concern was if we accidentally change the setting in yaml and expose to much data. so I followed the suggestion of the /ping endpoint as a light and working solution. ",
    "title": "feat: add /ping endpoint for uptime check"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2379",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "ðŸš€ New Features",
    "gitHubText": "### Related issue(s)\r\n<!-- Changes should have a linked issue for traceability, use closing words such as \"Closes\" or \"Fixes\" to automatically handle related issue status. -->\r\nRelated to #2341\r\n\r\n### Description\r\nAdds a helper method to indicate if authorization through identity is available or not. This will be used in Connectors to determine if Connectors was configured with the new Identity configuration variables and can be used instead of the built in authentication handling in Operate SDK\r\n\r\n### Is there anything else to consider?\r\n<!-- As a suggestion, were any alternatives considered? Is there any particular decision you'd like to highlight? -->\r\n\r\n### Acceptance criteria\r\n- [x] Code changes are covered by tests\r\n- [x] Design review requested or not required\r\n\n",
    "title": "feat: provider helper in sdk for authorization available"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2356",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "ðŸš€ New Features",
    "gitHubText": "### Related issue(s)\r\nFixesÂ [#2304](https://github.com/camunda-cloud/identity/issues/2304)\r\n\r\n### Description\r\nOptimize is currently implementing collectionÂ [camunda/product-hub#1322](https://github.com/camunda/product-hub/issues/1322)Â for which it will use identity's user search. Given the scope ofÂ [camunda/product-hub#739 (comment)](https://github.com/camunda/product-hub/issues/739#issuecomment-1674383817), user search may not be available in identity depending on identity configuration.â€¨Optimize needs a way to determine whether the user search is available so that we can hide the related parts in the Optimize UI.\r\n\r\n### Is there anything else to consider?\r\nBeside the identity configuration type, I'm also checking for null or empty baseUrl, but not sure if it's something could happen in prod (it's already checked with @RequiresBaseUrl over search method) or if this check is some how in scope of the issue.\r\n\r\n\n",
    "title": "feat: add a helper to determine identity user search availability"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2327",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "ðŸš€ New Features",
    "gitHubText": "### Related issue(s)\r\n<!-- Changes should have a linked issue for traceability, use closing words such as \"Closes\" or \"Fixes\" to automatically handle related issue status. -->\r\nCloses #2283, Closes #2284\r\n\r\n### Description\r\n<!-- Consider suggesting why this change is being made, for example has an issue been spotted? Have we found a more performant way to solve the problem? -->\r\n\r\nThis PR is an attempt to solve a few linked tasks. These are:\r\n\r\n1. Introduce the `GENERIC` auth type to allow connections that are not currently covered by our platform specific implementations \r\n2. Refactor the Keycloak and AzureAD implementations to use the generic implementation and overwrite where necessary to reduce duplication\r\n3. Implement the well-known configuration retrieval to allow the generic profile to be supported \r\n\r\n### Is there anything else to consider?\r\n<!-- As a suggestion, were any alternatives considered? Is there any particular decision you'd like to highlight? -->\r\n\r\n\r\n### Acceptance criteria\r\n- [x] Code changes are covered by tests\r\n- [x] Design review requested or not required\r\n\n",
    "title": "feat: implement generic auth proile"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2303",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "ðŸš€ New Features",
    "gitHubText": "### Related issue(s)\r\n<!-- Changes should have a linked issue for traceability, use closing words such as \"Closes\" or \"Fixes\" to automatically handle related issue status. -->\r\nCloses #2166 \r\n\r\n### Description\r\n<!-- Consider suggesting why this change is being made, for example has an issue been spotted? Have we found a more performant way to solve the problem? -->\r\n\r\nTo support a feature in our self manage offering for Tasklist, we need to expose the users assigned groups as part of the user details linked to a token. This PR adds a new mapper and also exposes this data as described in the parent issue.\r\n\r\n### Acceptance criteria\r\n- [x] Code changes are covered by tests\r\n- [x] Design review requested or not required\r\n\n",
    "title": "feat: expose user groups via SDK"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2296",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "ðŸš€ New Features",
    "gitHubText": "### Related issue(s)\r\n<!-- Changes should have a linked issue for traceability, use closing words such as \"Closes\" or \"Fixes\" to automatically handle related issue status. -->\r\nCloses #2278 \r\n\r\n### Description\r\n<!-- Consider suggesting why this change is being made, for example has an issue been spotted? Have we found a more performant way to solve the problem? -->\r\n\r\nThis PR adds the `read:users` permission to the Oprimize role.\r\n\n",
    "title": "feat: add read:users permission to the Optimize role"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2273",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "ðŸš€ New Features",
    "gitHubText": "### Related issue(s)\r\n<!-- Changes should have a linked issue for traceability, use closing words such as \"Closes\" or \"Fixes\" to automatically handle related issue status. -->\r\nFixes #2173 \r\n\r\n### Description\r\n<!-- Consider suggesting why this change is being made, for example has an issue been spotted? Have we found a more performant way to solve the problem? -->\r\n\r\nAs described in the parent issue, this PR contains the initial changes required for the Azure AD implementation\r\n\r\n### Is there anything else to consider?\r\n<!-- As a suggestion, were any alternatives considered? Is there any particular decision you'd like to highlight? -->\r\n\r\nAs we have limited time for this first change I have just opted to create a new implementation that duplicates some code. In further iterations I would like to ideally create a \"generic\" set of classes and then have both Keycloak and Azure AD implementations extend from that. This way, in instances where possible we can cover authentication paths but where something more specific is needed we can extend and handle it as necessary.\r\n\r\n### Acceptance criteria\r\n- [x] Code changes are covered by tests\r\n- [x] Design review requested or not required\r\n\n\n dlavrenuek: The `AbstractAuthentication` class has functionality to build `UserDetails`. There the `preferred_username` claim is used, however the AD access token does not seem to include it. Ideally we would have the claim name configurable",
    "title": "feat: implement Azure AD sdk profile"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2266",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "ðŸš€ New Features",
    "gitHubText": "### Related issue(s)\r\n<!-- Changes should have a linked issue for traceability, use closing words such as \"Closes\" or \"Fixes\" to automatically handle related issue status. -->\r\nFixes #2121 \r\n\r\n### Description\r\nThis PR adds code changes needed to connect to a [AWS Aurora database](https://aws.amazon.com/rds/aurora/?nc1=h_ls) using the [aws-advanced-jdbc-wrapper](https://github.com/awslabs/aws-advanced-jdbc-wrapper/tree/main) with [IAM plugin](https://github.com/awslabs/aws-advanced-jdbc-wrapper/blob/main/docs/using-the-jdbc-driver/using-plugins/UsingTheIamAuthenticationPlugin.md#how-do-i-use-iam-with-the-aws-advanced-jdbc-driver).\r\n\r\nThe solution is highly inspired by [the work done by the Web Modeler team](https://github.com/camunda/web-modeler/pull/4455/files).\r\n\r\n### Is there anything else to consider?\r\nI have tried running e2e tests with Aurora which turned out to be a bigger challenge because the AWS runners have certificates installed and specific env variables set to allow ssl communication with the Aurora database using IAM. I did not manage to successfully inject these into the Identity docker container.\r\n\r\n### Acceptance criteria\r\n- [x] Code changes are covered by tests\r\n- [x] Design review requested or not required\r\n\n\n dlavrenuek: You can see in [this run](https://github.com/camunda-cloud/identity/actions/runs/6695114101/job/18191414913) that the tests are executed successfully\n Ben-Sheppard: Seems like there are some test failures here, one being\r\n```\r\nCould not resolve placeholder 'spring.datasource.driver-class-name' in value \"${spring.datasource.driver-class-name}\"\r\n```\r\nShould that value be `${spring.datasource.driver-class-name:}` so there is a default value?\n dlavrenuek: Snyk has found a new Medium level vulnerability [CVE-2023-4586](https://www.cve.org/CVERecord?id=CVE-2023-4586) introduced by the new aws libraries: https://app.snyk.io/org/team-identity/project/a21f177d-2c09-4f89-96c2-4ef0236673c9/pr-check/ee1c8eea-43dc-41d0-af91-90417b7c1060\r\n\r\n> Affected versions of this package are vulnerable to Improper Certificate Validation. Certificate hostname validation is disabled by default in Netty 4.1.x which makes it potentially susceptible to Man-in-the-Middle attacks.\r\n\r\nAccording to this issue in Amazons' repository, certificate verification is enabled manually in the code, which makes the usage of the aws libraries not affected by this CVE\r\nhttps://github.com/aws/aws-sdk-java-v2/issues/4584#issuecomment-1760547020",
    "title": "feat: add support for AWS Aurora database"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2184",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "ðŸš€ New Features",
    "gitHubText": "### Related issue(s)\r\n<!-- Changes should have a linked issue for traceability, use closing words such as \"Closes\" or \"Fixes\" to automatically handle related issue status. -->\r\nCloses #2174 \r\n\r\n### Description\r\n<!-- Consider suggesting why this change is being made, for example has an issue been spotted? Have we found a more performant way to solve the problem? -->\r\n\r\nThis PR introduces the foundation of the new Identity Spring Boot Starter! \r\n\r\n### Is there anything else to consider?\r\n<!-- As a suggestion, were any alternatives considered? Is there any particular decision you'd like to highlight? -->\r\n\r\nThere is a change in the variable prefix I've used, I wanted to step more inline with the other components so opted for the prefix of `camunda.identity` ([REF](https://github.com/camunda-cloud/identity/compare/2174-implement-identity-spring-boot-starter?expand=1#diff-7eb4bddbc7ab3d7e3b4d562b4fc1079bb67fc2fd5718c3000270400a636518fcR12)). This means that an environment variable approach would result in variables like `CAMUNDA_IDENTITY_BASEURL` which IMO is a nicer experience and more understandable with the additional context.\r\n\r\n\n",
    "title": "feat: implement identity spring boot starter"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2148",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "ðŸš€ New Features",
    "gitHubText": "### Related issue(s)\r\n<!-- Changes should have a linked issue for traceability, use closing words such as \"Closes\" or \"Fixes\" to automatically handle related issue status. -->\r\nN/A\r\n\r\n### Description\r\n<!-- Consider suggesting why this change is being made, for example has an issue been spotted? Have we found a more performant way to solve the problem? -->\r\nThis change makes it so the implementation info is available inside MANIFEST.MF of the jar. This makes it easy to pull the Identity version that's being used in other project that depend on this SDK during runtime.\r\n\r\nI made this change to improve the E2E testing in Zeebe. We spin up a container of the Identity SNAPSHOT. This ivery fault-sensitive as the SNAPSHOT could change at any time. It also becomes an issue when supporting older versions, as the old SDK might not align with the SNAPSHOT version. By adding this information we can easily get the version that we depend on during runtime and use this for Docker image in our E2E tests.\r\n\r\n### Is there anything else to consider?\r\n<!-- As a suggestion, were any alternatives considered? Is there any particular decision you'd like to highlight? -->\r\n\r\n### Acceptance criteria\r\n- [ ] Code changes are covered by tests\r\n- [ ] Design review requested or not required\r\n\n\n remcowesterhoud: @Ben-Sheppard It seems the CI is timing out. Any ideas how to get this through? I'm convinced it's not because of my change ðŸ˜„ \n Ben-Sheppard: I'm also convinced its not your change @remcowesterhoud :D - usually rerunning does the trick, I'll keep an eye though\n Infra-connect: Successfully created backport PR for `v8.3`:\n- #2169",
    "title": "feat: add implementation info to SDK jar"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2161",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "ðŸš€ New Features",
    "gitHubText": "### Related issue(s)\r\n<!-- Changes should have a linked issue for traceability, use closing words such as \"Closes\" or \"Fixes\" to automatically handle related issue status. -->\r\nCloses #2159 \r\n\r\n### Description\r\n<!-- Consider suggesting why this change is being made, for example has an issue been spotted? Have we found a more performant way to solve the problem? -->\r\n\r\nTo better support our internal teams and customers with automations, this PR introduces initializer updates to allow not only the creation of tenants, but also the ability to assign members to these tenants via configuration.\r\n\r\n### Is there anything else to consider?\r\n<!-- As a suggestion, were any alternatives considered? Is there any particular decision you'd like to highlight? -->\r\n\r\n- The `<default>` tenant is something that should exist in the environment regardless of if other tenants are configured, in the existing structure the default tenant could have been overwritten and removed leading to an inconsistent state. I decided that moving the `<default>` tenant definition to a new section (`identity.environment`) protects against this somewhat and also denotes the importance of this tenant in the environment. I'm happy to discuss this approach though.\r\n\r\n### Acceptance criteria\r\n- [x] Code changes are covered by tests\r\n- [x] Design review requested or not required\r\n\n\n Infra-connect: Successfully created backport PR for `v8.3`:\n- #2170",
    "title": "feat: support initializing access rules by config"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2109",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "ðŸš€ New Features",
    "gitHubText": "### Related issue(s)\r\n<!-- Changes should have a linked issue for traceability, use closing words such as \"Closes\" or \"Fixes\" to automatically handle related issue status. -->\r\nCloses #1973 \r\n\r\n### Description\r\n<!-- Consider suggesting why this change is being made, for example has an issue been spotted? Have we found a more performant way to solve the problem? -->\r\n\r\nThis is described much better in the linked issue however this PR is the first step in enabling users with specific permissions to access the authorization data for an organization.\r\n\r\n### Is there anything else to consider?\r\n<!-- As a suggestion, were any alternatives considered? Is there any particular decision you'd like to highlight? -->\r\n\r\n* I have made some tweaks to the SDK, the `permissions` claim is found on JWTs from Auth0 but it was being skipped in the SDK due to audience reasons. I couldn't think of another way around this, additionally I have updated the Keycloak implementation of the `getPermissions` method to return an empty list vs a NPE on an invalid token structure, semantically the result is the same IMO.\r\n* In the context of retrieving all authorizations in an installation/organization it made sense to have the entity ID and type that the authorization relates to, I decided that instead of creating yet another representation to display the same data in a slightly different way, that its just easiest to add it to the existing responses.\r\n\r\n### Acceptance criteria\r\n- [x] Code changes are covered by tests\r\n- [x] Design review requested or not required\r\n\n",
    "title": "feat: implement GET /authorizations endpoint"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2343",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "ðŸ’Š Bugfixes",
    "gitHubText": "### Related issue(s)\r\n<!-- Changes should have a linked issue for traceability, use closing words such as \"Closes\" or \"Fixes\" to automatically handle related issue status. -->\r\nFixes #1929\r\n\r\n### Description\r\nFixes styling of the resource permissions checkboxes in Identity.\r\n\r\n<img width=\"742\" alt=\"Bildschirmfoto 2023-11-21 um 15 02 39\" src=\"https://github.com/camunda-cloud/identity/assets/20122620/9d9fbaf8-a4dc-48b1-ba02-d8f38d82a09d\">\r\n\r\n### Is there anything else to consider?\r\nThis should be tested manually, since no automatic tests for that behavior are available (at least none that would justify time investment)\r\n\r\n### Acceptance criteria\r\n- [ ] Code changes are covered by tests\r\n- [x] Design review requested or not required\r\n\n",
    "title": "fix: fix resource authorizations checkbox style"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2381",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "ðŸ’Š Bugfixes",
    "gitHubText": "<h3>Snyk has created this PR to fix one or more vulnerable packages in the `maven` dependencies of this project.</h3>\n\n\nAs this is a private repository, Snyk-bot does not have access. Therefore, this PR has been created automatically, but appears to have been created by a real user.\n\n#### Changes included in this PR\n\n- Changes to the following files to upgrade the vulnerable dependencies to a fixed version:\n    - management-api/pom.xml\n\n\n\n#### Vulnerabilities that will be fixed\n##### With an upgrade:\nSeverity                   | Priority Score (*)                   | Issue                   | Upgrade                   | Breaking Change                   | Exploit Maturity\n:-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------\n![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png \"medium severity\")  |  **551/1000**  <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.3  | Denial of Service (DoS) <br/>[SNYK-JAVA-ORGSPRINGFRAMEWORK-6091650](https://snyk.io/vuln/SNYK-JAVA-ORGSPRINGFRAMEWORK-6091650) |  `org.springframework.boot:spring-boot-starter-security:` <br> `3.1.5 -> 3.2.0` <br>  `org.springframework.boot:spring-boot-starter-web:` <br> `3.1.5 -> 3.1.6` <br>  |  No  | No Known Exploit \n\n(*) Note that the real score may have changed since the PR was raised.\n\n\n\n\n\n\n\n\n\n\n\nCheck the changes in this PR to ensure they won't cause issues with your project.\n\n\n\n------------\n\n\n\n**Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*\n\nFor more information:  <img src=\"https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIyMWZmZDNiNS1jMmViLTQ2NTctYmQ2NC0xZDBmZmY3ZDFjZTUiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjIxZmZkM2I1LWMyZWItNDY1Ny1iZDY0LTFkMGZmZjdkMWNlNSJ9fQ==\" width=\"0\" height=\"0\"/>\nðŸ§ [View latest project report](https://app.snyk.io/org/team-identity/project/afb108df-387e-4754-9fc5-4461836ac41c?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr)\n\nðŸ›  [Adjust project settings](https://app.snyk.io/org/team-identity/project/afb108df-387e-4754-9fc5-4461836ac41c?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings)\n\nðŸ“š [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities)\n\n[//]: # (snyk:metadata:{\"prId\":\"21ffd3b5-c2eb-4657-bd64-1d0fff7d1ce5\",\"prPublicId\":\"21ffd3b5-c2eb-4657-bd64-1d0fff7d1ce5\",\"dependencies\":[{\"name\":\"org.springframework.boot:spring-boot-starter-security\",\"from\":\"3.1.5\",\"to\":\"3.2.0\"},{\"name\":\"org.springframework.boot:spring-boot-starter-web\",\"from\":\"3.1.5\",\"to\":\"3.1.6\"}],\"packageManager\":\"maven\",\"projectPublicId\":\"afb108df-387e-4754-9fc5-4461836ac41c\",\"projectUrl\":\"https://app.snyk.io/org/team-identity/project/afb108df-387e-4754-9fc5-4461836ac41c?utm_source=github&utm_medium=referral&page=fix-pr\",\"type\":\"auto\",\"patch\":[],\"vulns\":[\"SNYK-JAVA-ORGSPRINGFRAMEWORK-6091650\"],\"upgrade\":[\"SNYK-JAVA-ORGSPRINGFRAMEWORK-6091650\"],\"isBreakingChange\":false,\"env\":\"prod\",\"prType\":\"fix\",\"templateVariants\":[\"priorityScore\"],\"priorityScoreList\":[551],\"remediationStrategy\":\"vuln\"})\n\n---\n\n**Learn how to fix vulnerabilities with free interactive lessons:**\n\n ðŸ¦‰ [Denial of Service (DoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr)\n\n\n Infra-connect: Backport failed for `v8.3`, because it was unable to cherry-pick the commit(s).\n\nPlease cherry-pick the changes locally and resolve any conflicts.\n```bash\ngit fetch origin v8.3\ngit worktree add -d .worktree/backport-2381-to-v8.3 origin/v8.3\ncd .worktree/backport-2381-to-v8.3\ngit switch --create backport-2381-to-v8.3\ngit cherry-pick -x 3217f618b3c352b76a5eb67270acc8867bfc3b52\n```",
    "title": "fix: upgrade org.springframework.boot:spring-boot-starter-security from 3.1.5 to 3.2.0"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2353",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "ðŸ’Š Bugfixes",
    "gitHubText": "[![Mend Renovate logo banner](https://app.renovatebot.com/images/banner.svg)](https://renovatebot.com)\n\nThis PR contains the following updates:\n\n| Package | Change | Age | Adoption | Passing | Confidence |\n|---|---|---|---|---|---|\n| [software.amazon.awssdk:sts](https://aws.amazon.com/sdkforjava) | `2.21.28` -> `2.21.29` | [![age](https://developer.mend.io/api/mc/badges/age/maven/software.amazon.awssdk:sts/2.21.29?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/software.amazon.awssdk:sts/2.21.29?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/software.amazon.awssdk:sts/2.21.28/2.21.29?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/software.amazon.awssdk:sts/2.21.28/2.21.29?slim=true)](https://docs.renovatebot.com/merge-confidence/) |\n| [software.amazon.awssdk:rds](https://aws.amazon.com/sdkforjava) | `2.21.28` -> `2.21.29` | [![age](https://developer.mend.io/api/mc/badges/age/maven/software.amazon.awssdk:rds/2.21.29?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/software.amazon.awssdk:rds/2.21.29?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/software.amazon.awssdk:rds/2.21.28/2.21.29?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/software.amazon.awssdk:rds/2.21.28/2.21.29?slim=true)](https://docs.renovatebot.com/merge-confidence/) |\n\n---\n\n> [!WARNING]\n> Some dependencies could not be looked up. Check the Dependency Dashboard for more information.\n\n---\n\n### Configuration\n\nðŸ“… **Schedule**: Branch creation - \"after 10pm every weekday,before 5am every weekday,every weekend\" in timezone Europe/Berlin, Automerge - At any time (no schedule defined).\n\nðŸš¦ **Automerge**: Enabled.\n\nâ™» **Rebasing**: Whenever PR becomes conflicted, or you tick the rebase/retry checkbox.\n\nðŸ”• **Ignore**: Close this PR and you won't be reminded about these updates again.\n\n---\n\n - [ ] <!-- rebase-check -->If you want to rebase/retry this PR, check this box\n\n---\n\nThis PR has been generated by [Mend Renovate](https://www.mend.io/free-developer-tools/renovate/). View repository job log [here](https://developer.mend.io/github/camunda-cloud/identity).\n<!--renovate-debug:eyJjcmVhdGVkSW5WZXIiOiIzNy41OS44IiwidXBkYXRlZEluVmVyIjoiMzcuNTkuOCIsInRhcmdldEJyYW5jaCI6Im1haW4ifQ==-->\n\n",
    "title": "fix: update aws-java-sdk-v2 monorepo to v2.21.29 (main)"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2350",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "ðŸ’Š Bugfixes",
    "gitHubText": "[![Mend Renovate logo banner](https://app.renovatebot.com/images/banner.svg)](https://renovatebot.com)\n\nThis PR contains the following updates:\n\n| Package | Change | Age | Adoption | Passing | Confidence |\n|---|---|---|---|---|---|\n| [software.amazon.awssdk:sts](https://aws.amazon.com/sdkforjava) | `2.21.26` -> `2.21.29` | [![age](https://developer.mend.io/api/mc/badges/age/maven/software.amazon.awssdk:sts/2.21.29?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/software.amazon.awssdk:sts/2.21.29?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/software.amazon.awssdk:sts/2.21.26/2.21.29?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/software.amazon.awssdk:sts/2.21.26/2.21.29?slim=true)](https://docs.renovatebot.com/merge-confidence/) |\n| [software.amazon.awssdk:rds](https://aws.amazon.com/sdkforjava) | `2.21.26` -> `2.21.29` | [![age](https://developer.mend.io/api/mc/badges/age/maven/software.amazon.awssdk:rds/2.21.29?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/software.amazon.awssdk:rds/2.21.29?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/software.amazon.awssdk:rds/2.21.26/2.21.29?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/software.amazon.awssdk:rds/2.21.26/2.21.29?slim=true)](https://docs.renovatebot.com/merge-confidence/) |\n| [software.amazon.awssdk:bom](https://aws.amazon.com/sdkforjava) | `2.21.26` -> `2.21.28` | [![age](https://developer.mend.io/api/mc/badges/age/maven/software.amazon.awssdk:bom/2.21.28?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/software.amazon.awssdk:bom/2.21.28?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/software.amazon.awssdk:bom/2.21.26/2.21.28?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/software.amazon.awssdk:bom/2.21.26/2.21.28?slim=true)](https://docs.renovatebot.com/merge-confidence/) |\n\n---\n\n> [!WARNING]\n> Some dependencies could not be looked up. Check the Dependency Dashboard for more information.\n\n---\n\n### Configuration\n\nðŸ“… **Schedule**: Branch creation - \"after 10pm every weekday,before 5am every weekday,every weekend\" in timezone Europe/Berlin, Automerge - At any time (no schedule defined).\n\nðŸš¦ **Automerge**: Enabled.\n\nâ™» **Rebasing**: Whenever PR becomes conflicted, or you tick the rebase/retry checkbox.\n\nðŸ‘» **Immortal**: This PR will be recreated if closed unmerged. Get [config help](https://togithub.com/renovatebot/renovate/discussions) if that's undesired.\n\n---\n\n - [ ] <!-- rebase-check -->If you want to rebase/retry this PR, check this box\n\n---\n\nThis PR has been generated by [Mend Renovate](https://www.mend.io/free-developer-tools/renovate/). View repository job log [here](https://developer.mend.io/github/camunda-cloud/identity).\n<!--renovate-debug:eyJjcmVhdGVkSW5WZXIiOiIzNy41OS44IiwidXBkYXRlZEluVmVyIjoiMzcuNTkuOCIsInRhcmdldEJyYW5jaCI6Im1haW4ifQ==-->\n\n",
    "title": "fix: update aws-java-sdk-v2 monorepo (main)"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2332",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "ðŸ’Š Bugfixes",
    "gitHubText": "<h3>Snyk has created this PR to fix one or more vulnerable packages in the `maven` dependencies of this project.</h3>\n\n\nAs this is a private repository, Snyk-bot does not have access. Therefore, this PR has been created automatically, but appears to have been created by a real user.\n\n#### Changes included in this PR\n\n- Changes to the following files to upgrade the vulnerable dependencies to a fixed version:\n    - management-api/pom.xml\n\n\n\n#### Vulnerabilities that will be fixed\n##### With an upgrade:\nSeverity                   | Priority Score (*)                   | Issue                   | Upgrade                   | Breaking Change                   | Exploit Maturity\n:-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------\n![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png \"medium severity\")  |  **490/1000**  <br/> **Why?** Has a fix available, CVSS 5.3  | Improper Certificate Validation <br/>[SNYK-JAVA-IONETTY-1042268](https://snyk.io/vuln/SNYK-JAVA-IONETTY-1042268) |  `software.amazon.awssdk:rds:` <br> `2.21.21 -> 2.21.26` <br>  |  No  | No Known Exploit \n\n(*) Note that the real score may have changed since the PR was raised.\n\n\n\n\n\n\n\n\n\n\n\nCheck the changes in this PR to ensure they won't cause issues with your project.\n\n\n\n------------\n\n\n\n**Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*\n\nFor more information:  <img src=\"https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI2YjQyYmMzYy04ZmIyLTRiMDgtYmFlZS1iNTM0MzE4N2I3OWQiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjZiNDJiYzNjLThmYjItNGIwOC1iYWVlLWI1MzQzMTg3Yjc5ZCJ9fQ==\" width=\"0\" height=\"0\"/>\nðŸ§ [View latest project report](https://app.snyk.io/org/team-identity/project/afb108df-387e-4754-9fc5-4461836ac41c?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr)\n\nðŸ›  [Adjust project settings](https://app.snyk.io/org/team-identity/project/afb108df-387e-4754-9fc5-4461836ac41c?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings)\n\nðŸ“š [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities)\n\n[//]: # (snyk:metadata:{\"prId\":\"6b42bc3c-8fb2-4b08-baee-b5343187b79d\",\"prPublicId\":\"6b42bc3c-8fb2-4b08-baee-b5343187b79d\",\"dependencies\":[{\"name\":\"software.amazon.awssdk:rds\",\"from\":\"2.21.21\",\"to\":\"2.21.26\"}],\"packageManager\":\"maven\",\"projectPublicId\":\"afb108df-387e-4754-9fc5-4461836ac41c\",\"projectUrl\":\"https://app.snyk.io/org/team-identity/project/afb108df-387e-4754-9fc5-4461836ac41c?utm_source=github&utm_medium=referral&page=fix-pr\",\"type\":\"auto\",\"patch\":[],\"vulns\":[\"SNYK-JAVA-IONETTY-1042268\"],\"upgrade\":[\"SNYK-JAVA-IONETTY-1042268\"],\"isBreakingChange\":false,\"env\":\"prod\",\"prType\":\"fix\",\"templateVariants\":[\"updated-fix-title\",\"priorityScore\"],\"priorityScoreList\":[490],\"remediationStrategy\":\"vuln\"})\n\n---\n\n**Learn how to fix vulnerabilities with free interactive lessons:**\n\n ðŸ¦‰ [Learn about vulnerability in an interactive lesson of Snyk Learn.](https://learn.snyk.io/?loc&#x3D;fix-pr)\n\n",
    "title": "fix: upgrade software.amazon.awssdk:rds from 2.21.21 to 2.21.26"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2312",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "ðŸ’Š Bugfixes",
    "gitHubText": "[![Mend Renovate](https://app.renovatebot.com/images/banner.svg)](https://renovatebot.com)\n\nThis PR contains the following updates:\n\n| Package | Change | Age | Adoption | Passing | Confidence |\n|---|---|---|---|---|---|\n| [software.amazon.awssdk:sts](https://aws.amazon.com/sdkforjava) | `2.21.20` -> `2.21.21` | [![age](https://developer.mend.io/api/mc/badges/age/maven/software.amazon.awssdk:sts/2.21.21?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/software.amazon.awssdk:sts/2.21.21?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/software.amazon.awssdk:sts/2.21.20/2.21.21?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/software.amazon.awssdk:sts/2.21.20/2.21.21?slim=true)](https://docs.renovatebot.com/merge-confidence/) |\n| [software.amazon.awssdk:rds](https://aws.amazon.com/sdkforjava) | `2.21.20` -> `2.21.21` | [![age](https://developer.mend.io/api/mc/badges/age/maven/software.amazon.awssdk:rds/2.21.21?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/software.amazon.awssdk:rds/2.21.21?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/software.amazon.awssdk:rds/2.21.20/2.21.21?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/software.amazon.awssdk:rds/2.21.20/2.21.21?slim=true)](https://docs.renovatebot.com/merge-confidence/) |\n| [software.amazon.awssdk:bom](https://aws.amazon.com/sdkforjava) | `2.21.20` -> `2.21.21` | [![age](https://developer.mend.io/api/mc/badges/age/maven/software.amazon.awssdk:bom/2.21.21?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/software.amazon.awssdk:bom/2.21.21?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/software.amazon.awssdk:bom/2.21.20/2.21.21?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/software.amazon.awssdk:bom/2.21.20/2.21.21?slim=true)](https://docs.renovatebot.com/merge-confidence/) |\n\n---\n\n> [!WARNING]\n> Some dependencies could not be looked up. Check the Dependency Dashboard for more information.\n\n---\n\n### Configuration\n\nðŸ“… **Schedule**: Branch creation - \"after 10pm every weekday,before 5am every weekday,every weekend\" in timezone Europe/Berlin, Automerge - At any time (no schedule defined).\n\nðŸš¦ **Automerge**: Enabled.\n\nâ™» **Rebasing**: Whenever PR becomes conflicted, or you tick the rebase/retry checkbox.\n\nðŸ”• **Ignore**: Close this PR and you won't be reminded about these updates again.\n\n---\n\n - [ ] <!-- rebase-check -->If you want to rebase/retry this PR, check this box\n\n---\n\nThis PR has been generated by [Mend Renovate](https://www.mend.io/free-developer-tools/renovate/). View repository job log [here](https://developer.mend.io/github/camunda-cloud/identity).\n<!--renovate-debug:eyJjcmVhdGVkSW5WZXIiOiIzNy40Ni4wIiwidXBkYXRlZEluVmVyIjoiMzcuNDYuMCIsInRhcmdldEJyYW5jaCI6Im1haW4ifQ==-->\n\n",
    "title": "fix: update aws-java-sdk-v2 monorepo to v2.21.21 (main)"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2307",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "ðŸ’Š Bugfixes",
    "gitHubText": "[![Mend Renovate](https://app.renovatebot.com/images/banner.svg)](https://renovatebot.com)\n\nThis PR contains the following updates:\n\n| Package | Change | Age | Adoption | Passing | Confidence |\n|---|---|---|---|---|---|\n| [software.amazon.awssdk:sts](https://aws.amazon.com/sdkforjava) | `2.21.17` -> `2.21.20` | [![age](https://developer.mend.io/api/mc/badges/age/maven/software.amazon.awssdk:sts/2.21.20?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/software.amazon.awssdk:sts/2.21.20?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/software.amazon.awssdk:sts/2.21.17/2.21.20?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/software.amazon.awssdk:sts/2.21.17/2.21.20?slim=true)](https://docs.renovatebot.com/merge-confidence/) |\n| [software.amazon.awssdk:rds](https://aws.amazon.com/sdkforjava) | `2.21.17` -> `2.21.20` | [![age](https://developer.mend.io/api/mc/badges/age/maven/software.amazon.awssdk:rds/2.21.20?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/software.amazon.awssdk:rds/2.21.20?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/software.amazon.awssdk:rds/2.21.17/2.21.20?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/software.amazon.awssdk:rds/2.21.17/2.21.20?slim=true)](https://docs.renovatebot.com/merge-confidence/) |\n| [software.amazon.awssdk:bom](https://aws.amazon.com/sdkforjava) | `2.21.17` -> `2.21.20` | [![age](https://developer.mend.io/api/mc/badges/age/maven/software.amazon.awssdk:bom/2.21.20?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/software.amazon.awssdk:bom/2.21.20?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/software.amazon.awssdk:bom/2.21.17/2.21.20?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/software.amazon.awssdk:bom/2.21.17/2.21.20?slim=true)](https://docs.renovatebot.com/merge-confidence/) |\n\n---\n\n> [!WARNING]\n> Some dependencies could not be looked up. Check the Dependency Dashboard for more information.\n\n---\n\n### Configuration\n\nðŸ“… **Schedule**: Branch creation - \"after 10pm every weekday,before 5am every weekday,every weekend\" in timezone Europe/Berlin, Automerge - At any time (no schedule defined).\n\nðŸš¦ **Automerge**: Enabled.\n\nâ™» **Rebasing**: Whenever PR becomes conflicted, or you tick the rebase/retry checkbox.\n\nðŸ”• **Ignore**: Close this PR and you won't be reminded about these updates again.\n\n---\n\n - [ ] <!-- rebase-check -->If you want to rebase/retry this PR, check this box\n\n---\n\nThis PR has been generated by [Mend Renovate](https://www.mend.io/free-developer-tools/renovate/). View repository job log [here](https://developer.mend.io/github/camunda-cloud/identity).\n<!--renovate-debug:eyJjcmVhdGVkSW5WZXIiOiIzNy40Ni4wIiwidXBkYXRlZEluVmVyIjoiMzcuNDYuMCIsInRhcmdldEJyYW5jaCI6Im1haW4ifQ==-->\n\n",
    "title": "fix: update aws-java-sdk-v2 monorepo to v2.21.20 (main)"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2300",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "ðŸ’Š Bugfixes",
    "gitHubText": "[![Mend Renovate](https://app.renovatebot.com/images/banner.svg)](https://renovatebot.com)\n\nThis PR contains the following updates:\n\n| Package | Change | Age | Adoption | Passing | Confidence |\n|---|---|---|---|---|---|\n| [software.amazon.awssdk:sts](https://aws.amazon.com/sdkforjava) | `2.21.15` -> `2.21.17` | [![age](https://developer.mend.io/api/mc/badges/age/maven/software.amazon.awssdk:sts/2.21.17?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/software.amazon.awssdk:sts/2.21.17?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/software.amazon.awssdk:sts/2.21.15/2.21.17?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/software.amazon.awssdk:sts/2.21.15/2.21.17?slim=true)](https://docs.renovatebot.com/merge-confidence/) |\n| [software.amazon.awssdk:rds](https://aws.amazon.com/sdkforjava) | `2.21.15` -> `2.21.17` | [![age](https://developer.mend.io/api/mc/badges/age/maven/software.amazon.awssdk:rds/2.21.17?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/software.amazon.awssdk:rds/2.21.17?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/software.amazon.awssdk:rds/2.21.15/2.21.17?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/software.amazon.awssdk:rds/2.21.15/2.21.17?slim=true)](https://docs.renovatebot.com/merge-confidence/) |\n| [software.amazon.awssdk:bom](https://aws.amazon.com/sdkforjava) | `2.21.15` -> `2.21.17` | [![age](https://developer.mend.io/api/mc/badges/age/maven/software.amazon.awssdk:bom/2.21.17?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/software.amazon.awssdk:bom/2.21.17?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/software.amazon.awssdk:bom/2.21.15/2.21.17?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/software.amazon.awssdk:bom/2.21.15/2.21.17?slim=true)](https://docs.renovatebot.com/merge-confidence/) |\n\n---\n\n> [!WARNING]\n> Some dependencies could not be looked up. Check the Dependency Dashboard for more information.\n\n---\n\n### Configuration\n\nðŸ“… **Schedule**: Branch creation - \"after 10pm every weekday,before 5am every weekday,every weekend\" in timezone Europe/Berlin, Automerge - At any time (no schedule defined).\n\nðŸš¦ **Automerge**: Enabled.\n\nâ™» **Rebasing**: Whenever PR becomes conflicted, or you tick the rebase/retry checkbox.\n\nðŸ”• **Ignore**: Close this PR and you won't be reminded about these updates again.\n\n---\n\n - [ ] <!-- rebase-check -->If you want to rebase/retry this PR, check this box\n\n---\n\nThis PR has been generated by [Mend Renovate](https://www.mend.io/free-developer-tools/renovate/). View repository job log [here](https://developer.mend.io/github/camunda-cloud/identity).\n<!--renovate-debug:eyJjcmVhdGVkSW5WZXIiOiIzNy40Ni4wIiwidXBkYXRlZEluVmVyIjoiMzcuNDYuMCIsInRhcmdldEJyYW5jaCI6Im1haW4ifQ==-->\n\n",
    "title": "fix: update aws-java-sdk-v2 monorepo to v2.21.17 (main)"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2213",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "ðŸ’Š Bugfixes",
    "gitHubText": "### Related issue(s)\r\n<!-- Changes should have a linked issue for traceability, use closing words such as \"Closes\" or \"Fixes\" to automatically handle related issue status. -->\r\nFixes #2157\r\n\r\n### Description\r\nUpgrade `c4-identity` library which includes a fix for assigning multiple tenants to an entity.\r\n\r\n### Is there anything else to consider?\r\n<!-- As a suggestion, were any alternatives considered? Is there any particular decision you'd like to highlight? -->\r\n\r\n### Acceptance criteria\r\n- [ ] Code changes are covered by tests\r\n- [x] Design review requested or not required\r\n\n\n Infra-connect: Backport failed for `v8.3`, because it was unable to cherry-pick the commit(s).\n\nPlease cherry-pick the changes locally.\n```bash\ngit fetch origin v8.3\ngit worktree add -d .worktree/backport-2213-to-v8.3 origin/v8.3\ncd .worktree/backport-2213-to-v8.3\ngit checkout -b backport-2213-to-v8.3\nancref=$(git merge-base d0756c082046edcc398fbc57df4c5ef5d104aea7 5fbf72aa74ed5daaedc814168408f50fe48d9fc9)\ngit cherry-pick -x $ancref..5fbf72aa74ed5daaedc814168408f50fe48d9fc9\n```",
    "title": "fix: update c4-identity to fix multiple tenants assignment"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2274",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "ðŸ’Š Bugfixes",
    "gitHubText": "<h3>Snyk has created this PR to fix one or more vulnerable packages in the `maven` dependencies of this project.</h3>\n\n\nAs this is a private repository, Snyk-bot does not have access. Therefore, this PR has been created automatically, but appears to have been created by a real user.\n\n#### Changes included in this PR\n\n- Changes to the following files to upgrade the vulnerable dependencies to a fixed version:\n    - management-api/pom.xml\n\n\n\n#### Vulnerabilities that will be fixed\n##### With an upgrade:\nSeverity                   | Priority Score (*)                   | Issue                   | Upgrade                   | Breaking Change                   | Exploit Maturity\n:-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------\n![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png \"medium severity\")  |  **490/1000**  <br/> **Why?** Has a fix available, CVSS 5.3  | Improper Certificate Validation <br/>[SNYK-JAVA-IONETTY-1042268](https://snyk.io/vuln/SNYK-JAVA-IONETTY-1042268) |  `software.amazon.awssdk:rds:` <br> `2.21.13 -> 2.21.15` <br>  |  No  | No Known Exploit \n\n(*) Note that the real score may have changed since the PR was raised.\n\n\n\n\n\n\n\n\n\n\n\nCheck the changes in this PR to ensure they won't cause issues with your project.\n\n\n\n------------\n\n\n\n**Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*\n\nFor more information:  <img src=\"https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI0ODI0OWJiOC04YzEwLTQzY2MtYmM3MS02M2MwY2Q0ZGVhZGQiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjQ4MjQ5YmI4LThjMTAtNDNjYy1iYzcxLTYzYzBjZDRkZWFkZCJ9fQ==\" width=\"0\" height=\"0\"/>\nðŸ§ [View latest project report](https://app.snyk.io/org/team-identity/project/afb108df-387e-4754-9fc5-4461836ac41c?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr)\n\nðŸ›  [Adjust project settings](https://app.snyk.io/org/team-identity/project/afb108df-387e-4754-9fc5-4461836ac41c?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings)\n\nðŸ“š [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities)\n\n[//]: # (snyk:metadata:{\"prId\":\"48249bb8-8c10-43cc-bc71-63c0cd4deadd\",\"prPublicId\":\"48249bb8-8c10-43cc-bc71-63c0cd4deadd\",\"dependencies\":[{\"name\":\"software.amazon.awssdk:rds\",\"from\":\"2.21.13\",\"to\":\"2.21.15\"}],\"packageManager\":\"maven\",\"projectPublicId\":\"afb108df-387e-4754-9fc5-4461836ac41c\",\"projectUrl\":\"https://app.snyk.io/org/team-identity/project/afb108df-387e-4754-9fc5-4461836ac41c?utm_source=github&utm_medium=referral&page=fix-pr\",\"type\":\"auto\",\"patch\":[],\"vulns\":[\"SNYK-JAVA-IONETTY-1042268\"],\"upgrade\":[\"SNYK-JAVA-IONETTY-1042268\"],\"isBreakingChange\":false,\"env\":\"prod\",\"prType\":\"fix\",\"templateVariants\":[\"updated-fix-title\",\"priorityScore\"],\"priorityScoreList\":[490],\"remediationStrategy\":\"vuln\"})\n\n---\n\n**Learn how to fix vulnerabilities with free interactive lessons:**\n\n ðŸ¦‰ [Learn about vulnerability in an interactive lesson of Snyk Learn.](https://learn.snyk.io/?loc&#x3D;fix-pr)\n\n",
    "title": "fix: upgrade software.amazon.awssdk:rds from 2.21.13 to 2.21.15"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2268",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "ðŸ’Š Bugfixes",
    "gitHubText": "### Related issue(s)\r\n<!-- Changes should have a linked issue for traceability, use closing words such as \"Closes\" or \"Fixes\" to automatically handle related issue status. -->\r\nFixes #2264 \r\n\r\n### Description\r\n<!-- Consider suggesting why this change is being made, for example has an issue been spotted? Have we found a more performant way to solve the problem? -->\r\n\r\nAs described in the linked issue, there appears to be a clashing of dependency versions for the jupiter artifacts, setting the versions via a `dependencyManagement` block doesn't seem to be successful (more exploration could be good here), so I have opted for a two fold approach:\r\n\r\n1. Set the Jupiter versions to a version that works\r\n2. Configure the maven failsafe plugin to fail the build if no tests are run during the verify stage for the management-api artifact.\r\n\r\nThe first point is clear, the second point is an extra layer of protection, we know that there are ITs that run during the validation phase in the `management-api` module so seeing no tests run is not expected (unless the tests are skipped using `-DskipTests=true` though the build is still successful in this instance.\r\n\r\n### Is there anything else to consider?\r\n<!-- As a suggestion, were any alternatives considered? Is there any particular decision you'd like to highlight? -->\r\n\r\nI tried to think of different ways to work through the dependency clashes but they felt at best like I was sticking a plaster over things and not letting Maven use its built in logic for dependency resolution (by for example excluding the dependency from spring-boot-test` or forcing the version).\r\n\r\nI believe that for now (and to get our builds back to a credible position) this change is sufficient. \r\n\n",
    "title": "fix: dependency issues causing tests to not run"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2265",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "ðŸ’Š Bugfixes",
    "gitHubText": "### Related issue(s)\r\n<!-- Changes should have a linked issue for traceability, use closing words such as \"Closes\" or \"Fixes\" to automatically handle related issue status. -->\r\nFixes #2263\r\n\r\n### Description\r\nUse `LEFT JOIN tenants` instead of `JOIN tenants` when retrieving access rules. A normal join leads to no rows being found although an access rule exists if it is not linked to at least one tenant \r\n\r\n### Is there anything else to consider?\r\n<!-- As a suggestion, were any alternatives considered? Is there any particular decision you'd like to highlight? -->\r\n\r\n### Acceptance criteria\r\n- [x] Code changes are covered by tests\r\n- [x] Design review requested or not required\r\n\n\n Infra-connect: Successfully created backport PR for `v8.3`:\n- #2269",
    "title": "fix: fix access rules retrieval"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2227",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "ðŸ’Š Bugfixes",
    "gitHubText": "[![Mend Renovate](https://app.renovatebot.com/images/banner.svg)](https://renovatebot.com)\n\nThis PR contains the following updates:\n\n| Package | Change | Age | Adoption | Passing | Confidence |\n|---|---|---|---|---|---|\n| [org.springframework.boot:spring-boot-starter](https://spring.io/projects/spring-boot) ([source](https://togithub.com/spring-projects/spring-boot)) | `3.1.4` -> `3.1.5` | [![age](https://developer.mend.io/api/mc/badges/age/maven/org.springframework.boot:spring-boot-starter/3.1.5?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/org.springframework.boot:spring-boot-starter/3.1.5?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/org.springframework.boot:spring-boot-starter/3.1.4/3.1.5?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/org.springframework.boot:spring-boot-starter/3.1.4/3.1.5?slim=true)](https://docs.renovatebot.com/merge-confidence/) |\n| [org.springframework.boot:spring-boot-configuration-processor](https://spring.io/projects/spring-boot) ([source](https://togithub.com/spring-projects/spring-boot)) | `3.1.4` -> `3.1.5` | [![age](https://developer.mend.io/api/mc/badges/age/maven/org.springframework.boot:spring-boot-configuration-processor/3.1.5?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/org.springframework.boot:spring-boot-configuration-processor/3.1.5?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/org.springframework.boot:spring-boot-configuration-processor/3.1.4/3.1.5?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/org.springframework.boot:spring-boot-configuration-processor/3.1.4/3.1.5?slim=true)](https://docs.renovatebot.com/merge-confidence/) |\n| [org.springframework.boot:spring-boot-autoconfigure](https://spring.io/projects/spring-boot) ([source](https://togithub.com/spring-projects/spring-boot)) | `3.1.4` -> `3.1.5` | [![age](https://developer.mend.io/api/mc/badges/age/maven/org.springframework.boot:spring-boot-autoconfigure/3.1.5?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/org.springframework.boot:spring-boot-autoconfigure/3.1.5?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/org.springframework.boot:spring-boot-autoconfigure/3.1.4/3.1.5?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/org.springframework.boot:spring-boot-autoconfigure/3.1.4/3.1.5?slim=true)](https://docs.renovatebot.com/merge-confidence/) |\n| [org.springframework.boot:spring-boot](https://spring.io/projects/spring-boot) ([source](https://togithub.com/spring-projects/spring-boot)) | `3.1.4` -> `3.1.5` | [![age](https://developer.mend.io/api/mc/badges/age/maven/org.springframework.boot:spring-boot/3.1.5?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/maven/org.springframework.boot:spring-boot/3.1.5?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/maven/org.springframework.boot:spring-boot/3.1.4/3.1.5?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/maven/org.springframework.boot:spring-boot/3.1.4/3.1.5?slim=true)](https://docs.renovatebot.com/merge-confidence/) |\n\n---\n\n> [!WARNING]\n> Some dependencies could not be looked up. Check the Dependency Dashboard for more information.\n\n---\n\n### Release Notes\n\n<details>\n<summary>spring-projects/spring-boot (org.springframework.boot:spring-boot-starter)</summary>\n\n### [`v3.1.5`](https://togithub.com/spring-projects/spring-boot/releases/tag/v3.1.5)\n\n[Compare Source](https://togithub.com/spring-projects/spring-boot/compare/v3.1.4...v3.1.5)\n\n#### :warning: Noteworthy Changes\n\n-   The behavior of `spring.jms.listener.concurrency` has been corrected to match the documentation ([#&#8203;37180](https://togithub.com/spring-projects/spring-boot/pull/37180)). If you were setting `spring.jms.listener.concurrency` without also setting `spring.jms.listener.max-concurrency`, please review your configuration when upgrading.\n\n#### :lady_beetle: Bug Fixes\n\n-   Constructor binding with a custom collection type does not work [#&#8203;37941](https://togithub.com/spring-projects/spring-boot/issues/37941)\n-   `@Order` does not work on (CommandLine|Application)Runner `@Bean` methods [#&#8203;37938](https://togithub.com/spring-projects/spring-boot/issues/37938)\n-   `@ComponentScan` on a test class is processed when creating a test context but is not included in the context's cache key [#&#8203;37924](https://togithub.com/spring-projects/spring-boot/issues/37924)\n-   Restarter creates memory leak in tests [#&#8203;37920](https://togithub.com/spring-projects/spring-boot/issues/37920)\n-   AOT processing fails when a `@WebServlet` found by scanning is annotated with `@MultipartConfig` [#&#8203;37883](https://togithub.com/spring-projects/spring-boot/issues/37883)\n-   Gradle plugin uses to-be-deprecated API for getting and setting file permissions [#&#8203;37881](https://togithub.com/spring-projects/spring-boot/issues/37881)\n-   Task executor metrics are not registered when using lazy initialization [#&#8203;37838](https://togithub.com/spring-projects/spring-boot/issues/37838)\n-   Gradle AOT processing tasks do not use project's Java toolchain [#&#8203;37826](https://togithub.com/spring-projects/spring-boot/issues/37826)\n-   `@ServiceConnection` is not found when used in an interface implemented by a test class [#&#8203;37671](https://togithub.com/spring-projects/spring-boot/issues/37671)\n-   Image building can fail when using GraalVM compilation and a remote Docker daemon [#&#8203;37665](https://togithub.com/spring-projects/spring-boot/issues/37665)\n-   NPE from Jetty's WebSocketUpgradeFilter when testing with `@SpringBootTest`, `@AutoConfigureMockMvc`, and MockMvc [#&#8203;37663](https://togithub.com/spring-projects/spring-boot/issues/37663)\n-   `@WebListener` does not work in a native image without additional reflection hints [#&#8203;37635](https://togithub.com/spring-projects/spring-boot/issues/37635)\n-   AspectJ transaction management with compile-time weaving does not work with spring.main.lazy-initialization=true [#&#8203;37632](https://togithub.com/spring-projects/spring-boot/issues/37632)\n-   IPv6 IP addresses cannot be used with RabbitMQ [#&#8203;37619](https://togithub.com/spring-projects/spring-boot/pull/37619)\n-   Unwanted Logback status messages are sometimes logged during startup [#&#8203;37600](https://togithub.com/spring-projects/spring-boot/issues/37600)\n-   Managed types for Neo4j are not used in Neo4j Data auto configuration [#&#8203;37594](https://togithub.com/spring-projects/spring-boot/issues/37594)\n-   fileMode and dirMode are not applied to all entries in an archive produced by BootJar [#&#8203;37588](https://togithub.com/spring-projects/spring-boot/issues/37588)\n-   Application fails to start when an optional config import cannot be resolved [#&#8203;37570](https://togithub.com/spring-projects/spring-boot/issues/37570)\n-   Contrary to the documentation, setting spring.jms.listener.concurrency alone configures the maximum concurrency [#&#8203;37553](https://togithub.com/spring-projects/spring-boot/issues/37553)\n-   Dependency management for kafka-server-common with a test classifier is missing [#&#8203;37542](https://togithub.com/spring-projects/spring-boot/issues/37542)\n-   RepackageMojo doesn't support 1 digit numerical values for project.build.outputTimestamp [#&#8203;37535](https://togithub.com/spring-projects/spring-boot/issues/37535)\n\n#### :notebook_with_decorative_cover: Documentation\n\n-   Document that 'spring.docker.compose.file' can be used to share Docker Compose configuration between applications [#&#8203;37886](https://togithub.com/spring-projects/spring-boot/issues/37886)\n-   Remove link to LiveReload website due to timeout [#&#8203;37691](https://togithub.com/spring-projects/spring-boot/issues/37691)\n-   Refer to ActiveMQ as ActiveMQ \"Classic\" [#&#8203;37615](https://togithub.com/spring-projects/spring-boot/issues/37615)\n-   Removal of spring.webflux.multipart.streaming is not documented [#&#8203;37609](https://togithub.com/spring-projects/spring-boot/issues/37609)\n-   Default value of spring.jmx.registration-policy is not documented [#&#8203;37596](https://togithub.com/spring-projects/spring-boot/issues/37596)\n-   Update documentation to align with Mockito 5 using the inline mock maker by default [#&#8203;37561](https://togithub.com/spring-projects/spring-boot/pull/37561)\n-   Add Javadoc since for AbstractAotMojo.getSession() [#&#8203;37547](https://togithub.com/spring-projects/spring-boot/issues/37547)\n-   Document support for Java 21 [#&#8203;37532](https://togithub.com/spring-projects/spring-boot/issues/37532)\n-   Use more idiomatic Kotlin in example for \"Map Health Indicators to Micrometer Metrics\" [#&#8203;37510](https://togithub.com/spring-projects/spring-boot/issues/37510)\n\n#### :hammer: Dependency Upgrades\n\n-   Upgrade to Byte Buddy 1.14.9 [#&#8203;37853](https://togithub.com/spring-projects/spring-boot/issues/37853)\n-   Upgrade to Couchbase Client 3.4.11 [#&#8203;37759](https://togithub.com/spring-projects/spring-boot/issues/37759)\n-   Upgrade to Dropwizard Metrics 4.2.21 [#&#8203;37897](https://togithub.com/spring-projects/spring-boot/issues/37897)\n-   Upgrade to Hibernate 6.2.13.Final [#&#8203;37854](https://togithub.com/spring-projects/spring-boot/issues/37854)\n-   Upgrade to HttpCore5 5.2.3 [#&#8203;37762](https://togithub.com/spring-projects/spring-boot/issues/37762)\n-   Upgrade to Infinispan 14.0.19.Final [#&#8203;37855](https://togithub.com/spring-projects/spring-boot/issues/37855)\n-   Upgrade to Jackson Bom 2.15.3 [#&#8203;37898](https://togithub.com/spring-projects/spring-boot/issues/37898)\n-   Upgrade to Jetty 11.0.17 [#&#8203;37856](https://togithub.com/spring-projects/spring-boot/issues/37856)\n-   Upgrade to Jetty Reactive HTTPClient 3.0.9 [#&#8203;37932](https://togithub.com/spring-projects/spring-boot/issues/37932)\n-   Upgrade to jOOQ 3.18.7 [#&#8203;37857](https://togithub.com/spring-projects/spring-boot/issues/37857)\n-   Upgrade to Micrometer 1.11.5 [#&#8203;37693](https://togithub.com/spring-projects/spring-boot/issues/37693)\n-   Upgrade to Micrometer Tracing 1.1.6 [#&#8203;37694](https://togithub.com/spring-projects/spring-boot/issues/37694)\n-   Upgrade to Neo4j Java Driver 5.13.0 [#&#8203;37793](https://togithub.com/spring-projects/spring-boot/issues/37793)\n-   Upgrade to Netty 4.1.100.Final [#&#8203;37858](https://togithub.com/spring-projects/spring-boot/issues/37858)\n-   Upgrade to Pooled JMS 3.1.4 [#&#8203;37764](https://togithub.com/spring-projects/spring-boot/issues/37764)\n-   Upgrade to R2DBC MySQL 1.0.5 [#&#8203;37859](https://togithub.com/spring-projects/spring-boot/issues/37859)\n-   Upgrade to Reactor Bom 2022.0.12 [#&#8203;37695](https://togithub.com/spring-projects/spring-boot/issues/37695)\n-   Upgrade to RxJava3 3.1.8 [#&#8203;37766](https://togithub.com/spring-projects/spring-boot/issues/37766)\n-   Upgrade to Spring AMQP 3.0.10 [#&#8203;37696](https://togithub.com/spring-projects/spring-boot/issues/37696)\n-   Upgrade to Spring Authorization Server 1.1.3 [#&#8203;37697](https://togithub.com/spring-projects/spring-boot/issues/37697)\n-   Upgrade to Spring Data Bom 2023.0.5 [#&#8203;37698](https://togithub.com/spring-projects/spring-boot/issues/37698)\n-   Upgrade to Spring Framework 6.0.13 [#&#8203;37816](https://togithub.com/spring-projects/spring-boot/issues/37816)\n-   Upgrade to Spring Integration 6.1.4 [#&#8203;37914](https://togithub.com/spring-projects/spring-boot/issues/37914)\n-   Upgrade to Spring Kafka 3.0.12 [#&#8203;37797](https://togithub.com/spring-projects/spring-boot/issues/37797)\n-   Upgrade to Spring LDAP 3.1.2 [#&#8203;37699](https://togithub.com/spring-projects/spring-boot/issues/37699)\n-   Upgrade to Spring Retry 2.0.4 [#&#8203;37700](https://togithub.com/spring-projects/spring-boot/issues/37700)\n-   Upgrade to Spring Security 6.1.5 [#&#8203;37701](https://togithub.com/spring-projects/spring-boot/issues/37701)\n-   Upgrade to Spring Session 3.1.3 [#&#8203;37702](https://togithub.com/spring-projects/spring-boot/issues/37702)\n-   Upgrade to Tomcat 10.1.15 [#&#8203;37902](https://togithub.com/spring-projects/spring-boot/issues/37902)\n-   Upgrade to UnboundID LDAPSDK 6.0.10 [#&#8203;37767](https://togithub.com/spring-projects/spring-boot/issues/37767)\n-   Upgrade to Undertow 2.3.10.Final [#&#8203;37933](https://togithub.com/spring-projects/spring-boot/issues/37933)\n\n#### :heart: Contributors\n\nThank you to all the contributors who worked on this release:\n\n[@&#8203;JinseongHwang](https://togithub.com/JinseongHwang), [@&#8203;bottlerocketjonny](https://togithub.com/bottlerocketjonny), [@&#8203;dependabot](https://togithub.com/dependabot)\\[bot], [@&#8203;erichaagdev](https://togithub.com/erichaagdev), [@&#8203;esperar](https://togithub.com/esperar), [@&#8203;izeye](https://togithub.com/izeye), [@&#8203;jbertram](https://togithub.com/jbertram), [@&#8203;jonasfugedi](https://togithub.com/jonasfugedi), [@&#8203;michael-simons](https://togithub.com/michael-simons), [@&#8203;nielsbasjes](https://togithub.com/nielsbasjes), [@&#8203;onobc](https://togithub.com/onobc), [@&#8203;sushant1987](https://togithub.com/sushant1987), [@&#8203;ttddyy](https://togithub.com/ttddyy), and [@&#8203;vpavic](https://togithub.com/vpavic)\n\n</details>\n\n---\n\n### Configuration\n\nðŸ“… **Schedule**: Branch creation - \"after 10pm every weekday,before 5am every weekday,every weekend\" in timezone Europe/Berlin, Automerge - At any time (no schedule defined).\n\nðŸš¦ **Automerge**: Enabled.\n\nâ™» **Rebasing**: Whenever PR becomes conflicted, or you tick the rebase/retry checkbox.\n\nðŸ”• **Ignore**: Close this PR and you won't be reminded about these updates again.\n\n---\n\n - [ ] <!-- rebase-check -->If you want to rebase/retry this PR, check this box\n\n---\n\nThis PR has been generated by [Mend Renovate](https://www.mend.io/free-developer-tools/renovate/). View repository job log [here](https://developer.mend.io/github/camunda-cloud/identity).\n<!--renovate-debug:eyJjcmVhdGVkSW5WZXIiOiIzNy4zMS41IiwidXBkYXRlZEluVmVyIjoiMzcuMzEuNSIsInRhcmdldEJyYW5jaCI6Im1haW4ifQ==-->\n\n",
    "title": "fix: update spring boot to v3.1.5 (main)"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2225",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "ðŸ’Š Bugfixes",
    "gitHubText": "### Related issue(s)\r\nrelated to https://github.com/camunda-cloud/identity/pull/2184\r\n\r\n\r\n### Description\r\nIn #2184 I have added artifact deployment for the new `-starter` and `-autoconfigure` artifacts, however this step fails due to being executed with Java 11 instead of Java 17 (see https://github.com/camunda-cloud/identity/actions/runs/6615156752/job/17966714643)\r\n\r\n### Is there anything else to consider?\r\n<!-- As a suggestion, were any alternatives considered? Is there any particular decision you'd like to highlight? -->\r\n\r\n### Acceptance criteria\r\n- [ ] Code changes are covered by tests\r\n- [ ] Design review requested or not required\r\n\n\n dlavrenuek: I will merge this PR to unblock the Operate team",
    "title": "fix: use java 17 to build new artifacts"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2191",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "ðŸ’Š Bugfixes",
    "gitHubText": "<h3>Snyk has created this PR to fix one or more vulnerable packages in the `maven` dependencies of this project.</h3>\n\n\nAs this is a private repository, Snyk-bot does not have access. Therefore, this PR has been created automatically, but appears to have been created by a real user.\n\n#### Changes included in this PR\n\n- Changes to the following files to upgrade the vulnerable dependencies to a fixed version:\n    - management-api/pom.xml\n\n\n\n#### Vulnerabilities that will be fixed\n##### With an upgrade:\nSeverity                   | Priority Score (*)                   | Issue                   | Upgrade                   | Breaking Change                   | Exploit Maturity\n:-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------\n![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png \"high severity\")  |  **875/1000**  <br/> **Why?** Currently trending on Twitter, Mature exploit, Recently disclosed, Has a fix available, CVSS 7.5  | Denial of Service (DoS) <br/>[SNYK-JAVA-ORGAPACHETOMCATEMBED-5953331](https://snyk.io/vuln/SNYK-JAVA-ORGAPACHETOMCATEMBED-5953331) |  `org.springframework.boot:spring-boot-starter-web:` <br> `3.1.4 -> 3.1.5` <br>  |  No  | Mature \n![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png \"medium severity\")  |  **551/1000**  <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.3  | Improper Input Validation <br/>[SNYK-JAVA-ORGAPACHETOMCATEMBED-5959654](https://snyk.io/vuln/SNYK-JAVA-ORGAPACHETOMCATEMBED-5959654) |  `org.springframework.boot:spring-boot-starter-web:` <br> `3.1.4 -> 3.1.5` <br>  |  No  | No Known Exploit \n![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png \"medium severity\")  |  **551/1000**  <br/> **Why?** Recently disclosed, Has a fix available, CVSS 5.3  | Incomplete Cleanup <br/>[SNYK-JAVA-ORGAPACHETOMCATEMBED-5959972](https://snyk.io/vuln/SNYK-JAVA-ORGAPACHETOMCATEMBED-5959972) |  `org.springframework.boot:spring-boot-starter-web:` <br> `3.1.4 -> 3.1.5` <br>  |  No  | No Known Exploit \n![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png \"medium severity\")  |  **651/1000**  <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 6.6  | Arbitrary Code Execution <br/>[SNYK-JAVA-ORGYAML-3152153](https://snyk.io/vuln/SNYK-JAVA-ORGYAML-3152153) |  `org.springframework.boot:spring-boot-starter-web:` <br> `3.1.4 -> 3.1.5` <br>  |  No  | Proof of Concept \n\n(*) Note that the real score may have changed since the PR was raised.\n\n\n\n\n\n\n\n\n\n\n\nCheck the changes in this PR to ensure they won't cause issues with your project.\n\n\n\n------------\n\n\n\n**Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*\n\nFor more information:  <img src=\"https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiIwMWRlMzMzYy0yZTJlLTRiMzctOTUxZi0zZGRmMTY1NWZmMzgiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjAxZGUzMzNjLTJlMmUtNGIzNy05NTFmLTNkZGYxNjU1ZmYzOCJ9fQ==\" width=\"0\" height=\"0\"/>\nðŸ§ [View latest project report](https://app.snyk.io/org/team-identity/project/afb108df-387e-4754-9fc5-4461836ac41c?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr)\n\nðŸ›  [Adjust project settings](https://app.snyk.io/org/team-identity/project/afb108df-387e-4754-9fc5-4461836ac41c?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings)\n\nðŸ“š [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities)\n\n[//]: # (snyk:metadata:{\"prId\":\"01de333c-2e2e-4b37-951f-3ddf1655ff38\",\"prPublicId\":\"01de333c-2e2e-4b37-951f-3ddf1655ff38\",\"dependencies\":[{\"name\":\"org.springframework.boot:spring-boot-starter-web\",\"from\":\"3.1.4\",\"to\":\"3.1.5\"}],\"packageManager\":\"maven\",\"projectPublicId\":\"afb108df-387e-4754-9fc5-4461836ac41c\",\"projectUrl\":\"https://app.snyk.io/org/team-identity/project/afb108df-387e-4754-9fc5-4461836ac41c?utm_source=github&utm_medium=referral&page=fix-pr\",\"type\":\"auto\",\"patch\":[],\"vulns\":[\"SNYK-JAVA-ORGAPACHETOMCATEMBED-5953331\",\"SNYK-JAVA-ORGAPACHETOMCATEMBED-5959654\",\"SNYK-JAVA-ORGAPACHETOMCATEMBED-5959972\",\"SNYK-JAVA-ORGYAML-3152153\"],\"upgrade\":[\"SNYK-JAVA-ORGAPACHETOMCATEMBED-5953331\",\"SNYK-JAVA-ORGAPACHETOMCATEMBED-5959654\",\"SNYK-JAVA-ORGAPACHETOMCATEMBED-5959972\",\"SNYK-JAVA-ORGYAML-3152153\"],\"isBreakingChange\":false,\"env\":\"prod\",\"prType\":\"fix\",\"templateVariants\":[\"updated-fix-title\",\"priorityScore\"],\"priorityScoreList\":[875,551,551,651],\"remediationStrategy\":\"vuln\"})\n\n---\n\n**Learn how to fix vulnerabilities with free interactive lessons:**\n\n ðŸ¦‰ [Denial of Service (DoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr)\n ðŸ¦‰ [Improper Input Validation](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr)\n\n\n Infra-connect: Backport failed for `v8.1`, because it was unable to cherry-pick the commit(s).\n\nPlease cherry-pick the changes locally.\n```bash\ngit fetch origin v8.1\ngit worktree add -d .worktree/backport-2191-to-v8.1 origin/v8.1\ncd .worktree/backport-2191-to-v8.1\ngit checkout -b backport-2191-to-v8.1\nancref=$(git merge-base 7918f55dc705b2e3322f61ce3b2c6c02ebf6e00c 1021ea3ebd786671584e9c3885e41be6c2225a0f)\ngit cherry-pick -x $ancref..1021ea3ebd786671584e9c3885e41be6c2225a0f\n```\n Infra-connect: Backport failed for `v8.2`, because it was unable to cherry-pick the commit(s).\n\nPlease cherry-pick the changes locally.\n```bash\ngit fetch origin v8.2\ngit worktree add -d .worktree/backport-2191-to-v8.2 origin/v8.2\ncd .worktree/backport-2191-to-v8.2\ngit checkout -b backport-2191-to-v8.2\nancref=$(git merge-base 7918f55dc705b2e3322f61ce3b2c6c02ebf6e00c 1021ea3ebd786671584e9c3885e41be6c2225a0f)\ngit cherry-pick -x $ancref..1021ea3ebd786671584e9c3885e41be6c2225a0f\n```\n Infra-connect: Backport failed for `v8.3`, because it was unable to cherry-pick the commit(s).\n\nPlease cherry-pick the changes locally.\n```bash\ngit fetch origin v8.3\ngit worktree add -d .worktree/backport-2191-to-v8.3 origin/v8.3\ncd .worktree/backport-2191-to-v8.3\ngit checkout -b backport-2191-to-v8.3\nancref=$(git merge-base 7918f55dc705b2e3322f61ce3b2c6c02ebf6e00c 1021ea3ebd786671584e9c3885e41be6c2225a0f)\ngit cherry-pick -x $ancref..1021ea3ebd786671584e9c3885e41be6c2225a0f\n```",
    "title": "fix: upgrade org.springframework.boot:spring-boot-starter-web from 3.1.4 to 3.1.5"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2180",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "ðŸ’Š Bugfixes",
    "gitHubText": "### Description\r\n<!-- Consider suggesting why this change is being made, for example has an issue been spotted? Have we found a more performant way to solve the problem? -->\r\n\r\nI noticed whilst putting together an example set of configuration for tenants that currently there is a exception when trying to assign members in configuration, which is:\r\n```\r\nCaused by: org.hibernate.LazyInitializationException: failed to lazily initialize a collection of role: io.camunda.identity.entity.AccessRule.tenants: could not initialize proxy - no Session\r\n```\r\n\r\nLooking at it I can see that the code fails when we try to retrieve the tenants for the access rule that we want to assign a member to, frustratingly we have tests for this behaviour but it seems that the Hibernate session is extending to allow the tests to pass...\r\n\r\nHaving [read a little about the options to resolve this](https://www.baeldung.com/hibernate-initialize-proxy-exception) I decided to opt for the `JOIN FETCH` approach as mentioned [here](https://www.baeldung.com/hibernate-initialize-proxy-exception#4-using-join-fetching) because using `FetchType.EAGER` is generally discouraged.\r\n\r\n### Acceptance criteria\r\n- [x] Code changes are covered by tests\r\n- [x] Design review requested or not required\r\n\n\n Infra-connect: Successfully created backport PR for `v8.3`:\n- #2183",
    "title": "fix: resolve lazy loading issue with tenants linked to access rules"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2133",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "ðŸ’Š Bugfixes",
    "gitHubText": "<h3>Snyk has created this PR to fix one or more vulnerable packages in the `maven` dependencies of this project.</h3>\n\n\nAs this is a private repository, Snyk-bot does not have access. Therefore, this PR has been created automatically, but appears to have been created by a real user.\n\n#### Changes included in this PR\n\n- Changes to the following files to upgrade the vulnerable dependencies to a fixed version:\n    - management-api/pom.xml\n\n\n\n#### Vulnerabilities that will be fixed\n##### With an upgrade:\nSeverity                   | Priority Score (*)                   | Issue                   | Upgrade                   | Breaking Change                   | Exploit Maturity\n:-------------------------:|-------------------------|:-------------------------|:-------------------------|:-------------------------|:-------------------------\n![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png \"medium severity\")  |  **539/1000**  <br/> **Why?** Has a fix available, CVSS 6.5  | Access Restriction Bypass <br/>[SNYK-JAVA-ORGAPACHETOMCATEMBED-5862028](https://snyk.io/vuln/SNYK-JAVA-ORGAPACHETOMCATEMBED-5862028) |  `org.springframework.boot:spring-boot-starter-web:` <br> `3.1.3 -> 3.1.4` <br>  |  No  | No Known Exploit \n![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png \"medium severity\")  |  **491/1000**  <br/> **Why?** Recently disclosed, Has a fix available, CVSS 4.1  | Incorrect Permission Assignment for Critical Resource <br/>[SNYK-JAVA-ORGSPRINGFRAMEWORKSECURITY-5905484](https://snyk.io/vuln/SNYK-JAVA-ORGSPRINGFRAMEWORKSECURITY-5905484) |  `org.springframework.boot:spring-boot-starter-security:` <br> `3.1.3 -> 3.1.4` <br>  |  No  | No Known Exploit \n\n(*) Note that the real score may have changed since the PR was raised.\n\n\n\n\n\n\n\n\n\n\n\nCheck the changes in this PR to ensure they won't cause issues with your project.\n\n\n\n------------\n\n\n\n**Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*\n\nFor more information:  <img src=\"https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJkYTNjY2M0YS1iYzI3LTQ1MDUtODcwNC1jZDAwMWUwNzhkYjIiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImRhM2NjYzRhLWJjMjctNDUwNS04NzA0LWNkMDAxZTA3OGRiMiJ9fQ==\" width=\"0\" height=\"0\"/>\nðŸ§ [View latest project report](https://app.snyk.io/org/team-identity/project/afb108df-387e-4754-9fc5-4461836ac41c?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr)\n\nðŸ›  [Adjust project settings](https://app.snyk.io/org/team-identity/project/afb108df-387e-4754-9fc5-4461836ac41c?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings)\n\nðŸ“š [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities)\n\n[//]: # (snyk:metadata:{\"prId\":\"da3ccc4a-bc27-4505-8704-cd001e078db2\",\"prPublicId\":\"da3ccc4a-bc27-4505-8704-cd001e078db2\",\"dependencies\":[{\"name\":\"org.springframework.boot:spring-boot-starter-security\",\"from\":\"3.1.3\",\"to\":\"3.1.4\"},{\"name\":\"org.springframework.boot:spring-boot-starter-web\",\"from\":\"3.1.3\",\"to\":\"3.1.4\"}],\"packageManager\":\"maven\",\"projectPublicId\":\"afb108df-387e-4754-9fc5-4461836ac41c\",\"projectUrl\":\"https://app.snyk.io/org/team-identity/project/afb108df-387e-4754-9fc5-4461836ac41c?utm_source=github&utm_medium=referral&page=fix-pr\",\"type\":\"auto\",\"patch\":[],\"vulns\":[\"SNYK-JAVA-ORGAPACHETOMCATEMBED-5862028\",\"SNYK-JAVA-ORGSPRINGFRAMEWORKSECURITY-5905484\"],\"upgrade\":[\"SNYK-JAVA-ORGAPACHETOMCATEMBED-5862028\",\"SNYK-JAVA-ORGSPRINGFRAMEWORKSECURITY-5905484\"],\"isBreakingChange\":false,\"env\":\"prod\",\"prType\":\"fix\",\"templateVariants\":[\"priorityScore\"],\"priorityScoreList\":[539,491],\"remediationStrategy\":\"vuln\"})\n\n---\n\n**Learn how to fix vulnerabilities with free interactive lessons:**\n\n ðŸ¦‰ [Access Restriction Bypass](https://learn.snyk.io/lesson/broken-access-control/?loc&#x3D;fix-pr)\n\n",
    "title": "fix: fix for 2 vulnerabilities"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2164",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "ðŸ’Š Bugfixes",
    "gitHubText": "### Related issue(s)\r\n<!-- Changes should have a linked issue for traceability, use closing words such as \"Closes\" or \"Fixes\" to automatically handle related issue status. -->\r\nFixes #1979 \r\n\r\n### Description\r\n<!-- Consider suggesting why this change is being made, for example has an issue been spotted? Have we found a more performant way to solve the problem? -->\r\n\r\nIt seems that with our current configuration, the Log4J2 system was proactively creating the file. This is producing the undesirable result and it turns out there is a property to toggle this feature ([REF](https://logging.apache.org/log4j/2.x/manual/appenders.html#fileappender)).\r\n\n\n Infra-connect: Successfully created backport PR for `v8.2`:\n- #2167\n Infra-connect: Successfully created backport PR for `v8.3`:\n- #2168",
    "title": "fix: only create log file when required"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2153",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "ðŸ’Š Bugfixes",
    "gitHubText": "### Related issue(s)\r\n<!-- Changes should have a linked issue for traceability, use closing words such as \"Closes\" or \"Fixes\" to automatically handle related issue status. -->\r\nFixes #2152 \r\n\r\n### Description\r\n<!-- Consider suggesting why this change is being made, for example has an issue been spotted? Have we found a more performant way to solve the problem? -->\r\n\r\nAs described in the linked issue, the `/tenants` page was failing to load due to the serviceWorker setting a body for `GET` calls. This PR changes the logic to avoid doing this specifically for `GET`\r\n\r\n### Acceptance criteria\r\n- [ ] Code changes are covered by tests\r\n- [ ] Design review requested or not required\r\n\n\n Infra-connect: Successfully created backport PR for `v8.3`:\n- #2156",
    "title": "fix: prevent body on GET call in serviceWorker"
  }
]