[
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/13203",
    "component": "Zeebe",
    "subcomponent": "Broker",
    "context": "Enhancements",
    "title": "Support Index State Management in OpenSearch exporter",
    "releaseNoteText": "Previously, users were unable to manage index lifecycles with Index State Management (ISM) in the OpenSearch Exporter. The need for manual index management through the OpenSearch UI was an inconvenience and an additional task for users. \nThe lack of ISM support in our OpenSearch Exporter was due to the absence of the necessary configuration code.\nSupport for ISM has been added to the OpenSearch Exporter in a manner similar to ILM, compatible with OpenSearch versions 1.3.x and 2.8.x. Furthermore, we have documented this feature in the OpenSearch Exporter docs to assist users in the configuration process.\nNow, users can conveniently configure the management of index lifecycles using ISM right from the OpenSearch Exporter. This upgrade eliminates the need for manual index management and aligns the OpenSearch Exporter with the Elasticsearch Exporter in terms of features."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14277",
    "component": "Zeebe",
    "subcomponent": "Gateway",
    "context": "Enhancements",
    "title": "Gateway supports multi-tenancy in signal broadcast RPCs",
    "releaseNoteText": " Previously, the Zeebe Gateway could not handle tenant-aware `BroadcastSignal` RPC calls, preventing multi-tenancy in signal broadcasts.\n This was due to the lack of a `tenantId` property in `BroadcastSignal` RPC requests, responses and signal-related records.\n To address this, we updated `BroadcastSignal` requests and responses to contain a `tenantId` property. We also included a `tenantId` property into signal-related records and added validation for correct `tenantId` in `BroadcastSignal` requests.\n Now, the Zeebe Gateway properly supports handling tenant-aware `BroadcastSignal` RPC calls, enabling multi-tenancy in signal broadcast RPCs."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15161",
    "component": "Zeebe",
    "subcomponent": "Java Client",
    "context": "Enhancements",
    "title": "Provide the Java client's interfaces for migrating a process instance",
    "releaseNoteText": "Users of the Java client were unable to migrate process instances through the API. \nIn the past, the interfaces required for migrating a process instance had not been made available for Java client users. \nThe interfaces of the Java client were provided. However, the full implementation was not done to unblock Operate early. \nUsers can now start development against these interfaces with the Java client, assisting in the migration of process instances."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14921",
    "component": "Zeebe",
    "subcomponent": "Java Client",
    "context": "Enhancements",
    "title": "Support migrating a process instance from the Java client",
    "releaseNoteText": "Users were unable to migrate process instances via the Java client's API. The logic behind the migration of several process instances, using a reusable migration plan, was inefficiently administered.\nThe Java client lacked a proper implementation of interfaces for managing the migration of a process instance.\nImplemented the necessary interfaces in the Java client for managing the migration of process instances. This made it easier to reuse a migration plan multiple times to handle several processes.\nUsers can now efficiently migrate process instances using the Java client's API. By reusing a migration plan, several process instances can be managed in a more streamlined manner. The system is no longer blocked from performing such processes."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/13558",
    "component": "Zeebe",
    "subcomponent": "Java Client",
    "context": "Enhancements",
    "title": "Java client supports multi-tenancy for BroadcastSignal RPC",
    "releaseNoteText": " \nThe Java client could not support multi-tenancy while broadcasting tenant-aware signals in Zeebe. Attempting to broadcast a signal of a tenant without required authorization or providing invalid/missing tenant id led to PERMISSION_DENIED or INVALID_ARGUMENT errors.\n \nThere was no optional `tenantId` property/method in `BroadcastSignalCommand` to support multi-tenancy during broadcasting of signals. \n \nThe Java client now exposes an optional `tenantId` property/method in `BroadcastSignalCommand` to allow multi-tenancy support. The system's ability to recognize and appropriately respond to situations such as unauthorized access or improper arguments has been improved.\n \nIn the updated system, when attempting to broadcast a signal of a tenant using the Java client, users will need to provide a proper `tenantId`. Upon correct authorization and tenant id, multi-tenancy will be supported seamlessly. Incorrect access or arguments will still result in PERMISSION_DENIED or INVALID_ARGUMENT errors, maintaining system security and integrity."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15153",
    "component": "Zeebe",
    "subcomponent": "Go Client",
    "context": "Enhancements",
    "title": "Go client commands allow specifying a `tenantId` / `tenantIds` property",
    "releaseNoteText": "Prior to this bug fix, users of the Zeebe Go client were unable to specify `tenantId` and `tenantIds` when executing key commands, therefore it did not support multi-tenancy.\nThis was due to the omission of methods to set a `tenantId` in the `DeployResource`, `CreateProcessInstance`, `PublishMessage`, `BroadcastSignal`, `EvaluateDecision` commands, and `tenantIds` in the `ActivateJobs` command in the Zeebe Go client.\nMethods for setting `tenantId` were added to the `DeployResource`, `CreateProcessInstance`, `PublishMessage`, `BroadcastSignal`, `EvaluateDecision` commands, and `tenantIds` to the `ActivateJobs` command of the Zeebe Go client.\nNow, the Zeebe Go client allows users to specify `tenantId` and `tenantIds`, hence supporting multi-tenancy. Those commands can now be utilized in multi-tenant settings, allowing for better convenience and improved functionalities."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14477",
    "component": "Zeebe",
    "subcomponent": "Go Client",
    "context": "Enhancements",
    "title": "Add multi-tenancy support in the Zeebe Go client",
    "releaseNoteText": "Users of the Zeebe Go client were unable to utilize multi-tenancy, restricting their ability to separate, isolate and secure different tenant environments.\nThe Zeebe Go client did not historically include multi-tenancy support in its architecture, resulting in limited functionality when managing multiple tenants.\nThe issue was slated for attention and underwent investigation, resulting in multiple sub-tasks needed to support multi-tenancy. However, the issue was de-prioritized and not resolved in this update.\nAt the moment, the Zeebe Go client still does not offer multi-tenancy support as the feature has been de-prioritized. The solution timeline for this feature is currently unclear."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/6150",
    "component": "Zeebe",
    "subcomponent": "Go Client",
    "context": "Enhancements",
    "title": "go-client: Job worker polling backoff mechanism",
    "releaseNoteText": " Previously, errors received from the gateway by the job poller in the Go client did not trigger a delay for the next poll for available jobs, potentially causing inefficient use of resources.\n The Go client lacked a backoff mechanism to delay the next job poll following a gateway error, unlike its Java counterpart.\n A backoff mechanism was implemented for the Go client's job worker, mirroring the functionality of the Java client's backoff process. This includes a default exponential backoff mechanism with configurable settings, and the option for users to apply their own backoff strategy.\n Now, when there's an error received on the poll for available jobs, the Go client's job worker applies the backoff mechanism, efficiently managing resources. The added customization options also give users greater control over error handling."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15675",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "title": "Process instance migration supports multi-tenancy",
    "releaseNoteText": "Process instance migration failed to support multi-tenancy, causing commands unauthorized for specific tenants to be improperly processed.\nThe system was not able to discern commands appropriate for the tenant that owned the process instance intended for migration, thus allowing unauthorized commands to pass through.  \nThe function for process instance migration was updated to verify each command's authorization before allowing migration. \nNow, the system correctly rejects any command that is not authorized to work with the tenant that owns the intended process instance. This improves the reliability of the process instance migration for multitenant environments."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15659",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "title": "Revert: Migrate task instances with recreated event subscriptions",
    "releaseNoteText": "Users experienced trouble with process instance migrations due to unsuccessful recreation of event subscriptions. This malfunction was observed after the implementation of Migrate task instances (#15407). \nThis was because the underlying logic in #15407 did not properly account for the resetting of timers during a migration, leading to malfunctioning event subscriptions.\nA decision was made to revert the changes introduced by #15407 to alleviate the problem. This was the best immediate solution, as it was not possible to implement a quick fix for the bug until the minor release.\nWith the reversal of changes, the process instance migrations are now operating as expected without any issues related to event subscriptions. Users are able to smoothly navigate through the application without encountering the previously faced trouble."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15568",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "title": "Sync Process Instance Migration gRPC documentation with gateway.proto",
    "releaseNoteText": "In the past, users encountered confusion and inconsistencies because the response codes and their explanations were not in sync between our Camunda 8 gRPC API documentation and the 'gateway.proto' file.\nThe discrepancy occurred due to some updates and modifications that were implemented in the 'gateway.proto' file, but these changes did not reflect in the gRPC documentation in a timely manner.\nWe have now updated and synchronized the gRPC documentation with the 'gateway.proto' file. We made certain that the error code explanations in both resources are identical and consistent.\nCurrently, users can expect a consistent and unified view of the response codes and their explanations across our Camunda 8 gRPC API documentation and the 'gateway.proto' file. This synchronization will enhance the user experience by removing any discrepancies and potential confusion.\n"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15407",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "title": "Migrate task instances with recreated event subscriptions",
    "releaseNoteText": "Previously, during the migration of a process instance, issues arose when there existed event subscriptions in the process instance or events in the target process that needed to be subscribed to. This complexity was due to the fact that event subscriptions existed on other partitions than the process instance, particularly message subscriptions.\nThe problem was due to the structure of the system where event subscriptions could exist on partitions separate from the process instance, making migration of these instances complicated.\nThe engineering team addressed this by implementing a mechanism to unsubscribe each active element instance in the process instance and concurrently subscribing to new events in the target process for each migrated element instance.\nNow, the migration process is smoother with an efficient way of dealing with existing event subscriptions. Event subscriptions can properly migrate alongside their respective process instances, meaning users will experience fewer migration-related issues."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15391",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "title": "Allow setting `scope` parameter for OAuth2 authentication",
    "releaseNoteText": " Previously, users were not able to set the `scope` parameter for OAuth2 authentication when using the Zeebe java client. This prevented support for authentication via Microsoft Entra ID (Azure AD) and generic OpenID Connect which required this parameter.\n The limitation was due to the absence of a feature in the Zeebe client for setting the `scope` parameter in the token request, a necessary specification particularly for OAuth2 authentication involving Microsoft Entra ID among others.\n A provision has been added in the Zeebe java client to support the setting of the `scope` parameter in the token request. This involved integrating support for environment variable configuration.\n Now, the `scope` parameter can be set in the Zeebe java client, which enables successful OAuth2 authentication for Microsoft Entra ID (Azure AD) and generic OpenID Connect. Users can now specify this `scope` parameter based on their specific authentication requirements."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15389",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "title": "Configure `identity-sdk` through `identity-spring-boot-starter` in the Gateway",
    "releaseNoteText": " Previously, any update to the Identity configuration had to be manually added to Zeebe's configuration classes, creating a dependence of Zeebe on Identity properties.\n This issue arose due to Zeebe's inability to integrate the `identity-spring-boot-starter` and dynamically adapt to changes in the Identity configuration properties.\n Adjustments were made to the `IdentityInterceptor`, `Gateway`, `StandaloneGateway`, and `EmbeddedGatewayService` classes. They now create an `Identity` instance using an `IdentityConfiguration` instance, which is passed as a parameter to the constructors of these classes. The `StandaloneBroker` class now also passes the `IdentityConfiguration` bean, which allows for automatic configuration of Identity properties.\n Zeebe can now integrate the `identity-spring-boot-starter` and dynamically adapt to changes in the Identity configuration properties. This decouples it from manual updates every time a new Identity configuration property is added."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15382",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "title": "Add integration test to `MultiTenancyOverIdentityIT` test",
    "releaseNoteText": "Previously, users could experience inconsistent behaviour while using the `MultiTenancyOverIdentityIT` test because it lacked necessary integration tests.\nThis was due to the absence of exhaustive IT tests in the `MultiTenancyOverIdentityIT` class.\nComprehensive integration tests were added to the `MultiTenancyOverIdentityIT` test, using `shouldCompleteJobForTenant` as an example.\nNow when using the `MultiTenancyOverIdentityIT` test, users can expect consistent and reliable results due to the improved integration testing."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15366",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "title": "Support `UpdateJobTimeout` RPC in Go Client",
    "releaseNoteText": "Previously, the Go Client did not support the `UpdateJobTimeout` RPC function, limiting its capabilities.\nThis issue arose because the Go Client was initially designed without including the Job Update Timeout command, similar to the Job Update Retries command.\nThe `UpdateJobTimeout` RPC was incorporated into the Go Client's functionalities.\nNow, users can successfully use the `UpdateJobTimeout` RPC in the Go Client, enhancing its operational scope and effectiveness."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15306",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "title": "Support ElasticSearch 8.9+",
    "releaseNoteText": " Users experiencing bugs and common vulnerabilities and exposures (CVEs) were compelled to upgrade their ElasticSearch instances to 8.9.\n The ElasticSearch dependencies in Zeebe were not initially upgraded to support 8.9+ even though frequent minor updates from ElasticSearch typically provide necessary bug-fixes.\n The update was implemented by manually updating the ElasticSearch dependencies in Zeebe to version 8.9+. In addition, the dependency in zeebe-analytics was also updated to support this.\n The system is now aligned with the upgraded ElasticSearch version 8.9+, subsequently helping users to address bugs and CVEs in the updated ElasticSearch instances."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15193",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "title": "Update `broker.yaml.template` and `broker.standalone.yaml.template` files in order to allow user to disable form exporting",
    "releaseNoteText": "Users were previously unable to disable form exporting as there was no option provided in the `broker.yaml.template` and `broker.standalone.yaml.template` files.\nThe provision to disable form exporting had not been incorporated into the `broker.yaml.template` and `broker.standalone.yaml.template` files. \nThe `broker.yaml.template` and `broker.standalone.yaml.template` files were updated to include an option that allows users to disable form exporting.\nUsers are now able to disable form exporting by modifying the option in the `broker.yaml.template` and `broker.standalone.yaml.template` files."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15140",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "title": "Export compensation subscription events",
    "releaseNoteText": "Previously, compensation record for BPMN compensation events were not exported to Elasticsearch and OpenSearch.\nThis omission occurred due to lack of implementation in the data export phase of the Zeebe engine.\nWe addressed this by following our development guide, making necessary changes to support record values in the Elasticsearch exporter as suggested by pull request [#15392](https://github.com/camunda/zeebe/pull/15392).\nNow, the system correctly exports BPMN compensation event records to both Elasticsearch and OpenSearch."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15114",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "title": "Migrate task instances with variables",
    "releaseNoteText": " Previously, users were unable to migrate task instances that contained variables. This limitation hindered process flexibility and adaptability.\n This issue was a result of the system not having a `VariableIntent` `MIGRATED` functionality. There was neither a process to update variable properties (`bpmnProcessId`, `processDefinitionVersion`, `processDefinitionKey`), nor a command processor to apply these changes to all variable instances in a process.\n A new `VariableIntent` called `MIGRATED` was introduced to assist in the migration of variables associated with task instances. An event applier was added to update crucial variable properties such as `bpmnProcessId`, `processDefinitionVersion`, and `processDefinitionKey`. Furthermore, the `ProcessInstanceMigration` command processor was amended to append a `Variable:MIGRATED` event for each variable and child element instance of the process. Care was taken to clear the variable's `value` when appending the event to avoid exceeding the result records batch size.\n Now, users can migrate task instances with variables. Variables remain in their original scope, ensuring continuity of process functionality. This update has improved the flexibility of the process instance, allowing for variable use in activated jobs and updates through Set Variables RPC."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15113",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "title": "Migrate service task instances (including jobs)",
    "releaseNoteText": " Users were not able to migrate service task instances, including jobs, from one process definition to another. This limitation hindered the flexibility and dynamism of process management.\n The system lacked the necessary mechanism to modify service task properties during migration, and no `JobIntent` was available to signify a job migration. The `ProcessInstanceMigration` command processor was not set up to handle service type `SERVICE_TASK`, which left the job's events unprocessed during migration.\n A new `JobIntent` named `MIGRATED` was introduced. An event applier was added to update the job properties, including `bpmnProcessId`, `processDefinitionVersion`, `processDefinitionKey`, and `elementId` during migration. Furthermore, the `ProcessInstanceMigration` command processor was updated to append `ProcessInstance:ELEMENT_MIGRATED` event for each child instance and a `Job:MIGRATED` event if the child is a service task.\n Users can now successfully migrate service task instances, including jobs, from one process definition to another. All job commands remain functional post-migration ensuring seamless process management."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15063",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "title": "Introduce new compensation subscription record",
    "releaseNoteText": " \nPreviously, there was no way to track compensation events within processed activities. This impacted the functionality as it became hard to find completed activities with compensation handlers and track the state of these handlers.\n \nThis limitation was due to the absence of a dedicated record for compensation events. This could store essential data like tenant id, process instance key, process definition key, element ids, compensation throw event details, and local variables involved with the compensation handler.\n \nA new record for compensation events was added. The lifecycle states (CREATED, TRIGGERED, COMPLETED, DELETED) for this record were introduced, each state change triggering an update to the subscription in the state.\n \nNow users can track compensation handlers' progress and identify the resources involved effortlessly. Hence it enhances the clarity and manageability of completed activities involving compensation events, providing a more transparent view to the end user."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15034",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "title": "Add Job Update Timeout intents",
    "releaseNoteText": "Consequence:\nPreviously, if no deadline was found for a job in its state or the job itself was not found in the state, the system would return a `NOT_FOUND` rejection. \nCause:\nThe issue stemmed from the `JobIntent` class not having an `UPDATE_TIMEOUT` command to check the state for the job and its deadline. Additionally, `JOB_DEADLINES` ColumnFamily, keyed by the deadline-jockey, could not be updated directly and had no access to the old deadline information.\nTwo new intents were added to the `JobIntent` class: `UPDATE_TIMEOUT` and `TIMEOUT_UPDATED`. The `UPDATE_TIMEOUT` command finds the job in the state and checks its deadline. The `TIMEOUT_UPDATED` event updates the `JOB_DEADLINES` ColumnFamily by removing the existing job entry and inserting a new one simultaneously. This was facilitated by retrieving old deadline data stored in the state through the JobRecord.\nCurrently, if a job or its deadline can't be found in the state, the `UPDATE_TIMEOUT` intent issues a `NOT_FOUND` rejection. The `TIMEOUT_UPDATED` intent successfully modifies the deadline in the state, effectively updating the `JOB_DEADLINES` ColumnFamily."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14965",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "title": "I can execute a process with an intermediate compensation throw event",
    "releaseNoteText": " Previously, users could deploy a process with a compensation intermediate throw event but were unable to execute this process. The records for the compensation intermediate throw event did not have the correct event type.\n The system was not handling the invocation of compensation handlers correctly, preventing the completion of a process instance that contained a compensation intermediate throw event.\n We implemented a fix that enables the correct activation and completion of the compensation intermediate throw event. We have also ensured that the records for the compensation intermediate throw event now have the event type `COMPENSATION`. \n Users can now fully execute a process that includes a compensation intermediate throw event. The compensation intermediate throw event is both activated and completed successfully, with the correct event type set in the records."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14944",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "title": "The record for a compensation event has the correct event type",
    "releaseNoteText": " Previously, when executing a BPMN process, the recording of compensation events might have been ambiguous or omitted due to a missing event type 'COMPENSATION' for process instance value type events. \n This situation was created because the BPMN system didn't have a dedicated 'COMPENSATION' type for its bpmnEventType field, causing the compensation events to be inaccurately or not categorized.\n A new event type 'COMPENSATION' has been introduced and applied to all related BPMN compensation events (including but not limited to compensation intermediate throwing events, compensation end events, compensation boundary events, and compensation start events). \n Now, all events related to BPMN compensation event have the appropriate 'COMPENSATION' event type. Users can now accurately track compensation events, leading to more precise process execution monitoring in BPMN workflows.\n"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14943",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "title": "I can deploy a process with BPMN compensation event subprocesses",
    "releaseNoteText": " Users were unable to deploy a process with BPMN compensation event subprocesses due to existing validation rules. The system disallowed this action, limiting the complexity of processes that could be deployed.\n The current validation rules did not permit compensation start events within event subprocesses. The limitations came from restrictions that insisted that a compensation start event must exist within an event subprocess and that a compensation event subprocess should reside inside an embedded subprocess with no allowance made for the process level.\n The validation rules were modified to allow for compensation start events within event subprocesses. The said constraints related to the compensation start event and compensation event subprocesses were effectively attended to. \n Now, users can successfully deploy a process with BPMN compensation event subprocesses. The process deployment capability is now more versatile, supporting complex processes inclusive of compensation start events and event subprocesses."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14942",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "title": "I can deploy a process with BPMN compensation events",
    "releaseNoteText": "Previously, users were unable to deploy processes with BPMN compensation events due to existing validation rules.\nThe underlying issue was caused by the system's strict validation rules not allowing specific BPMN elements such as Compensation intermediate throwing event, Compensation end event, Compensation boundary event, and Activity with compensation marker (i.e., a compensation handler). \nWe adjusted the validation rules to now allow these specific BPMN elements. At the same time, we maintained safeguards by ensuring set restrictions such as a compensation boundary event should have no outgoing sequence flows while a compensation handler should have no incoming and outgoing sequence flows and no boundary events.\nUsers are now able to successfully deploy processes incorporating BPMN compensation events, while still adhering to essential restrictions for proper functioning."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14858",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "title": "Add support to Java client for Form deletion",
    "releaseNoteText": "Previously, the Java clients were unable to delete forms due to a limitation in the system. \nThe issue was rooted in the fact that support for form deletion was not incorporated in the initial development of the Java client. \nDevelopers rectified this by adding the necessary support in the Java client to allow form deletions seamlessly.\nNow, Java clients can effectively delete forms as required, improving overall usage and experience."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14701",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "title": "Replace Dependabot with Renovate",
    "releaseNoteText": "\nUsers observed unnecessary complexity in the repository due to both Dependabot and Renovate operating simultaneously. This dual operation did not cover all use cases efficiently.\nThe deployment of both Dependabot and Renovate within the repo primarily caused this complexity. Dependabot was not fulfilling all requirements, failing to match Renovate's comprehensive utility.\nTo streamline the process, all requirements from Dependabot were transferred to Renovate, and Dependabot was subsequently removed from the system. Additional tests were completed to validate all the future PRs being created. Also, Dependabot's Github related actions were removed.\nNow, users will notice a simpler and more efficient repo operation, with Renovate covering all necessary dependency updates. Future PR validation is also ensured. Dependabot related elements will no longer exist in the GitHub actions configuration."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14562",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "title": "Use caching for Forms",
    "releaseNoteText": " Before this update, Forms were retrieved from the state without utilizing any caching mechanism, which affected the performance of retrieval\n Caching was not implemented in the DbFormState, which resulted in a lack of efficiency in form retrieval, especially when compared to the handling of Decision and Process resources in their respective DbState classes.\n We introduced a caching mechanism to DbFormState, similar to what is already in place for the Decision and Process states. \n Now, the Forms retrieval is faster and more efficient, providing significant performance improvements."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14285",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "title": "Support general-usage tenant-providing interceptor for SM",
    "releaseNoteText": " Zeebe 8.3.0 users who wanted to set up their own tenant provider could not do this because the software only supported multi-tenancy over Identity.\n The tenant-providing logic was closely tied to the Identity token in the IdentityInterceptor making it impossible to extend or adapt it for custom solutions.\n We expanded the `InterceptorUtil` class to provide the `AUTHORIZED_TENANTS_KEY` gRPC context key. We also ensured that data provided by custom tenant-providing interceptors are used or ignored, based on multi-tenancy being enabled or disabled. \n Now, users can implement a custom tenant-providing interceptor with much greater flexibility. The software can utilize the data provided by custom tenant-providing interceptors when multi-tenancy is enabled, and does not when it is disabled. Users can also override the Identity tenant-providing interceptor with their custom interceptors when multi-tenancy is enabled.",
    "partOfEpic": [
      13237,
      12653
    ]
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14279",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "title": "Support multi-tenancy for Resource Deletion",
    "releaseNoteText": " \nPreviously, the system did not support tenant-aware resource deletion, meaning that resources could not be looked up using authorized tenant IDs, which restricted multi-tenancy.\n \nThe `ResourceDeletionProcessor` was not designed to lookup resources using authorized tenant IDs.\n \nWe have updated the `ResourceDeletionProcessor` to be capable of looking up resources using authorized tenant IDs. This included creating unit tests and QA tests for tenant-aware resource deletion. We also re-enabled the tests in the `TenantAwareTimerStartEventTest`.\n \nNow, the system supports tenant-aware resource deletion, allowing resources to be found using authorized tenant IDs and supporting multi-tenancy."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14272",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "title": "Support deletion of the Forms through DeleteResource gRPC",
    "releaseNoteText": "Users were previously unable to delete Forms using the DeleteResource gRPC, which limited their ability to manage resources within Zeebe.\nThe omission of Forms in the implementation of the DeleteResource gRPC function occurred during the development of epic #12874.\nThe DeleteResource gRPC function was updated to include capability for Form resource deletion.\nUsers are now able to delete Forms via the DeleteResource gRPC in Zeebe, providing more comprehensive resource management."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/13433",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "title": "Keep track of event types in the Engine",
    "releaseNoteText": "Previously, users were unable to keep track of the event types that triggered elements in Grafana as this feature wasn't available.\nThis limitation was due to the `ProcessEngineMetrics` class not tracking event types for the `element_instance_events_total` metric in the Engine. Consequently, this data wasn't being registered, preventing users from viewing these metrics in Grafana.\nWe rectified this issue by updating the `ProcessEngineMetrics` class to accommodate event types. We then updated the `element_instance_events_total` metric to include activated, completed, and terminated element instances in the data tracked by Grafana.\nNow, users are able to visualize and track event types that trigger elements in Grafana, streamlining the troubleshooting process and enhancing the overall user experience."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/13410",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "title": "Indicate the type of activated/completed/terminated catch events in Grafana",
    "releaseNoteText": "\nPreviously, there was an incident of the Engine lagging behind in processing new records. This was observed through Grafana, however, the specific type of catch event taking CPU time was not identifiable. The issue needed further investigation in other log resources for a root cause analysis.\nThe problem was caused by a timer catch event that continuously generated new records, overloading the thread pool. In Grafana's `General Overview -> Current Events` view, it wasn't indicated which type of catch event was causing the issue, limiting the detailed investigation. \nThe solution involved adding the catch event types to the aforementioned Grafana view. Thus, each element instance contained its own type which improved detailed logging. \nNow, the Grafana `General Overview -> Current Events` view portrays each element's type, providing a more comprehensive and accessible log. This enhancement speeds up troubleshooting and negates a need for seeking additional log resources. The view is now equipped with activation, completion, and termination information of catch events."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/13336",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "title": "Zeebe supports multi-tenancy for BPMN signal events",
    "releaseNoteText": "Previously, Zeebe did not have multi-tenancy support for signal broadcasting. \nThis was because the `SignalSubscriptionState` could not persist the `tenantId`, and the engine was unable to process `SignalRecord` commands with a tenant id.\nTo address this, elements were worked on including: ensuring the `SignalSubscriptionState` could persist the `tenantId`, enabling the engine to process `SignalRecord` commands with a tenant id, and updating all impacted Elasticsearch/Opensearch record templates to include a `tenantId`. Additionally, a data migration for ProcessingStates used by signal broadcasting was provided.\nZeebe now supports multi-tenancy for BPMN signal broadcasting, allowing more flexible and secure handling of signal broadcasts across different tenants.",
    "partOfEpic": [
      1774,
      12653
    ]
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15220",
    "component": "Zeebe",
    "subcomponent": "Broker",
    "context": "Bug Fixes",
    "title": "Failed or cancelled topology change operations block further operations",
    "releaseNoteText": "Previously, any failed or cancelled topology change operations blocked subsequent operations. ClusterTopologyManager failed to clear the flag which signaled an ongoing topology change operation when a change was cancelled or an operation failed to initialize.\nThe issue was caused because the 'onGoingTopologyChangeOperation' flag in the ClusterTopologyManager was only reset (set to 'false') when an operation was successful. If an operation was cancelled or failed to initialize, this flag wasn't appropriately reset, thus blocking subsequent operations.\nWe adjusted the 'ClusterTopologyManager' so that the 'onGoingTopologyChangeOperation' is set to 'false' not only when an operation is successful, but also after an operation fails or is cancelled.\nNow, even in the event of failed or cancelled topology change operations, subsequent operations can be initiated without being blocked. The system handles operations more efficiently, providing a smoother user experience."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15219",
    "component": "Zeebe",
    "subcomponent": "Broker",
    "context": "Bug Fixes",
    "title": "Topology received via gossip can bypass merge with locally persisted topology",
    "releaseNoteText": "Previously, brokers had an issue where they might forget about already applied operations. This was due to the new topology received over gossip before the `TopologyInitializer` had finished its process, subsequently bypassing the merge with the locally persisted topology.\nTechnically, this occurred because the local topology was uninitialized at the time the new topology was received. This led to the new topology being preserved without merging with the local one.\nThe issue was resolved by ensuring that topology updates received via gossip do not overwrite an uninitialized topology. The only exception was made for the `GossipInitializer`.\nBrokers now recall already applied operations accurately. Topology updates received over gossip will not overwrite an uninitialized topology, with the previously stated exception applying. This ensures the accurate preservation and merging of both new and local topologies."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14509",
    "component": "Zeebe",
    "subcomponent": "Broker",
    "context": "Bug Fixes",
    "title": "Potential inconsistency in raft after restoring from a backup",
    "releaseNoteText": "  \nAfter restoring Zeebe brokers from backup, some partitions encountered an error that could lead to potential inconsistency. This problem was caused by a misalignment in the log index which failed to delete a certain number of older operations. This problem resulted in your Zeebe cluster either being safe to restart if there is no new data written, or a partial data loss if new user requests have been processed. High disk usage was also observed due to logs not being compacted.\n  \nThe issue occurred because, after restoring from the backup, the raft metastore - a component of the Zeebe broker which stores crucial data required for the raft consensus algorithm, including the current term and voted candidate - was empty, leading to an incorrect restart of the term from 1. This caused an illegal state exception and resulted in the cluster going into an inconsistent state where new data could not be written if the inconsistency occurs after restore, or potential loss of newly written data.\n  \nMade modifications using the approved changes in [PR #15272](https://github.com/camunda/zeebe/pull/15272). The changes include adjusting the state of the raft metastore, ensuring the correct term is used after restoration from backup, safeguarding from deletions that can lead to inconsistencies.\n  \nFollowing the fix, after restoring from a backup, there is no inconsistency found and the raft continues to work in all scenarios. There is no potential for data loss irrespective of whether new data is written or not. The logs are also compacted regularly, avoiding high disk usage."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14465",
    "component": "Zeebe",
    "subcomponent": "Broker",
    "context": "Bug Fixes",
    "title": "Deleting a process definition only checks running instances and doesn't ignore banned instances",
    "releaseNoteText": " Users were previously unable to delete a process definition if there was a banned instance associated with it. Additionally, if the definition was deleted and the command was sent to a partition with a banned instance, the command would keep retrying indefinitely, causing state inconsistency.\n The issue occurred because the check for running instances during the deletion of a process definition did not account for banned instances. This gap in the check meant that definitions with banned instances were always rejected. Furthermore, this check was only carried out on the partition that received the command, leading to possible discrepancies between partitions.\n The check for running instances during deletion of a process definition has been updated to exclude banned instances. The check has also been improved to consider all partitions, ensuring consistent behaviour across different partitions.\n Users can now delete a process definition even when there are banned instances. Moreover, the deletion command will not continuously retry on a partition with banned instances, ensuring that users can consistently start new instances across all partitions."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14109",
    "component": "Zeebe",
    "subcomponent": "Broker",
    "context": "Bug Fixes",
    "title": "Static user task assignee value treated as number (leads to an exception)",
    "releaseNoteText": "Users encountered an exception when setting a user task assignee to a static number value. This resulted in improper activation of the user task.\nThe issue was rooted in the system interpreting a static number value as an actual 'NUMBER' data type, instead of treating it as a 'STRING' as expected, leading to a 'NUMBER' type exception.\nThis issue was fixed by ensuring that static number values are treated as string values by the system, not as number literals. This was achieved by enhancing the system's interpretation mechanism.\nWith this fix, users can now set user task assignees to static number values without encountering any exceptions. The system appropriately treats these static number values as 'STRING' data types, ensuring successful activation of user tasks."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15677",
    "component": "Zeebe",
    "subcomponent": "Gateway",
    "context": "Bug Fixes",
    "title": "Interceptor order is not respected",
    "releaseNoteText": " \nUsers experienced mismatched gRPC interceptor calls, as the order was not always consistent with the configuration settings.\n \nThe underlying issue was a disregard for the specific order in which interceptors were added to the configuration within the `IdentityRepository`.\n \nThe `IdentityRepository` was refactored to ensure the interceptors are initiated in the same order as they have been added in the configuration.\n \nUsers will now observe that gRPC interceptors are consistently called in the precise order as they are added in the configuration, as per the user's preference.\n"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14176",
    "component": "Zeebe",
    "subcomponent": "Java Client",
    "context": "Bug Fixes",
    "title": "ActivateJobsCommandImpl throws NullPointerException when ZeebeClientProperties.getDefaultJobWorkerName is null",
    "releaseNoteText": " Users encountered a `NullPointerException` when trying to implement the `ZeebeClientProperties` and returning `null` for the `getDefaultJobWorkerName`.\n This issue arose because the `ActivateJobsCommandImpl` constructor attempted to set the `workerName` during construction and anticipated the `config.getDefaultJobWorkerName()` to be non-null. Null values were not supported by the underlying builder, causing a `NullPointerException`.\n The software was adjusted to ensure that the builder falls back to the default worker name only if it hasn't been set by a client while constructing the final command.\n Now, even if the Zeebe Client Properties returns `null` for the `getDefaultJobWorkerName`, the service continues to function without throwing a `NullPointerException`. The builder will use the default worker name when none is set by the client during construction of the final command. This results in a more robust system with improved error handling."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15726",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "title": "After cancellation, a completed change operation should not overwrite updated topology",
    "releaseNoteText": "In the past, the software failed to maintain updated topology changes upon the cancellation of a change operation. This led to the overwrite and subsequent loss of crucial topology adjustments, causing disruptive breaches in the network infrastructure. \nThe root problem was from a sequencing misalignment in the cancellation and overwrite mechanisms. When a change operation was cancelled, the programming logic did not consider subsequent topology updates that occurred during the operation, resulting in the overwrite of the updated topology.\nA modification in the programming logic was implemented to account for topology updates during a change operation. Should a cancellation occur, the system would then preserve these updates instead of overwriting them.\nToday, after a change operation is cancelled, any completed topology updates are preserved and not overwritten. This ensures the integrity of the network infrastructure is maintained, leading to a more robust, reliable system."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15447",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "title": "Special characters in static inputs should not be escaped",
    "releaseNoteText": "Users previously noticed that special characters in static inputs were incorrectly escaped. For example, a simple input like `Hello\\n\\nYOU!` turned into `\"Hello\\\\\\\\n\\\\\\\\nYOU!\"`.\nThis occurred due to an oversight in the string processing component of the system. The function responsible for handling static input was incorrectly escaping special characters.\nThe team adjusted the static input handler to properly treat special characters. We modified the string processing functionality to prevent unwanted character escape.\nNow, the system correctly handles special characters in static inputs. Inputs like `Hello\\n\\nYOU!` are preserved exactly as intended, improving overall system reliability and user experience."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15445",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "title": "Verify correct behaviour of input mappings after bump to FEEL 1.17.3",
    "releaseNoteText": "Users had to deal with special characters inside input mappings being escaped. \nThe execution of FEEL 1.17.3 was escaping special characters, due to an issue in the coding of the software.\nThe software was updated to FEEL 1.17.3, which contained the appropriate code to correct this issue. \nUsers can now incorporate special characters inside input mappings without them being automatically escaped."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15381",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "title": "NullPointerException when join is retried after a restart",
    "releaseNoteText": " \nUsers testing the impact of pod restarts during dynamic scaling faced a `NullPointerException` error which led to the partition join being stuck. \n \nThe issue was caused by the fact that when a join request was retried after a server B restart, the server B was assumed by the leader to already have the configuration. When an append request was sent to server B, a null pointer exception was returned as the configuration was held in memory and was lost when server B restarted.\n \nTwo fixes were implemented:\n1. The leader was updated to resend the configuration when it receives a duplicate join request allowing the follower to handle the null pointer exception by rejecting the AppendRequest. This allowed the follower to eventually receive the new configuration.\n2. The `configurationIndex` was added to the `AppendResponse` so that the leader can resend the configuration if necessary.\n \nNow, when a pod restart happens during dynamic scaling and a join request is retried, a null pointer exception doesn't stop the partition join process. Instead, the leader correctly resends the configuration ensuring system stability."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15343",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "title": "IT timeout due to `ScaleResiliencyTest.ScaleDown`",
    "releaseNoteText": " Users had experienced frequent test failures caused by timeout during operations. This issue prevented merging of some changes, impacting workflow and productivity.\n The issue was linked to the `ScaleResiliencyTest.ScaleDown` process. An error occurred when Broker-2 attempted to leave partition 2, but couldn't due to Broker-1's unavailability. The commit of the first part of the joint consensus happened before Broker-2 restart. Upon Broker-1 restart, sometimes the partition 2 in Broker-2 was not yet up, causing the leader to commit the second part of the joint consensus before Broker-2 was ready. This led to Broker-2 waiting indefinitely for partition 2 to be ready, resulting in a system timeout and test failure.\n The issue was resolved by implementing a system control to ensure that the Broker-2 becomes ready prior to the start up of Broker-1. This provided a proper sequence for the commit of both parts of the joint consensus, ensuring the readiness of partitions before any further actions were taken.\n Users can now smoothly conduct their operations and tests without encountering timeout issues. The fix allows for changes to be merged successfully, enhancing workflow and productivity. Known issues with the `ScaleResiliencyTest.ScaleDown` process have been addressed, ensuring seamless operation."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15194",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "title": "Form events are not replayed",
    "releaseNoteText": "\nUsers experienced the loss of deployed forms in versions 8.3 and 8.4.0-alpha1, which led to incidents being raised at User Tasks due to these missing forms. As a consequence, the system failed to replay the form events, causing discrepancies in the state.\nThe underlying issue was a glitch in the replay mode where form events were missed, causing the system not to add them to the state. This led to a scenario where existing jobs of User Tasks were still referring to the key of the missing forms.\nThe technical team addressed the glitch in the replay mode to ensure all form events were duly recorded and added to the state. Simultaneously, measures were taken to prevent the loss of deployed forms.\n \nNow, when instances activate User Task with form id set, no Incident arises as no Form is missing. All form events are replayed correctly in the system, maintaining the state accuracy. Also, users no longer have to redeploy the form, eliminating chances of repeatedly experiencing the bug."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15188",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "title": "Could not update new leader for partition",
    "releaseNoteText": " The update of a new leader for a partition was unsuccessful, causing difficulties in managing the partition, which could potentially impact the efficiency of data processing and handling.\n The system expected to find a non-null value for the current leader term but found a null value instead, resulting in an inability to update the new leader for the partition. The specific causing factors weren't discernible from the log.\n An investigation was conducted to clarify the issue and subsequently, appropriate adjustments and validations were implemented to ensure the correct retrieval and assignment of the non-null leader term value, thereby optimizing the leader update process.\n A partition leader now gets updated accurately and efficiently, ensuring improved management of partitions and data processing. Any future attempts to update a new leader for a partition should no longer encounter the same problem."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15129",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "title": "Avoid executing process according to unprocessed commands referring to source process definition",
    "releaseNoteText": " In the past, during migration of process instances not entirely in a wait state, Zeebe would incorrectly execute the process instance as if the migration had not happened. This was due to unprocessed commands referring to the source process definition being on the log stream with a higher position than the process instance migration command.\n The cause of this issue was that the system wasn't designed to handle process instances that weren't entirely in a wait state during migration. There was a lack of clarity on how to deal with this issue.\n The issue was fixed by choosing not to trust the process definition key in such commands, but rather to look up the process definition from the state when processing commands. This solution was less restrictive, allowing for migrations of process instances in active execution rather than only those in a complete wait state.\n Currently, during process instance migration, the process instance correctly executes after migration, whether or not the process instance is entirely in a wait state. No unprocessed commands mess up the execution. The log stream now has a more robust design that simplifies reasoning about the system and brings consistency to implementation."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15047",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "title": "8.3.x release benchmark - Authorization data unavailable: The Token can't be used before xxx",
    "releaseNoteText": "\nPreviously, user workflows were unexpectedly halted due to a DEAD partition, marked as such because of a JWT Authorization data unavailability issue. The error message \"Token can't be used before xxx\" was thrown, indicating that the system was attempting to utilize the JWT token before the allowed time. \nThis issue was rooted in a timing discrepancy. The `Instant.now()` method, which sets the `JWT#issuedAt` value, was used in a cluster with a different timezone setting. The discrepancy resulted in the JWT validation throwing an exception.\n \nThe fix considered the importance of JWT's only role as a data transport method that does not require signing and is not used outside Zeebe. Therefore, we implemented two changes:\n- The `issuedAt` claim was removed as it was redundant.\n- The JWT validation was disabled to prevent this issue from recurring for existing JWT tokens.\nIn replacement of the removed claim, we created our own validation to ensure the presence of `authorized_tenants` JWT, which can be done by checking for null when fetching the claim. \nAs it stands, users can use Zeebe without encountering a DEAD partition due to JWT token unavailability. Furthermore, there is additional validation for the `authorized_tenants` JWT to maintain system integrity. Despite the changes, the performance of workflows and operations of previous versions remain intact."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14931",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "title": "A deployed form is not mentioned in the `DeploymentEventImpl.toString()`",
    "releaseNoteText": " In the past, when users deployed a form using the Java client, the output of the DeploymentEvent did not include the deployed form. This happened even though the form was successfully deployed, causing a discrepancy in the logged information.\n The underpinning cause of this issue was that the `DeploymentEventImpl.toString()` function was not engineered to include forms in its output.\n We modified the `DeploymentEventImpl.toString()` function's implementation. The change ensures it includes forms as part of the printed output when a form is deployed with the Java client.\n Now, when a form is deployed using the Java client, the output of the DeploymentEvent correctly prints the deployed form. This adjustment provides users with accurate information about their deployed resources.\n"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14924",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "title": "IOPS doubled in CW44",
    "releaseNoteText": "Consequence:\nIn CW44, intermittent nodes were observed to perform at a rate of upto 8000 IOPS, which was double the rate of ~4000 IOPS noted in previous benchmarks. This inconsistent performance was puzzling as all nodes were expected to maintain a benchmark rate of 4000 IOPS regardless of their leadership status on partitions.\nCause:\nAfter investigation, the underlying cause of the issue was found to be duplicate metrics. Certain nodes and their metrics were being counted twice, causing an apparent doubling of the IOP rate.\nFix:\nThe issue was rectified by revising the method used to count metrics and removing the potential for duplication. This fix followed a similar resolution approach to an earlier CPU related problem (ref: camunda/zeebe/pull/13386).\nResult:\nWith the correction of the metrics calculation, the nodes now perform consistently without apparent unexpected surges in IOPS. The performance rate of the nodes is now reliably at 4000 IOPS, eliminating the intermittent spikes observed previously."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14884",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "title": "Improve resilience of remote stream restart",
    "releaseNoteText": " Previously, brokers inappropriately requested all members to restart their streams whenever a new member was added, which created disruptions. Moreover, in the case of failure, no retries were implemented, which could result in interrupted and unknown streams on the brokers.\n The primitive behaviour of brokers where they sent restart requests to all members, not just gateways, was the cause of the issue. Coupled with the lack of a retry mechanism for failures, this led to susceptibility to errors and interruptions in the stream.\n The software was modified to ensure that restart requests are now directed exclusively at gateways, and not all members. Additionally, a retry mechanism with back-off strategy was introduced to handle any request failures.\n As a result of this fix, when a new member is added, only gateways restart their streams, reducing disruption significantly. If a request fails, the system automatically retries till it succeeds, ensuring constant and uninterrupted streams on the brokers."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14837",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "title": "UDP resource leaks",
    "releaseNoteText": " \nIn versions CW42 and CW43, a major UDP resource leak was observed, which eventually resulted in all UDP listening connections reaching their maximum limit. This constraint prevented any new listening connections from being created, leading to substantial DNS failures. As a result, data could no longer be exported, causing the system to run out of disk space.\n \nThe issue emerged due to a constant increase in the number of open UDP connections, specifically related to the Java process. This behaviour did not conform to the expected setup where only two constant connections should be operating, namely the Netty unicast service and sporadic temporary connections.\n \nThe error was identified and a patch was applied to control the creation and maintenance of UDP connections by the Java process. The fix ensured that only the necessary UDP connections (Netty unicast service and temporary ones) would be operational.\n \nWith the implementation of the patch, the system is now equipped to correctly maintain the UDP connections. This results in the conservation of system resources, as indicated by adequate free disk space and efficient data export processes. Furthermore, the risk of systemic DNS failures due to UDP listening connection overflow is now mitigated."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14814",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "title": "Leaked message subscriptions cause constant processing load",
    "releaseNoteText": " \nIn Zeebe 8.2.12 Cluster, system users experienced a consistent high processing load due to leaked message subscription issues.\n \nThe problem was caused by a discrepancy between partitions. On one partition, the subscription column had an entry marked as correlating that continued to send `PROCESS_MESSAGE_SUBSCRIPTION/CORRELATE` repetitively. However, another partition lacked this entry. The underlying issue was a lost `MESSAGE_SUBSCRIPTION/CORRELATE` or `MESSAGE_SUBSCRIPTION/DELETE` command, leading to the persistence of the subscription entry. \n \nThe technical fix addressed this persistent state and handled non-interrupting subscriptions by removing them from the state immediately, then sending `MESSAGE_SUBSCRIPTION/CORRELATE` one time. It also resolved the issue by cleaning up the subscription once the receiving partition rejected it. \n \nAs a result of this fix, the partition issues are resolved, reducing system load. The Zeebe 8.2.12 Cluster no longer has repeating `PROCESS_MESSAGE_SUBSCRIPTION/CORRELATE` commands, which reduces rejections and optimizes the overall processing capacity of the software. The system no longer gets stuck in a broken state and operates with improved efficiency."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14773",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "title": "Job streams not aggregated correctly on gateway",
    "releaseNoteText": " Previously, the aggregation of job streams on the gateway was not functioning correctly. Despite having the same parameters, two identical job streams were assigned different server stream IDs.\n This issue originated from our usage of `ArrayProperty` and `ArrayValue`. The `equals` and `hashCode` implementations for these relied on the last modification being reflected in their internal buffer, which was not always accurate.\n We addressed this issue by redefining the consistency between `equals` and `hashCode` methods for `ArrayProperty` and `ArrayValue` to reliably reflect each modification.\n Now job streams with identical parameters will be accurately aggregated on the gateway and correctly assigned the same server stream ID."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14771",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "title": "Job streams not registered on broker after some time",
    "releaseNoteText": "Before the fix, brokers were losing track of registered job streams after a certain period of operation. Despite this, the gateway actuator reported the streams as being connected to all brokers, leading to inconsistent gateway and broker stream states. \nThe underlying cause of the issue was that when a broker was locally removed and re-added, the system did not send a restart streams request to the gateway. Essentially, the streams were not recreated as expected, leading to the said inconsistency.\nA correction was made to rectify the flawed process. The system was tweaked to ensure that a restart streams request is sent to the gateway each time a broker rejoins, prompting the streams to be recreated successfully.\nWith the fix applied, the system ensures that the gateway and broker stream states remain consistent as they are supposed to. Brokers no longer lose track of the job streams, thus resolving the initial issue."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14699",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "title": "IllegalStateException: found snapshot 0. A previous snapshot is most likely corrupted",
    "releaseNoteText": " Users experienced an `IllegalStateException` during the startup of the Zeebe broker, indicating a likely corruption. This problem caused the broker to get stuck in a bootloop, impacting its availability and functionality.\n The broker encountered snapshot 0 due to inconsistencies in saving state resulting from a failure of the underlying file I/O operations. Notably, the system had a race condition where the checksum file was created but was left empty due to an unexpected crash during the write operation.\n A safeguard was implemented to handle file I/O operations more intelligently, preventing the creation of empty checksum files and ensuring a smoother recovery from unexpected crashes. Synchronization issues around snapshotting were also resolved.\n The Zeebe broker now handles potential data corruption more gracefully. On bootup, file integrity checks are robust and the chance of handling and recovering from \"snapshot 0\" errors has been significantly improved. The broker's overall stability, in terms of data consistency and recoverability, has been enhanced.\n"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14663",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "title": "Regular OOM when replicating large state due to messaging timeouts",
    "releaseNoteText": " Users experienced repeated out-of-memory errors when replicating a snapshot larger than 4GB. This disruption was mainly caused by the 'NettyMessagingService' timeout executor.\n The error occurred due to the code design where the scheduled lambda closed over the complete message. When the snapshot replication involved large files, messages being in double (or at worst, triple) digit megabytes quickly exhausted memory resources.\n We've revisited the underlying code to optimize memory usage. Now, instead of closing over the complete message, which was used only for logging, we've kept its scope to a bare minimum, preventing unnecessary piling up of memory resources.\n With this fix, Zeebe Version 8.3.1, users would no longer face out-of-memory issues during large snapshot replication, significantly improving the stability and reliability of the service."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14624",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "title": "MsgpackReaderException when reading TenantId",
    "releaseNoteText": "\nUsers of release-8-3-0 benchmark encountered a MsgpackReaderException error while trying to read TenantId. This resulted in the system being unable to process certain records due to not being able to determine the string type. This error prevented the job push feature, which is in its experimental phase, from being usable.\nThe problem was due to the job streamer, an object that exists once per broker and is shared between partitions on the broker. If a broker is leader for more than one partition, iterating through the tenants in the `JobActivationPropertiesImpl` failed as the ArrayProperty was not thread-safe. Processing was impacted because iteration over a supposed immutable `ArrayProperty` led to mutation of the object.\nAn immediate fix was carried out by cloning the job activation properties in the `BpmnJobActivationBehavior`, before obtaining the list of tenants. Since a copy was created, the issue of multiple partitions iterating over the same `ArrayProperty` was resolved.\nRelease-8-3-0 benchmark can now correctly process records without encountering the MsgpackReaderException. Users can utilise the job push feature without fear of the system throwing exceptions due to mutation of the ArrayProperty."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14588",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "title": "NPE in `PendingMessageSubscriptionChecker`",
    "releaseNoteText": "\nUsers experienced an unexpected NullPointerException (NPE) when the PendingMessageSubscriptionChecker ran concurrently to the stream processor in the Camunda Zeebe engine. This issue caused a disruption in regular processing and a drop in availability.\nThe issue was caused by a race condition. The PendingMessageSubscriptionChecker and the stream processor were both accessing a shared in-memory state concurrently. However, this state was not spread over multiple calls, leading to synchronization issues between the transient state and the persisted state.\nThe development team addressed this issue by adding a null check in PendingMessageSubscriptionChecker, which safely skips any subscription record that is removed from the state. This ensured that the MessageSubscriptionRecord could be either correlated or deleted from the state without causing an error.\nWith the fix applied, the PendingMessageSubscriptionChecker can now run concurrently to the stream processor without any issues. Therefore, there is no disruption in regular processing, ensuring constant availability. The race condition is now less likely to occur, promoting stability in the system. Users are no longer experiencing the NullPointerException (NPE), which improves the overall user experience."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14458",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "title": " NullPointerException: Cannot invoke `PartitionContext.getComponentHealthMonitor()` because `this.context` is null ",
    "releaseNoteText": "\nDuring the shutdown process, users experienced a NullPointerException (NPE) when the healthcheck service attempted to query the health of a Zeebe partition.\nThe NPE occurred due to the system attempting to invoke the `PartitionContext.getComponentHealthMonitor()` on a null context item. This was a result of the Partition Context not being fully initialized or being prematurely disposed of during the shutdown process.\nA validity check was introduced before invoking the `getComponentHealthMonitor()` method. If the context is null, the system will deem the component as unhealthy, thus preventing the NullPointerException.\nUsers no longer encounter a NullPointerException during shutdown. Instead, if a Zeebe partition's context is null during shutdown, the system will safely report the component as unhealthy."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14047",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "title": "Snapshot version of  benchmark application (starter/worker) doesn't work with SaaS",
    "releaseNoteText": " Users were unable to connect the benchmark starter and worker to SaaS clusters, forcing them to use outdated versions and impacting their ability to generate datasets and execute game days. \n The cause was a misconfigured client builder within the benchmark applications, which was set to use plaintext rather than establishing a proper, secure connection with the SaaS clusters. Despite the presence of a configuration for Transport Layer Security (TLS), this wasn't utilized as intended.\n The TLS configuration within the benchmark applications was corrected so that it is used properly, establishing a secure connection with the SaaS clusters. \n Users can now connect the benchmark starter and worker to SaaS clusters smoothly and without any errors, improving the usability and reliability of these benchmark applications.\n"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/13870",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "title": "Scheduled tasks should avoid overloading the streamprocessor",
    "releaseNoteText": "In the past, the workflow engine was burdened by numerous scheduled tasks that would append commands to the log stream and repeatedly overload the stream processor, causing system inefficiencies and potential outages. \nThis issue mainly arose because tasks like `JobTimeoutTrigger` lacked any constraints preventing them from appending the same `Job:TIME_OUT` command multiple times if the stream processor became blocked. Furthermore, there were possible concurrency issues with the `DueDateTimerChecker`, which could lead to more timers being scheduled under high load.\nA new approach was adopted, using a local cache in state checkers like `DueDateTimerChecker`. The cache stores the keys of tasks that have been written, preventing the same task from being written again. Furthermore, lossy cache-based prototype was built and enhanced with LRU (Least Recently Used) eviction for efficiency. This solution has been extended to other local checkers such as job timeout and message expiry.\nWith the new fix in place, the same tasks are not repeatedly written, significantly reducing the system load and preventing potential blockages. Now, timers, job timeouts, and message expiries make efficient use of caching to ensure smooth operation of the system. Moreover, the limit on timer triggers and the addition of rate limits between different categories further reinforces system efficiency."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/13551",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "title": "EvaluateDecision request returns double quoted string",
    "releaseNoteText": " Users extracting string output from their decision evaluation using the EvaluateDecisionCommand in Zeebe experienced the unexpected behaviour of the output being enclosed in extra quotation marks.\n This issue was caused by an unusual handling of string transformations in Zeebe's DMN engine. Instead of rendering the strings in a standard format, the engine was enclosing the string in an additional set of quotes. \n Adjustments were made to the procedure Zeebe used for transforming and returning strings in the EvaluateDecisionCommand. The fix removed the additional set of quotes that were being added in the transformation process. \n Now, when a user extracts the output of their decision evaluation using the EvaluateDecisionCommand in Zeebe, the extracted string is not enclosed in any additional quotation marks. The output is returned in the standard, expected format."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/9859",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "title": "Inconsistent/unnecessary escaping of characters in inputs",
    "releaseNoteText": " \nIn the past, users were experiencing inconsistency and unnecessary escaping of special characters in inputs, including double escaping and escaping of unicode characters. This issue was notably impacting connectors and was causing failure of jobs due to escaped double quotes.\n \nThe cause of this issue was rooted in the way the FEEL engine was handling escaping of characters. In particular, characters in dynamic inputs were being unnecessarily escaped, and this was happening inconsistently with static inputs. This issue was affected by the handling of newline characters and unicode characters. \n \nWe implemented a bug fix on the FEEL engine that rectified the unnecessary and inconsistent escaping of special characters and ensured that unicode characters are not escaped in input mappings. This fix involved bumping the FEEL version to 1.17.3. We also ensured that static inputs are also handled correctly, in that special characters in them are not escaped.\n \nUsers can now pass special characters in strings without the fear of inconsistent or unnecessary escaping of these characters. Newline and unicode characters are handled correctly, and the handling is consistent across static and dynamic inputs. The new behaviour has been communicated via the release notes and the support team is aware of the change. The documentation on Input Mappings has been updated to reflect the new behaviour.\n"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/8938",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "title": "Feel error are causing bad user experiences",
    "releaseNoteText": "\nUsers encountered incidents with unclear messages when creating instances with invalid feel expressions. This issue was particularly frustrating for users working with larger, more complex feel expressions, and for those who did not have access to the logs for error troubleshooting.\nThe underlying issue was that the Feel engine's response did not adequately communicate the problem when evaluating invalid feel expressions. Instead of providing pertinent details that pointed toward a resolution, the response simply declared the result as 'NULL'.\nWe upgraded the FEEL engine to a newer version (1.17.0) and utilized its new API, which returns not just the evaluation result, but also potential warnings. We've integrated these warnings into the incident messages to provide users with more context about problems.\nNow, when an instance is created with an invalid FEEL expression, the incident message includes not only the expectation and the result but also any warnings associated with the evaluation. These warnings provide specific details about the fault, like unknown functions or invalid types, leading to improved transparency and troubleshooting."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/15287",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Documentation",
    "title": "Add missing steps to developer handbook",
    "releaseNoteText": "Developers were previously left confused and dealing with bug reports and failing CI due to incomplete instructions in the developer handbook, specifically for adding new record types.\nSteps for including a new value type in the engine's supported value types and for Zeebe Process Test (ZPT) support were mistakenly omitted from the developer handbook.\nWe added the two missing steps into the developer handbook, providing clear instructions for adding new record types.\nNow, developers can follow the enhanced, detailed guide in the developer handbook, reducing the chance of bug reports or CI failures related to the addition of new record types."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/pull/15744",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Merged Pull Requests",
    "title": "build(topology): add backward compatibility check for protobuf messages",
    "releaseNoteText": "Users experienced failures when attempting to use older versions of protobuf messages with the topology module, as there was no support for backward compatibility.\nThis issue originated from a lack of checks in the system to ensure backward compatibility for protobuf messages in the topology module.\nA maven plugin was added to prevent backward incompatible changes to protobuf messages in the topology module.\nUsers are now able to use both older and newer versions of protobuf messages with the topology module, as it can successfully manage backward compatibility."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/pull/15727",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Merged Pull Requests",
    "title": "fix(topology): do not update the topology if the operation was cancelled",
    "releaseNoteText": "Previously, there was an issue where even after the cancelation of an operation, the changes associated with it were still sporadically applied to the topology.\nThis was caused by the flawed comparison within the operation that failed to validate the status of the topology and its suitability to handle changes.\nThe issue was resolved by implementing a check that compared the expected topology with the current one before enacting any changes. This check ensures changes are only applied when the operation remains valid and hasn't been cancelled.\nNow, when an operation is cancelled, the system no longer applies associated changes to the topology, ensuring consistency and averting potential inaccuracies in the updated topology."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/pull/15709",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Merged Pull Requests",
    "title": "fix(topology): maintain gossip state update ordering",
    "releaseNoteText": "Prior to the fix, users were experiencing an issue where the gossip state updates were not maintained in an orderly manner, leading to system inconsistencies and unreliable data exchange.\nThe issue was caused by a deficiency in the implementation of the gossip protocol in our topology module. This defect resulted in updates not being consistently ordered, which consequently affected system stability.\nThe engineering team addressed this issue by refining the algorithm within the topology module. They reinforced the logic to ensure the ordering of gossip state updates, thus eliminating the inconsistency previously observed.\nWith this fix, users can now rely on the proper functioning of the gossip protocol in our system. Gossip state updates are correctly sequenced, preventing any related system inconsistencies and enhancing overall data integrity."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/pull/15629",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Merged Pull Requests",
    "title": "deps(github-tags): Update actions/upload-artifact action to v4 (main)",
    "releaseNoteText": "Apologies for the confusion, the provided text does not contain any information about a specific issue but rather an update. Please provide a GitHub issue text with a suitable problem description. If the update is what you want to be transformed into a release note, more details about what it addressed and its impact would be helpful."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/pull/15619",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Merged Pull Requests",
    "title": "fix(github): new project automation token for add-to-projects",
    "releaseNoteText": "Users who tried to automate new projects by adding them to existing projects through automation tokens were unable to do so. \nThe existing coding structure did not cater for the translation of the new project automation token into an instruction to add new projects to existing ones. \nThe system was updated to recognise and deliver the instructions bundled in the new project automation token. \nUsers can now effortlessly automate the addition of new projects to existing ones using their automation tokens."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/pull/15618",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Merged Pull Requests",
    "title": "Revert \"fix(github): use project admin token for project query\"",
    "releaseNoteText": "Based on the request, here is the release note:\nPreviously, users experienced an issue where the project query was not functioning correctly. \nThis was due to the use of project admin tokens in the project query, introduced in commit 6bcd02082e5e6d5a6087560c113c4e650e7f61f7. \nTo rectify this, we have completed a revert of the mentioned commit, removing the use of the project admin token. \nNow, the project queries are functioning correctly and no longer use project admin tokens."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/pull/15617",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Merged Pull Requests",
    "title": "fix(github): fixed for add to projects workflow",
    "releaseNoteText": " Users previously encountered a warning on how variables were set during the 'add to projects' workflow, and incorrect conditions also caused the 'has-project' step to malfunction. Also, usage of the GitHub repo token returned no results due to lack of project query access.\n The warning resulted from improper setting of variables. Also, the 'has-project' step's malfunction was based on the problem that it expected a boolean value and not a string. The lack of results returned using the GitHub repo token was due to it not having access privileges for project queries.\n The variable settings were correctly configured, resolving the warning. The 'if' conditions were fixed to work with string inputs instead of booleans. A project admin token was implemented in place of the GitHub repo token to execute project queries successfully since it had the required access privileges.\n Now, no warning would appear regarding the set variables. 'Has-project' step will work without any issues since it can now process string results. Finally, users receive results from project queries because the project admin token provides the necessary access."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/pull/15508",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Merged Pull Requests",
    "title": "Handle job worker back pressure/yield as an expected case",
    "releaseNoteText": " Users previously observed job worker back pressure as an error state, which was logged heavily. Some users might have experienced slower than optimal performance when a job worker yielded under high load.\n Job worker back pressure was not correctly categorized in the system. The logging level was too high causing an overflow of logs, distracting attention from actual error states. The issue was due to the tight coupling of the gateway push-to-client logic with the `AggregatedClientStream` and `ClientStreamImpl` classes.\n The code was refactored to downgrade job worker back pressure from an error state to an expected state. Logs were downgraded to trace level, and the gateway's push-to-client logic was separated from the `AggregatedClientStream` and `ClientStreamImpl` classes into its own class, `ClientStreamPusher`.\n Job worker back pressure is now tracked as an expected state rather than an error, enabling users to focus on actual error states. Less frequently occurring logs declutter the system, increasing its performance. New metric visualization aids in identifying when a worker is slower than the others in the same deployment, making it easier for users to diagnose issues with specific streams."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/pull/15390",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Merged Pull Requests",
    "title": "Fix the current ES dashboard",
    "releaseNoteText": " The Elasticsearch (ES) dashboard was not functioning due to some defects. The external exporting feature was broken which severely affected the user experience.\n The failure was triggered by recent changes in the code that caused the ES dashboard to malfunction when exported for sharing externally. \n We corrected this issue by restoring the earlier version of the dashboard and re-exporting it. This was achieved by importing the correct configuration of the dashboard from a valid template.\n The ES dashboard is now fully operational. Users can now export the dashboard externally without any issues, providing a smoothly functioning and improved user experience."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/pull/15341",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Merged Pull Requests",
    "title": "fix: set correct generation",
    "releaseNoteText": "Users previously observed that generations were not correctly set when using the system.\nThe issue was caused by the use of incorrect variables for reference within the system's technical configuration.\nThe correct variables were identified and referenced appropriately to set generations as initially intended.\nNow, as a user, you will observe that generations are set correctly in the system, preventing any further variable misalignments."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/pull/15340",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Merged Pull Requests",
    "title": "Calculate the right versions",
    "releaseNoteText": "Users were unable to calculate the correct versions as this functionality was broken.\nThe version calculation feature was broken due to the absence of patch versions in the system.\nThe missing part responsible for version calculation was brought back and reinstated without major refactoring.\nUsers can now accurately calculate the version numbers using the reinstated functionality."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/pull/15317",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Merged Pull Requests",
    "title": "refactor: allow easy disabling via secret removal",
    "releaseNoteText": "Users previously found it difficult to disable Transactional Consistency Control (TCC) when it was needed because the system design did not allow for an easy disabling mechanism.\nThe system architecture was originally set up in such a way that disabling TCC required a complex process, instead of simply removing a secret.\nThe system was refactored to implement a simplified approach where removing a secret quickly disables TCC.\nCurrently, users can effortlessly disable TCC whenever required by simply getting rid of the secret. This amendment enhances user control and the overall system flexibility."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/pull/15201",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Merged Pull Requests",
    "title": "Replay deployed forms",
    "releaseNoteText": " Users were unable to replay deployed forms because Form records were not included in the supported Engine records previously. \n The root cause of this problem was that the source code failed to include 'Form' records in the set of supported records for playback in the Engine.\n Developers have refreshed the codebase to encompass 'Form' records in the supported Engine records, hence enabling them to be replayed.\n Now users can efficiently replay deployed forms as 'Form' records are included in the list of supported Engine records. A regression test case has been added to verify this behavior and ensure system robustness."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/pull/14631",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Merged Pull Requests",
    "title": "Aa update ci after 8.3 minor release",
    "releaseNoteText": "The CI was outdated and not in sync after the 8.3 minor release, affecting the performance and stability of subsequent builds. \nThis occurred because the CI was not updated after the 8.3 minor release, leaving it incompatible with the new release changes. \nThe CI was updated comprehensively to match the 8.3 minor release standards, and refactored to ensure compatibility.\nThe CI is now up-to-date with the 8.3 minor release, and all builds are running smoothly and as expected.\n"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/pull/14217",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Merged Pull Requests",
    "title": "fix(clients/go): client does not retry on permanent errors",
    "releaseNoteText": " Previously, the system did not stop on permanent errors regardless of the behavior of the credential provider. This made the client try to execute operations repetitively causing a waste of system resources. \n The client was not configured correctly to distinguish between permanent errors and temporary errors. Permanent errors were not flagged as non-retryable, so the client kept on trying to execute the commands unnecessarily.\n Changes were made to ensure that commands stop on encountering permanent errors regardless of the behavior of the credential provider. Permanent errors were marked as non-retryable, while all other errors defaulted to the retry predicate. \n Now, when a command encounters a permanent error, it immediately stops the operation avoiding unnecessary execution, and thus, functions more efficiently.\n"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/pull/8203",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Merged Pull Requests",
    "title": "[Backport stable/1.2] Fixes flakiness with the ElasticsearchExporter integration tests",
    "releaseNoteText": " Previously, users might have experienced flakiness while running ElasticsearchExporter integration tests. This interfered with accurate testing outcomes.\n The instability occurred due to a lack of synchronization between processes within the testing environment.\n We applied a backport of alteration #8194 to our `stable/1.2` branch, effectively resolving synchronization issues.\n Now, when users execute ElasticsearchExporter integration tests, they experience a stable, predictable testing process. Benefit from more reliable test results, contributing to a smoother and more efficient development workflow."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/operate/issues/6083",
    "component": "Operate",
    "subcomponent": "Misc",
    "context": "💊 Bugfixes",
    "title": "fix(backend): Extract IndexLifecycleManagementIT. Get rid of ArchiverITRepository",
    "releaseNoteText": "In the past, system integration was not optimal as the IndexLifecycleManagementIT and ArchiverITRepository components were not separate, causing inefficiency and potential confusion.\nThis situation occurred due to the IndexLifecycleManagementIT and ArchiverITRepository components being combined rather than separately addressed in the backend codebase.\nWe responded by extracting the IndexLifecycleManagementIT from the combination and got rid of ArchiverITRepository from the system's backend.\nNow, the system's integration is more streamlined and efficient, making it easier for users to identify and manage the processes related to IndexLifecycleManagementIT; and the ArchiverITRepository component is no longer present, thus simplifying system management."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/operate/issues/6078",
    "component": "Operate",
    "subcomponent": "Misc",
    "context": "💊 Bugfixes",
    "title": "fix(backend): Remove unused items",
    "releaseNoteText": "The backend tests included unused items, decluttering the tests and potentially causing confusion or misleading results.\nDuring the process of coding, certain items remained in the tests that were no longer relevant or necessary. This was an oversight during the process of iterating and refining the product.\nThe unnecessary items were identified and systematically removed from all backend tests.\nNow, the backend tests are more clear and concise. This ensures an optimized testing process and more reliable result interpretation."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/operate/issues/6065",
    "component": "Operate",
    "subcomponent": "Misc",
    "context": "💊 Bugfixes",
    "title": "fix(backend): Configure number of shards per index #5920",
    "releaseNoteText": " Users were unable to configure the number of shards and replicas per index, leading to limitations in customizing their setup. \n This was due to the absence of a feature in the backend to support per index shards and replicas configuration.\n Developers added support for per index shards and replicas configuration. This implementation expands the capabilities of the application.\n Users can now seamlessly configure the quantity of shards and replicas per index, granting enhanced control and customization over their setups.\n"
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/operate/issues/6030",
    "component": "Operate",
    "subcomponent": "Misc",
    "context": "💊 Bugfixes",
    "title": "fix(backend): Workaround for search_after issue in OS java client (#5788)",
    "releaseNoteText": "Users experienced problems with the search_after function in the OS Java client due to the CamundaPatchedSearchRequest.\nThe issue was rooted in the technical engineering of the product. Specifically, the CamundaPatchedSearchRequest was a factor causing the function to fail.\nA Map-based model replaced the CamundaPatchedSearchRequest.\nNow, users should encounter no issues with the search_after function in the OS Java client. The swap to a Map-based model has rectified the underlying cause."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2412",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "🚀 New Features",
    "title": "feat: allow multiple root urls",
    "releaseNoteText": "Prior to this fix, users were only able to define a single root URL in the KeycloakClient object, limiting the configuration options.\nThe root URL field in the KeycloakClient object was not designed to handle comma-separated values, thus not allowing multiple root URLs.\nThe code was modified to allow the root URL field on the KeycloakClient object to be interpreted as a comma-separated value field.\nUsers can now specify multiple root URLs in the KeycloakClient object, enabling more flexible configuration options."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2406",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "🚀 New Features",
    "title": "feat: support custom client ids for camunda application",
    "releaseNoteText": "\nUsers were unable to use custom client ids for Camunda applications as the Identities had only predefined client ids.\nThe application lacked the ability to map variable definitions the same way as it processes secrets.\nWe introduced a feature allowing the definition of custom client ID through environment variables. This covers all Camunda applications including Operate, Optimize, Tasklist, Connectors, Console, and Web-Modeler  \nResult:\nNow, users can define custom client IDs for all the Camunda applications, allowing for greater flexibility and customization. This change is also reflected in the updated documentation."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2380",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "🚀 New Features",
    "title": "feat: add /ping endpoint for uptime check",
    "releaseNoteText": "The system was not providing a way to check its uptime, which could decrease manageability and the ability to monitor system health.\nThe system hadn't been initially designed with a specific endpoint for uptime checking and the built-in Spring endpoint could have introduced undesirable changes if not accurately handled.\nA new /ping endpoint was added that returns a 2xx status and is accessible without authentication. This was implemented as a less complex and safer approach than modifying the health details and jwt filters.\nThe system now features a /ping endpoint allowing users to check its uptime without authentication, improving monitoring and control over system health. Changes in jwt filters weren't needed, minimizing the risk of accidental over-exposure of data."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2379",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "🚀 New Features",
    "title": "feat: provider helper in sdk for authorization available",
    "releaseNoteText": "In the past, users were unable to determine if authorization through identity was available or not in the SDK, leading to uncertainty when configuring Connectors.\nThe issue arose due to the lack of a helper method in the SDK to indicate the availability of authorization through identity.\nDevelopers added a helper method in the SDK to show if authorization via identity is available. This method will ascertain if Connectors have been set up with new Identity configuration variables, superseding the built-in authentication handling mechanism in the Operate SDK.\nUsers can now check if authorization through identity is accessible directly from the SDK, streamlining the configuration process for Connectors."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2356",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "🚀 New Features",
    "title": "feat: add a helper to determine identity user search availability",
    "releaseNoteText": "Previously, the user search feature in the Optimize UI appeared even when this feature was not supported by the underlying system configuration. This could result in confusion or an inconsistent user experience. \nThe application did not have a mechanism to determine the availability of the user search feature based on Identity's configurations. Besides that, the system didn't check for null or empty baseUrl, which could lead to process failures. \nWe have added a helper function that checks whether user search is available, taking into consideration the Identity configuration type and the baseUrl. This helper allows the app to detect the operational status of the user search feature. \nCurrently, if user search is not available due to Identity configuration restrictions, the related UI elements are hidden in the Optimize interface. This ensures a more consistent and error-free user experience."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2327",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "🚀 New Features",
    "title": "feat: implement generic auth proile",
    "releaseNoteText": " \nPreviously, users were unable to use authentication connections that were not covered by the platform specific implementations.\n \nThe system was primarily designed to only support Keycloak and AzureAD implementations, and lacked detailed `GENERIC` authentication type catering to custom connections.\n \nA `GENERIC` authentication type was introduced where we incorporated well-known configuration retrieval, enabling connections not covered by specific platform implementations. Further, we also revamped the Keycloak and AzureAD implementations to leverage this generic profile, thereby significantly reducing code duplication.\n \nNow, users can enjoy smooth and successful connections that may not be encompassed by our platform-specific implementations. Through our generic implementation and the newly incorporated well-known configuration retrieval, the system can support a broader range of connections."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2303",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "🚀 New Features",
    "title": "feat: expose user groups via SDK",
    "releaseNoteText": "Previously, users couldn't see the groups they belonged to via the SDK when using the self-manage feature for Tasklist. This lack of visibility made it difficult for them to gauge their access and permissions.\nThis issue was due to the absence of a mapper in the system that could extract and expose user groups as part of the user details linked to a token.\nThe engineering team developed and implemented a new mapper to specifically address this missing functionality. Furthermore, the associated user data was exposed as outlined in the parent issue.\nNow, users can view the groups they are assigned to within the Tasklist self-manage offering. The system seamlessly surfaces this information in user details connected to a token."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2296",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "🚀 New Features",
    "title": "feat: add read:users permission to the Optimize role",
    "releaseNoteText": "Previously, users with the Optimize role were unable to access user details due to a lack of necessary permissions. \nThis issue was triggered because the `read:users` permission was not included in the Optimize role's configuration.\nThe `read:users` permission has been duly added to the Optimize role.\nNow, users assigned the Optimize role are able to access and read user details."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2273",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "🚀 New Features",
    "title": "feat: implement Azure AD sdk profile",
    "releaseNoteText": "Users were unable to authenticate using Azure AD as it was not supported in previous releases.\nThe underlying platform had not integrated Azure AD SDK, hence it lacked the necessary functionality for Azure AD authentication.\nWe implemented the Azure AD SDK profile, created a new set of classes dedicated to Azure AD authentication, and adjusted the `AbstractAuthentication` class to make the `preferred_username` claim name configurable.\nUsers can now authenticate using Azure AD in the current release. Uniquely configured preferred_username claims are also catered for, enhancing user flexibility and platform versatility."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2266",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "🚀 New Features",
    "title": "feat: add support for AWS Aurora database",
    "releaseNoteText": " \nUsers were previously unable to connect to an AWS Aurora database using the software, due to inadequate support for such operations in the codebase.\n \nThe software lacked the necessary code changes needed to connect to an AWS Aurora database using the AWS-advanced-jdbc-wrapper with IAM plugin, preventing users from establishing successful connections.\n \nCode was modified to add the necessary support, notably incorporating elements from the solutions provided by the Web Modeler team. The changes involved using the AWS-advanced-jdbc-wrapper with IAM plugin to establish the connection to AWS Aurora database.\n \nUsers can now successfully connect to an AWS Aurora database using the software. The system resolves the appropriate 'spring.datasource.driver-class-name' placeholder correctly, and the resultant operation of connection to the AWS Aurora database is covered by tests to ensure stability and reliability. However, users should note that, due to a new medium-level vulnerability CVE-2023-4586 introduced by the new AWS libraries, proper certificate validation should be manually enabled in the code to avoid Man-in-the-Middle attacks."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2184",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "🚀 New Features",
    "title": "feat: implement identity spring boot starter",
    "releaseNoteText": " Previously, users lacked the basic foundation for the new Identity Spring Boot Starter. The separate variable prefix caused a disjointed, chaotic experience for users manipulating environment variables.\n The issue was generated due to the absence of an Identity Spring Boot Starter. This unattended gap in the ecosystem resulted in inefficiencies and limited integration options. Additionally, the arbitrary naming of variable prefixes amplified the confusion.\n A new Identity Spring Boot Starter has been implemented. Attention was given to the improvement of the variable prefix system and has been updated to 'camunda.identity'. This new approach aligns better with existing components and provides enhanced context.\n Now, users have access to the new Identity Spring Boot Starter which has an improved variable prefix system. This feature offers better integration options and a more streamlined, user-friendly experience by adopting a unified and intuitive naming convention. Environment variables such as 'CAMUNDA_IDENTITY_BASEURL' provide additional context, leading to a more graspable user experience."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2148",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "🚀 New Features",
    "title": "feat: add implementation info to SDK jar",
    "releaseNoteText": "Previously, the implementation information was not accessible inside the MANIFEST.MF of the jar, making it difficult to fetch the Identity version in the dependent projects during runtime. This situation was particularly problematic during E2E testing in Zeebe, causing problems when dealing with Identity SNAPSHOTs and supporting older versions.\nThe SDK was not initially designed to provide implementation info, and consequently, the Identity version could not be retrieved during runtime in dependent projects.\nThe code was altered to include the implementation info in the MANIFEST.MF of the jar in the SDK. This information allows dependent projects to fetch the Identity version during runtime.\nNow, the Identity version can be readily identified during runtime in dependent projects, including for Docker images in E2E tests. This change enhances compatibility and fault tolerance when dealing with Identity SNAPSHOTs and older versions."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2161",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "🚀 New Features",
    "title": "feat: support initializing access rules by config",
    "releaseNoteText": "Earlier, users were unable to assign members to tenants through configuration, causing inconvenience to internal teams and consumers trying to automate tasks. It was also possible that the \"default\" tenant could be overwritten and removed, resulting in an inconsistent state.\nThe issue arose due to the lack of initializer updates that would support the assignment of members to tenants via configuration. Also, the \"default\" tenant definition was not in a secure location, making it susceptible to overwriting and removal.\nChanges have been made to introduce initializer updates, enabling the assignment of members to tenants through configuration. Additionally, the \"default\" tenant definition has been moved to a new section, \"identity.environment\", to prevent it from being overwritten and removed.\nNow, users can assign members to tenants via configuration seamlessly, enhancing automation capabilities. Furthermore, the \"default\" tenant will always exist in the environment, ensuring a consistent state."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2109",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "🚀 New Features",
    "title": "feat: implement GET /authorizations endpoint",
    "releaseNoteText": " Users with specific permissions were unable to access the authorization data of an organization due to an issue in the SDK where the 'permissions' claim on JWTs from Auth0 was being overlooked.\n The issue was caused by the SDK overlooking the 'permissions' claim on JWTs from Auth0 because of audience reasons. On a similar note, the Keycloak system was returning a Null Pointer Exception (NPE) rather than an empty list when tokens structuring was invalid.\n The SDK and Keycloak implementations were tweaked to handle permissions properly. The 'permissions' claim on the JWTs is now recognized by the SDK, and the Keycloak implementation has been altered to return an empty list instead of an NPE when encountering an invalid token structure.\n Users with requisite permissions can now successfully access the authorization data for an organization. This first step towards a broader feature rollout significantly improves user experience by eliminating previous technical constraints."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2343",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "💊 Bugfixes",
    "title": "fix: fix resource authorizations checkbox style",
    "releaseNoteText": "Users were experiencing incorrect styling of the resource permissions checkboxes in Identity.\nThe issue was caused by a bug in the CSS that dictates the visual representation of these checkboxes, which resulted in inconsistencies and reduced usability. \nThe styles associated with the resource permissions checkboxes were adjusted to ensure they display properly in all supported environments and browsers. \nThe resource permissions checkboxes in Identity now display correctly, providing users with a more consistent experience when managing permissions."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2381",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "💊 Bugfixes",
    "title": "fix: upgrade org.springframework.boot:spring-boot-starter-security from 3.1.5 to 3.2.0",
    "releaseNoteText": " Prior to this fix, users could have potentially experienced a Denial of Service (DoS) attack due to a vulnerability in the security features of our system.\n The problem was that the `org.springframework.boot:spring-boot-starter-security` was outdated (version 3.1.5).\n The issue was resolved by updating the `org.springframework.boot:spring-boot-starter-security` dependency from version 3.1.5 to 3.2.0, and also the `org.springframework.boot:spring-boot-starter-web` from 3.1.5 to 3.1.6.\n The system is now more secure with the new updates and users are no longer exposed to the DoS attack through the mentioned vulnerability. These changes maintain system stability without disruption to user experience."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2353",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "💊 Bugfixes",
    "title": "fix: update aws-java-sdk-v2 monorepo to v2.21.29 (main)",
    "releaseNoteText": " The system was using an outdated version of aws-java-sdk-v2 (v2.21.28), affecting the operation and compatibility with newer versions of AWS services like STS and RDS.\n The issue was caused by the continuous integration system not having been set up to automatically update the aws-java-sdk-v2 from v2.21.28 to the latest version, v2.21.29.\n The problem was fixed by updating the aws-java-sdk-v2 monorepo to v2.21.29. Both the STS and RDS services were successfully updated to the newer version.\n The system now operates with the latest version of aws-java-sdk-v2 (v2.21.29). This provides enhanced compatibility with AWS services like STS and RDS, improving stability and overall runtime efficiency."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2350",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "💊 Bugfixes",
    "title": "fix: update aws-java-sdk-v2 monorepo (main)",
    "releaseNoteText": "Users were running outdated versions of various Amazon Web Service's Java SDK v2 packages, including 'sts', 'rds', and 'bom'. These outdated versions may expose the system to vulnerabilities and prevent the utilisation of updated features. \nThe system was using deprecated versions of several AWS Java SDK v2 packages (sts 2.21.26, rds 2.21.26, bom 2.21.26). The adoption and confidence in these outdated packages were evidently low, posing potential risks for system integrity.\nUpdated the aws-java-sdk-v2 monorepository to incorporate the latest versions of several packages. Software.amazon.awssdk:sts was updated from 2.21.26 to 2.21.29, software.amazon.awssdk:rds from 2.21.26 to 2.21.29, and software.amazon.awssdk:bom from 2.21.26 to 2.21.28.\nUsers are now operating on the most recent versions of software.amazon.awssdk:sts, rds, and bom. This ensures that they benefit from the most recent updates, features and security patches, plus the highest level of adoption and confidence in these packages."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2332",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "💊 Bugfixes",
    "title": "fix: upgrade software.amazon.awssdk:rds from 2.21.21 to 2.21.26",
    "releaseNoteText": " The older version of `software.amazon.awssdk:rds` (2.21.21) was in use, which led to an instance of improper certificate validation, posing as a significant vulnerability.\n `software.amazon.awssdk:rds` in its iteration of 2.21.21 version contained an identified vulnerability (SNYK-JAVA-IONETTY-1042268) associated with improper certificate validation.\n The issue was resolved by upgrading `software.amazon.awssdk:rds` from version 2.21.21 to its secured counterpart, version 2.21.26 as per the recommendation of Snyk. \n With the application of this fix, the system now uses the updated `software.amazon.awssdk:rds` package (2.21.26), effectively eliminating the previously identified vulnerability. This ensures the correct validation of certificates, enhancing the overall security of the system."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2312",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "💊 Bugfixes",
    "title": "fix: update aws-java-sdk-v2 monorepo to v2.21.21 (main)",
    "releaseNoteText": "Users experienced an outdated version of AWS Java SDK v2, thus, potentially running into discrepancies, incompatibilities, or missing out on recent improvements provided by the newer version. \nThe software depended on an older version of the AWS Java SDK v2 (v2.21.20). \nThe AWS Java SDK v2 was updated from v2.21.20 to v2.21.21 for the software.amazon.awssdk:sts, software.amazon.awssdk:rds, and software.amazon.awssdk:bom packages.\nUsers now interact with the system employing the updated AWS Java SDK v2.21.21, which offers a more up-to-date, secure, and stable environment, ensuring maximum compatibility and taking advantage of the latest features from AWS."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2307",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "💊 Bugfixes",
    "title": "fix: update aws-java-sdk-v2 monorepo to v2.21.20 (main)",
    "releaseNoteText": "Users experienced outdated dependencies while using the application, particularly those related to AWS Java SDK v2, as it ran on version 2.21.17.\n \nThe issue was caused by the application's software packages, specifically software.amazon.awssdk:sts, software.amazon.awssdk:rds, and software.amazon.awssdk:bom, being linked to an older version of the AWS Java SDK v2.\nThe AWS Java SDK v2 dependencies in the application were updated to the latest version, 2.21.20.\nThe application now operates with the most recent version of the AWS Java SDK v2 (2.21.20). This effectively updates the dependencies, improving overall system reliability and performance."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2300",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "💊 Bugfixes",
    "title": "fix: update aws-java-sdk-v2 monorepo to v2.21.17 (main)",
    "releaseNoteText": "\nPreviously, the application was using an outdated version (v2.21.15) of the aws-java-sdk, which might have caused issues with some functionality related to the software.amazon.awssdk:rds, software.amazon.awssdk:sts and the software.amazon.awssdk:bom packages.\nThe issue was caused due to the use of an outdated AWS Java SDK version. The versions of sts, rds, and bom components of the AWS Java SDK were all at v2.21.15. \nThe AWS Java SDK version was updated to v2.21.17 for the sts, rds, and bom components. This was achieved by merging the relevant pull request, which upgraded the monorepo to v2.21.17 from v2.21.15.\nWith this fix, the application now uses the updated v2.21.17 version of the AWS Java SDK. Consequently, the app now benefits from the help of improvements, enhancements, and bug fixes released in the v2.21.17 version."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2213",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "💊 Bugfixes",
    "title": "fix: update c4-identity to fix multiple tenants assignment",
    "releaseNoteText": " Previously, users experienced an issue where multiple tenants were being assigned to a single entity. \n This occured due to a glitch in the `c4-identity` library that incorrectly allowed for multi-tenant assignment.\n We've resolved the issue by upgrading the `c4-identity` library which included a necessary fix to prevent multiple tenants from being assigned to a single entity. \n Now, the system properly assigns tenants to entities with each entity having a unique tenant. This improvement is implemented and active in the latest version."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2274",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "💊 Bugfixes",
    "title": "fix: upgrade software.amazon.awssdk:rds from 2.21.13 to 2.21.15",
    "releaseNoteText": " Users may have experienced invalid or untrustworthy certificates due to improper certificate validation from the vulnerability of the software.amazon.awssdk:rds version 2.21.13 in the management-api.\n The triggered issue stemmed from a flawed version of software.amazon.awssdk:rds (2.21.13), which had issues with certificate validation. The vulnerability had severity rated as medium, potentially disrupting the secure function of the software.\n The issue was addressed by upgrading software.amazon.awssdk:rds from version 2.21.13 to version 2.21.15. The changes were executed in the maven dependencies within the file - management-api/pom.xml, to deploy the fixed version of the package.\n Now, software robustness is reinforced with the upgrade, and users have reduced risk from an unsecured certificate, hence ensuring a more secure experience. Unsafe certificate validation is no longer an issue with software.amazon.awssdk:rds version 2.21.15."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2268",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "💊 Bugfixes",
    "title": "fix: dependency issues causing tests to not run",
    "releaseNoteText": " Previously, users were unable to execute tests due to dependency issues.\n This was caused by a clash of dependency versions for the Junit Jupiter artifacts. The use of a `dependencyManagement` block to set the versions was not successful.\n Two solutions were applied. First, the Jupiter versions were set to a version that works. Second, the Maven failsafe plugin was configured to fail the build if no tests were run during the verify stage for the management-api artifact.\n Now, users can successfully run tests. The build fails as expected if no tests are run during the validation phase in the `management-api` module, adding an extra layer of protection. Maven is now allowed to use its built-in logic for dependency resolution."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2265",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "💊 Bugfixes",
    "title": "fix: fix access rules retrieval",
    "releaseNoteText": "Previously, users were unable to retrieve access rules if they were not linked to at least one tenant, even when an access rule existed\nThis problem was due to the use of a normal \"JOIN tenants\" when retrieving access rules instead of \"LEFT JOIN tenants\"\nWe have adjusted the technical logic by replacing \"JOIN tenants\" with \"LEFT JOIN tenants\" in the access rules retrieval process\nCurrently, users can successfully retrieve existing access rules even if they are not associated with any tenant."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2227",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "💊 Bugfixes",
    "title": "fix: update spring boot to v3.1.5 (main)",
    "releaseNoteText": "Without the details of the specific GitHub issue, I cannot provide a complete release note. However, I can give you an example based on the given repository update:\n Before this fix, the application was running on an older version (3.1.4) of the Spring Boot framework.\n Regular updates are crucial for the application to function optimally and securely. The previous version of Spring Boot had some noted bugs and deficiencies that could have affected the application's performance.\n The application dependencies were updated from Spring Boot version 3.1.4 to version 3.1.5. This upgrade encompasses simultaneous enhancements to the 'spring-boot-starter', 'spring-boot-configuration-processor', 'spring-boot-autoconfigure', and 'spring-boot' packages.\n With these updates now applied, the application operates on a much more stable and secure version of Spring Boot. Consequently, all improvements and bug-fixes wrapped into version 3.1.5 of Spring Boot are now a part of the application, optimizing its functionality."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2225",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "💊 Bugfixes",
    "title": "fix: use java 17 to build new artifacts",
    "releaseNoteText": "Previously, artifact deployment for the `-starter` and `-autoconfigure` failed due to an incompatibility with Java 11. \nThe root cause of the issue was the use of Java 11 for executing these steps, which was not compatible with the required Java 17.\nThe solution to this issue was to update the environment to build new artifacts using Java 17 instead of Java 11.\nNow, the `-starter` and `-autoconfigure` artifacts correctly deploy due to the compatibility with Java 17."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2191",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "💊 Bugfixes",
    "title": "fix: upgrade org.springframework.boot:spring-boot-starter-web from 3.1.4 to 3.1.5",
    "releaseNoteText": "Consequence:\nPreviously, users were experiencing a range of security vulnerabilities when utilizing the application due to outdated dependencies. Concerns included denial of service (DoS) attacks, improper input validation, incomplete cleanup, and arbitrary code execution. \nThe cause of these vulnerabilities was the use of the 3.1.4 version of org.springframework.boot:spring-boot-starter-web. This version had previously identified vulnerabilities that left the system open to possible breaches. \nTo remedy these vulnerabilities, we upgraded the org.springframework.boot:spring-boot-starter-web from version 3.1.4 to 3.1.5 in the management-api/pom.xml file. \nResult:\nUsers can now utilize the application with improved security measures in place. The upgrade to the org.springframework.boot:spring-boot-starter-web dependency to version 3.1.5 has mitigated the prior identified vulnerabilities, leading to fewer opportunities for breaches and a safer user experience."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2180",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "💊 Bugfixes",
    "title": "fix: resolve lazy loading issue with tenants linked to access rules",
    "releaseNoteText": " Users previously encountered an exception when attempting to assign members in configuration for tenants. This was due to a `LazyInitializationException` from Hibernate which prevented the successful initialization of a collection of role: `io.camunda.identity.entity.AccessRule.tenants`.\n The exception occurred because the code failed to retrieve the tenants for the access rule that we want to assign a member to. This issue arose from the Hibernate session extending and causing a disruption in the process.\n The issue was corrected by implementing the `JOIN FETCH` approach. This decision was made after evaluating various options to resolve the `LazyInitializationException`. Using `FetchType.EAGER` was avoided due to the general discouragement in its usage.\n Users can now successfully assign members in configuration for tenants without facing any exceptions. This fix has enhanced the fluidity of the configuration process, improving overall user experience."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2133",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "💊 Bugfixes",
    "title": "fix: fix for 2 vulnerabilities",
    "releaseNoteText": " Previously, users could potentially be exposed to two security vulnerabilities, Access Restriction Bypass and Incorrect Permission Assignment for Critical Resource, which posed a risk to the security of the software.\n These vulnerabilities occurred because of outdated packages in the Maven dependencies of the project, specifically `org.springframework.boot:spring-boot-starter-web:` and `org.springframework.boot:spring-boot-starter-security:` versions 3.1.3.\n We have upgraded these vulnerable dependencies to a secure version, 3.1.4, as reflected in the changes to the management-api/pom.xml file.\n Now, the system is secure against the mentioned vulnerabilities, significantly reducing the potential security risks to our users. Users can now leverage the functionalities of the software while being assured of their security."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2164",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "💊 Bugfixes",
    "title": "fix: only create log file when required",
    "releaseNoteText": " Previously, the Log4J2 system was creating a log file prematurely, even when it was not required. \n In the past, the software's configuration was set for Log4J2 to proactively create the logfile, which caused unintended results for users. \n The issue was resolved by modifying the configuration settings and toggling off the feature that allowed for proactive file creation. \n Now, the system only generates log files when necessary, thus enhancing user experience and system performance."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda-cloud/identity/issues/2153",
    "component": "Identity",
    "subcomponent": "Misc",
    "context": "💊 Bugfixes",
    "title": "fix: prevent body on GET call in serviceWorker",
    "releaseNoteText": "Previously, the `/tenants` page failed to load, inhibiting smooth user experience.\nThis occurred because the serviceWorker was setting a body for `GET` calls, which was causing the error.\nA modification in logic was implemented to prevent the serviceWorker from setting a body for `GET` calls. \nWith this fix, the `/tenants` page now loads successfully."
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14967",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "gitHubText": "## Description \r\n\r\nAs a user, I can deploy a process with a compensation end event. Now, I want to execute the process. In the first step, we want to ignore the invocation of compensation handlers. \r\n\r\n- I can complete a process instance that contains a compensation end event.\r\n- The compensation end event is activated and completed.\r\n- The records for the compensation end event have the event type `COMPENSATION`.\r\n\r\n![Screenshot from 2023-11-02 14-01-15](https://github.com/camunda/zeebe/assets/4305769/97ea69e5-3f3c-4393-82bf-c58bfe11e353)\r\n\r\nrequires #14942 \r\nrequires #14944 \n",
    "title": "I can execute a process with a compensation end event",
    "releaseNoteText": "Previously, a process model with a compensation end event could not be deployed to the engine. Compensation end event support has been added to the engine, although the compensation handler is not invoked. This is a partial implementation as part of the overall implementation of support for compensation events, which is ongoing. As a result, in this release a process model containing a compensation end event may be deployed and executed. "
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14939",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Enhancements",
    "gitHubText": "## Description\r\n\r\nThe BPMN model API supports compensation events already. But, creating a process via builder with compensation events is verbose and feels clunky. \r\n\r\nSee the example here:\r\n\r\n```java\r\nBpmn.createExecutableProcess(\"compensation-process\")\r\n            .startEvent()\r\n            .userTask(\"A\")\r\n            .boundaryEvent()\r\n            .compensateEventDefinition()\r\n            .compensateEventDefinitionDone()\r\n            .compensationStart()\r\n            .userTask(\"undo-A\")\r\n            .compensationDone()\r\n            .endEvent()\r\n            .done();\r\n```\r\n\r\n![Screenshot from 2023-11-01 11-10-35](https://github.com/camunda/zeebe/assets/4305769/94e1f994-a3cd-4c4b-9045-01acd38e6ac5)\r\n\r\nWe will need the builder to create test cases for compensation events. Let's make the builder smoother for better test cases.\r\n\r\nSee the proposal based on other events. The final solution may look different. \r\n\r\n```java\r\nBpmn.createExecutableProcess(\"compensation-process\")\r\n        .startEvent()\r\n        .userTask(\"A\")\r\n        .boundaryEvent()\r\n        .compensation(c -> c.userTask(\"undo-A\"))\r\n        .moveToActivity(\"A\")\r\n        .endEvent()\r\n        .done();\r\n\r\n// or via nested lambda definition \r\nBpmn.createExecutableProcess(\"compensation-process\")\r\n        .startEvent()\r\n        .userTask(\"A\")\r\n        .boundaryEvent(\r\n            \"compensate\",\r\n            boundaryEvent -> boundaryEvent.compensation().userTask(\"undo-A\"))\r\n        .endEvent()\r\n        .done();\r\n``` \r\n\r\n\n",
    "title": "I can model a process with compensation events via BPMN model API",
    "releaseNoteText": "A new API for modeling a BPM compensation event has been implemented with this release. It is now accomplished with the following code:\r\n```java\r\n  Bpmn.createProcess().startEvent()\r\n    .userTask(\"task\")\r\n    .boundaryEvent(\"boundary\")\r\n    .compensation(c -> c.userTask(\"compensate\"))\r\n    .moveToActivity(\"task\")\r\n    .endEvent(\"theend\")\r\n    .done(); "
  },
  {
    "release": "8.4.0",
    "url": "https://github.com/camunda/zeebe/issues/14329",
    "component": "Zeebe",
    "subcomponent": "Misc",
    "context": "Bug Fixes",
    "gitHubText": "**Describe the bug**\r\n\r\n[E2E tests](https://camunda.slack.com/archives/C013MEVQ4M9/p1694702959002779) failed because the send-ping job is not completed. All the instances that did not complete are stuck in `send-message-ping` or `send-message-pong` tasks.\r\n\r\n![image](https://github.com/camunda/zeebe/assets/1997478/2b210594-5e5e-47cf-8bae-15a5ba3c0006)\r\n\r\nLooking at the logs, publishing message failed due to resource exhausted (there were broker restarts and leader change around this time). So the job was failed with a retry backoff of 30 seconds. However, the worker logs and the exported records show that these jobs were not re-activated. \r\n\r\nHere is screenshot showing exported records in elastic search for some of these jobs.\r\n\r\n![tmp-78371-iIZGT5mhN4lx-image](https://github.com/camunda/zeebe/assets/1997478/4dbb026d-31e2-42f2-8d0b-a98e872cc481)\r\n\r\n![tmp-78371-p0xwE2yFVwKD-image](https://github.com/camunda/zeebe/assets/1997478/5a38c06e-ff69-4830-a4fd-51cb7d7f5621)\r\n\r\n![tmp-78371-23tXS7l4N8jV-image](https://github.com/camunda/zeebe/assets/1997478/512e83ae-291b-45ef-8d78-29e1c1bc9887)\r\n\r\nFor a job that retried after backoff, you can see those records in ES. But we don't see them for the above jobs. Here is a an example of such a job were retrying after backoff worked successfully (from the same cluster and for same job type). \r\n![tmp-78371-cK9yR3YYFP5d-image](https://github.com/camunda/zeebe/assets/1997478/dec2c231-cb4a-4f03-ac87-7f94c54fb7b4)\r\n\r\nAlso ran `zbctl activate jobs \"send-message-ping\"` locally against this cluster. But it returned  no jobs, even though operate shows these jobs as not completed.\r\n\r\n**To Reproduce**\r\n\r\nNot sure. But I think restarts and leader change may have something to do with this.\r\n\r\n**Expected behavior**\r\n\r\nFailed jobs should be re-activatable after backoff timeout.\r\n\r\n**Log/Stacktrace**\r\n\r\nThere were no relevant error in Zeebe. But here are the logs around 14.09.2023 16:33 CEST. All uncompleted tasks were created around 2023-09-14 16:32 - 16:33\r\n[downloaded-logs-20230915-105930.zip](https://github.com/camunda/zeebe/files/12618554/downloaded-logs-20230915-105930.zip)\r\n\r\nHere is the snapshot of partition 3 on which these jobs exist. \r\n[snapshot-partition-3.zip](https://github.com/camunda/zeebe/files/12617504/snapshot-partition-3.zip)\r\n\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.1.16-SNAPSHOT (8.1.16-SNAPSHOT-stable-8.1-8522cb63)\r\n\n\n korthout: ZPA triage:\n- when this happens it's quite bad, those jobs will not picked up again\n- we have tests for this, so it must have been a special situation\n- may be hard to reproduce\n- marking this as `later` unless it's reported again\n deepthidevaki: Happened again twice:\r\n1. Stable/8.0 https://camunda.slack.com/archives/C013MEVQ4M9/p1696265270192779 \r\n2. Stable/8.2 https://camunda.slack.com/archives/C013MEVQ4M9/p1696265619453009 \r\n\r\nIn both cases, there were broker restarts when the job failed.\n abbasadel: ZPA triage:\n- We see this issue happening more often now (today's morning and lastweek)\n- We don't think this is related to the latest changes since it also happens on 8.0 and 8.3\n- We think we should start investigating this and give it a high priority as 'upcoming' \n\n deepthidevaki: All E2E tests are failing every week due to this bug. It would be good to fix this asap. If the fix is not planned, could you please disable this `ping-pong` process in e2e tests? It doesn't make sense to run the tests if it keeps failing.\n megglos: reached out on slack on this, see https://camunda.slack.com/archives/C037W9NMATG/p1700578334906099\n nicpuppa: From the snapshot provided me and @tmetzke saw that the `JOB_BACKOFF` column family is empty. Possible scenarios:\r\n\r\n- Job Failed event is not correctly replayed, after the restart new leader doesn't know about the failed job that needs to be retried\r\n- JOB_BACKOFF column family might be [cleaned up ](https://github.com/camunda/zeebe/blob/1c6c3cc2ec75bdb298c1e644131a586edbf0f501/engine/src/main/java/io/camunda/zeebe/engine/state/instance/DbJobState.java#L241)\n Zelldon: @nicpuppa and I had a closer look together.\r\n\r\nWe have seen in the state that in deed there is no entries for JOB_BACKOFF, but still there are jobs in state FAILED.\r\n\r\n\r\nAfter this we check the code in more depth, and tried to understand how this could happen. Based on the previous comment we check the Migration code, etc. Based on the code we would expect that the migration is not executed if there is still an job existing, but where not sure about the second condition https://github.com/camunda/zeebe/blob/1c6c3cc2ec75bdb298c1e644131a586edbf0f501/engine/src/main/java/io/camunda/zeebe/engine/state/instance/DbJobState.java#L246\r\n\r\n\r\nWe wrote an Engine test, with snapshot and replay. Here everything worked fine. This means without migration there is no issue.\r\n\r\nLater we came up with an integration test which involves restarting the node and transitioning (this will cause migration to run).\r\n\r\n\r\n```java\r\n\r\n  @Test\r\n  public void test() {\r\n    // given\r\n    final String jobType = \"test\";\r\n    clientRule.createSingleJob(jobType);\r\n\r\n    final var activateResponse =\r\n        clientRule\r\n            .getClient()\r\n            .newActivateJobsCommand()\r\n            .jobType(jobType)\r\n            .maxJobsToActivate(1)\r\n            .send()\r\n            .join();\r\n    final var jobKey = activateResponse.getJobs().get(0).getKey();\r\n\r\n    final Duration backoffTimeout = Duration.ofDays(30);\r\n    clientRule\r\n        .getClient()\r\n        .newFailCommand(jobKey)\r\n        .retries(1)\r\n        .retryBackoff(backoffTimeout)\r\n        .send()\r\n        .join();\r\n\r\n    // when\r\n    clusteringRule.triggerAndWaitForSnapshots();\r\n    clusteringRule.restartBroker(clusteringRule.getLeaderForPartition(1).getNodeId());\r\n    clusteringRule.getClock().addTime(backoffTimeout.plus(backoffTimeout));\r\n\r\n    // then\r\n    Awaitility.await()\r\n        .until(\r\n            () ->\r\n                clientRule\r\n                    .getClient()\r\n                    .newActivateJobsCommand()\r\n                    .jobType(jobType)\r\n                    .maxJobsToActivate(1)\r\n                    .send()\r\n                    .join(),\r\n            r -> !activateResponse.getJobs().isEmpty());\r\n  }\r\n```\r\n\r\n\r\nIt turns out that this test fails. After restart and increasing the time, the job will never become active again.\r\n\r\n\r\nAdding some logs to the `DbJobState`:\r\n\r\n```diff\r\n--- a/engine/src/main/java/io/camunda/zeebe/engine/state/instance/DbJobState.java\r\n+++ b/engine/src/main/java/io/camunda/zeebe/engine/state/instance/DbJobState.java\r\n@@ -244,6 +244,14 @@ public final class DbJobState implements JobState, MutableJobState {\r\n           final var backoff = key.first().getValue();\r\n           final var job = jobsColumnFamily.get(jobKey);\r\n           if (job == null || job.getRecord().getRetryBackoff() != backoff) {\r\n+            //\r\n+            LOG.error(\r\n+                \"DeleteExisting job {} from Backoff CF (backoff= {}, recordBackoff={})\",\r\n+                key,\r\n+                backoff,\r\n+                job.getRecord().getRetryBackoff());\r\n+            final State state = getState(jobKey.getValue());\r\n+            LOG.error(\"State of job is {}\", state);\r\n             backoffColumnFamily.deleteExisting(key);\r\n           }\r\n\r\n```\r\n\r\nWe can see when running the test that if the leader goes down:\r\n```\r\n13:36:14.604 [Broker-0] [Startup] [zb-actors-1] DEBUG io.camunda.zeebe.broker.system - Shutdown was called with context: BrokerStartupContextImpl{broker=0}\r\n13:36:14.604 [Broker-0] [Startup] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Shutdown Broker Admin Interface\r\n13:36:14.606 [Broker-0] [Startup] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Shutdown Partition Manager\r\n```\r\n\r\nA small election loop starts where both nodes became first candidate and follower again. In both cases they run the migration, when become follower again and delete the job from the Backoff column family:\r\n\r\n```\r\n## Broker 1\r\n13:36:17.458 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.engine.state.migration - Starting JobBackoffCleanupMigration migration (8/16)\r\n13:36:17.460 [Broker-1] [ZeebePartition-1] [zb-actors-1] ERROR io.camunda.zeebe.broker.process - DeleteExisting job DbCompositeKey{first=DbLong{1703853368362}, second=DbForeignKey[inner=DbLong{2251799813685256}, columnFamily=JOBS, match=Full, skip=io.camunda.zeebe.db.impl.DbForeignKey$$Lambda/0x00007f34746a94a0@29b35c5f]} from Backoff CF (backoff= 1703853368362, recordBackoff=2592000000)\r\n13:36:17.460 [Broker-1] [ZeebePartition-1] [zb-actors-1] ERROR io.camunda.zeebe.broker.process - State of job is FAILED\r\n13:36:17.460 [Broker-1] [ZeebePartition-1] [zb-actors-1] DEBUG io.camunda.zeebe.engine.state.migration - JobBackoffCleanupMigration migration completed in 2 ms.\r\n\r\n## Broker 2\r\n13:36:17.458 [Broker-2] [ZeebePartition-1] [zb-actors-0] INFO  io.camunda.zeebe.engine.state.migration - Starting JobBackoffCleanupMigration migration (8/16)\r\n13:36:17.460 [Broker-2] [ZeebePartition-1] [zb-actors-0] ERROR io.camunda.zeebe.broker.process - DeleteExisting job DbCompositeKey{first=DbLong{1703853368362}, second=DbForeignKey[inner=DbLong{2251799813685256}, columnFamily=JOBS, match=Full, skip=io.camunda.zeebe.db.impl.DbForeignKey$$Lambda/0x00007f34746a94a0@29b35c5f]} from Backoff CF (backoff= 1703853368362, recordBackoff=2592000000)\r\n13:36:17.460 [Broker-2] [ZeebePartition-1] [zb-actors-0] ERROR io.camunda.zeebe.broker.process - State of job is FAILED\r\n13:36:17.460 [Broker-2] [ZeebePartition-1] [zb-actors-0] DEBUG io.camunda.zeebe.engine.state.migration - JobBackoffCleanupMigration migration completed in 2 ms.\r\n```\r\n\r\nLater Broker-1 becomes leader (on the next migration there is nothing to do):\r\n\r\n```\r\n13:36:22.451 [Broker-1] [raft-server-1] [raft-server-1-1] INFO  io.atomix.raft.impl.RaftContext - RaftServer{raft-partition-partition-1} - Transitioning to LEADER\r\n\r\n....\r\n13:36:22.456 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.engine.state.migration - Starting JobBackoffCleanupMigration migration (8/16)\r\n13:36:22.456 [Broker-1] [ZeebePartition-1] [zb-actors-1] DEBUG io.camunda.zeebe.engine.state.migration - JobBackoffCleanupMigration migration completed in 0 ms.\r\n13:36:22.456 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.engine.state.migration - Finished JobBackoffCleanupMigration migration (8/16)\r\n```\r\n\r\n\r\n\r\nBased on the output of the error log we added we can see that  the condition doesn't work as expected \r\n\r\nhttps://github.com/camunda/zeebe/blob/1c6c3cc2ec75bdb298c1e644131a586edbf0f501/engine/src/main/java/io/camunda/zeebe/engine/state/instance/DbJobState.java#L246\r\n\r\n```\r\n(backoff= 1703853368362, recordBackoff=2592000000)\r\n```\r\n\r\n\r\n\r\n\r\n\r\n**Side note:**\r\n\r\nI feel there is something going on with the role changes/transitions. It transitioned to follower in term two, then it should transition to follower on term 3 but it says later it does on term 2 🤔 Completes this again and then starts with term three. Maybe @deepthidevaki @oleschoenburg wants to look into.\r\n\r\n<details>\r\n<pre>\r\n13:36:17.471 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Transition to FOLLOWER on term 2 completed\r\n13:36:17.471 [Broker-1] [Exporter-1] [zb-fs-workers-0] DEBUG io.camunda.zeebe.broker.exporter - Recovered exporter 'Exporter-1' from snapshot at lastExportedPosition 23\r\n13:36:17.471 [Broker-1] [Exporter-1] [zb-fs-workers-0] DEBUG io.camunda.zeebe.broker.exporter - Configure exporter with id 'test-recorder'\r\n13:36:17.471 [Broker-1] [Exporter-1] [zb-fs-workers-0] DEBUG io.camunda.zeebe.broker.exporter - Set event filter for exporters: ExporterEventFilter{acceptRecordTypes={EVENT=true, COMMAND_REJECTION=true, SBE_UNKNOWN=true, COMMAND=true, NULL_VAL=true}, acceptValueTypes={DECISION=true, JOB=true, DEPLOYMENT=true, VARIABLE_DOCUMENT=true, ESCALATION=true, FORM=true, PROCESS_INSTANCE_MIGRATION=true, SBE_UNKNOWN=true, VARIABLE=true, SIGNAL=true, MESSAGE_BATCH=true, ERROR=true, TIMER=true, MESSAGE=true, MESSAGE_SUBSCRIPTION=true, PROCESS_INSTANCE=true, SIGNAL_SUBSCRIPTION=true, USER_TASK=true, PROCESS_MESSAGE_SUBSCRIPTION=true, DECISION_EVALUATION=true, PROCESS_INSTANCE_MODIFICATION=true, DECISION_REQUIREMENTS=true, MESSAGE_START_EVENT_SUBSCRIPTION=true, PROCESS_EVENT=true, DEPLOYMENT_DISTRIBUTION=true, PROCESS_INSTANCE_RESULT=true, PROCESS_INSTANCE_CREATION=true, PROCESS_INSTANCE_BATCH=true, RESOURCE_DELETION=true, CHECKPOINT=true, NULL_VAL=true, INCIDENT=true, PROCESS=true, JOB_BATCH=true, COMMAND_DISTRIBUTION=true}}\r\n13:36:18.523 [Broker-1] [ClusterTopologyManagerService] [zb-actors-0] WARN  io.camunda.zeebe.topology.gossip.ClusterTopologyGossiper - Failed to sync with gateway\r\n13:36:18.811 [Broker-1] [ClusterTopologyManagerService] [zb-actors-1] WARN  io.camunda.zeebe.topology.gossip.ClusterTopologyGossiper - Failed to sync with gateway\r\n13:36:19.949 [Broker-1] [raft-server-1] [raft-server-1-1] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{raft-partition-partition-1}{role=FOLLOWER} - Node priority 2 < target priority 3. Not triggering election.\r\n13:36:22.449 [Broker-1] [raft-server-1] [raft-server-1-1] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{raft-partition-partition-1}{role=FOLLOWER} - No heartbeat from a known leader since 8018ms\r\n13:36:22.449 [Broker-1] [raft-server-1] [raft-server-1-1] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{raft-partition-partition-1}{role=FOLLOWER} - Sending poll requests to all active members: [DefaultRaftMember{id=2, type=ACTIVE, updated=2023-11-29T12:36:04.116Z}, DefaultRaftMember{id=0, type=ACTIVE, updated=2023-11-29T12:36:04.116Z}]\r\n13:36:22.449 [Broker-1] [raft-server-1] [raft-server-1-1] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{raft-partition-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=2, type=ACTIVE, updated=2023-11-29T12:36:04.116Z} for next term 3\r\n13:36:22.449 [Broker-1] [raft-server-1] [raft-server-1-1] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{raft-partition-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=0, type=ACTIVE, updated=2023-11-29T12:36:04.116Z} for next term 3\r\n13:36:22.450 [Broker-1] [raft-server-1] [raft-server-1-1] WARN  io.atomix.raft.roles.FollowerRole - RaftServer{raft-partition-partition-1}{role=FOLLOWER} - Poll request to 0 failed: io.atomix.cluster.messaging.MessagingException$NoSuchMemberException: Expected to send a message with subject 'raft-partition-partition-1-poll' to member '0', but member is not known. Known members are '[Member{id=1, address=0.0.0.0:1032, properties={brokerInfo=EADJAAAABAABAAAAAQAAAAMAAAADAAAAAAABCgAAAGNvbW1hbmRBcGkMAAAAMC4wLjAuMDoxMDMxBQABAQAAAAEMAAAOAAAAOC40LjAtU05BUFNIT1QFAAEBAAAAAQ==}}, Member{id=2, address=0.0.0.0:1036, properties={brokerInfo=EADJAAAABAACAAAAAQAAAAMAAAADAAAAAAABCgAAAGNvbW1hbmRBcGkMAAAAMC4wLjAuMDoxMDM1BQABAQAAAAEMAAAOAAAAOC40LjAtU05BUFNIT1QFAAEBAAAAAQ==}}, Member{id=gateway, address=0.0.0.0:1038, properties={event-service-topics-subscribed=KIIDAGpvYnNBdmFpbGFibOU=}}]'.\r\n13:36:22.450 [Broker-1] [raft-server-1] [raft-server-1-1] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{raft-partition-partition-1}{role=FOLLOWER} - Received accepted poll from DefaultRaftMember{id=2, type=ACTIVE, updated=2023-11-29T12:36:04.116Z}\r\n13:36:22.450 [Broker-1] [raft-server-1] [raft-server-1-1] INFO  io.atomix.raft.impl.RaftContext - RaftServer{raft-partition-partition-1} - Transitioning to CANDIDATE\r\n13:36:22.450 [Broker-1] [raft-server-1] [raft-server-1-1] INFO  io.atomix.raft.roles.CandidateRole - RaftServer{raft-partition-partition-1}{role=CANDIDATE} - Starting election\r\n13:36:22.450 [Broker-1] [raft-server-1] [raft-server-1-1] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{raft-partition-partition-1} - Set term 3\r\n13:36:22.450 [Broker-1] [raft-server-1] [raft-server-1-1] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{raft-partition-partition-1} - Voted for 1\r\n13:36:22.450 [Broker-1] [raft-server-1] [raft-server-1-1] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{raft-partition-partition-1}{role=CANDIDATE} - Requesting votes for term 3\r\n13:36:22.450 [Broker-1] [raft-server-1] [raft-server-1-1] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{raft-partition-partition-1}{role=CANDIDATE} - Requesting vote from DefaultRaftMember{id=2, type=ACTIVE, updated=2023-11-29T12:36:04.116Z} for term 3\r\n13:36:22.450 [Broker-1] [raft-server-1] [raft-server-1-1] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{raft-partition-partition-1}{role=CANDIDATE} - Requesting vote from DefaultRaftMember{id=0, type=ACTIVE, updated=2023-11-29T12:36:04.116Z} for term 3\r\n13:36:22.451 [Broker-1] [raft-server-1] [raft-server-1-1] WARN  io.atomix.raft.roles.CandidateRole - RaftServer{raft-partition-partition-1}{role=CANDIDATE} - io.atomix.cluster.messaging.MessagingException$NoSuchMemberException: Expected to send a message with subject 'raft-partition-partition-1-vote' to member '0', but member is not known. Known members are '[Member{id=1, address=0.0.0.0:1032, properties={brokerInfo=EADJAAAABAABAAAAAQAAAAMAAAADAAAAAAABCgAAAGNvbW1hbmRBcGkMAAAAMC4wLjAuMDoxMDMxBQABAQAAAAEMAAAOAAAAOC40LjAtU05BUFNIT1QFAAEBAAAAAQ==}}, Member{id=2, address=0.0.0.0:1036, properties={brokerInfo=EADJAAAABAACAAAAAQAAAAMAAAADAAAAAAABCgAAAGNvbW1hbmRBcGkMAAAAMC4wLjAuMDoxMDM1BQABAQAAAAEMAAAOAAAAOC40LjAtU05BUFNIT1QFAAEBAAAAAQ==}}, Member{id=gateway, address=0.0.0.0:1038, properties={event-service-topics-subscribed=KIIDAGpvYnNBdmFpbGFibOU=}}]'.\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Transition to FOLLOWER on term 3 requested.\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] DEBUG io.camunda.zeebe.broker.system - Partition role transitioning from FOLLOWER to CANDIDATE in term 3\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Prepare transition from FOLLOWER on term 2 to FOLLOWER\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Prepare transition from FOLLOWER on term 2 to FOLLOWER - preparing Admin API\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Prepare transition from FOLLOWER on term 2 to FOLLOWER - preparing BackupApiRequestHandler\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Prepare transition from FOLLOWER on term 2 to FOLLOWER - preparing ExporterDirector\r\n13:36:22.451 [Broker-1] [StreamProcessor-1] [zb-actors-1] DEBUG io.camunda.zeebe.logstreams - Paused replay for partition 1\r\n13:36:22.451 [Broker-1] [Exporter-1] [zb-fs-workers-1] DEBUG io.camunda.zeebe.broker.exporter - Closed exporter director 'Exporter-1'.\r\n13:36:22.451 [Broker-1] [raft-server-1] [raft-server-1-1] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{raft-partition-partition-1}{role=CANDIDATE} - Received successful vote from DefaultRaftMember{id=2, type=ACTIVE, updated=2023-11-29T12:36:04.116Z}\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Prepare transition from FOLLOWER on term 2 to FOLLOWER - preparing SnapshotDirector\r\n13:36:22.451 [Broker-1] [raft-server-1] [raft-server-1-1] INFO  io.atomix.raft.impl.RaftContext - RaftServer{raft-partition-partition-1} - Transitioning to LEADER\r\n13:36:22.451 [Broker-1] [SnapshotDirector-1] [zb-actors-1] DEBUG io.camunda.zeebe.scheduler.ActorTask - Discard job io.camunda.zeebe.broker.system.partitions.impl.AsyncSnapshotDirector$$Lambda/0x00007f34749a5230 QUEUED from fastLane of Actor SnapshotDirector-1.\r\n13:36:22.451 [Broker-1] [raft-server-1] [raft-server-1-1] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{raft-partition-partition-1}{role=CANDIDATE} - Cancelling election\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Prepare transition from FOLLOWER on term 2 to FOLLOWER - preparing StreamProcessor\r\n13:36:22.451 [Broker-1] [StreamProcessor-1] [zb-actors-1] DEBUG io.camunda.zeebe.logstreams - Closed stream processor controller StreamProcessor-1.\r\n13:36:22.451 [Broker-1] [raft-server-1] [raft-server-1-1] INFO  io.atomix.raft.impl.RaftContext - RaftServer{raft-partition-partition-1} - Found leader 1\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Prepare transition from FOLLOWER on term 2 to FOLLOWER - preparing InterPartitionCommandService\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Prepare transition from FOLLOWER on term 2 to FOLLOWER - preparing BackupManager\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Prepare transition from FOLLOWER on term 2 to FOLLOWER - preparing BackupStore\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Prepare transition from FOLLOWER on term 2 to FOLLOWER - preparing QueryService\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Prepare transition from FOLLOWER on term 2 to FOLLOWER - preparing Migration\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Prepare transition from FOLLOWER on term 2 to FOLLOWER - preparing ZeebeDb\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Prepare transition from FOLLOWER on term 2 to FOLLOWER - preparing LogStream\r\n13:36:22.451 [Broker-1] [LogStream-1] [zb-actors-1] INFO  io.camunda.zeebe.logstreams - Close appender for log stream logstream-raft-partition-partition-1\r\n13:36:22.451 [Broker-1] [LogStream-1] [zb-actors-1] INFO  io.camunda.zeebe.logstreams - On closing logstream logstream-raft-partition-partition-1 close 1 readers\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Prepare transition from FOLLOWER on term 2 to FOLLOWER - preparing LogStorage\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Preparing transition from FOLLOWER on term 2 completed\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Transition to FOLLOWER on term 3 starting\r\n\r\n</pre>\r\n</details>\n deepthidevaki: > It transitioned to follower in term two, then it should transition to follower on term 3 but it says later it does on term 2\r\n\r\n`Transition to FOLLOWER on term 2 completed`\r\nIt transitioned to Follower on term 2. \r\n\r\n```\r\nTransition to FOLLOWER on term 3 requested.\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] DEBUG io.camunda.zeebe.broker.system - Partition role transitioning from FOLLOWER to CANDIDATE in term 3\r\n13:36:22.451 [Broker-1] [ZeebePartition-1] [zb-actors-1] INFO  io.camunda.zeebe.broker.system - Prepare transition from FOLLOWER on term 2 to FOLLOWER\r\n```\r\nThen it is \"Preparing\" transition from Follower on term 2. \r\n\r\n```\r\nTransition to FOLLOWER on term 3 starting\r\n```\r\nThen it is transitioning to term 3.\r\n\r\nSo everything looks good here.\n oleschoenburg: Hm, I guess this was my bad then :disappointed: https://github.com/camunda/zeebe/commit/1d82e6efc279043b9ff582cf5f66c04afbf53146\r\nI even raised it as a question here: https://github.com/camunda/zeebe/pull/13886#discussion_r1294250574\r\n\r\nI guess the cause is that the record backoff is not supposed to be the point in time where the backoff \"expires\", i.e. what's in the backoff CF.\n Zelldon: On fail we use the `getRecurringTime` to store the job in the backoff column\r\n\r\n```java\r\n  @Override\r\n  public void fail(final long key, final JobRecord updatedValue) {\r\n    if (updatedValue.getRetries() > 0) {\r\n      if (updatedValue.getRetryBackoff() > 0) {\r\n->        addJobBackoff(key, updatedValue.getRecurringTime());\r\n        updateJob(key, updatedValue, State.FAILED);\r\n      } else {\r\n        updateJob(key, updatedValue, State.ACTIVATABLE);\r\n      }\r\n    } else {\r\n      updateJob(key, updatedValue, State.FAILED);\r\n      makeJobNotActivatable(updatedValue.getTypeBuffer(), updatedValue.getTenantId());\r\n    }\r\n  }\r\n```\r\n\r\nLater we check against the backoff of the record. I guess this is the issue?\r\n\r\nhttps://github.com/camunda/zeebe/blob/1c6c3cc2ec75bdb298c1e644131a586edbf0f501/engine/src/main/java/io/camunda/zeebe/engine/state/instance/DbJobState.java#L246\n Zelldon: Thanks @deepthidevaki I'm always (still) confused by these logs.\r\n\r\nI guess what I miss is the to term which it is transitioning to like instead of\r\n\r\n```\r\nPrepare transition from FOLLOWER on term 2 to FOLLOWER\r\n```\r\n\r\n```\r\nPrepare transition from FOLLOWER on term 2 to FOLLOWER on term 3\r\n```\n oleschoenburg: > On fail we use the getRecurringTime to store the job in the backoff column\r\n\r\nYeah I think that's it :facepalm: \n Zelldon: I created https://github.com/camunda/zeebe/pull/15419 to fix the general issue.\r\n\r\n@camunda/zeebe-process-automation please take a look regarding how to handle already stuck jobs, I would like to hand this over to you from here on. \n abbasadel: Thanks @koevskinikola for mentioning this, I removed the label \"version\". it seems they were put mistakely by the change-log command.\n abbasadel: As discussed with @nicpuppa , we also need to backport this fix to 8.1, 8.2 plus 8.3",
    "title": "Failed jobs are not reactivate after backoff",
    "releaseNoteText": "A combination of migrating a broker to a new version and jobs that had failed with a backoff time specified could result in jobs becoming stuck and not retried after a broker migration to a new version. This was due to a bug in the algorithm used to delete orphaned jobs from the queue. The algorithm has been fixed and failed jobs with a backoff time now function as expected after migration."
  }
]