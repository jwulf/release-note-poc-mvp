[
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14019",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\n\r\nFor multi-tenancy we need to store a tenant id in the state. We've decided to do this using a new object: `DbTenantAwareKey`.\r\n\r\nThis key will implement the `DbKey` interface. It will always contain a `DbString` which is used to store the tenantId. Besides this it can wrap any other key.\r\n\r\nEg:\r\n```java\r\npublic record DbTenantAwareKey<WrappedKey extends DbKey>(DbString tenantKey, WrappedKey wrappedKey)\r\n    implements DbKey\r\n```\n",
    "title": "Create `DbTenantAwareKey`"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13989",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nZeebe auth data (ex. the user’s tenant access list) should be sent from the gateway to the broker in a way that is easily extendable in the future. Extendability is important since Zeebe might need to support user permissions (https://github.com/camunda/product-hub/issues/495) in the future, and it would be better if we already have the code that can contain these future extensions without any significant changes.\r\n\r\nSince Zeebe auth data may be used in all types of Zeebe Records, it would be better to place it in the `RecordMetadata` since:\r\n* It is a single change that makes auth data available to all records.\r\n* Conceptually, auth data is more closely related to `Intent`s which are already placed in the `RecordMetadata`.\r\n* It doesn't pollute the record value since auth data is irrelevant to the outcome of commands.\r\n\r\nWhen placed in the `RecordMetadata` the auth data should be encoded so that:\r\n* It can be extracted only when needed\r\n* It can be extended without any further changes to the `RecordMetadata` strucutre\r\n\r\n### AC\r\n- [x] Auth data is contained in `RecordMetadata`\r\n- [x] Auth data is encoded within the `RecordMetadata`\r\n- [x] Auth data contains a flag to indicate the mechanism used to encode/decode it\r\n- [x] Auth data is contained in `ExecuteCommandRequest` (to enable Gateway-to-Broker requests)\r\n\n",
    "title": "Add auth data to Zeebe records"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13988",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nZeebe auth data (ex. the user’s tenant access list) should be sent from the gateway to the broker in a way that is easily extendable in the future. Extendability is important since Zeebe might need to support user permissions (https://github.com/camunda/product-hub/issues/495) in the future, and it would be better if we already have the code that can contain these future extensions without any significant changes.\r\n\r\nIdeally, the data interchange protocol/mechanism should be pluggable, so that Zeebe has the ability to move to something different in the future.\r\n\r\nFor this iteration, unsigned JWT tokens will be used, for the following reasons:\r\n* Using JWT is future-proof.\r\n   * If we decide to export auth data in the future, we can use signed JWTs (JWS), so users have a high confidence that the auth data is valid.\r\n* JWTs are already used by other C8 components, so we ensure wider compatibility.\r\n* We already have TLS secure communication so we can trust unsigned JWT tokens (for this iteration).\r\n* Lower implementation effor than other options (ex. MsgPack), as JWT libraries already provide APIs to work with different Java types.\r\n\r\n## AC\r\n- [x] A general API for auth data encoding is available.\r\n- [x] A general API for auth data decoding is available.\r\n- [x] Auth data can be encoded in a JWT token string\r\n- [x] Auth data can be decoded from a JWT token string\n",
    "title": "Provide an API for data interchange of Zeebe auth data"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13987",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nA `TenantAccessChecker` class should be implemented in the `engine` module. It should be used by `*Processor` classes, or any other classes that process `Command` records created by Client requests. The purpose of the class is to determine if the User making the request has access to the requested Tenant data.\r\n\r\nThe class should provide the following methods:\r\n* `hasAccess(String tenantId, , List<String> authorizedTenants)` - to determine if a user has access to data from a given tenant. The output can be `Either<Exception, String>` providing the tenantId.\r\n   * If the `tenantId` is listed in the `RecordValueWithTenantPermissions`, the `tenantId` is returned. Otherwise, an error is raised.\r\n* `hasFullAccess(List<String> tenantIds, List<String> authorizedTenants)` - to determine if a user has access to all of the `tenantIds` provided in a list. This will be used later for job polling.\r\n\r\n:information_source: We might expand or refactor this class according to different needs as development on the multi-tenancy topic progresses.\r\n\r\n### AC\r\n- [x] A `TenantAccessChecker` class\r\n- [x] Test coverage for the `TenantAccessChecker` class\n",
    "title": "Provide a `TenantAccessChecker` API for multi-tenancy"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/8263",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nCame up in a recent [incident review ](https://docs.google.com/document/d/1E03kB3UCfM3l8X0xU7RjzqvOnX8dk4GtKAmH5-_vY60/edit) that it would be nice to see whether instances are blacklisted or not, maybe also the rate of blacklisting.\r\n\r\nI realized that we added a while ago the metrics for that, but never added this to the dashboard, see https://github.com/camunda-cloud/zeebe/pull/6715. \r\n\r\n**Describe the solution you'd like**\r\n\r\nEither show a graph of blacklisted instance count or just some indication whether instances are blacklisted.\r\n\r\nI had yesterday a **hard time**  to find a good visualization, since the count is only exported if there was a new blacklisted instance. If the pod is restarted the metric is not exported. _It might make sense to refill the metric on restart_ OR we need to work with it, but this would mean we can only show limited data, see below.\r\n\r\n**Example A - Show an indication that instances are blacklisted**\r\n\r\n![gauge-blacklisted](https://user-images.githubusercontent.com/2758593/143187841-3f9d4a4c-50b9-49de-95b0-43650d479103.png)\r\n![gauge-blacklisted-general](https://user-images.githubusercontent.com/2758593/143187843-2c6c629b-0687-463a-82de-970f39a4e372.png)\r\n\r\nIn general I like this, since it shows directly whether there is something wrong.\r\nThe problem here is if the time frame is smaller (were no instances are blacklisted) than this indication is green. :-1: \r\n\r\n**Example B - Graph**\r\n\r\nShowing a graph is not that fruitful, since as described above the metric is not always exported. \r\n![graph-blacklisted](https://user-images.githubusercontent.com/2758593/143187948-f6d36e3e-1b15-48a6-82ba-872d58592ca5.png)\r\n\r\nShowing zero for null values\r\n![graph-zero-blacklisted](https://user-images.githubusercontent.com/2758593/143188265-1b85b331-2306-40d2-94df-0132255c767c.png)\r\n\r\n**Example C - Rate**\r\n\r\nThe rate of such metric is also not really useful, since the change is too rare.\r\n\r\n![graph-rate-blacklisted](https://user-images.githubusercontent.com/2758593/143188214-3b44348e-3b0c-4464-85ed-5ccbb62ed60f.png)\r\n\r\n\r\n**Example D - Table**\r\n![table-blacklisted](https://user-images.githubusercontent.com/2758593/143188772-b439d832-b192-47ed-8b6e-dbe31842a7b9.png)\r\n\r\nOther alternative would be to show in a table the recent count, but this is also very limited (to the time frame) and other\r\n\r\n**Describe alternatives you've considered**.\r\n\r\nIdeally we would export the metric always, this would simplify things enormous. Then we can choose better one or more of the possible visualization from above.\r\n\r\n**Additional context**\r\n\r\nBlacklisting always shows that something problematic happened in the process execution (processing). An exception was thrown during the execution, which mostly an indication of a bug. \r\n\r\nI would like to find a good way how we can visualize it and hope someone has some comments, opinion or ideas.\r\n\r\n\\cc @pihme\r\n\n\n pihme: What about exposing the metric as a rate and visualizing it with a heat map? \n npepinpe: Prioritized as planned for now under the assumption we won't work on a better long term alternative to blacklisting in the next quarter, and this will already be an improvement in terms of visibility. Before opening a PR for this, please discuss and decide on the visualization/metric that we want.\n Zelldon: The question for me is really what we want to achieve with the metric.\r\n\r\nDo we want to know really how many (exact) are blacklisted? Then Prometheus might be not the best fit, but if we just want to have an indicator that SOMETHING is blacklisted then this could work:\r\n \r\n![blacklist](https://user-images.githubusercontent.com/2758593/148549724-9548b692-0f78-44a6-9783-0692e0bd9919.png)\r\n\r\nHere we could change the colors and shown values to something like Blacklisted (if x > 0) and nothing (if x <= 0).I think this is similar what I have shown above with example A. This would potentially already help.\r\n\n pihme: What about exposing the metric as a rate and visualizing it with a heat map?\n Zelldon: This wouldn't help since the metric is not exported all the time. Plus what should tell me the heatmap would be the question?\n pihme: I would expect to see a change in the blacklisting rate.\r\n\r\nKinda like here:\r\n![image](https://user-images.githubusercontent.com/14032870/148551718-06e323ce-d576-4e46-bd78-130a9e97a4d5.png)\r\n\r\nIf it is not exported for some time, I will just see a black column, but I can always make the time scale wider and then I should see data pretty much for all the time, because each export would be aggregated in the rate (I hope).\r\n\r\nThen I could look at a big enough time scale and if my blacklisting suddenly jumps from 2 per day to 200 per day, that would be my signal. And if the jump is correlated to e.g. the point there was an update or a pod restart, that would also be interesting.\r\n\r\nThis is all speculation though, one would have to see what it actually looks like.\n Zelldon: Thanks @pihme \r\n\r\nyeah so it would look like this\r\n\r\n15 mins\r\n![heatmap](https://user-images.githubusercontent.com/2758593/148553374-792367bc-f419-43cf-a476-70b9f9244280.png)\r\n\r\n90 days\r\n![heatmap2](https://user-images.githubusercontent.com/2758593/148553377-ae5ce979-9e40-4134-a8c1-76a636c77043.png)\r\n\r\nProblem is that we not store data long enough on our prometheus server. So there can be data deleted which contained some blacklisting, which is why I ask what we want to see. The real count or just an indication all in all I think our current metric doesn't work well. Ideally we would report always or on bootstrap with the current value of blacklisted instances.\r\n\n pihme: Thanks @Zelldon. Yeah I regularly forget we forget data. I think 90 days would be good enough, and yes, the visualization looks like what I had in mind. But if the history is 30 days or less, it becomes less useful. So I see your point.\n Zelldon: I think the best would be as described as alternative above:\r\n\r\n> **Describe alternatives you've considered.**\r\n> Ideally we would export the metric always, this would simplify things enormous. Then we can choose better one or more of the possible visualization from above.\r\n\r\nThis means at least on restart we export the metric at least once. \n Zelldon: @remcowesterhoud since you mentioned to me that you had to look at a cluster with zdb to find out whether something was blacklisted might be useful for you to just have this in the metrics :) \n menski: Right now we don't see value to implement this, we have other ways to identify blacklisted instances, which also allow us to see the corresponding process instance keys, i.e. logs or zdb.\n\nHopefully at one point we can get rid of blacklisting\n korthout: ZPA triage:\r\n- we want to gain more insights into the frequency of blacklisting before choosing to replace the concept\n remcowesterhoud: Attaching this to the \"Blacklisting replacement\" as having insights in these metrics may be useful for this epic.\n Zelldon: Reopening since we still need panels on the dashboard. \n koevskinikola: ZPA triage:\r\n- @Zelldon you mentioned [HERE](https://github.com/camunda/zeebe/pull/12606#issuecomment-1528862192) that you'll add the Grafana dashboards. Is this still your plan, or ZPA should do something from our side?\r\n  - ZPA also has this issue marked as `upcoming` so we would do it soon either way.",
    "title": "Show blacklisting in the Grafana Dashboard"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14497",
      "component": "Zeebe",
      "subcomponent": "Gateway",
      "context": "Enhancements"
    },
    "gitHubIssueText": "All clients should be able to use the default tenant. Even the client is not associated to any tenants in Identity.\r\n\r\n```java\r\n@Test\r\n@Disabled(\"Not yet supported: https://github.com/camunda/zeebe/issues/14497\")\r\nvoid shouldDenyDeployProcessWhenNoTenantAssociated() {\r\n  // given\r\n  try (final var client = createZeebeClient(ZEEBE_CLIENT_ID_WITHOUT_TENANT)) {\r\n    // when\r\n    final Future<DeploymentEvent> result =\r\n        client.newDeployResourceCommand().addProcessModel(process, \"process.bpmn\").send();\r\n\r\n    // then\r\n    assertThat(result)\r\n        .describedAs(\"Expect that process can be deployed for the default tenant\")\r\n        .succeedsWithin(Duration.ofSeconds(10));\r\n  }\r\n}\r\n```\n\n remcowesterhoud: We shouldn't implement this, details https://camunda.slack.com/archives/C05Q0EXE9V4/p1696235964583069\n korthout: For those reading this and wonder what's in the internal slack thread:\r\n\r\nWe're not implementing to support a specific use case.\r\n\r\nThe use-case is as follows:\r\n- Team A utilized a 8.2 cluster\r\n- They want to onboard Team B on the same cluster\r\n- They update to 8.3, activate MT, all their data is owned by the default (`<default>`) tenant\r\n- Team B should not be able to see data from Team A that already used the cluster, so they should not be able to access the default tenant\r\n- This can be configured explicitly in Identity",
    "title": "Allow using the default tenant even when no tenant associated to requester"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14278",
      "component": "Zeebe",
      "subcomponent": "Gateway",
      "context": "Enhancements"
    },
    "gitHubIssueText": "null\n",
    "title": "Gateway supports multi-tenancy in EvaluateDecision RPC"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14276",
      "component": "Zeebe",
      "subcomponent": "Gateway",
      "context": "Enhancements"
    },
    "gitHubIssueText": "null\n",
    "title": "Gateway supports multi-tenancy in PublishMessage RPCs"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14254",
      "component": "Zeebe",
      "subcomponent": "Gateway",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nThe Gateway supports receiving and forwarding (to the Broker) `ActivateJobs` and `StreamActivatedJobs` RPC calls with a List of `tenantId`s.\r\n\r\nThe IDs specify the tenants whose jobs the Client/user wants to activate/process.\r\n\r\nNote: if the tenant ids list is empty, all of the user's authorized tenants should be used instead.\r\n\r\nThe following error codes may be returned:\r\n* PERMISSION_DENIED (code: 7) \r\n   * when a user attempts to poll/stream jobs for a tenant they are not authorized for, when multi-tenancy is enabled.\r\n* INVALID_ARGUMENT (code: 3)\r\n   * For a provided (non-default) tenant id, when multi-tenancy is disabled\r\n   * For an invalid tenant id (i.e. doesn't match the pre-defined format), when multi-tenancy is enabled.\n",
    "title": "Job polling/pushing in the Gateway supports multi-tenancy"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14211",
      "component": "Zeebe",
      "subcomponent": "Gateway",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nThe Gateway supports receiving and forwarding (to the Broker) `CreateProcessInstance*` RPC calls with a `tenantId`.\r\n\r\n### AC\r\n- [x] The following gRPC messages contain a new `tenantId` property:\r\n       - [ ] `CreateProcessInstanceRequest`\r\n       - [ ] `CreateProcessInstanceWithResultRequest`\r\n       - [ ] `CreateProcessInstanceResponse`\r\n- [x] The `createProcessInstance(...)` Gateway endpoint passes the gRPC `CreateProcessInstanceRequest#tenantId` property to the `BrokerCreateProcessInstanceRequest`. The following scenarios are possible as well:\r\n      - If multi-tenancy is disabled (see #13237), the `BrokerCreateProcessInstanceRequest#tenantId` is set to `<default>`.\r\n      - If multi-tenancy is enabled, and `BrokerCreateProcessInstanceRequest#tenantId` is `null`, the request is rejected.\r\n- [x] The `createProcessInstanceWithResult(...)` Gateway endpoint passes the gRPC `CreateProcessInstanceWithResultRequest#tenantId` property to the `BrokerCreateProcessInstanceWithResultRequest`. The following scenarios are possible as well:\r\n      - If multi-tenancy is disabled (see #13237), the `BrokerCreateProcessInstanceWithResultRequest#tenantId` is set to `<default>`.\r\n      - If multi-tenancy is enabled, and `BrokerCreateProcessInstanceWithResultRequest#tenantId` is `null`, the request is rejected.\n",
    "title": "Gateway supports multi-tenancy in CreateProcessInstance* RPCs"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14041",
      "component": "Zeebe",
      "subcomponent": "Gateway",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nProvide a Gateway configuration flag: `zeebe.gateway.multiTenancy.enabled` - default **FALSE** (disabled)\r\n  * The flag should toggle multi-tenancy for the Zeebe cluster.\r\n  * An `enabled` flag is preferred, as additional MT-related configuration properties might be added in the future (stretch goal).\n",
    "title": "I can use a Gateway configuration flag for multi-tenancy"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13989",
      "component": "Zeebe",
      "subcomponent": "Gateway",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nZeebe auth data (ex. the user’s tenant access list) should be sent from the gateway to the broker in a way that is easily extendable in the future. Extendability is important since Zeebe might need to support user permissions (https://github.com/camunda/product-hub/issues/495) in the future, and it would be better if we already have the code that can contain these future extensions without any significant changes.\r\n\r\nSince Zeebe auth data may be used in all types of Zeebe Records, it would be better to place it in the `RecordMetadata` since:\r\n* It is a single change that makes auth data available to all records.\r\n* Conceptually, auth data is more closely related to `Intent`s which are already placed in the `RecordMetadata`.\r\n* It doesn't pollute the record value since auth data is irrelevant to the outcome of commands.\r\n\r\nWhen placed in the `RecordMetadata` the auth data should be encoded so that:\r\n* It can be extracted only when needed\r\n* It can be extended without any further changes to the `RecordMetadata` strucutre\r\n\r\n### AC\r\n- [x] Auth data is contained in `RecordMetadata`\r\n- [x] Auth data is encoded within the `RecordMetadata`\r\n- [x] Auth data contains a flag to indicate the mechanism used to encode/decode it\r\n- [x] Auth data is contained in `ExecuteCommandRequest` (to enable Gateway-to-Broker requests)\r\n\n",
    "title": "Add auth data to Zeebe records"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13988",
      "component": "Zeebe",
      "subcomponent": "Gateway",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nZeebe auth data (ex. the user’s tenant access list) should be sent from the gateway to the broker in a way that is easily extendable in the future. Extendability is important since Zeebe might need to support user permissions (https://github.com/camunda/product-hub/issues/495) in the future, and it would be better if we already have the code that can contain these future extensions without any significant changes.\r\n\r\nIdeally, the data interchange protocol/mechanism should be pluggable, so that Zeebe has the ability to move to something different in the future.\r\n\r\nFor this iteration, unsigned JWT tokens will be used, for the following reasons:\r\n* Using JWT is future-proof.\r\n   * If we decide to export auth data in the future, we can use signed JWTs (JWS), so users have a high confidence that the auth data is valid.\r\n* JWTs are already used by other C8 components, so we ensure wider compatibility.\r\n* We already have TLS secure communication so we can trust unsigned JWT tokens (for this iteration).\r\n* Lower implementation effor than other options (ex. MsgPack), as JWT libraries already provide APIs to work with different Java types.\r\n\r\n## AC\r\n- [x] A general API for auth data encoding is available.\r\n- [x] A general API for auth data decoding is available.\r\n- [x] Auth data can be encoded in a JWT token string\r\n- [x] Auth data can be decoded from a JWT token string\n",
    "title": "Provide an API for data interchange of Zeebe auth data"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13987",
      "component": "Zeebe",
      "subcomponent": "Gateway",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nA `TenantAccessChecker` class should be implemented in the `engine` module. It should be used by `*Processor` classes, or any other classes that process `Command` records created by Client requests. The purpose of the class is to determine if the User making the request has access to the requested Tenant data.\r\n\r\nThe class should provide the following methods:\r\n* `hasAccess(String tenantId, , List<String> authorizedTenants)` - to determine if a user has access to data from a given tenant. The output can be `Either<Exception, String>` providing the tenantId.\r\n   * If the `tenantId` is listed in the `RecordValueWithTenantPermissions`, the `tenantId` is returned. Otherwise, an error is raised.\r\n* `hasFullAccess(List<String> tenantIds, List<String> authorizedTenants)` - to determine if a user has access to all of the `tenantIds` provided in a list. This will be used later for job polling.\r\n\r\n:information_source: We might expand or refactor this class according to different needs as development on the multi-tenancy topic progresses.\r\n\r\n### AC\r\n- [x] A `TenantAccessChecker` class\r\n- [x] Test coverage for the `TenantAccessChecker` class\n",
    "title": "Provide a `TenantAccessChecker` API for multi-tenancy"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13237",
      "component": "Zeebe",
      "subcomponent": "Gateway",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\n\nFor multi-tenancy, the goal of the Zeebe Gateway is to:\n1. Provide data on the user's authorized tenants.\n2. Forward this data (through the `RecordMetadata`) to the Zeebe Broker through a `Broker*Request` instance.\n\nFull support for multi-tenancy in the Zeebe gateway will be provided once https://github.com/camunda-cloud/identity/issues/1819 is completed.\n\nFor this issue, the Zeebe Gateway should (only) support the following:\n\n* Provide a Gateway configuration flag: `zeebe.gateway.multiTenancy.enabled` - default **FALSE** (disabled)\n  * The flag should toggle multi-tenancy for the Zeebe cluster.\n  * An `enabled` flag is preferred, as we will probably add additional multi-tenancy configuration properties in the future.\n* Expand the [`IdentityInterceptor`](https://github.com/camunda/zeebe/blob/1434d4682eba842ef022ec500cd80ebe7cc29a2a/gateway/src/main/java/io/camunda/zeebe/gateway/interceptors/impl/IdentityInterceptor.java#L65) class. After the token is successfully verified the following should be performed:\n   1. Provide a list of tenant ids the user has access to:\n      * If multi-tenancy is disabled, the list should only contain the `<default>` tenant id (see #13235)\n      * If multi-tenancy is enabled, the Identity SDK should be used to fetch the list of tenant ids (out-of-scope for this issue)\n   2. [Proposal] Place the list of tenant IDs in a [gRPC context](https://grpc.github.io/grpc-java/javadoc/io/grpc/Context.html) to be forwarded to the `EndpointManager`/`RequestMapper` (consider doing it through a helper class).\n* We should provide a general (auth) API that the IdentityInterceptor class will implement. The goal is to provide an Identity-independent use of multi-tenancy, i.e. users can use a multi-tenant Zeebe without the need of Camunda Identity.\n\n**AC:**\n\n```[tasklist]\n### Tasks\n- [ ] https://github.com/camunda/zeebe/issues/14041\n- [x] The Gateway can set/get values from a gRPC context\n- [x] `IdentityInterceptor` class is expanded according to the description above.\n- [ ] https://github.com/camunda/zeebe/issues/14284\n- [ ] https://github.com/camunda/zeebe/issues/14307\n```\n---\n\n```[tasklist]\n### Out of scope\n- [ ] https://github.com/camunda/zeebe/issues/14285\n- [ ] https://github.com/camunda/zeebe/issues/14396\n```\n\n\n romansmirnov: @koevskinikola & @korthout,\r\n\r\n>* [...] After the token is successfully verified the following should be performed:\r\n>    * Provide a list of tenant ids the user has access to:\r\n>       * If multi-tenancy is disabled, the list should only contain the <default> tenant id (see https://github.com/camunda/zeebe/issues/13235)\r\n\r\nDoes that mean, that if tenancy is disabled, it will still ask Identity to provide a list of tenants?\r\n\r\nAnd another question: What happens if tenancy is disabled and Identity is not configured (i.e., Zeebe is used \"standalone\")? Does the Gateway still provide the `default` tenant to include it in the respective broker requests?\n korthout: >if tenancy is disabled, it will still ask Identity to provide a list of tenants?\r\n\r\nI don't think so, because that's the behavior when multi-tenancy is enabled.\r\n\r\n>If multi-tenancy is enabled, the Identity SDK should be used to fetch the list of tenant ids\r\n\r\nApart from the description, I would also find it very odd if Identity is still asked.\r\n\r\n---\r\n\r\n> What happens if tenancy is disabled and Identity is not configured (i.e., Zeebe is used \"standalone\")? Does the Gateway still provide the default tenant to include it in the respective broker requests?\r\n\r\nI would assume that the default tenant would be added to the tenant access ids list, so it can be used in the broker requests where the default tenant is assumed when multi-tenancy is disabled. This means data isolation can be guaranteed.\n abbasadel: I moved https://github.com/camunda/zeebe/issues/14285 out of scope and will link it to a different issue.\r\n\n korthout: @abbasadel I've marked #14286 as closing this issue. I think that's fine for Zeebe, as #14307 is Helm Charts related and not blocking Zeebe.\n abbasadel: Make sense. Thanks @korthout \r\n",
    "title": "Support multi-tenancy in the Gateway"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14255",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nThe JobWorker API of the Zeebe Java client supports tenant-aware job workers by integrating the new behavior provided by #13560.\r\n\r\nThe JobWorker API should:\r\n* Provide methods for specifying single or multiple tenant identifiers for which jobs will be polled/streamed.\r\n* Pick up tenant IDs from the `defaultJobWorkerTenantIds` client configuration property if nothing is defined.\n",
    "title": "The JobWorker API of the java client support multi-tenancy"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13752",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nWhenever I'm using the Java client with variables, I can end up writing `job.getVariablesAsMap().get(\"name\")` a lot. Of course, I can use `getVariablesAsType` with a record; but a shorthand method would be useful.\r\n\r\n**Describe the solution you'd like**\r\n`job.getVariable(\"name\")`.\r\n\r\nUnder the hood, this makes a call to `job.getVariablesAsMap()`, and *caches* the map (to make it as performant as deserialising to a map, then accessing various variables in user code), then returns `.get(\"name\")` from that map.\r\n\r\n**Additional Context**\r\n\r\nYes, there are better ways to do the whole thing - including deserialising to an object/class/record or using Springboot. \r\n\r\nHowever, for first experience with the platform (including C8 Platform training), reducing ceremonial boilerplate will be good. \r\n\n\n korthout: I believe this would be useful (reasonable desire -> `impact/medium`), and the effort is `small`\n korthout: ZPA Triage:\n- the simple solution (this just being a convenience method) seems low effort, size: x-small\n- a more performant solution could be done later\n- @jwulf would you be willing to contribute this change?\n- we're marking it as later for our own priorities\n remcowesterhoud: @abbasadel FYI the team thinks that this is low-hanging fruit. However, it doesn't show up on the board. Could we improve the query in the board to also include issues that are x-small and medium impact?",
    "title": "job.getVariable ergonomic method in Java client"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13560",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\nThe Java client provides a `ActivateJobsCommandImpl` and `StreamJobsCommandImpl` commands for processing tenant-aware jobs in Zeebe. These commands should support multi-tenancy by exposing an optional `tenantIds` property/method.\r\n\r\nThe `tenantIds` property should allow the Java Client to specify a single, or multiple tenant IDs for which the client will poll/stream Jobs from the Zeebe Gateway/Broker.\r\n\r\nThe following error codes may be returned:\r\n* PERMISSION_DENIED (code: 7) \r\n   * when a user attempts to poll/stream jobs for a tenant they are not authorized for, when multi-tenancy is enabled.\r\n* INVALID_ARGUMENT (code: 3)\r\n   * For a provided (non-default) tenant id, when multi-tenancy is disabled\r\n   * For an invalid tenant id (i.e. doesn't match the pre-defined format), when multi-tenancy is enabled.\r\n\r\n### AC\r\n\r\n- A `ClientProperties#DEFAULT_JOB_WORKER_TENANT_IDS` with value `zeebe.client.worker.tenantIds` is defined.\r\n- The `ZeebeClientBuilderImpl` class is expanded with a `defaultJobWorkerTenantIds` property.\r\n       - The `ZeebeClientBuilderImpl#withProperties(...)` method may set the `defaultJobWorkerTenantIds` property to a value defined by `zeebe.client.worker.tenantIds` in a `.properties` file (see point above).\r\n- The `ActivateJobsCommandImpl` and `StreamJobsCommandImpl` commands provide a new `tenantIds(List<String> tenantIds)` method.\r\n       - The command will set the `tenantId`s to the value of `zeebe.client.worker.tenantIds` if provided through a `.properties` file.\r\n       - The default value if the `tenantIds` property should be `['<default>']`.\r\n\r\n\r\n### Blocks\r\n- Connectors team\n",
    "title": "Java client supports multi-tenancy for the ActivateJobs/StreamJobs commands"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13559",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "null\n",
    "title": "Java client supports multi-tenancy for PublishMessage RPC"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13557",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "null\n\n koevskinikola: @korthout did you plan to add anything specific to the description of this issue?\n korthout: @koevskinikola No, I just needed an issue reference to refer to from the code (in todos).\n\nSince none of the tasks in the umbrella issue were converted to issues yet I expected that this would come later when we'd start work on it.",
    "title": "Java client supports multi-tenancy for EvaluateDecision RPC"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13536",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\nThe Java client provides a `CreateProcessInstanceCommand` and `CreateProcessInstanceWithResultCommand` for creating tenant-aware process instances in Zeebe. These commands should support multi-tenancy by exposing an optional `tenantId` property/method.\r\n\r\nThe following error codes may be returned:\r\n* PERMISSION_DENIED (code: 7) \r\n   * when a user attempts to start a process instance of a tenant they are not authorized for, when multi-tenancy is enabled.\r\n* INVALID_ARGUMENT (code: 3)\r\n   * For a provided tenant id, when multi-tenancy is disabled\r\n   * For a missing tenant id, when multi-tenancy is enabled\r\n   * For an invalid tenant id (i.e. doesn't match the pre-defined format), when multi-tenancy is enabled.\r\n\r\n### Blocks\r\n- [Web Modeler](https://github.com/camunda/web-modeler/issues/5058)\n",
    "title": "Java client supports multi-tenancy for CreateProcessInstance* RPCs"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13473",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\n\r\nTo simplify migration to job streaming, we will integration the new `StreamJobsCommand` into the job worker API.\r\n\r\nAs job streaming still under development, this will be an opt-in feature initially, and should be disabled by default, such that the job worker behaves just as it used to unless streaming is enabled.\r\n\r\nWith this issue, the job worker builder should:\r\n\r\n- [ ] Expose a opt-in method for streaming\r\n- [ ] Expose an additional timeout API to set the stream timeout (if any). We cannot reuse the existing `requestTimeout`, since that's used for the polling mechanism, and will likely be much smaller than the streaming timeout.\r\n\r\nWhen opted in, the worker will:\r\n\r\n- [ ] Open a long living stream on `open`, using the same parameters as for the `ActivateJobsCommand` (where applicable).\r\n- [ ] Jobs activated via the stream are handled exactly like jobs activated via `ActivateJobsCommand`\r\n- [ ] Jobs activated via the stream do not count towards the `remainingJobs` which control polling; since polling serves to back fill older jobs, we don't want continuous load on a stream to prevent that.\r\n- [ ] Polling should still work and remain completely independent from streaming.\r\n- [ ] If the stream is closed while the worker is still opened, it should be re-opened. We can reuse the back off mechanism on error.\r\n- [ ] Close the stream when closing itself (and the stream is not reopened)\r\n\r\n\n",
    "title": "Stream jobs using job worker"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13460",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThis feature is related to #13040 . Using Java Client I want to complete `.newModifyProcessInstanceCommand()` by using only a single variable\r\n\r\n**Describe the solution you'd like**\r\n`client.newModifyProcessInstanceCommand(job).variable(\"name\", value)`\r\n\r\n**Describe alternatives you've considered**\r\n`client.newModifyProcessInstanceCommand(job).variables(Map.of(\"name\", value))`, but `Map.of()` is available only with Java 9 or above\r\n\n",
    "title": "newModifyProcessInstanceCommand: complete command with single variable"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13458",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThis feature is related to #13040 . Using Java Client I want to complete `.newThrowErrorCommand()` by using only a single variable\r\n\r\n**Describe the solution you'd like**\r\n`client.newThrowErrorCommand(job).variable(\"name\", value)`\r\n\r\n**Describe alternatives you've considered**\r\n`client.newThrowErrorCommand(job).variables(Map.of(\"name\", value))`, but `Map.of()` is available only with Java 9 or above\r\n\n",
    "title": "newThrowErrorCommand: complete command with single variable"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13456",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThis feature is related to #13040  Using Java Client I want to complete `.newFailCommand()` by using only a single variable\r\n\r\n**Describe the solution you'd like**\r\n`client.newFailCommand(job).variable(\"name\", value)`\r\n\r\n**Describe alternatives you've considered**\r\n`client.newFailCommand(job).variables(Map.of(\"name\", value))`, but Map.of() is available only with Java 9 or above\r\n\n",
    "title": "newFailCommand: complete command with single variable"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13451",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThis feature is related to #13040. Using Java Client I want to complete `.newBroadcastingSignalCommand()` by using only a single variable\r\n\r\n**Describe the solution you'd like**\r\n`client.newBroadcastingSignalCommand(job).variable(\"name\", value)`\r\n\r\n**Describe alternatives you've considered**\r\n`client.newBroadcastingSignalCommand(job).variables(Map.of(\"name\", value))`, but Map.of() is available only with Java 9 or above\r\n\n",
    "title": "newBroadcastingSignalCommand: complete command with single variable"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13449",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThis feature is related to #13040. Using Java Client I want to complete `.newEvaluateDecisionCommand()` by using only a single variable\r\n\r\n**Describe the solution you'd like**\r\n`client.newEvaluateDecisionCommand(job).variable(\"name\", value)`\r\n\r\n**Describe alternatives you've considered**\r\n`client.newEvaluateDecisionCommand(job).variables(Map.of(\"name\", value))`, but Map.of() is available only with Java 9 or above\r\n\n",
    "title": "newEvaluateDecisionCommand: complete command with single variable"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13447",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThis feature is related to #13040. Using Java Client I want to complete .newPublishMessageCommand() by using only a single variable\r\n\r\n**Describe the solution you'd like**\r\n`client.newPublishMessageCommand(job).variable(\"name\", value)`\r\n\r\n**Describe alternatives you've considered**\r\n`client.newPublishMessageCommand(job).variables(Map.of(\"name\", value))`, but `Map.of()` is available only with Java 9 or above\r\n\n",
    "title": "newPublishMessageCommand: complete command with single variable"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13443",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThis feature is related to #13040. Using Java Client I want to complete `.newCreateInstanceCommand()` by using only a single variable\r\n\r\n**Describe the solution you'd like**\r\n`client.newCreateInstanceCommand(job).variable(\"name\", value)`\r\n\r\n**Describe alternatives you've considered**\r\n`client.newCreateInstanceCommand(job).variables(Map.of(\"name\", value))`,  but `Map.of()` is available only with Java 9 or above\r\n\n",
    "title": "newCreateInstanceCommand: complete command with single variable"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13428",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\n\r\nI would like to use virtual threads for my job worker executor, and also measure the time a job spends waiting, once submitted, before it's processed. Right now, all I can configure is the number of threads in the job worker's pool. Additionally, that thread pool is always global per client.\r\n\r\n**Describe the solution you'd like**\r\n\r\nI'd like to use a thread-per-task execution model, relying on virtual threads for execution. By providing my own executor I can also instrument it more easily.\r\n\r\n**Describe alternatives you've considered**\r\n\r\n- I can re-implement the job worker myself - cumbersome, I'd rather not do that.\r\n- I can instrument the job handler. This doesn't count time spent in the executor's queue, however.\r\n- I can have the job handler forward it to my own custom executor. This is the best workaround, but it feels unnecessary.\r\n\r\n**Additional context**\r\n\r\nThis is very low priority, but it's definitely a nice to have :)\r\n\n\n koevskinikola: ZPA triage:\n- Setting the `scope/clients-java`. The issue deals with improving performance and UX, so adding the appropriate labels.\n- The Java client still supports Java 8, which conflicts with this feature.\n- @npepinpe we're closing this issue since we don't see a possibility in implementing it in our Java client. If you have any ideas on how to implement this, please provide them. If not in the official Java client, maybe they can be implemented in community projects.\n npepinpe: I'm not sure how allowing custom executors for job workers conflicts with Java 8, but I guess me saying Virtual Threads made it sound scary ;)\r\n\r\nIf I just open a PR for it, will you consider it? It's what, 20 lines of code?\n npepinpe: OK so this was slightly more than 20 lines of code, but still not too big. Mostly tests and comments :upside_down_face: ",
    "title": "Allow custom job worker executors"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13321",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nThe Java client provides a `DeployResourceCommand` for performing resource deployments to Zeebe. This command should support multi-tenancy by exposing an **optional** `tenantId` property/method.\r\n\r\nThe following error codes may be returned:\r\n* PERMISSION_DENIED (code: 7) \r\n   * when a user attempts to access data of a tenant they are not authorized for, when multi-tenancy is enabled.\r\n* INVALID_ARGUMENT (code: 3)\r\n   * For a provided tenant id, when multi-tenancy is disabled\r\n   * For a missing tenant id, when multi-tenancy is enabled\r\n   * For an invalid tenant id (i.e. doesn't match the pre-defined format), when multi-tenancy is enabled.\r\n\r\n### AC\r\n- [x] A `ClientProperties#DEFAULT_TENANT_ID` with value `zeebe.client.tenantId` is defined.\r\n- [x] The `ZeebeClientBuilderImpl` class is expanded with a `defaultTenantId` property.\r\n       - The `ZeebeClientBuilderImpl#withProperties(...)` method may set the `defaultTenantId` property to a value defined by `zeebe.client.tenantId` in a `.properties` file (see point above).\r\n- [x] The `DeployResourceCommand` provides a new `tenantId(String tenantId)` method.\r\n       - The command will set the `tenantId` to the value of `zeebe.client.tenantId` if provided through a `.properties` file.\r\n       - The default value if the `tenantId` property should be `null`.\r\n\r\n### Blocked by\r\n- #13319\r\n\r\n### Blocks\r\n- [Web Modeler](https://github.com/camunda/web-modeler/issues/5058)\r\n- [Desktop Modeler](https://github.com/camunda/camunda-modeler/issues/3716)\n",
    "title": "Java client supports multi-tenancy for DeployResource RPC"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12122",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nSometimes, I may want to specify a larger than the default (4 MB) inbound `MAX_MESSAGE_SIZE` for the client. For example, when I create a process instance with a result where the resulting variables are larger than the default 4 MB.\r\n\r\n**Describe the solution you'd like**\r\nAdd a configuration option to the client to specify an inbound max message size applied to the gRPC responses. The default should be the current hardcoded 4 MB.\r\n\r\n**Describe alternatives you've considered**\r\n- specify it for each command: this is hard to implement at this time as the channel is set up on client construction\r\n- don't allow specifying it: the current behavior throws errors in the client when the gateway responds with larger than 4MB messages (which is possible already).\r\n\r\n**Additional context**\r\n- https://github.com/camunda/zeebe/pull/11902#issuecomment-1480460031\r\n- https://github.com/camunda/zeebe/issues/12104\r\n\n",
    "title": "Configure the client's inbound max_message_size"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/4700",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\n\r\n> We want to provide metrics about our zeebe workers to better monitor them. Specifically, how many jobs a worker has scheduled.\r\n\r\nThe goal here is to align the Java client with the Go client and allow users to better monitor their workers. See #4500 \r\n\r\n**Describe the solution you'd like**\r\n\r\nI would like to be able to add a metrics facade (whether an interface or something like Micrometer is to be discussed) to a job worker to monitor the amount of jobs currently enqueued. You can have a look at the Go solution in #4501 \r\n\r\n**Describe alternatives you've considered**\r\n\r\nImplementing your own JobWorker (which is, after all, a QoL feature).\r\n\n\n npepinpe: Proposal would be to add the following metrics:\r\n\r\n- Count of jobs activated\r\n  - The rate can be derived from this count\r\n- Count of jobs handled\r\n  - The rate can be derived from this count\r\n  - The amount of non-handled jobs can also be derived by subtracting both series\r\n\r\nAs the last one may not be so accurate, we could also provide a count of the queued jobs. This would be a sum of remaining jobs and a new counter related to jobs streamed. We can't just use `remainingJobs` since that is only used for polling, and we don't want to use it for streaming as well since polling should remain independent.\r\n\r\nWe will limit it to this, as most of the other things can be effectively measured by users at the moment:\r\n\r\n- Observing the back off supplier can be done by wrapping the default one at the moment and instrumenting the one method for it\r\n- Observing the executor can be done by passing a custom, instrumented executor\r\n",
    "title": "Introduce JobWorker metrics for the Java client"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13024",
      "component": "Zeebe",
      "subcomponent": "zbctl",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nMany of the recently reported [CVEs](https://github.com/camunda/zeebe/issues/12649) were related to the `zbctl` in the docker image, which only used in debugging and troubleshooting of SaaS clusters.\r\n\r\n**Describe the solution you'd like**\r\nRemove `zbctl` from the docker images from 8.3 going forward. This would eliminate any CVEs reported on our docker images that are golang related. \r\n\r\n\n\n megglos: ZDP-Triage:\n- would be nice to get out of the way, might be worth combining it with #12959 \n npepinpe: The simplest approach is to remove it from the distribution entirely. Would that be alright? Otherwise, we can remove it only from the Dockerfile.\n megglos: > The simplest approach is to remove it from the distribution entirely. Would that be alright? Otherwise, we can remove it only from the Dockerfile.\r\n\r\nI guess that would be fine 🤔  we attach it as artifact on every release anyway\n npepinpe: I've opened the PR where it's removed from the Dockerfile. It's also not that big a deal to do it then, and I guess it's a smaller breaking change :shrug:\r\n\r\nI'm pretty 50/50 on this honestly.\n megglos: > I've opened the PR where it's removed from the Dockerfile. It's also not that big a deal to do it then, and I guess it's a smaller breaking change 🤷\r\n> \r\n> I'm pretty 50/50 on this honestly.\r\n\r\nif it's already done on the dockerfile that's also fine then, the smaller scope would give us more flexibility to revise that decision ^^",
    "title": "Remove zbctl from 8.3 images going forward"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14555",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\n\r\nForms were built parallel to use building Multi-Tenancy. As a result Forms were not implemented tenant-aware. We should make sure we store forms in the state by tenant.\n",
    "title": "Support Multi-tenant Form deployments"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14302",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\n\r\nAdd those tests:\r\n* Verify that if a user task is activated with a formId, but the form is not yet deployed an Incident should be raised\r\n* Verify that if an Incident is raised, and a form with the same formId is deployed the Incident is resolved\r\n* Verify that no Incident are raised if the form is already deployed before user task activation\n",
    "title": "Add tests to verify Incident behaviour when form not found during user task activation"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14270",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\n\r\nSince we will support returning the Form tenantId in the DeployResource gRPC, it needs to be supported in the Java client as well. The implementation should include the following:\r\n\r\n* `tenantId` field should be added to `Form` and `FormImpl` classes in the Java client module.\n",
    "title": "Receive tenantId in the Form deployment response"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14269",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\n\r\n* [gateway] map the tenantId from the FormRecord to the gRPC response.\r\n* [engine] new Form-related Record/RecordValues will need to have a `tenantId` property.\n",
    "title": "Add tenantId to the Form deployment gRPC response"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14268",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\nThis issue is created to support versioning for Forms. Even if we support versioning internally, still, the latest version of the Form will be exported on user task activation. Supporting the versioning will prevent migration issues if we decided to support versioning for clients later on. The implementation of this issue will include:\r\n\r\n* Implementation of a version manager class similar to `ProcessVersionManager`\r\n* Implementation of a version info class similar to `ProcessVersionInfo`\r\n* Creating a new column family to retrieve the key with `formId` and `version` values.\r\n* Integrating the version manager into `DbFormState` class.\n",
    "title": "Support versioning of the Forms in the state"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14248",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\n\r\nDeployment record should now be exported with also form metadata information. The implementation is expected to include following:\r\n\r\n* Update `zeebe-record-deployment-template.json`, adding form metadata properties\r\n\r\nBlocked by #14132 \n",
    "title": "Export Deployment Record with form metadata"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14222",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\n\r\nTo allow other teams to display deployed forms, `Zeebe` should export forms to `Opensearcg`. The implementation is expected to include following:\r\n\r\n* Create a new index template `zeebe-record-form-template.json` with all the form fields, the `indexPattern` property should be `zeebe-record_form_*`\r\n* Add `FORM` as new index `ValueType`\r\n* Enabling the creation of the new `form` index\n",
    "title": "Export Form Deployment record to Opensearch"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14187",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nZeebe has a relatively high CPU load when idle. With no processes or clients running it uses in our test installation 10% CPU. It is running in docker, 'docker run camunda/zeebe:8.2.12'. The CPU is used by 4 actor threads (ActorThead.java), each of them around 2.5% CPU. As our application runs not many processes (maybe 1-10 per day) this waists resources.\r\n\r\n**Describe the solution you'd like**\r\nIn ActorThread.java the ActorTaskRunnerIdleStrategy is initialized with:\r\nBackoffIdleStrategy(100, 100, 1, TimeUnit.MILLISECONDS.toNanos(1));\r\nTherefore the ActorThread will poll for work in 1ms interval causing the CPU load.\r\nCan all parameters or only the last parameter for BackoffIdleStrategy() be configured in environment variables of the docker image? Then we can set e.g. a 100ms polling intervall which should use 100 times less CPU.\r\n\r\n**Describe alternatives you've considered**\r\nSetting variables ZEEBE_BROKER_THREADS_CPUTHREADCOUNT and ZEEBE_BROKER_THREADS_IOTHREADCOUNT to 1 causes half the CPU load, but having only 1 thread in each pool does not sound right.\r\n\r\n**Additional context**\r\nConsider a higher the default value of the polling interval. As probably many zeebe instances are running with defaults, you can save a lot of CPU load, power consumtion and the climate!\r\nAnd for production recommend the current  low interval.\r\n\n\n npepinpe: It probably wouldn't hurt too much to have a default higher value in general. Whenever new work is submitted to the task, we will wake up the thread anyway (if, for example, it was parked). The one caveat is for timer tasks, as these cannot wake up the thread. So if you set the parked time to 100ms, for example, then you could miss low timeouts by 100ms.\r\n\r\nBut we could probably already increase it to 10 or even 20ms :+1: \n npepinpe: A quick test with 20ms as the park timeout showed no noticeable performance difference on medium and high throughput, and a 1-3% (constantly fluctuating) CPU usage by the java process. So we can definitely increase this value I think.\r\n\r\nMaking it configurable is a helpful to tweak, but I don't know how useful it is in the long term - I suspect we can find a good general value for most users :shrug: Though having it configurable definitely helps in running various benchmarks :smile: \n npepinpe: You can see in this screenshot when it flattens, this is when the system is idle:\r\n\r\n![image](https://github.com/camunda/zeebe/assets/43373/65970f7a-9d6d-46e5-9dc9-e52d7023f24a)\r\n\r\nAnd this is the normal system on idle, with a 1ms park time instead of 20ms:\r\n\r\n![image](https://github.com/camunda/zeebe/assets/43373/45c2b6e3-4239-4bb2-9aaa-0c719c398bbe)\r\n\r\nIt's a bit hard to see, but difference is that with 1ms, CPU usage is 0.04, and with 20ms, it's 0.01-0.02, so halved. And I believe the only downside is that you risk a 20ms added delay on your timers - of course, assuming your clock is fine grained. A coarse grained clock may be sleeping for me, but then it'd also be sleeping for longer than 1ms :smile: \n amalzahn: i guess when the threadcount is increased the low intervall will actually decrease zeebe performace as the idle threads will consume CPU load. So this intervall should be made configurable to adjust to different usecases. Maybe also different intervals for CPU and IO threads so you can have a lower interval with the timer tasks and a higher on the other. A different IdleStrategy without spinning may also increase performance with many threads, but I'm no expert here.\r\nThis is not our usecase, but I imagine some users will increase thread count.\n Zelldon: Triage:\r\n\r\n * related https://github.com/camunda/zeebe/issues/4231\r\n * @npepinpe wants to do it!",
    "title": "zeebe running with relatively high idle CPU load"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14139",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\n\r\n* The `DeploymentEventImpl` class constructor should be updated to map Form metadata.\r\n* `Form` class should be created to hold Form metadata in the client side.\r\n\r\nBlocked by #14131 \n",
    "title": "Add support to Java client for returning Form metadata in DeployResource gRPC response"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14138",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\n\r\nOn activation of a user tasks that references to a Form by formId, the key of the latest version of the Form should be fetched from the state. Then, the fetched form key should be put into custom headers of the Job record. The implementation is expected to include following:\r\n\r\n* Add `formId` expression to `JobWorkerProperties`\r\n* Add a method to `UserTaskTransformer` for transforming `formId`\r\n* Update `BpmnJobBehaviour.encodeHeaders` to put transformed `formId` into job custom headers\r\n* If form not found by given `formId` on user task activation, `Either.left<Failure>` should be returned from `BpmnJobBehaviour.createNewJob` method. Therefore, incident will be raised. Later on, a form with the given `formId` can be deployed and the incident can be resolved by `ResolveIncident` gRPC.\n",
    "title": "Export form key of the latest form on user task activation"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14137",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\n\r\nA method should be added to retrieve the latest form by form id to `DbFormState` class.\n",
    "title": "Implement query to retrieve the latest form by formId from the state"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14136",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\n\r\nTo allow other teams to display deployed forms, `Zeebe` should export forms to `Elasticsearch`. The implementation is expected to include following:\r\n\r\n* Create a new index template `zeebe-record-form-template.json` with all the form fields, the `indexPattern` property should be `zeebe-record_form_*`\r\n* Add `FORM` as new index `ValueType`\r\n* Enabling the creation of the new `form` index\n",
    "title": "Export Form Deployment record to Elasticsearch"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14135",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\n\r\nA `Start Event` can be linked to a deployed form by the new `formId` field. The implementation is expected to include following:\r\n\r\n* Add the new `formId` attribute into the `ZeebeFormDefinition`\r\n* Update `ZeebeElementValidator.validate()` method in order to enable `zeebe` to validate also for group of fields. In the case, a start event with linked form is valid if only one field between `formKey` and `formId` is present\n",
    "title": "Allow binding forms to start events by formId"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14134",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\n\r\nA `User Task` can be linked to a deployed form by the new `formId` field. The implementation is expected to include following:\r\n\r\n* Add the new `formId` attribute into the `ZeebeFormDefinition`\r\n* Update `ZeebeElementValidator.validate()` method in order to enable `zeebe` to validate also for group of fields. In the case, a user task with linked form is valid if only one field between `formKey` and `formId` is present\n",
    "title": "Allow binding forms to user tasks by formId"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14133",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\n\r\nThis issue is created to implement state behaviour of the Form. It is expected to include following:\r\n\r\n* Create Form DB classes\r\n* Create Form column family definitions\r\n* Create a method to store the Form\r\n* Create a method to retrieve the latest form by form id\r\n* Create a method to retrieve a Form by key\n",
    "title": "Save Form to the state"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14132",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\n\r\nThis issue is created to implement the processing of the Form deployment in the engine. It is expected to include following:\r\n\r\n* Create Form intent\r\n* Create Form transformer\r\n* Create Form applier\r\n* Include Form metadata to deployment record\r\n* Transform deployed Form in `DeploymentCreateProcessor` class\n",
    "title": "Handle Form Deployment in the Engine"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14131",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\n\r\nThis issue is created to update the `DeployResource` gRPC endpoint for supporting Form deployments.. It is expected to include following:\r\n\r\n* Add Form metadata response definition to `DeployResource` gRPC in `gateway.proto` file\r\n* Fill Form metadata in gateway `ResponseMapper` class\r\n\r\nBlocked by #14132 \n",
    "title": "Implement Form Deployment gRPC endpoint in the Gateway"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13516",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\nIT tests for the Job Push is missing. To cover the happy path scenario end to end, we need to implement IT tests. ~~These cases should also cover closing a stream which will actually test the `onClose` hook.~~ Please refer to these two discussions for more details: \r\nhttps://github.com/camunda/zeebe/pull/13351#discussion_r1263414875\r\nhttps://github.com/camunda/zeebe/pull/13351#discussion_r1263834385\r\n\n\n berkaycanbc: @koevskinikola Based on the investigation we did, the only way to trigger `onClose` handler is to call `onComplete` or `onError` from the server side which is not the case for our implementation. Therefore, we will opt-out tests for it. At the same time, we want to keep it in case we decide to call `onComplete` later on. (e.g. during server shutdown)\r\n\r\nCc: @npepinpe ",
    "title": "Add integration tests for Job Push"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13465",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\n\nWe need to validate user input before registering workers to the job stream. That will provide two main benefits:\n- We will be able to return better validation error messages to the client\n- Without validation, worker can register to the stream with a wrong job type and later no jobs will be pushed. We will prevent that with validation.\n\n**Expected validation checks:**\n- type is blank (empty string, null)\n- ~worker is blank (empty string, null)~ this is going to be optional as we do in polling mechanism\n- timeout less than 1\n",
    "title": "Validate user input before registering a worker to the job stream"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13429",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\n\r\nWith the new `StreamJobs` RPC introduced via #13351, we can now easily stream jobs from a Zeebe cluster. As a first step, we'll introduce a new command in `ZeebeClient` which will wrap the underlying gRPC call and integrate it into our client.\r\n\r\nWe'll introduce a new command, `StreamJobsCommand`, and a new API, `ZeebeClient#newStreamJobsCommand`. This will follow closely the `ActivateJobsCommand`, and have three steps:\r\n\r\n1. Set the job type (REQUIRED)\r\n2. Set a job consumer (REQUIRED)\r\n3. Set metadata (e.g. worker, job timeout, etc.) (OPTIONAL)\r\n\r\n### Long living stream\r\n\r\nSince this call is meant to be a long living stream, it diverges from our normal calls, which are all meant to eventually end.\r\n\r\n#### Request timeout\r\n\r\nFor example, the client has a default request timeout which is applied everywhere. This is counterproductive for this feature. Instead, **we will not apply the default request timeout here**. Users can still provide one which will be respected, but by default there will be no request timeout.\r\n\r\n#### Consumer\r\n\r\nSince the stream is long living, we don't want to wait for the command to complete before passing along the results. As opposed to `ActivateJobsCommand`, the `StreamJobsCommand` will take in a consumer as a mandatory build step, and results will be piped there. Future integration in the job worker can then do whatever with it, including passing it to the `JobHandler`.\r\n\r\n#### Cancellation/termination\r\n\r\nUsing the standard gRPC tools, you would normally cancel the stream either by throwing an exception (which is sent over the client's `StreamObserver#onError`), or by using `CancellableContext`. Closing the underlying transport channel would also work.\r\n\r\nThis doesn't fit the current `ZeebeClient` abstraction, and refactoring that goes beyond the scope of this issue. So instead, we'll piggyback on top of `Future#cancel(boolean)` - by calling this method, the user will be able to terminate the job stream gracefully, completing the future and notifying the server that the call is closed.\n\n npepinpe: Blocked by https://github.com/camunda/zeebe/issues/13430",
    "title": "Implement ZeebeClient wrapper for the StreamJobs RPC"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13426",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nSupport tenant validation when an incident is resolved.\r\n\r\n:information_source: No changes on the Client side, as the tenant ownership is clear from the `IncidentRecord`, and the user's tenant id list is obtained from the gateway.\r\n\r\n```[tasklist]\r\n### Task breakdown\r\n- [x] The `ResolveIncidentProcessor` resolves an incident only if the tenant that owns the incident is accessible by the user making the request.\r\n- [x] The `ResolveIncidentProcessor` rejects a command if the tenant that owns the incident is **not** accessible by the user making the request.\r\n- [x] ~Gateway provides a user's tenant id list with the broker request when a ResolveIncident RPC call is made~ (Done with #13989 and #14283)\r\n```\r\n\r\n### Blocked by\r\n- #13345\r\n- #13346 \n\n koevskinikola: Closing as https://github.com/camunda/zeebe/pull/14467 resolves this issue.",
    "title": "A ResolveIncident request/command supports tenant validation"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13425",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nSupport tenant validation when an error is thrown for a job.\r\n\r\n:information_source: No changes on the Client side, as the tenant ownership is clear from the `JobRecord`, and the user's tenant id list is obtained from the gateway.\r\n\r\n```[tasklist]\r\n### Task breakdown\r\n- [x] The `JobThrowErrorProcessor` throws an error on a job only if the tenant that owns the job is accessible by the user making the request.\r\n- [x] The `JobThrowErrorProcessor` rejects a command if the tenant that owns the job is **not** accessible by the user making the request.\r\n- [x] ~Gateway provides a user's tenant id list with the broker request when a ThrowError RPC call is made~ (Done with #13989 and #14283)\r\n```\r\n\r\n### Blocked by\r\n- #13345\n\n koevskinikola: PR https://github.com/camunda/zeebe/pull/14466 resolved this issue.",
    "title": "A ThrowError request/command supports tenant validation"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13388",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nSupport multi-tenancy when setting variables on a particular scope (e.g. process instance, flow element instance).\r\n\r\n:information_source: No changes on the Client side, as the tenant ownership is clear from the `ElementInstance`, and the user's tenant id list is obtained from the gateway.\r\n\r\n```[tasklist]\r\n### Task breakdown\r\n- [ ] The `VariableDocumentRecord` and `VariableRecord` contain a new `tenantId` property (see #13235)\r\n- [ ] The `VariableState` contains a `tenantId` key, and can store and fetch variables by `tenantId`\r\n- [x] Elasticsearch/Opensearch `variable-template.json` and `variable-document-template.json` contain  a`tenantId` property.\r\n- [x] The `UpdateVariableDocumentProcessor` updates variables of a given scope only if the tenant that owns the scope is accessible by the user making the request.\r\n- [x] The `UpdateVariableDocumentProcessor` rejects a command if the tenant that owns the \"scope\" is **not** accessible by the user making the request.\r\n- [x] The `VariableBehavior` class appends the `tenantId` from the provided \"scope\" to all `VariableRecord` instances.\r\n- [x] ~Gateway provides a user's tenant id list with the broker request when a `SetVariables` RPC call is made~ (authorized tenants are passed in the `RecordMetadata`)\r\n- [x] Follow-up (`VariableDocumentRecord` and `VariableRecord`) events contain a `tenantId` inherited from the scope that was updated.\r\n```\r\n\r\n### Additional context\r\n- The `VariableDocumentRecord` class needs to contain a `tenantId` property only when used as a response to the client request, and as an event to be exported.\r\n\r\n### Blocked by\r\n- #13279\r\n\n\n koevskinikola: Update: All (impacted) Elasticsearch/Opensearch record templates have been updated to include a `tenantId` property with #13520.",
    "title": "Support multi-tenancy for setting variables on a process instance scope"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13354",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "# Description\r\nIf a process contains a signal start event we must make sure to unsubscribe this signal.\r\n\r\nIf the deleted process was the latest, the previous version will become the new latest version. If this previous version contains a signal start event we should make sure that the signal subscription is created and process instances are started as expected.\r\n\r\nBlocked by: #9769 \n",
    "title": "Delete and recreate signal subscription of previous version"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13349",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\n\r\nThe process deleting event will mark the process definition as pending deletion.\r\n\r\n- Add the `DELETING` intent to the `ProcessIntent`\r\n- Add an `EventApplier` to handle the `DELETING` intent\r\n    - The applier must change the state the `PersistedProcess` to `PENDING_DELETION`\r\n        - Don't forget to change the state in the `CoulmnFamily`, as well as in the cache.\r\n    - This means a new method must be created on the `DbProcessState`\r\n        - Include tests!\r\n\r\n**Out of scope**\r\n- Writing the event will happen in a different issue\r\n\r\n**Blocked by**\r\n#13348 \n",
    "title": "Add and handle Process Deleting event"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13348",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\n\r\nWe need to keep track of process definitions that are pending deletion. For this we will introduce a `state` on the `PersitedProcess`.\r\n\r\n- Add a `state` property on the `PersistedProcess`\r\n- State should be a new enum (`PeristedProcessState`)\r\n- As of now we will know 2 states:\r\n    - `ACTIVE` - This is the default.\r\n    - `PENDING_DELETION` - Used to mark a process for deletion\r\n\r\nIn the future the states could be extended with other states, e.g. `SUSPENDED`.\r\n\n",
    "title": "Add a `state` to `PersistedProcess`"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13347",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nSupport tenant validation when a job's retries are updated.\r\n\r\n:information_source: No changes on the Client side, as the tenant ownership is clear from the `JobRecord`, and the user's tenant id list is obtained from the gateway.\r\n\r\n```[tasklist]\r\n### Task breakdown\r\n- [x] The `JobUpdateRetriesProcessor` updates a job only if the tenant that owns the job is accessible by the user making the request.\r\n- [x] The `JobUpdateRetriesProcessor` rejects a command if the tenant that owns the job is **not** accessible by the user making the request.\r\n- [x] ~Gateway provides a user's tenant id list with the broker request when a UpdateJobRetries RPC call is made~ (Done with #13989 and #14283)\r\n```\r\n\r\n### Blocked by\r\n- #13345\n\n koevskinikola: PR https://github.com/camunda/zeebe/pull/14466 resolved this issue.",
    "title": "An UpdateJobRetries request/command supports tenant validation"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13346",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nSupport tenant validation when a job is failed.\r\n\r\n:information_source: No changes on the Client side, as the tenant ownership is clear from the `JobRecord`, and the user's tenant id list is obtained from the gateway.\r\n\r\n```[tasklist]\r\n### Task breakdown\r\n- [x] The `JobFailProcessor` completes a job only if the tenant that owns the job is accessible by the user making the request.\r\n- [x] The `JobFailProcessor` rejects a command if the tenant that owns the job is **not** accessible by the user making the request.\r\n- [x] The `IncidentRecord` has a `tenantId` property.\r\n- [x] The `IncidentState` can persist the new `tenantId` key.\r\n- [x] The `JobFailProcessor` creates an `IncidentRecord` with the `tenantId` of the `JobRecord`.\r\n- [x] The Elasticsearch/Opensearch `incident-template.json` has a `tenantId` property.\r\n- [x] ~Gateway provides a user's tenant id list with the broker request when a FailJob RPC call is made~ (Done with #13989 and #14283)\r\n```\r\n\r\n### Blocked by\r\n- #13345\n\n koevskinikola: Closing as https://github.com/camunda/zeebe/pull/14467 fully resolves this issue.",
    "title": "A FailJob request/command supports tenant validation"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13343",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\nThis ColumnFamily must contains **all** running process instances. We must create a migration script that initially fills this ColumnFamily.\r\n\r\n- Create the migration script\r\n    - Include tests!\r\n\r\n**Blocked by**\r\n#13340 \n",
    "title": "Migrate running process instance into `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13342",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\nWhen a process has reached an end state we must remove it from this ColumnFamily\r\n\r\n- Add a method to remove data from this ColumnFamily\r\n    - Include tests!\r\n- When a process is completed use this method to remove it from the ColumnFamily\r\n- When a process is terminated use this method to remove it from the ColumnFamily\r\n- When a call activity is completed / terminated use this method to remove it from the ColumnFamily\r\n\r\nPlease also change the `ElementInstanceStateTest#shouldNotLeakMemoryOnRemoval`. We must change the processInstanceRecord that's generated here to be of element type PROCESS.\r\n\r\n**Blocked by**\r\n#13340 \n",
    "title": "Remove from `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13341",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\nIn #13340 we have created this ColumnFamily. We now need to fill it with data.\r\n\r\n- Add a method to insert data into this ColumnFamily\r\n    - Include tests!\r\n- Use this method to insert data when starting a new process instance\r\n    - Also when starting a process anywhere\r\n- Use this method to insert data upon calling a CallActivity\r\n\r\n**Blocked by**\r\n#13340 \n",
    "title": "Insert into the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13340",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\nWe will need a way to find out if there are any process instances for a definition key. We need this so when a process instance is terminated/completed we can check if there are other process instances still running. \r\nIf this is not the case, and the deployment is pending deletion we can write the followup events to fully delete the resource.\r\n\r\n- Create the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY ` in `ZbColumnFamililes`\r\n- Create this ColumnFamily in the `DbElementInstanceState`\r\n\r\n**Out of scope:**\r\n- Doing any actions on this ColumnFamily. Inserting and deleting data will happen in:\r\n    - #13341 \r\n    - #13342\r\n    - #13343\n",
    "title": "Add `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13337",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nSupport multi-tenancy for timer start events and timer boundary/intermediate events.\r\n\r\n:information_source: For timers, the Gateway and Client code doesn't need to be adjusted. Timers are always processed in the engine, and inherit the `tenantId` from the process definition or process instance.\r\n\r\n```[tasklist]\r\n### Tasks\r\n- [x] The `TimerRecord` class contains a `tenantId` property\r\n- [x] The `TimerInstance` class contains a `tenantId` property\r\n- [ ] The `TimerInstanceState` has a `tenantId` key and can store and get instances by it\r\n- [x] Elasticsearch/Opensearch `timer-template.json` files have a `tenantId` property\r\n- [x] Timer start events inherit the `tenantId` from the process definition in the `DeploymentCreateProcessor`\r\n- [x] `TriggerTimerProcessor` propagates the `tenantId` from the `TimerRecord` to any follow-up records.\r\n- [x] `DueDateChecker` propagates the `tenantId` from the `TimerInstance` to any follow-up records.\r\n- [ ] Documentation on timer start events is expanded.\r\n- [ ] Add E2E multi-tenancy tests for timer events\r\n```\r\n\r\nFor the release candidate, we've disabled the timer events feature for non-default tenants.\r\n\r\nIt's easy to-re-enable:\r\n- remove TIMER [here](https://github.com/camunda/zeebe/blob/58ed10ab83e56c7970025911e8dd694aa054f3d9/engine/src/main/java/io/camunda/zeebe/engine/processing/deployment/model/validation/UnsupportedMultiTenantFeaturesValidator.java#L33-L34)\r\n- replace the tests cases [here](https://github.com/camunda/zeebe/blob/58ed10ab83e56c7970025911e8dd694aa054f3d9/engine/src/test/java/io/camunda/zeebe/engine/processing/multitenancy/TenantAwareTimerEventTest.java#L69-L237)\r\n- un-ignore the test case [here](https://github.com/camunda/zeebe/blob/80d849648b7de82f55ee1de624e3f797df0ce3bc/engine/src/test/java/io/camunda/zeebe/engine/processing/incident/TimerIncidentTest.java#L237)\r\n\r\n### Blocked by\r\n- https://github.com/camunda/zeebe/issues/13238\n\n koevskinikola: Update: All (impacted) Elasticsearch/Opensearch record templates have been updated to include a `tenantId` property with https://github.com/camunda/zeebe/issues/13520.\n korthout: I moved this to in progress, because I wanted to start with it, but ended up working on other thing instead. \n romansmirnov: > The `TimerInstanceState` has a `tenantId` key and can store and get instances by it\r\n\r\nAs in other cases, I decided against implementing the `TimerInstanceState` to be tenant -aware.",
    "title": "Support multi-tenancy for BPMN timer events"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13335",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\n\r\n- Add a new `Intent` to the `ResourceDeletionIntent` class named `DELETING`\r\n- In the `ResourceDeletionProcessor` write the `DELETING` event\r\n    - Currently only the DMN resource are part of this processor.\r\n    - We are writing a `DELETED` event to the log, and are writing this as a response to the client.\r\n    - Before this we should write the `DELETING` event to the log.\r\n    - The response to the client should be changed to be `DELETING` as wel\r\n    - After the response we must still write the `DELETED` tot he log.\n",
    "title": "Add Resource Deleting intent"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13320",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nThe engine should support working with multi-tenancy during deployments.\r\n\r\n### AC\r\n- [x] The `RecordValueWithTenant` interface (see #13235) is implemented by the following classes:\r\n       - [ ] `DeploymentRecord`\r\n       - [ ] `ProcessMetadata`\r\n       - [ ] `ProcessRecord`\r\n       - [ ] `DecisionRequirementsMetadataRecord`\r\n       - [ ] `DecisionRequirementsRecord`\r\n       - [ ] `DecisionRecord`\r\n- [x] `DeploymentRecord` implements the `RecordValueWithTenantPermissions` interface (see #13235)\r\n- [ ] ~~`DeploymentCreateProcessor` rejects the command if the `tenantId` isn't found in the list of `tenantIds` available to the user.~~\r\n- [x] The `DeploymentCreateProcessor`/`BpmnResourceTransformer`/`DmnResourceTransformer` propagates the `tenantId` to the BPMN/DMN resources and their appropriate Records.\r\n- [x] The `DeploymentState`, `ProcessState`, and `DecisionState`  include a `tenantId` key when an appropriate resource is added.\r\n- [x] The unit test coverage is extended to cover the new tenant properties.\r\n\r\n\r\n\r\n### Blocked by\r\n- #13235\n\n koevskinikola: :question: Do we need to tackle the `DeploymentDistributionRecord` as well? \r\n\r\nThe record is deprecated, so it would only be triggered by existing \"in-progress\" deployments. We can classify this as old data, and map this deployment to the `<default>` tenant.\n remcowesterhoud: > DeploymentCreateProcessor rejects the command if the tenantId isn't found in the list of tenantIds available to the user.\r\n\r\nThis will happen in the gateway, so I won't do this as part of this issue.\n remcowesterhoud: > ❓ Do we need to tackle the DeploymentDistributionRecord as well?\r\n\r\nI don't think this record needs it. This record doesn't know any details about a process. It was used to store a key + partition id in the state. This kept track of what deployment needed to be distributed to what partition. But the actual command that got distributed was the `Deployment.DISTRIBUTE`, which is actually the `DeploymentRecord`.\n koevskinikola: Closing this issue as all the sub-tasks have been completed.",
    "title": "Engine can process multi-tenant DeploymentRecord commands"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13319",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nThe Gateway supports receiving and forwarding (to the Broker) DeployResource RPC calls with a `tenantId`.\r\n\r\n### AC\r\n- [x] The following gRPC messages contain a new `tenantId` property:\r\n       - [ ] `DeployResourceRequest`\r\n       - [ ] `ProcessMetadata`\r\n       - [ ] `DecisionMetadata`\r\n       - [ ] `DecisionRequirementsMetadata`\r\n- [x] The `deployResource(...)` Gateway endpoint passes the gRPC `DeployResourceRequest#tenantId` property to the `BrokerDeployResourceRequest`. The following scenarios are possible as well:\r\n      - If multi-tenancy is disabled (see #13237), the `BrokerDeployResourceRequest#tenantId` is set to `<default>`.\r\n      - If multi-tenancy is enabled, and `DeployResourceRequest#tenantId` is `null`, the deployment is rejected.\r\n- [ ] ~The `BrokerDeployResourceRequest#tenantIds` is set to the list of user accessible tenant ids provided through a gRPC context by the Identity SDK.~\r\n       -  The user's authorization list will be set in the `RecordMetadata`. It will be implemented through #13989 and #13237 \r\n\r\n### Blocked by\r\n- #13320\n",
    "title": "Gateway supports multi-tenancy in deployment RPCs"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13318",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nTo support multi-tenancy when polling or pushing jobs, a job should be activated when the `jobType` **AND** `tenantId` match.\r\nWe also want to be able to poll or push jobs for multiple tenants at once.\r\n\r\nAs a result, a list of `tenantIds` needs to be included in the `JobBatchRecord`s and `JobActivationProperties` classes, to ensure that filtering jobs by `jobType` **AND** `tenantId` is possible.\r\n\r\n\r\n```[tasklist]\r\n### AC\r\n- [x] The `JobBatchRecord` class provides a `tenantIds` property (List\\<String\\>).\r\n- [x] The `JobActivationProperties` class provides a `tenantIds` property (List\\<String\\>).\r\n- [x] ~The `JobBatchRecord` class implements the `RecordValueWithTenantPermissions` interface~ (see #13989 and #14283)\r\n- [x] ~The `JobActivationProperties` class implements the `RecordValueWithTenantPermissions` interface~ (see #13989 and #14283)\r\n- [x] ~The Gateway adds a list of tenant ids assigned to a user to the`JobBatchRecord` or `JobActivationProperties` instances for each `ActivateJobs`/`StreamJobs` RPC call.~ (see #14283)\r\n```\n",
    "title": "`JobBatchRecord` and `JobActivationProperties` provide tenant information"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13317",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nJob polling and pushing should filter jobs by `jobType` and `tenantId` before the jobs are activated.\r\n\r\n\r\n```[tasklist]\r\n### Tasks\r\n- [x] ~`JobBatchCollector` queries jobs by `jobType` and `tenantIds` (multiple tenantIds are possible)~ Done in #13345\r\n- [x] The JobStreamer API provides a new method `streamFor(final DirectBuffer jobType, final DirectBuffer tenantId)` that returns a `JobStream` for a given `jobType` and `tenantId`. Done with #14397\r\n- [x] `BpmnJobActivationBehavior` [queries for](https://github.com/camunda/zeebe/blob/main/engine/src/main/java/io/camunda/zeebe/engine/processing/bpmn/behavior/BpmnJobActivationBehavior.java#L67) `JobStream` by `jobType` and `tenantId`.\r\n```\r\n\r\n### Additional context\r\n* Once the `JobStreamer` API is adjusted, the [`RemoteJobStreamer` class](https://github.com/camunda/zeebe/blob/49a7709a3dc947d5579aff3c19c7d166c8a00167/transport/src/main/java/io/camunda/zeebe/transport/stream/impl/RemoteStreamerImpl.java#L62-L85) will need to be adjusted to filter the set of job streams for a given `jobType` by the `tenantIds` they specify (in the `JobActivationProperties` as well.\r\n* The ZDP team will need to be involved here, to expand the `JobStreamer` API to provide a new method `streamFor(final DirectBuffer jobType, final DirectBuffer tenantId)` that returns a `JobStream` for a given `jobType` and `tenantId`. \n\n koevskinikola: Closing as PR #14464 resolves this issue.",
    "title": "Jobs are pushed/polled only for requested tenants"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13316",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nSupport tenant validation when a job is completed.\r\n\r\n:information_source: No changes on the Client side, as the tenant ownership is clear from the `JobRecord`, and the user's tenant id list is obtained from the gateway.\r\n\r\n```[tasklist]\r\n### Task breakdown\r\n- [x] The `JobCompleteProcessor` completes a job only if the tenant that owns the job is accessible by the user making the request.\r\n- [x] The `JobCompleteProcessor` rejects a command if the tenant that owns the job is **not** accessible by the user making the request.\r\n- [x] ~Gateway provides a user's tenant id list with the broker request when a CompleteJob RPC call is made~ (Done with #13989 and #14283)\r\n```\r\n\r\n### Blocked by\r\n- #13345\n\n koevskinikola: PR https://github.com/camunda/zeebe/pull/14466 resolved this issue.",
    "title": "A CompleteJob request/command supports tenant validation"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13288",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\n\nAdd multi-tenancy support for the following process instance operations:\n* [Process instance modification](https://docs.camunda.io/docs/next/apis-tools/grpc/#modifyprocessinstance-rpc)\n* [Cancel process instance](https://docs.camunda.io/docs/next/apis-tools/grpc/#cancelprocessinstance-rpc)\n* [Set variables](https://docs.camunda.io/docs/next/apis-tools/grpc/#setvariables-rpc)\n\nTo support each operation, the following changes need to be applied:\n- The engine validates tenant access in the appropriate processors (`ProcessInstanceModificationProcessor`, `ProcessInstanceCommandProcessor`, `UpdateVariableDocumentProcessor`).\n- The Gateway propagates tenant data to the Broker.\n- No client changes are needed for these operations. The tenant access is validated by comparing the following information: \n  - The tenant ownership is acquired from the State. \n  - The list of tenants the user is assigned to is acquired from the Identity SDK in the Gateway and forwarded to the engine through the command record (modification, cancel, set variables).\n\n#### Expected behaviors\n\nFor each of the process instance operations, the following applies:\n* A process instance operation is only possible if the user/token has access to the tenant owning the process instance.\n\n```[tasklist]\n### Task breakdown\n- [ ] https://github.com/camunda/zeebe/issues/14550\n- [ ] https://github.com/camunda/zeebe/issues/14520\n- [ ] https://github.com/camunda/zeebe/issues/13388\n```\n\n### Blocked by\n- #13279\n",
    "title": "Support multi-tenancy for process instance operations (modify, cancel, set variables)"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13279",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nThis issue covers multi-tenancy support for the core operations of BPMN processes in Zeebe. The following should be covered, i.e. any command/event records produced by the following actions should contain a `tenantId`:\r\n* Starting a process instance\r\n* Starting a process instance with results\r\n* Starting a CallActivitiy\r\n* Evaluating a Decision through a Business Rule task.\r\n* Child instance creation.\r\n\r\n#### Expected behaviors\r\nThe following behavior should be supported:\r\n* A process instance is always owned by a tenant. A process instance can’t be shared across tenants.\r\n* All child instances are owned by the same tenant as the process instance. That means that any `ProcessInstanceRelated` records are owned by the same tenant as the process instance.\r\n* A `CallActivity` is started from a process definition with the following criteria:\r\n   1. The process definition is owned by the same tenant as the parent process instance.\r\n   2. If nothing is found, an `Incident` is raised.\r\n* A `Decision` is evaluated if it has the following criteria:\r\n   1. The Decision is owned by the same tenant as the parent process instance.\r\n   2. If nothing is found, an `Incident` is raised.\r\n* Filtering process definitions by version remains the same as the non-multi-tenant approach, i.e. if no version is specified, the latest version of the process definition owned by the tenant is used. (provided by #13238)\r\n* A process instance can be started only if a tenant id is provided with the client request. The following scenarios are possible:\r\n   * A tenant id is provided:\r\n      1. A process instance is started if the user making the request has access to the tenant owning the process definition.\r\n      2. The request/command is rejected if the user doesn't have correct access to the tenant.\r\n   * A tenant id isn't provided:\r\n      1. A process instance is started if the user only has access to a single tenant, and a single process definition can be found that can be mapped to that tenant.\r\n      2. The request/command is rejected if the user has access to multiple tenants or no process definition is found.\r\n\r\n:information_source: In the implementation, first focus on implementing the simple path, with a `tenantId` explicitly provided in the client request/command record. The goal is to deliver an MVP as soon as possible.\r\n\r\n```[tasklist]\r\n### Task breakdown\r\n- [x] Engine can process multi-tenant ProcessInstanceCreationRecord commands with tenant id\r\n- [x] Engine propagates tenant id to process instance elements.\r\n- [x] Engine can start CallActivity by tenant id\r\n- [ ] https://github.com/camunda/zeebe/issues/14211\r\n- [ ] https://github.com/camunda/zeebe/issues/13536\r\n- [x] Notify Operate and Optimize teams that core BPMN process events are available from Zeebe\r\n- [ ] ~CreateProcessInstance* RPCs documentation is expanded with multi-tenancy~\r\n- [ ] ~Engine can evaluate Business Rule task Decision by tenant id~ (moved to DMN)\r\n- [ ] ~Multi-tenancy data migration is provided for `ElementInstanceState`, `BannedInstanceState`~ WON'T DO\r\n```\r\n\r\n\r\n### Blocked by\r\n- https://github.com/camunda/zeebe/issues/13238\n\n koevskinikola: The task `Engine can evaluate Business Rule task Decision by tenant id` was moved to the https://github.com/camunda/zeebe/issues/13282 issue, as it fits better in the DMN scope of this Epic.\n koevskinikola: The `ElementInstanceState` and `BannedInstanceState` processing state won't be made tenant-aware, i.e. no `tenantId` properties will be added, and no data migration will be performed for them.\r\n\r\nWhy:\r\n* The data and operations within `ElementInstanceState` are internal to the engine, and aren't exposed to User requests. Furthermore, `ElementInstanceState` operations work with data KEYS (processDefinition, processInstance) which are unique across all tenants. As such, we see little value in introducing the additional overhead of introducing tenant-based data isolation for this state.\r\n  * On the other hand, we save up on implementation effort and time by not making this state tenant-aware.\r\n  * For operations like `CancelProcessInstance` and `DeleteResource` where we interact with the `ElementInstanceState` we can already validate the tenant against the `ProcessState` (either in the processor or within the state), ensuring that no unauthorized operations reach the `ElementInstanceState`.\r\n* The data and operations within `BannedInstanceState` are only accessible through the management/admin API which by its nature requires full access to the data. Furthermore, the `BannedInstanceState` only works with `processInstanceKey`s which are unique across tenants. As a result, we see little value in introducing tenant-based data isolation in these states.\r\n  * In addition, we plan to migrate away from using `BannedProcessInstances` with https://github.com/camunda/product-hub/issues/686.\n koevskinikola: The task `CreateProcessInstance* RPCs documentation is expanded with multi-tenancy` has been transfered to https://github.com/camunda/camunda-platform-docs/issues/2561.\n koevskinikola: The task `Engine can start CallActivity by tenant id` was already completed in https://github.com/camunda/zeebe/pull/14328.\n korthout: While we don't migrate `ElementInstanceState` and `BannedInstanceState` to be `TenantAware` right now, we could always add this later. The benefit would be further isolation of data. While keys are unique, the data is accessible when the engine makes a mistake due to a bug. By further isolating the data, we'd avoid such cases. But this can be done in a future iteration.",
    "title": "Support multi-tenancy for core BPMN process operations"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13253",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nCritical issues are defined as a \"stop the world\" issue. This means they require immediate attention. Currently, when an issue is labeled as critical there's no way of knowing, unless the person who added the label notifies the team. If this doesn't happen, there's a good chance we won't notice until the next triage.\r\n\r\n**Describe the solution you'd like**\r\nWe can easily add a workflow that checks if an issue gets labelled as critical. If this is the case we could send a notification to the Zeebe slack channel to notify the engineers that attention is required.\r\n\r\n**Describe alternatives you've considered**\r\nPeople could ping us manually when they do this. But that's a manual step and could be forgotten.\r\n\r\n**Additional context**\r\nN/A\r\n\n\n remcowesterhoud: ZPA Triage:\n- Helps us identify critical bugs more quickly\n- Removes a manual step that could be forgotten otherwise\n- Marking it as `upcoming` as we want to be aware of critical bugs asap\n- We already send slack message from our CI. Should be easy to take inspiration from these other places.\n- Low effort, potentially high impact.\n megglos: ZDP-Triage:\n- could help us being faster to react\n- need to clarify who reacts on it\n- right now we would assume this is done by zeebe engineers who can also escalate directly when identifying a critical issue\n- marking as later for now",
    "title": "Notify Zeebe Team in Slack when an issue is labelled as critical"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13238",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\nThis issue covers multi-tenancy support for the Deployment functionality of Zeebe. The following behavior should be supported:\r\n\r\n* If multi-tenancy is enabled:\r\n   * If no tenant id is provided, the deployment is rejected.\r\n      * Reason: Zeebe will support shared deployments in the future that might be mapped to this call.\r\n   * If a tenant id is provided, the resources are owned by a single tenant.\r\n   * Resource versioning is separated by tenant, i.e. process definitions/decisions owned by different tenants with the same `processId` have separate version counts in each tenant.\r\n* If multi-tenancy is disabled:\r\n   * All deployments and their resources are mapped to the `<default>` tenant.\r\n\r\n:warning: Note: the user/m2m token must have access to the tenant. Otherwise, the deployment command is rejected.\r\nA user/token shouldn’t be allowed to deploy to a tenant they don’t have access to.\r\n\r\n#### Task breakdown\r\n\r\n```[tasklist]\r\n### Tasks\r\n- [ ] https://github.com/camunda/zeebe/issues/13320\r\n- [ ] https://github.com/camunda/zeebe/issues/13322\r\n- [ ] https://github.com/camunda/zeebe/issues/13319\r\n- [ ] https://github.com/camunda/zeebe/issues/13321\r\n- [x] Notify Web/Desktop Modeler and Operate teams that multi-tenancy Deployments are available in Zeebe\r\n- [ ] https://github.com/camunda/zeebe/issues/14019\r\n- [ ] https://github.com/camunda/zeebe/issues/13315\r\n```\r\n\r\n\r\n### Blocked by\r\n- #13235\r\n- #13237 (soft blocker, functionality can be implemented, excluding user tenant id list)\n\n koevskinikola: :question: We should consider if the deprecated `DeployProcess` RPC should be adjusted as well.\n koevskinikola: Documentation on this component is tracked in https://github.com/camunda/camunda-platform-docs/issues/2561. As a result, I'm removing `DeployResource RPC documentation is expanded with multi-tenancy` from the tasklist in this issue.",
    "title": "Support multi-tenancy for deployments"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13040",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nAs a programmer writing Zeebe Job Handlers, I want to complete a job returning only a single variable value.\r\n\r\nThe `newCompleteCommand` offers only a parameter `.variables()` where I have to wrap the value in a Map beforehand.\r\n\r\nThis is overhead.\r\n\r\n**Describe the solution you'd like**\r\n`client.newCompleteCommand(job).variable(\"name\", value)...`\r\n\r\n**Describe alternatives you've considered**\r\nPromote the Java feature with `Map.of(\"name\", value)` giving examples in the docs and Zeebe examples. \r\n\r\nOlder programmers, that learned Java with versions older than 8 are not aware of this shortcut.\r\n\r\n**Additional context**\r\nIt came up in a migration Workshop with a customer.\n\n Gireesh2002: Acdording to [javadoc.io]https://javadoc.io/doc/io.camunda/zeebe-client-java/1.2.1/index-all.html\r\nMy Suggesstion\r\nbefor\r\njobClient.newCompleteCommand(job.getKey()).variables(variables);\r\nafter\r\nSystem.out.println(job.getElementId());   // returns element Id (or) null\r\n                  Map variables = job.getVariablesAsMap();  // Get variables\r\njobClient.newCompleteCommand(job.getKey()).variables((Map.of(\"newVariable\",\"VariableFromClient\"));\n korthout: ZPA triage:\n- seems like a reasonable request\n- should be a good first issue\n- @aleksander-dytko please consider if you want the team to work on this from PM perspective\n aleksander-dytko: @korthout thanks for the mention. I don't see it as a priority at the moment - changing priority level to `Later`.\n megglos: ZDP-Triage:\n- not affecting ZDP",
    "title": "As a Zeebe Java user, I want to complete a Job with a single variable"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12975",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nWhen configuring async message TTL checking, I need to know the health of my system. A key metric here is the number of \"buffered\" messages. When this number is steadily climbing, I might need to configure the TTL checker differently to ensure that messages expire fast enough and don't accumulate.\r\n\r\n**Describe the solution you'd like**\r\nExport more message related metrics. Similar to the banned instance metrics, these should restore on recovery so that the reported counts are accurate even when brokers restart.\r\n\r\n**Describe alternatives you've considered**\r\nNot exposing metrics for this directly, instead relying on the exported record stream. This is more complicated and requires a separate application continously running.\r\n\r\n**Additional context**\r\n\r\nrelated to https://jira.camunda.com/browse/SUPPORT-17177\r\n\n\n Zelldon: :bulb: Just as a thought we might want to consider this a general metric for all column families. \r\n\n megglos: ZDP-Planning:\n- low hanging fruit that can be done shortly\n- solution needs to be discussed still though\n megglos: @rodrigo-lourenco-lopes @oleschoenburg  we likely would need to backport this (at least back to 8.1) for a particular customer to make use of, is that feasible? \n rodrigo-lourenco-lopes: > @rodrigo-lourenco-lopes @oleschoenburg we likely would need to backport this (at least back to 8.1) for a particular customer to make use of, is that feasible?\r\n\r\nYes, it should be possible :) ",
    "title": "Export number of buffered messages as metrics"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12942",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "In our case，the directory of `/usr/local/zeebe/data/raft-partition/partitions/1` has about 60 .log files with each one has 128MB size data,and we have 3 patitions,so all the files‘s size almost reach 66GB.When we do backup api,it always failed by throwing exception like below:\r\n![image](https://github.com/camunda/zeebe/assets/12196018/ea47d5f7-9fe6-40dc-9d62-92ece0445266)\r\n\r\n`\r\n{'backupId': 1685672012, 'status': 'FAILED', 'failureReason': \"Backup on partition 3 failed due to java.util.concurrent.CompletionException: software.amazon.awssdk.core.exception.SdkClientException: Unable to execute HTTP request: Acquire operation took longer than the configured maximum time. This indicates that a request cannot get a connection from the pool within the specified maximum time. This can be due to high request rate.\\nConsider taking any of the following actions to mitigate the issue: increase max connections, increase acquire timeout, or slowing the request rate.\\nIncreasing the max connections can increase client throughput (unless the network interface is already fully utilized), but can eventually start to hit operation system limitations on the number of file descriptors used by the process. If you already are fully utilizing your network interface or cannot further increase your connection count, increasing the acquire timeout gives extra time for requests to acquire a connection before timing out. If the connections doesn't free up, the subsequent requests will still timeout.\\nIf the above mechanisms are not able to fix the issue, try smoothing out your requests so that large traffic bursts cannot overload the client, being more efficient with the number of times you need to call AWS, or by increasing the number of hosts sending requests.}\r\n`\r\nI also see the code source,S3BackupStore.java create NettyNioAsyncHttpClient with static number 45s of ConnectionAcquireTimeout,and other configuration used the default value.There is no any chance to customizing httpClient configurtation data .\r\nI'm not sure if i have missing somthing important document.Could you help me with this problem?  Thanks.\r\n@oleschoenburg \r\n![image](https://github.com/camunda/zeebe/assets/12196018/fd472cf0-be41-4d2f-ae4e-222a4b1725c9)\r\n\r\n\n\n oleschoenburg: Thanks for reporting @codingman1990 :+1: \r\n\r\nPlease correct me if I'm wrong but the situation is basically this:\r\nWhen a partition has a lot of log segments, uploading them in parallel will cause some uploads to wait for longer than 45 seconds on an available connection. This results in an exception and failure to complete the backup.\r\n\r\nWe should definitely improve this, although I'm not sure if just configuring the connection acquisition timeout is the right way to go about it.\r\n\r\nIn my opinion, the issue is rather that there is no limiting of parallel uploads. This is an issue here, with 66 log segments. It will also become an issue for large states, especially if #12483 is merged. So I would suggest to rather limit the number of concurrent uploads. The limit can be configurable of course.\r\n\r\n\r\n@codingman1990 Slightly unrelated but I was a bit surprised when I read that you have >60 log segments. I'd have expected much lower numbers. Did you adjust the snapshot interval or know any other reasons why your brokers keep more segments than usual?\r\n\r\n\n codingman1990: > Thanks for reporting @codingman1990 👍\r\n> \r\n> Please correct me if I'm wrong but the situation is basically this: When a partition has a lot of log segments, uploading them in parallel will cause some uploads to wait for longer than 45 seconds on an available connection. This results in an exception and failure to complete the backup.\r\n> \r\n> We should definitely improve this, although I'm not sure if just configuring the connection acquisition timeout is the right way to go about it.\r\n> \r\n> In my opinion, the issue is rather that there is no limiting of parallel uploads. This is an issue here, with 66 log segments. It will also become an issue for large states, especially if #12483 is merged. So I would suggest to rather limit the number of concurrent uploads. The limit can be configurable of course.\r\n> \r\n> @codingman1990 Slightly unrelated but I was a bit surprised when I read that you have >60 log segments. I'd have expected much lower numbers. Did you adjust the snapshot interval or know any other reasons why your brokers keep more segments than usual?\r\n\r\nIn my situation,there are abount 200 running instances at the same time.I almost had not edit any configuration except something like ES,BackupStore.The full configuration data is like below(Some sensitive data are marsked):\r\n\r\n`\r\n{\r\n    \"network\": {\r\n        \"host\": \"0.0.0.0\",\r\n        \"portOffset\": 0,\r\n        \"maxMessageSize\": \"4MB\",\r\n        \"advertisedHost\": \"***\",\r\n        \"commandApi\": {\r\n            \"host\": \"0.0.0.0\",\r\n            \"port\": 26501,\r\n            \"advertisedHost\": \"***\",\r\n            \"advertisedPort\": 26501,\r\n            \"address\": \"0.0.0.0:26501\",\r\n            \"advertisedAddress\": \"***\"\r\n        },\r\n        \"internalApi\": {\r\n            \"host\": \"0.0.0.0\",\r\n            \"port\": 26502,\r\n            \"advertisedHost\": \"***\",\r\n            \"advertisedPort\": 26502,\r\n            \"address\": \"0.0.0.0:26502\",\r\n            \"advertisedAddress\": \"***\"\r\n        },\r\n        \"security\": {\r\n            \"enabled\": false,\r\n            \"certificateChainPath\": null,\r\n            \"privateKeyPath\": null\r\n        },\r\n        \"maxMessageSizeInBytes\": 4194304\r\n    },\r\n    \"cluster\": {\r\n        \"initialContactPoints\": [\r\n            \"***\",\r\n            \"***\",\r\n            \"***\"\r\n        ],\r\n        \"partitionIds\": [\r\n            1,\r\n            2,\r\n            3\r\n        ],\r\n        \"nodeId\": 1,\r\n        \"partitionsCount\": 3,\r\n        \"replicationFactor\": 3,\r\n        \"clusterSize\": 3,\r\n        \"clusterName\": \"camunda-zeebe\",\r\n        \"heartbeatInterval\": \"PT0.25S\",\r\n        \"electionTimeout\": \"PT2.5S\",\r\n        \"membership\": {\r\n            \"broadcastUpdates\": false,\r\n            \"broadcastDisputes\": true,\r\n            \"notifySuspect\": false,\r\n            \"gossipInterval\": \"PT0.25S\",\r\n            \"gossipFanout\": 2,\r\n            \"probeInterval\": \"PT1S\",\r\n            \"probeTimeout\": \"PT0.1S\",\r\n            \"suspectProbes\": 3,\r\n            \"failureTimeout\": \"PT10S\",\r\n            \"syncInterval\": \"PT10S\"\r\n        },\r\n        \"raft\": {\r\n            \"enablePriorityElection\": true\r\n        },\r\n        \"messageCompression\": \"NONE\"\r\n    },\r\n    \"threads\": {\r\n        \"cpuThreadCount\": 3,\r\n        \"ioThreadCount\": 3\r\n    },\r\n    \"data\": {\r\n        \"directory\": \"/usr/local/zeebe/data\",\r\n        \"logSegmentSize\": \"128MB\",\r\n        \"snapshotPeriod\": \"PT5M\",\r\n        \"logIndexDensity\": 100,\r\n        \"diskUsageMonitoringEnabled\": true,\r\n        \"diskUsageReplicationWatermark\": 0.87,\r\n        \"diskUsageCommandWatermark\": 0.85,\r\n        \"diskUsageMonitoringInterval\": \"PT1S\",\r\n        \"backup\": {\r\n            \"store\": \"S3\",\r\n            \"s3\": {\r\n                \"bucketName\": \"zeebe-proc\",\r\n                \"endpoint\": \"***\",\r\n                \"region\": \"cn-hangzhou\",\r\n                \"accessKey\": \"***\",\r\n                \"secretKey\": \"***\",\r\n                \"apiCallTimeout\": \"PT3M\"\r\n            }\r\n        },\r\n        \"logSegmentSizeInBytes\": 134217728,\r\n        \"freeDiskSpaceCommandWatermark\": 5052946022,\r\n        \"freeDiskSpaceReplicationWatermark\": 4379219886\r\n    },\r\n    \"exporters\": {\r\n        \"elasticsearch\": {\r\n            \"jarPath\": null,\r\n            \"className\": \"io.camunda.zeebe.exporter.ElasticsearchExporter\",\r\n            \"args\": {\r\n                \"index\": {\r\n                    \"prefix\": \"zeebe-record\"\r\n                },\r\n                \"authentication\": {\r\n                    \"password\": \"***\",\r\n                    \"username\": \"***\"\r\n                },\r\n                \"url\": \"***\"\r\n            },\r\n            \"external\": false\r\n        }\r\n    },\r\n    \"gateway\": {\r\n        \"network\": {\r\n            \"host\": \"0.0.0.0\",\r\n            \"port\": 26500,\r\n            \"minKeepAliveInterval\": \"PT30S\"\r\n        },\r\n        \"cluster\": {\r\n            \"initialContactPoints\": [\r\n                \"0.0.0.0:26502\"\r\n            ],\r\n            \"requestTimeout\": \"PT15S\",\r\n            \"clusterName\": \"zeebe-cluster\",\r\n            \"memberId\": \"gateway\",\r\n            \"host\": \"0.0.0.0\",\r\n            \"advertisedHost\": \"0.0.0.0\",\r\n            \"port\": 26502,\r\n            \"advertisedPort\": 26502,\r\n            \"membership\": {\r\n                \"broadcastUpdates\": false,\r\n                \"broadcastDisputes\": true,\r\n                \"notifySuspect\": false,\r\n                \"gossipInterval\": \"PT0.25S\",\r\n                \"gossipFanout\": 2,\r\n                \"probeInterval\": \"PT1S\",\r\n                \"probeTimeout\": \"PT0.1S\",\r\n                \"suspectProbes\": 3,\r\n                \"failureTimeout\": \"PT10S\",\r\n                \"syncInterval\": \"PT10S\"\r\n            },\r\n            \"security\": {\r\n                \"enabled\": false,\r\n                \"certificateChainPath\": null,\r\n                \"privateKeyPath\": null\r\n            },\r\n            \"messageCompression\": \"NONE\"\r\n        },\r\n        \"threads\": {\r\n            \"managementThreads\": 1\r\n        },\r\n        \"security\": {\r\n            \"enabled\": false,\r\n            \"certificateChainPath\": null,\r\n            \"privateKeyPath\": null\r\n        },\r\n        \"longPolling\": {\r\n            \"enabled\": true\r\n        },\r\n        \"interceptors\": [],\r\n        \"initialized\": true,\r\n        \"enable\": false\r\n    },\r\n    \"backpressure\": {\r\n        \"enabled\": true,\r\n        \"algorithm\": \"VEGAS\",\r\n        \"aimd\": {\r\n            \"requestTimeout\": \"PT1S\",\r\n            \"initialLimit\": 100,\r\n            \"minLimit\": 1,\r\n            \"maxLimit\": 1000,\r\n            \"backoffRatio\": 0.9\r\n        },\r\n        \"fixed\": {\r\n            \"limit\": 20\r\n        },\r\n        \"vegas\": {\r\n            \"alpha\": 3,\r\n            \"beta\": 6,\r\n            \"initialLimit\": 20\r\n        },\r\n        \"gradient\": {\r\n            \"minLimit\": 10,\r\n            \"initialLimit\": 20,\r\n            \"rttTolerance\": 2.0\r\n        },\r\n        \"gradient2\": {\r\n            \"minLimit\": 10,\r\n            \"initialLimit\": 20,\r\n            \"rttTolerance\": 2.0,\r\n            \"longWindow\": 600\r\n        }\r\n    },\r\n    \"experimental\": {\r\n        \"maxAppendsPerFollower\": 2,\r\n        \"maxAppendBatchSize\": \"32KB\",\r\n        \"disableExplicitRaftFlush\": false,\r\n        \"rocksdb\": {\r\n            \"columnFamilyOptions\": {},\r\n            \"enableStatistics\": false,\r\n            \"memoryLimit\": \"512MB\",\r\n            \"maxOpenFiles\": -1,\r\n            \"maxWriteBufferNumber\": 6,\r\n            \"minWriteBufferNumberToMerge\": 3,\r\n            \"ioRateBytesPerSecond\": 0,\r\n            \"disableWal\": false\r\n        },\r\n        \"raft\": {\r\n            \"requestTimeout\": \"PT5S\",\r\n            \"maxQuorumResponseTimeout\": \"PT0S\",\r\n            \"minStepDownFailureCount\": 3,\r\n            \"preferSnapshotReplicationThreshold\": 100,\r\n            \"preallocateSegmentFiles\": true\r\n        },\r\n        \"partitioning\": {\r\n            \"scheme\": \"ROUND_ROBIN\",\r\n            \"fixed\": []\r\n        },\r\n        \"queryApi\": {\r\n            \"enabled\": false\r\n        },\r\n        \"consistencyChecks\": {\r\n            \"enablePreconditions\": false,\r\n            \"enableForeignKeyChecks\": false,\r\n            \"settings\": {\r\n                \"enablePreconditions\": false,\r\n                \"enableForeignKeyChecks\": false\r\n            }\r\n        },\r\n        \"engine\": {\r\n            \"messages\": {\r\n                \"ttlCheckerBatchLimit\": 2147483647,\r\n                \"ttlCheckerInterval\": \"PT1M\"\r\n            }\r\n        },\r\n        \"features\": {\r\n            \"enableYieldingDueDateChecker\": false,\r\n            \"enableActorMetrics\": false,\r\n            \"enableBackup\": true,\r\n            \"enableMessageTtlCheckerAsync\": false\r\n        },\r\n        \"maxAppendBatchSizeInBytes\": 32768\r\n    },\r\n    \"executionMetricsExporterEnabled\": false,\r\n    \"processing\": {\r\n        \"maxCommandsInBatch\": 1\r\n    }\r\n}\r\n`\r\n@oleschoenburg Could you please point out the problem.\n megglos: ZDP-Triage:\n- to be discussed at planning as flagged by Ole\n megglos: ZDP-Planning:\r\n- will become more likely with sst partitioning enabled",
    "title": "Support S3 backup httpclient custom configuration."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12878",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nIn the BPMN model API, a `send task` or `message throw event` should be able to reference a message definition.\r\n\r\n* a `send task` or `message throw event` can have an extension element `messageDefinition` with\r\n  * an attribute `messageName` that references the name of the message\r\n    * has either a static value or an expression\r\n    * must not be empty\r\n  * an attribute `correlationKey` that references the correlation key of the message\r\n    * has either a static value or an expression\r\n    * must not be empty\r\n  * an attribute `messageId` that references the id of the message\r\n    * has either a static value or an expression\r\n  * an attribute `timeToLive` that references the time to live of the message\r\n    * has either a static value or an expression\r\n*  a `send task` or `message throw event` must have either an extension element `messageDefinition` or `taskDefinition` but not both\r\n\r\n\r\nSample XML with static attributes:\r\n\r\n```\r\n<bpmn:sendTask id=\"send_message\">\r\n  <bpmn:extensionElements>\r\n    <zeebe:messageDefinition \r\n        messageName=\"foo\" \r\n        correlationKey=\"abc\" \r\n        messageId=\"abc\" \r\n        timeToLive=\"PT10S\"/>\r\n  </bpmn:extensionElements>  \r\n</bpmn:sendTask> \r\n\r\n\r\n<bpmn:intermediateThrowEvent id=\"Event_1nqoocy\">\r\n  <bpmn:extensionElements>\r\n    <zeebe:messageDefinition \r\n        messageName=\"foo\" \r\n        correlationKey=\"abc\" \r\n        messageId=\"abc\" \r\n        timeToLive=\"PT10S\"/>\r\n  </bpmn:extensionElements>\r\n  <bpmn:messageEventDefinition id=\"MessageEventDefinition_1wizp0b\" />\r\n</bpmn:intermediateThrowEvent>\r\n```\r\n\r\nSample XML with expression attributes:\r\n\r\n```\r\n<bpmn:sendTask id=\"send_message\">\r\n  <bpmn:extensionElements>\r\n    <zeebe:messageDefinition \r\n        messageName=\"=messageName\" \r\n        correlationKey=\"=correlationKey\" \r\n        messageId=\"=messageId\" \r\n        timeToLive=\"=timeToLive\"/>\r\n  </bpmn:extensionElements>  \r\n</bpmn:sendTask> \r\n\r\n<bpmn:intermediateThrowEvent id=\"Event_1nqoocy\">\r\n  <bpmn:extensionElements>\r\n    <zeebe:messageDefinition \r\n        messageName=\"=messageName\" \r\n        correlationKey=\"=correlationKey\" \r\n        messageId=\"=messageId\" \r\n        timeToLive=\"=timeToLive\"/>\r\n  </bpmn:extensionElements>\r\n  <bpmn:messageEventDefinition id=\"MessageEventDefinition_1wizp0b\" />\r\n</bpmn:intermediateThrowEvent>\r\n```\r\n\n\n korthout: Hi @lzgabel 👋 Sorry for the delay. I wanted to look at your [pull request](https://github.com/camunda/zeebe/pull/12879), but I want to understand the big picture before reviewing it.\r\n\r\nAs I understand, this issue is one of several parts of:\r\n- #1021?\r\n\r\nIn order to allow a user to model a process where a Send Task (or Messsage Throw Event) sends a message, we need to bind it to some Message Definition. Which is what this issue is about.\r\n\r\nIn your suggestion, the binding is achieved through a new `messageDefinition` extension element in the zeebe namespace, informing the message name, correlation key, message id and time-to-live of the message that should be published.\r\n\r\n❓ How did you come to this design? Was there a specific reason why you chose this over any other?\r\n\r\nFor example, there already exists a `MessageEventDefinition` in the BPMN spec that we could attempt to re-use for this. Here is an example of pure BPMN.\r\n\r\n```xml\r\n<bpmn:definitions ...>\r\n  <bpmn:process id=\"Process_02p7q4p\" isExecutable=\"true\">\r\n    ...\r\n    <bpmn:intermediateThrowEvent id=\"Event_145tbfx\">\r\n      <bpmn:messageEventDefinition id=\"MessageEventDefinition_0inu3y4\" messageRef=\"Message_1nb8aa6\" />\r\n    </bpmn:intermediateThrowEvent>\r\n    ...\r\n  </bpmn:process>\r\n  <bpmn:message id=\"Message_1nb8aa6\" name=\"order_placed\" />\r\n  ...\r\n</bpmn:definitions>\r\n```\n lzgabel: Hi @korthout. Thanks for you reply.  I'm sorry for starting the implementation without agreeing on this issue. :bow:\r\n\r\n> How did you come to this design? Was there a specific reason why you chose this over any other?\r\n\r\nActually, what I first thought was that this behavior is considered to be an internal implementation of the Zeebe engine, and should be create a new `messageDefinition` extension element in the zeebe namespace. 😄 \r\n\r\n> there already exists a MessageEventDefinition in the BPMN spec that we could attempt to re-use for this. Here is an example of pure BPMN.\r\n\r\nYes. You are absolutely right. 👍  I've checked the BPMN spec again, I will take your suggestion.\r\n\r\n---\r\n🤔 I plan to add an extension element under message, WDYT? :\r\n\r\n```xml\r\n<bpmn:message id=\"Message_1nb8aa6\" name=\"order_placed\">\r\n  <bpmn:extensionElements>\r\n    <zeebe:publish correlationKey=\"= orderId\" timeTolive=\"PT10S\" messageId=\"= uuid()\" />\r\n  </bpmn:extensionElements>\r\n</bpmn:message>\r\n```\r\n\r\n--- \r\nAt the same time, I think we can also support the `message end event`. 🚀 \r\n\r\n\r\n\r\n\n korthout: > I plan to add an extension element under message\r\n\r\n@lzgabel Thanks for checking with me. I don't know what is best. \r\n\r\nAn argument for putting this under the message is that this allows several tasks/events to publish the same message with the same details (correlation key, TTL, message id).\r\n\r\nBut the counter-argument is that this does not allow changing any of these for tasks/events that want to publish the same message but with different details.\r\n\r\nI'll check with our Modeling experts to see what fits best. I'll attempt to make a decision on this early next week. Is that okay for you, @lzgabel?\n lzgabel: > I'll check with our Modeling experts to see what fits best. I'll attempt to make a decision on this early next week. Is that okay for you, @lzgabel?\r\n\r\nYes. Looking forward to the conclusion.\n nikku: @korthout to mirror my internal comment here:\r\n\r\n* I'd love to clearly separate a message from \"stuff that is being done with the message, in the context of a flow / process execution\".\r\n* To support this I'd keep what is relevant in the _execution flow_ on the flow element (intermediate catch event in this case)\n barmac: Hi, I'd like to add my two cents. First, it's great that you are looking into this feature, and I appreciate it a lot that you pulled in Modeler devs :)\r\n\r\nLet's examine how we solve modeling problems with each of the solutions:\r\n\r\n| Problem | Properties defined on shared `bpmn:Message` | Properties defined on individual events via `bpmn:MessageEventDefinition`|\r\n|-|-|-|\r\n| I want to add another event which publishes message of given name with the same correlation key, TTL etc. | Create a new event and select shared message | Copy existing event |\r\n| I want to modify properties for a specific event | Create a new message with the same name but different propertie | Modify properties on the event |\r\n| I want to modify properties on all events | Modify in single place | Modify on each event |\r\n| I want to create an event with a different variable as correlation key | Create a new message with the same name | Change on individual event |\r\n\r\nSetting different properties for the same message name leads to redundancy in the diagram (multiple messages of the same name but different extension elements). I believe this is not what we want, therefore I'd support setting event-related properties on the event definition.\r\n\r\nNote that all of that can be also applied to catch event. \r\n\r\n---\r\n\r\nI started writing this before Nico's comment but we discussed this in the morning and have 100% agreement :)\r\n\r\nProposal:\r\n\r\n```xml\r\n<bpmn:endEvent id=\"Message_1nb8aa6\" name=\"order_placed\">\r\n  <bpmn:messageEventDefinition>\r\n    <bpmn:extensionElements>\r\n      <zeebe:publish correlationKey=\"= orderId\" timeTolive=\"PT10S\" />\r\n    </bpmn:extensionElements>\r\n  </bpmn:messageEventDefinition>\r\n</bpmn:endEvent>\r\n```\n barmac: Question: What would `messageId` be used for?\n nikku: > Question: What would `messageId` be used for?\r\n\r\n@barmac I asked myself the same question. It is another way to ensure idempotent message delivery ([internal ref](https://camunda.slack.com/archives/CSQ2E3BT4/p1686047258957109)).\n nikku: @barmac and as we discussed, based on https://github.com/camunda/zeebe/issues/12878#issuecomment-1578613868, imagine this (attaching subscription information to the message definition, not the `bpmn:Message` as it is currently the case):\r\n\r\n```xml\r\n<bpmn:intermediateCatchEvent id=\"Message_1nb8aa6\" name=\"order_processed\">\r\n  <bpmn:messageEventDefinition messageRef=\"order_processed_message\">\r\n    <bpmn:extensionElements>\r\n      <zeebe:subscribe correlationKey=\"= orderId\" />\r\n    </bpmn:extensionElements>\r\n  </bpmn:messageEventDefinition>\r\n</bpmn:intermediateCatchEvent>\r\n```\n lzgabel: 👋 Hi Guys. Will this [#12878 (comment)](https://github.com/camunda/zeebe/issues/12878#issuecomment-1578613868) be the final conclusion?\n korthout: Hi @lzgabel I've asked our internal Modeling experts. @nikku and @barmac gave great input, and we'll likely take that route. However, I don't want to take the opportunity away for others to voice their opinions. I set Monday, June 12th, as the deadline for input, so let's await that. I'll update this issue on Tuesday with the outcome.\n jonathanlukas: I would also prefer the approach @nikku proposes. While a message correlation surely is related to the message itself, the implementation details are more part of the event definition of each bpmn element.\r\n\r\nAlso, I think in Camunda 7, the message throw event was able to select a message element to refer to which still leads to confusion...\n korthout: Thanks everyone for your input 👏 \r\n\r\nNo opinions against the proposal were raised. Let's move forward with the following:\r\n\r\n- a new extension element that can be added to the `bpmn:messageEventDefinition` (of Intermediate Throw Event, or End Event) and/or to the `bpmn:sendTask` to specify the details of the message to publish (i.e. correlation key, time to live, message id)\r\n- let's name this extension element `zeebe:publishMessage` to make the XML easier to understand/read for humans\r\n- the message to publish can be referenced directly from the `bpmn:messageEventDefinition` and from the `bpmn:sendTask` according to BPMN spec using the `messageRef` attribute (the referenced message contains the message name).\r\n\r\nHere follows an example with three different elements all publishing the same message with different message details (i.e. correlation key, time to live, message id).\r\n\r\n```xml\r\n<bpmn:definitions ...>\r\n  <bpmn:process id=\"Process_02p7q4p\" isExecutable=\"true\">\r\n    ...\r\n    <bpmn:intermediateThrowEvent id=\"Event_145tbfx\">\r\n      <bpmn:messageEventDefinition id=\"MessageEventDefinition_0inu3y4\" messageRef=\"Message_1nb8aa6\">\r\n        <bpmn:extensionElements>\r\n          <zeebe:publishMessage correlationKey=\"= orderId\" timeTolive=\"PT10S\" messageId=\"= orderId\" />\r\n        </bpmn:extensionElements>\r\n      </bpmn:messageEventDefinition>\r\n    </bpmn:intermediateThrowEvent>\r\n    ...\r\n    <bpmn:sendTask id=\"Activity_1kgyl85\" messageRef=\"Message_1nb8aa6\">\r\n      <bpmn:extensionElements>\r\n        <zeebe:publishMessage correlationKey=\"= some_other_order_id\" />\r\n      </bpmn:extensionElements>\r\n    </bpmn:sendTask>\r\n    ...\r\n    <bpmn:endEvent id=\"Message_1nb8aa6\" name=\"order_placed\">\r\n      <bpmn:messageEventDefinition messageRef=\"Message_1nb8aa6\">\r\n        <bpmn:extensionElements>\r\n          <zeebe:publishMessage correlationKey=\"= orderId\" timeTolive=\"PT1H\" />\r\n        </bpmn:extensionElements>\r\n      </bpmn:messageEventDefinition>\r\n    </bpmn:endEvent>\r\n    ...\r\n  </bpmn:process>\r\n  <bpmn:message id=\"Message_1nb8aa6\" name=\"order_placed\" />\r\n  ...\r\n</bpmn:definitions>\r\n```\r\n\r\n\r\n\n lzgabel: @korthout. Thanks. ❤️ \r\nI'll be pushing the latest commit for this feature in the next few days.",
    "title": "Add message definition extension elements"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12796",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nWhen pushing a job fails, we should trigger the `JobYieldProcessor` to make the related job available for long polling.\r\n\r\n**Describe the solution you'd like**\r\n`JobIntent.YIELD` command should be appended when job push fails. This will be achieved through registering the error handler to `RemoteJobStreamErrorHandlerService` inside `JobStreamServiceStep`.\r\n\r\n**Describe alternatives you've considered**\r\n/\r\n\r\n**Additional context**\r\nBlocked by:\r\n* https://github.com/camunda/zeebe/issues/12541\r\n\n",
    "title": "Provide Error Handler implementation for Job Streamer"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12793",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\n\r\nWe can configure a raft request timeout that is applicable to all requests send between raft leaders and followers. InstallRequest send snapshot files which are considerably larger than other raft requests. As a result sometimes it takes longer to send the request and get a response, especially on networks with higher latency between brokers. This can result in timeout exception and the snapshot replication will have to restart from the beginning (See #11496). \r\n\r\nTo workaround, we can increase raft request timeout. But increasing this can affect how fast failures are detected, how fast requests are retried etc. So it would be better to be able to set a higher request timeout just of InstallRequest.\r\n\r\n**Describe the solution you'd like**\r\n\r\nExpose a configuration to set request timeout for InstallRequest. This should be different from the existing raft request timeout configuration.\r\n\r\n**Describe alternatives you've considered**\r\n\r\n- Fix #11496. This will not solve the issue, but reduces the impact if the request timesout.\r\n- Instead of sending the files as it is, split into to smaller requests. This would be a better solution, but requires more effort. https://github.com/camunda/zeebe/issues/12795 \r\n\r\n**Additional context**\r\nRelated to https://jira.camunda.com/browse/SUPPORT-16901 \r\n\n\n megglos: Triage:\n- it allows us to provide a better workaround in cases where the snapshot transmission takes a longer time (depending on the size of a single snapshot chunk up to 64MB) => we could just increase the timeout for installs but not everything\n- me moved it into ready already to get to it asap for the next patch (we have to do it next week)",
    "title": "Allow configuring request timeout for InstallRequest"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12696",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\n\r\nSome process models can sometimes cause whole partitions to become completely bricked. For example, process models with a combination of loops, large multi-instance collection spawning activities, or straight-through processing (e.g. FEEL script tasks, undefined tasks, manual tasks, etc.) can cause us to write a large amount of follow up records, much faster than we can process or export. This can lead to:\r\n\r\n- The exported system being overloaded, e.g. Operate cannot import fast enough, Elasticsearch runs out of disk space, etc.\r\n- Runaway process instances cannot be canceled because the command is either not accepted, or written so far down the log that it takes hours to get there, at which point we've written millions more follow up records.\r\n- Node is out of disk space, leading to all partitions stopping.\r\n\r\n**Describe the solution you'd like**\r\n\r\nWhile preventing such situations is ideal, as a first step, we would like to provide an escape hatch for when they arise by allowing users to forcefully terminate a process without having to wait for all events to be processed.\r\n\r\nThis means providing a way to bypass the causality chain of the partition by directly modifying the state projection.\r\n\r\nWe would have to:\r\n\r\n1. Provide an API endpoint (whether a management or client endpoint is to be determined) where users can forcefully terminate a PI by key\r\n2. The leader of the partition for this PI must then modify the state to mark that PI as forcefully terminated. **It may not be possible to write to disk anymore, so we may have to simply modify the state**.\r\n3. The state modification must be replicated to all followers, much like the exporter state. This is to ensure consistency between all nodes should an election occur.\r\n4. Once a PI is forcefully terminated, then all records associated it still have to be read but can be skipped/not processed/replayed. **We cannot delete them from the log - this not only breaks the append-only contract, but is much more complicated**\r\n5. The records must also be skipped by the exporters. However, we may want to notify the external systems that the PI has been forcefully terminated somehow, so we may need some solution here. Maybe we still need to write a record at the end of the log specifying it was forcefully terminated.\r\n\r\nIn terms of UX, it would be great to be able to differentiate between normal termination and forceful termination, if only for monitoring and auditing purposes.\r\n\r\n**Describe alternatives you've considered**\r\n\r\n- Static analysis of process models to detect endless loops. This may not always be possible, and there may very well be valid use cases for such. Unclear how easy that is, especially things like a process with a call activity where the called ID is a variable which could point to the process itself.\r\n- Fair/weighted process scheduling. A rather complex solution, where the processor ensures that one PI cannot overwhelm the partition at the cost of other PIs or external user commands. This could be achieved by performing more look-ahead and determining whether it's safe to continue processing a given PI or if it should yield, and writing deferred computation records to resume it.\r\n\r\n**Additional context**\r\n\r\nSeveral incidents occurred recently which were the results of users deploying looping, straight-through processes, which were producing too writing more than the system could process/export. One was writing an exponential number of follow up records, which led to an unrecoverable out of disk space, where there was nothing to compact yet, so processing could never resume.\r\n\n\n korthout: @npepinpe I worry that termination of the process instance is considered a loss of data.\r\n\r\nInstead of termination of the process instance, I'd rather see something like suspension of the process instance. For example, by creating a process instance-level [Incident](https://docs.camunda.io/docs/components/concepts/incidents/) with a new `ErrorType` that prevents any further progress unless they are attempts to repair the instance, e.g. Modification, SetVariable, Migration, etc. This aids in visibility and allows users to repair the instance.\r\n\r\nSuch a solution is also what we are investigating in relation to:\r\n- #5121 \r\n- https://github.com/camunda/product-hub/issues/686\r\n\r\nI understand if that would not make it into the first iteration, but I'd hope we can replace the termination logic with what I've described here in some future iteration.\n Zelldon: Yesterday I was also thinking about suspending process instances just for the same use case, from the operations point of view, to get my system healthy again or avoid an imbalance of exporting and processing. \r\n\r\nNot sure whether the proposed solutions above are optimal, due to the state changes without processing a command. \r\n\r\n----\r\n\r\nWhat I thought we could do is have a runtime transient toggle that can be flipped for every process instance. If we see that a process instance makes problems we could flip it and the process instance will be ignored. Kind of similar to being blacklisted, but not persistent. If we restart the instance can be executed again. \r\n\r\n**What does ignored mean?** \r\n\r\nThere could be two things either, completely skip all commands, but then you will lose the progress. OR you simply append the current command for the process instance at the end of the log, without processing it. This allows to make progress still on other instances, and later again on the PI if it is no longer suspended. (BTW IMHO commands shouldn't be exported anyway (https://github.com/camunda/zeebe/issues/6749))\r\n\r\nWe need to allow certain commands like canceling still be processed for such PI. This allows to clean up, similar to we should allow for blacklisted instances.\r\n\r\n_______\r\n\r\n\r\nTo develop this further I could imagine that we could at some point have this even automatically, like normal OS scheduling or as we know in K8. If you have X process instances and Y CPUs than a PI is only allowed to take Y/X CPU time, if more we could suspend it for a time frame (append the current command at the end such it will be processed later).\r\n\n korthout: 🤔 Interesting ideas @Zelldon. \r\n\r\nYielding the commands (appending them to the end of the log instead of processing them) would solve parts of these issues, but not the fork bomb case where the log keeps expanding. Consider a fork bomb that's been running for a few minutes unnoticed. There are likely already thousands if not millions of unprocessed commands for that instance on the log. These will all be yielded to back of the log continuously. New commands will take at least the time that it takes to yield all those commands.\r\n\r\nI'm personally more in favor of skipping the commands. This puts the process instance in a state of limbo just like a blacklisted process instance. All we need is a way to start up processing again, and to allow the user to cancel the instance. \r\n\r\n- We could persist the skipped commands. As soon as we want to continue, we write these back to the log and continue processing. This has two downsides: disk usage and fork bomb would continue. So I don't think we should do this.\r\n- We could ignore the skipped commands. As soon as we want to continue, the user can usePI modification / migration to re-start the processing, or they can simply cancel the instance. \r\n\r\nLastly, I was wondering how you'd keep track of a runtime flag for each process instance? If a user has large state (millions of running PI) then this would consume a lot of memory. Why not persist it like blacklisting?\n deepthidevaki: > Not sure whether the proposed solutions above are optimal, due to the state changes without processing a command.\r\n\r\nI think it is possible to do this correctly. When we force \"suspend\" (for lack of a better word) a process, we should bypass the normal stream processing process. That means, we do not write the command to the logstream, but submit to StreamProcessor actor as a task to be processed immediately. When processing this task, it should generate followup events  to suspend the process. The followup event could be also the error record/incident record which @korthout mentioned. The follow up event should somehow refer to the invisible command that should have been at the current processingPosition (yet to figure out how to do it.). The event get's replicated and the followers will have the same state as the leader. Send the response only after this follow up event is committed, so that we know for sure that the process is suspended.\r\n\r\nSide note:- In general, it would make sense to not write user commands to the logstream. Only follow up command and events are required for deterministic replay. \n megglos: ZDP-Triage:\n- may be a topic that affects both teams\n- if such incidents occur we can't recover in an easy way (you would need to manually update the rocksDb sate to e.g. blacklist the instance)\n\n@abbasadel would be curious on the triage outcome of your team, as this would be crucial to mitigate incidents where we experience malicious processes\n megglos: @felix-mueller this is the feature I mentioned in the stakeholder round today in order to allow us to better handle incidents that are caused by malicious processes\n megglos: ZDP-Planning:\n- ZDP is picking this up to assess solutions for this asap to allow engineers to handle incidents better going forward\n remcowesterhoud: ZPA Triage:\r\n- The solution of bypassing the log is unrelated to ZPA, as we only build things on top of the log stream.\r\n- We are interested in what happens to the process instance. The state of this PI is our responsibility\r\n- Please reach out to us when you need involvement on deciding what happens with the PI! We are happy to support 🙂 \r\n- We're not planning to work on this unless we need to support the ZDP team. As a result we will remove it from our project board.\n Zelldon: First of all, I want to thank you all for your ideas and brain dumps you have done here. :bow: \r\n\r\nI thought about this for a while and also re-read all your comments. If you're interested, I used the following [gdoc](https://docs.google.com/document/d/1NDKjq7osuzYoFd5Jtj9Xhe5_buw8rVU6nLIqBDQwJ74/edit) to summarize and better assess potential solutions. \r\n\r\nI have a potential solution in mind, let me shortly explain it to you.\r\n\r\n## Proposal:\r\n\r\nI think we can combine several ideas together to get the best out of it. \r\n\r\nIt makes sense to have a two-step process implemented, which allows to first ban the instance and secondly (if wished) to cancel the instance.\r\n\r\nWe provide an API (potentially actuator) where we can send “ban instance X” request. We access the ZeebeDB with a separate context, as we do with the Exporters, and mark the process instance as banned. I think this should bring us a quick win and feels potentially easy to implement. \r\n\r\nThe ban column family is less frequented, which should allows us to already stop the processing for a specific PI, without conflicts. It might make sense to think about the blacklisting cache again, whether we really need it or we just delete it (otherwise we have to update flag).\r\n\r\nThe banning of the instance should be confirmed with an event on the log, this allows to replicate the state change, and followers can do the same.\r\n\r\nPlease be aware as soon as the instance is banned, there is right now no way to recover. [The process instance is in limbo, as ](https://github.com/camunda/zeebe/issues/12696#:~:text=I%27m%20personally%20more%20in%20favor%20of%20skipping%20the%20commands.%20This%20puts%20the%20process%20instance%20in%20a%20state%20of%20limbo%20just%20like%20a%20blacklisted%20process%20instance.%20All%20we%20need%20is%20a%20way%20to%20start%20up%20processing%20again%2C%20and%20to%20allow%20the%20user%20to%20cancel%20the%20instance.)[Nico Korthout](mailto:nico.korthout@camunda.com) said so nicely. To handle this differently I see this out of the scope of this feature since we expect it to use only for severe issues where the cluster is otherwise not recoverable (and/or can make no progress).\r\n\r\nYou may ask why is that actually the case. Because we likely skip commands which are then lost. Furthermore, there is right now no way to remove some PI from the banned instances list. A potential follow-up could be to allow removing instances from that list and applying PI modification (to repair instances).\r\n\r\nWe could think of [persisting the skipped records into the state](https://github.com/camunda/zeebe/issues/12696#:~:text=We%20could%20persist%20the%20skipped%20commands.%20As%20soon%20as%20we%20want%20to%20continue%2C%20we%20write%20these%20back%20to%20the%20log%20and%20continue%20processing.%20This%20has%20two%20downsides%3A%20disk%20usage%20and%20fork%20bomb%20would%20continue.%20So%20I%20don%27t%20think%20we%20should%20do%20this.), but I would rather not do that, since it could cause severe other issues due to limited space, etc. It is also not clear to me whether the gains are outweighed by the costs since we can also easily run process instance modification to recover such.\r\n\r\nThe approach above is a combination of multiple others mentioned before, and I feel it is best to not make it too easy to erase customer data, which is why cancelation is/can be the second step. \r\n\r\nGenerally, cancelation involves more work, due to the child's cancelation, etc. This should happen in the processing, with a separate command. Since it might fail as well, due to large nested instances, etc, but with this approach it is fine since it is already banned in this case. \r\n\r\nIn order to allow the cancelation we need to make sure that we implement the [cancelation for banned instances.](https://github.com/camunda/zeebe/issues/12772)\r\n\r\n## Next\r\n\r\nAs a next step, I would like to spend some time doing a POC and play a bit around with that. Please feel free to raise any concerns with this approach. I'm open for discussions of course.\r\n",
    "title": "Forcefully terminate a process instance"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12655",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\nAfter a release we must send a list of fixed issues related to support tickets. For this we look at the `support` label on issues. It's easy to forget to add this label.\n\n**Describe the solution you'd like**\nIntroduce a GitHub action that checks new issues and comments in issues if the text contains `SUPPORT-XXXX`. If it find any the action should add the `support` label.\n\n**Describe alternatives you've considered**\nN/A\n\n**Additional context**\nN/A\n\n\n remcowesterhoud: @abbasadel fyi 🙂 \n megglos: ZDP-Triage:\n- would be great to be done for all C8 teams actually\n- @megglos will take this over as part of the support/engineering collaboration\n",
    "title": "Automatically add `support` label to support related issues"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12575",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\n\r\nI think that it is not a good idea to collect snapshot files in the list. \r\nCheck it here: https://github.com/camunda/zeebe/blob/02d1df8ac1b6f5484f1e9d4c1f19c8a5712176b1/snapshot/src/main/java/io/camunda/zeebe/snapshots/impl/SnapshotChecksum.java#L41. \r\nBecause, after a while, the amount of snapshot files should grow. So, we could instead call the `.forEachOrdered` method to calculate the snapshot. I will provide the PR soon to see this in action and benchmark this.\r\n\r\n**Describe the solution you'd like**\r\nWe should call `Stream.forEachOrdered` instead of collecting snapshots `File`s in the list.\r\n\r\n**Describe alternatives you've considered**\r\nWe could use `Stream.forEach` but as I can understand the order is important, so we shouldn't do this.\n\n aivinog1: @Zelldon Hello 👋 \nI think that this could be addressed to the ZDP Team (sorry if I am wrong 😅)\nCould we assign this to someone, please? In our load test environment, we see a little degradation in creating snapshots and we are suspecting this part of the code.\nThank you in advance 🙂\n Zelldon: Looks like @deepthidevaki will look into your PR, thanks for providing it :)",
    "title": "Improve the traversing of snapshot files"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12548",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nI understand that [the Actor metrics](https://github.com/camunda/zeebe/blob/a3d2002a0eb74cc701572b4e02abfeb16d6a48a8/scheduler/src/main/java/io/camunda/zeebe/scheduler/ActorMetrics.java) are in the experimental phase, but it would be nice if there exists the Actor dashboard out of the box 🙂\r\n\r\n**Describe the solution you'd like**\r\nA separate file, called, for example, actor.json in [the Grafana dashboards](https://github.com/camunda/zeebe/tree/a3d2002a0eb74cc701572b4e02abfeb16d6a48a8/monitor/grafana) folder.\r\n\r\n**Describe alternatives you've considered**\r\nAdd these metrics to the existing dashboard, for example, [zeebe.json](https://github.com/camunda/zeebe/blob/a3d2002a0eb74cc701572b4e02abfeb16d6a48a8/monitor/grafana/zeebe.json). But I think that it would be more convenient if we have this in a separate dashboard (some users may decide to not enable these metrics thus the dashboard is not needed for them)\n\n megglos: ZDP-Triage:\r\n- seems like a good first issue to pickup during an upcoming onboarding",
    "title": "Provide Grafana Dashboards for the Actor metrics"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12541",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThe following `Processor` classes are able to push an activated job to a client.\r\n\r\n- [JobWorkerTaskProcessor](https://github.com/camunda/zeebe/blob/319813a684325c97556fb59013eb8b88ea27b3f2/engine/src/main/java/io/camunda/zeebe/engine/processing/bpmn/task/JobWorkerTaskProcessor.java#L54) / [BpmnJobBehavior](https://github.com/camunda/zeebe/blob/4d46a4947e6d3ac72cb4e0af324f4c978b591989/engine/src/main/java/io/camunda/zeebe/engine/processing/bpmn/behavior/BpmnJobBehavior.java#L81-L92)\r\n- [JobTimeOutProcessor](https://github.com/camunda/zeebe/blob/ad1d5c92a3d4d6009da3a9d968238b83b9dd5c5c/engine/src/main/java/io/camunda/zeebe/engine/processing/job/JobTimeOutProcessor.java#L20)\r\n- [JobFailProcessor](https://github.com/camunda/zeebe/blob/813c86f780f54496fcd089810704c1a4e4958141/engine/src/main/java/io/camunda/zeebe/engine/processing/job/JobFailProcessor.java)\r\n- [JobRecurProcessor](https://github.com/camunda/zeebe/blob/813c86f780f54496fcd089810704c1a4e4958141/engine/src/main/java/io/camunda/zeebe/engine/processing/job/JobRecurProcessor.java)\r\n- [ResolveIncidentProcessor](https://github.com/camunda/zeebe/blob/18dd3c5e8df9bd2164e4d0fc73f5429c9d38b05c/engine/src/main/java/io/camunda/zeebe/engine/processing/incident/ResolveIncidentProcessor.java)\r\n\r\n**Describe the solution you'd like**\r\nThe `BpmnJobActivationBehavior` class is able to use the `JobStreamer` API to push jobs. The following steps need to be performed:\r\n\r\n- [x] For an available `JobStream` get `JobActivationProperties`\r\n- [x] Set `deadline` for `JobRecord` (using `JobActivationProperties`)\r\n- [x] Set `variables` for `JobRecord` (using `JobActivationProperties`) \r\n- [x] Set `worker` for `JobRecord` (using `JobActivationProperties`)\r\n- [x] Activate job using a `JobBatchRecord`/`JobBatchIntent.ACTIVATE`\r\n- [x] Push `JobRecord` on the `JobStream` through a `SideEffectProducer`\r\n\r\n**Describe alternatives you've considered**\r\n/\r\n\r\n**Additional context**\r\nBlocked by:\r\n* https://github.com/camunda/zeebe/issues/12083\r\n\n",
    "title": "Jobs are pushed from relevant processors"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12539",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\n\r\nCreate a processor to process `ProcessInstanceBatch.TERMINATE` commands.\r\n\r\n- Add a method to the BpmnStateBehavior to get a defined amount of child elements, starting at a specific key.\r\n- Create the processor\r\n- Write a `ProcessInstance.TERMINATE` command for each of the element instance keys in the list\r\n- If the `index` is available:\r\n    - Get the next `BATCH_SIZE` + 1 child instances of the `batchElementInstanceKey`\r\n    - Create a new `ProcessInstanceBatch` record\r\n    - Write a new `ProcessInstanceBatch.TERMINATE` command\r\n- If no `index` is available:\r\n    - Have a 🍪 Nothing else to do\n",
    "title": "Create `ProcessInstanceBatch.TERMINATE` processor"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12538",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\n\r\nContainer elements (process, subprocess, etc.) all terminate their child instances when `onTerminate` is called. For all these elements this method calls the `BpmnStateTransitionBehavior#terminateChildInstances` method. This method needs to be changed to make use of the new `ProcessInstanceBatch` command with the `TERMINATE` intent.\r\n\r\n- Modify the `BpmnStateTransitionBehavior#terminateChildInstances `:\r\n    - Create a `ProcessInstanceBatch` record\r\n        - `batchElementInstanceKey` will be the key of the container element\r\n        - `index` will be empty as this is the first batch command\r\n    - Write the a `ProcessInstanceBatch.TERMINATE` command using the created record\n",
    "title": "Use the `ProcessInstanceBatch Command` when terminating container elements"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12537",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\nThe `ProcessInstanceBatch` record and intent will be used to perform certain actions (terminating/activating children) on a process instance in batches.\r\n\r\nThe record will contain the following data:\r\n- `batchElementInstanceKey` - The element instance key of the element that the batch is executed. E.g., the key of a subprocess which will terminate all its children.\r\n- ~~`childElementInstanceKeys` - The element instance keys of all elements for which the batch action needs to be performed. E.g., all child instance for which a `TERMINATE` command needs to be written.~~\r\n- `index` - The index to keep track of where we are in the batch. Depending on the Intent this index can be something different. For `TERMINATE` this will be the element instance key of the first child in the next batch.\r\n\r\nThere will be only 1 intent for now:\r\n- `TERMINATE`\n",
    "title": "Create `ProcessInstanceBatch` Record and Intent"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12416",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nIt is hard to tune the JVM if I cannot override the default `-Xms128m`. I can't override it via `JAVA_OPTS` because it is used at first, and then goes explicit `-Xms128m` (and finally applied):\r\n```\r\nexec \"$JAVACMD\" $JAVA_OPTS -Xms128m -XX:+ExitOnOutOfMemoryError -Dfile.encoding=UTF-8...\r\n```\r\n\r\n**Describe the solution you'd like**\r\nI would like to move all JVM options into the default Docker environment variable (`JAVA_OPTS`) so that users could override it easily (`-Xms` included).\r\n\r\n**Describe alternatives you've considered**\r\nWe could just remove `-Xms128m` from the default options, but I can't predict the consequences.\r\n\r\n**Additional context**\r\nSee the details [here](https://camunda-platform.slack.com/archives/C6WGNHV2A/p1681141205701259).\r\nI would like to see the changes backported in 8.0, 8.1, 8.2 as well 🙂\n\n remcowesterhoud: @megglos Do you know which team should be responsible for this one? I feel like this is more of a shared responsibility and I don't see it listed in the team split document.\n megglos: this affects the dist module (which is shared? 😅 ), the script is generated via the `appassembler-maven-plugin`, which has a feature request for exactly this open since 2016 😅 \r\nhttps://github.com/mojohaus/appassembler/issues/48\r\n\r\nJust synced with @Zelldon on this, the best would probably be to remove the Xms flag from the plugin config and stick to jvm defaults\r\nhttps://github.com/camunda/zeebe/blob/main/dist/pom.xml#L290\n remcowesterhoud: Oh right, I don't know how I missed that in the split document 🤦 \r\n\r\nI'll assign it to both our project in that case\n aivinog1: Hey @remcowesterhoud! If this is not occupied I could provide the PR to fix it:\r\n> Just synced with @Zelldon on this, the best would probably be to remove the Xms flag from the plugin config and stick to jvm defaults\n remcowesterhoud: @aivinog1 Thanks! It's not occupied so go ahead. I'll assign the issue to you 👍 ",
    "title": "Remove the default un-overridable `-Xms128m` value"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12382",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nCurrently the zeebe process is run by the root user in the zeebe docker image:\r\n```\r\nroot@5ce6a5346a36:/usr/local/zeebe# ps aux\r\nUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\r\nroot         1  0.3  0.0   1940   448 ?        Ss   14:26   0:00 tini -- /usr/local/bin/startup.sh\r\nroot         7  197  1.4 4216156 295668 ?      Sl   14:26   0:05 /opt/java/openjdk/bin/java -Xms128m -XX:+ExitOnOutOfMemoryError -Dfile.encoding=UTF-8 -classpath /usr/local/zeebe/config:/usr/local/zeebe/lib/* -Dapp.name=broker -Dapp.pid=7 -Dapp.repo=/usr/local/zeebe\r\nroot        42  1.0  0.0   6880  3364 pts/0    Ss   14:26   0:00 /bin/bash\r\nroot        55  0.0  0.0   8476  2808 pts/0    R+   14:26   0:00 ps aux\r\n```\r\n\r\nTo harden the security of the docker image it should **default** to run it with an unprivileged user instead, [see the OWASP recommendation](https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-2-set-a-user).\r\n\r\n**Describe the solution you'd like**\r\nThe zeebe image **should by default run with an unprivileged user.** [There is already a zeebe user setup with uid 1000](https://github.com/camunda/zeebe/blob/main/Dockerfile#L105-L111) it's just not used.\r\n\r\n**Additional context**\r\nRelates to https://github.com/camunda/product-hub/issues/717\r\nsupport for running with an unprivileged user was added with https://github.com/camunda/zeebe/issues/11784\r\nRelates to https://jira.camunda.com/browse/SUPPORT-15969\n\n mvawork: Please review my pull request.\r\n\r\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code class=\"notranslate\"># ps -aux\r\nUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\r\nzeebe        1  0.1  0.0   2504   580 pts/0    Ss   12:28   0:00 tini -- /usr/local/bin/startup.sh /bin/bash\r\nzeebe       12 35.8  1.7 12603416 455936 pts/0 Sl+  12:28   1:18 /opt/java/openjdk/bin/java -XX:+ExitOnOutOfMemoryError -Dfile.encoding=UTF-8 -classpath /usr/local/zeebe/config:/usr/local/zeeb\r\nroot       100  0.0  0.0   2612   536 pts/1    Ss   12:28   0:00 /bin/sh\r\nroot       109  0.0  0.0   8892  3308 pts/1    R+   12:32   0:00 ps -aux\r\n# \r\n</code></pre><div class=\"zeroclipboard-container position-absolute right-0 top-0\">\r\n    <clipboard-copy aria-label=\"Copy\" class=\"ClipboardButton btn js-clipboard-copy m-2 p-0 tooltipped-no-delay\" data-copy-feedback=\"Copied!\" data-tooltip-direction=\"w\" value=\"root@5ce6a5346a36:/usr/local/zeebe# ps aux\r\nUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\r\nroot         1  0.3  0.0   1940   448 ?        Ss   14:26   0:00 tini -- /usr/local/bin/startup.sh\r\nroot         7  197  1.4 4216156 295668 ?      Sl   14:26   0:05 /opt/java/openjdk/bin/java -Xms128m -XX:+ExitOnOutOfMemoryError -Dfile.encoding=UTF-8 -classpath /usr/local/zeebe/config:/usr/local/zeebe/lib/* -Dapp.name=broker -Dapp.pid=7 -Dapp.repo=/usr/local/zeebe\r\nroot        42  1.0  0.0   6880  3364 pts/0    Ss   14:26   0:00 /bin/bash\r\nroot        55  0.0  0.0   8476  2808 pts/0    R+   14:26   0:00 ps aux\" tabindex=\"0\" role=\"button\" style=\"display: inherit;\">\r\n      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-copy js-clipboard-copy-icon m-2\">\r\n    <path d=\"M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z\"></path><path d=\"M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z\"></path>\r\n</svg>\r\n      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-check js-clipboard-check-icon color-fg-success m-2 d-none\">\r\n    <path d=\"M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z\"></path>\r\n</svg>\r\n    </clipboard-copy>\r\n  </div></div>\r\n\n jessesimpson36: I proposed an [alternative PR ](https://github.com/camunda/zeebe/pull/13418)to accomplish the same goal, without gosu.  I believe there was a concern that if you set `USER zeebe` about whether you could exec into the container as root, and I did verify manually that you can run the process as root via `docker run --user root ...`  as well as `docker exec -it --user root ...`.\r\n\r\nThis change will require a helm chart change, but I'm on the team that works on that, so coordinating that change shouldn't be much of a challenge.",
    "title": "Docker: Run the zeebe process with an unprivileged user by default"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12283",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nI want to be able to open processes modeled with `zeebe-bpmn-model` in the Desktop Modeler as a C8 process by default. Currently, I need to set a lot of attributes by hand.\r\n\r\n```java\r\nDefinitions definitions = modelInstance.newInstance(Definitions.class);\r\n            definitions.setTargetNamespace(BPMN20_NS);\r\n            //definitions.setExporter(\"Camunda Modeler\");\r\n            //definitions.setExporterVersion(\"5.8.0\");\r\n            definitions.setAttributeValueNs(\"http://camunda.org/schema/modeler/1.0\",\"modeler:executionPlatform\",\"Camunda Cloud\");\r\n            modelInstance.setDefinitions(definitions);\r\n```\r\n\r\n**Describe the solution you'd like**\r\nProvide reasonable defaults for these attributes.\r\n\r\n**Describe alternatives you've considered**\r\nNone, the workaround above works as well.\r\n\r\n**Additional context**\r\nRequested by @superbeagle \n\n korthout: ZPA triage:\n\n- @remcowesterhoud mentioned that we should indicate somewhere in these attributes that this was generated with the zeebe-bpmn-model\n- @koevskinikola reproduced the issue already and it wasn't very pleasant\n- @koevskinikola the values should be defaults and can be changed by the user via the API (at least like above, or perhaps with an improved API)\n- @remcowesterhoud we should also add the other missing attributes\n- our own priority for this is low, but we think it's a good first issue for new contributors\n\n",
    "title": "Zeebe BPMN Model should provide reasonable defaults for definition attributes"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12085",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\nA job pushed to the `JobStreamer` API may fail to be handed over to the client due to client failure.\r\n\r\n## AC\r\n* A new `JobIntent.YIELD` intent is added.\r\n* A new `JobYield` processor is added. The processor will perform logic similar to the `JobFailProcessor`, i.e. make a job `ACTIVATABLE` again.\r\n\r\n## Additional context\r\n* The implementation of this issue should then be used in the job push `ErrorHandler` implementation.\n",
    "title": "Job Yield Processor is implemented to be used for Job Push fallback"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12000",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThis is part of the break-down of the following epic https://github.com/camunda/product-hub/issues/120 .\r\n\r\n**Describe the solution you'd like**\r\nIn order to add native support for oauth token authentication in Camunda 8 SM the Zeebe Gateway needs to get extended to perform oauth authentication token validation. The scope of this feature is just authentication, authorisation is to be added in a follow-up. Ideally the solution is making use of the identity-sdk for token verification.\r\n\r\n**Describe alternatives you've considered**\r\nAn alternative would be adding an auth proxy component to the Camunda 8 SM stack, however this adds more complexity to the infrastructure.\r\n\r\n**Additional context**\r\n- https://github.com/camunda-community-hub/zeebe-keycloak-interceptor Community project\r\n- Identity interceptor prototype https://github.com/npepinpe/zeebe-identity-interceptor/blob/main/src/main/java/org/camunda/community/zeebe/interceptors/identity/Interceptor.java\r\n\r\n\r\nRelates to:\r\n- https://jira.camunda.com/browse/SUPPORT-15807\n",
    "title": "OAuth Auth Token authentication support in Zeebe Gateway"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11920",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "- ## Adjust `EndEventProcessor` to broadcast a signal on activation\r\n\t- Introduce a new `SignalEndEventBehavior`\r\n\t- When Signal End Event activates:\r\n\t\t- Apply input mappings\r\n\t\t- Transition to activated\r\n\t\t- Write `Signal:Broadcast` command\r\n\t\t- Apply output mappings\r\n\t\t- Transition to complete the element\n",
    "title": "Support Broadcast signal for Signal End Events"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11919",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "- ## Adjust `IntermediateThrowEventProcessor` to broadcast a signal on activation\r\n\t- When Signal Intermediate Throw Event activates:\r\n\t\t- Apply input mappings\r\n\t\t- Transition to activated\r\n\t\t- Write `Signal:Broadcast` command\r\n\t\t- Apply output mappings\r\n\t\t- Transition to complete the element\r\n\r\n\n",
    "title": "Support Broadcast signal for Signal Intermediate Throw Events"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11708",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\n\r\nIn order to register job streams to the gateway, we will need to add a new gRPC API for this. This API will be a unidirectional stream (from server to client). \r\n\r\nIt should take in as parameter the same activation properties as the job worker (minus anything related to long polling and the likes), but it will be a long-living stream. Later, when implementing it on the client side and on the gateway, it's important that its keep-alive be configured properly.\r\n\r\nIt returns a stream of single `ActivatedJob`. We can discuss if we want to keep the batching properties, as this could be a useful optimization, but I would propose to ignore it for the alpha target and measure its impact once we have the complete end-to-end pipeline.\n",
    "title": "Add gRPC job stream API"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/10031",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThe BPMN symbol for the inclusive gateway is supported with\r\n- #9747 \r\n\r\nHowever, only the diverging behavior was added. Users may want to use the converging behavior as well.\r\n\r\n**Describe the solution you'd like**\r\n- remove the validation that restricts the number of incoming sequence flows to an inclusive gateway to max 1.\r\n- only activate the inclusive gateway when (see discussion https://github.com/camunda/zeebe/pull/9747#discussion_r925770530):\r\n  - there are no active children of its flow scope instance (we can check this already),\r\n  - or if all incoming sequence flows were taken at least once (we can check this already),\r\n  - or if no path can be found from any of the active children to the inclusive gateway (but we can't check this yet).\r\n\r\n**Describe alternatives you've considered**\r\nUsers can use a combination of parallel and exclusive gateways to build similar joining logic.\r\n\r\n**Additional context**\r\nSpec: https://www.omg.org/spec/BPMN/2.0.2/PDF\r\n\r\nProduct-Hub:\r\n- https://github.com/camunda/product-hub/issues/364\r\n\n\n lzgabel: Hi @remcowesterhoud. I'm working on this recently, please assign this task to me. Thanks. ❤️\n lzgabel: Hi @remcowesterhoud. Sorry to bother you, :bow:  because I'm currently implementing this feature, and I need to get all child instances in the scope, but when I rebase the changes on top of  `main`, I found that you removed this `BpmnStateBehavior#getChildInstances` method by this [commit](https://github.com/camunda/zeebe/pull/12604/commits/0f3f458221a72b2fae27e9a1f5bc7dad890c4395), so I will add it back.\n remcowesterhoud: > Hi @remcowesterhoud. Sorry to bother you, :bow:  because I'm currently implementing this feature, and I need to get all child instances in the scope, but when I rebase the changes on top of  `main`, I found that you removed this `BpmnStateBehavior#getChildInstances` method by this [commit](https://github.com/camunda/zeebe/pull/12604/commits/0f3f458221a72b2fae27e9a1f5bc7dad890c4395), so I will add it back.\n\nIt was unused 😄\n\nOf course, go ahead and put it back if you need it!\n remcowesterhoud: @koevskinikola assigning you on the issue, so it's clear on our board you are reviewing it\n aisong: I'm really looking forward to this feature. I wonder when it will be released?\n lzgabel: Hi @aisong. This feature is available after version `8.3.0-alpha2`. It also brings another problem, which I am currently solving: #13070 \r\n\r\nBTW. `8.3.0` will be released this October.\n aisong: > Hi @aisong. This feature is available after version `8.3.0-alpha2`. It also brings another problem, which I am currently solving: #13070\r\n> \r\n> BTW. `8.3.0` will be released this October.\r\n\r\nOK,thank you. I'll be waiting for your good product.\n jschulenklopper: Looking forwards to this. This is related to the error message in the Modeler that reads \"An <Inclusive Gateway> with more than one incoming <Sequence Flow> is not supported by Camunda 8.2\", right? And delivery of this feature will deprecate the warning that's on https://docs.camunda.io/docs/components/modeler/bpmn/inclusive-gateways/?\r\n\r\nAnd for the modeller, the missing feature will be delivered per https://github.com/camunda/camunda-modeler/issues/3613 ?\r\n\r\n\n korthout: Great to hear you're all excited about this feature 🚀 \r\n\r\n@jschulenklopper That's all correct! You can already tryout the feature in `8.3.0-alpha2`, but Modeler does not take this into account yet (which is why it raises the warning). As @lzgabel mentioned, this is planned to be fully available in `8.3.0` (release planned for October 2023)\n korthout: Due to several limitations, we had to revert the implementation of the converging inclusive gateway:\r\n- https://github.com/camunda/zeebe/issues/13640\r\n\r\nTherefore, I'm re-opening this issue.\n lzgabel: Hi @korthout. I'll continue to look into this issue in the next few days. If you have a better solution, please let me know. 🙇 \n korthout: @lzgabel Sorry, I haven't spent time on this topic in a while, and I don't yet have ideas other than those that I wrote [here](https://github.com/camunda/zeebe/issues/13070#issuecomment-1649387794). So, we need some way to store in the state that a sequence flow is active and expect to process a command to activate the target element at some point in the future. If you want to work on this feature, I think we need to tackle this part first.",
    "title": "Support BPMN Converging Inclusive Gateway"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14509",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nAfter restoring from backup, some partitions encountered [following error](https://console.cloud.google.com/logs/query;cursorTimestamp=2023-09-26T11:26:03.072149905Z;endTime=2023-09-26T11:56:36.088Z;pinnedLogId=2023-09-26T11:26:01.585896295Z%2Fh1uykcs15coafgsq;query=logName:%22stdout%22%0Aresource.type%3D%22k8s_container%22%0Aresource.labels.container_name%3D%22zeebe%22%0Aresource.labels.namespace_name%3D%22eab43606-a898-467f-967f-21a757fc69b7-zeebe%22%0Aresource.labels.location%3D%22europe-west1%22%0Aresource.labels.project_id%3D%22camunda-saas-int-chaos%22%0Aresource.labels.cluster_name%3D%22worker-chaos-1%22%0Aresource.labels.pod_name%3D%22zeebe-1%22%0AjsonPayload.context.partitionId%3D%2226%22%0Atimestamp%3D%222023-09-26T11:26:01.585896295Z%22%0AinsertId%3D%22h1uykcs15coafgsq%22;startTime=2023-09-26T10:56:36.088Z;summaryFields=jsonPayload%252Fcontext%252FpartitionId,resource%252Flabels%252Fpod_name:false:32:beginning?project=camunda-saas-int-chaos).\r\n\r\n```\r\njava.lang.IllegalStateException: Expected to delete index after 3982, but it is lower than the commit index 3983. Deleting committed entries can lead to inconsistencies and is prohibited.\r\n\tat io.atomix.raft.storage.log.RaftLog.deleteAfter(RaftLog.java:168) ~[zeebe-atomix-cluster-8.2.13.jar:8.2.13]\r\n\tat io.atomix.raft.roles.PassiveRole.tryToAppend(PassiveRole.java:565) ~[zeebe-atomix-cluster-8.2.13.jar:8.2.13]\r\n\tat io.atomix.raft.roles.PassiveRole.appendEntries(PassiveRole.java:511) ~[zeebe-atomix-cluster-8.2.13.jar:8.2.13]\r\n\tat io.atomix.raft.roles.PassiveRole.handleAppend(PassiveRole.java:367) ~[zeebe-atomix-cluster-8.2.13.jar:8.2.13]\r\n```\r\n\r\nWhat happened:\r\n\r\nFirst Zeebe-1 votes for Zeebe-2 and Zeebe-2 becomes leader.\r\n```\r\nINFO 2023-09-26T11:25:57.870482436Z [jsonPayload.context.partitionId: 26] [resource.labels.podName: zeebe-1] RaftServer{raft-partition-partition-26}{role=FOLLOWER} - Accepted PollRequest{term=0, candidate=2, lastLogIndex=3982, lastLogTerm=2}: candidate's log is up-to-date\r\nINFO 2023-09-26T11:25:57.887139637Z [jsonPayload.context.partitionId: 26] [resource.labels.podName: zeebe-1] RaftServer{raft-partition-partition-26}{role=FOLLOWER} - Accepted VoteRequest{term=1, candidate=2, lastLogIndex=3982, lastLogTerm=2}: candidate's log is up-to-date\r\nINFO 2023-09-26T11:25:57.892834402Z [jsonPayload.context.partitionId: 26] [resource.labels.podName: zeebe-1] RaftServer{raft-partition-partition-26} - Found leader 2\r\n```\r\nSee above. `term` is 1, but the lastLogTerm is 2. This is because after restore, raft metastore is empty. So it restarts the term from 1. This is ok so far. But after Zeebe-2 becomes the leader and commits its InitialEntry at index `3983`, Zeebe-0 starts election probably because it did not receive any heartbeat from the leader `Zeebe-2`.\r\n\r\n```\r\nINFO 2023-09-26T11:26:01.569385207Z [jsonPayload.context.partitionId: 26] [resource.labels.podName: zeebe-1] RaftServer{raft-partition-partition-26}{role=FOLLOWER} - Accepted PollRequest{term=1, candidate=0, lastLogIndex=3982, lastLogTerm=2}: candidate's log is up-to-date\r\nINFO 2023-09-26T11:26:01.577404370Z [jsonPayload.context.partitionId: 26] [resource.labels.podName: zeebe-1] RaftServer{raft-partition-partition-26}{role=FOLLOWER} - Accepted VoteRequest{term=2, candidate=0, lastLogIndex=3982, lastLogTerm=2}: candidate's log is up-to-date\r\nINFO 2023-09-26T11:26:01.582069421Z [jsonPayload.context.partitionId: 26] [resource.labels.podName: zeebe-1] RaftServer{raft-partition-partition-26} - Found leader 0\r\n```\r\nZeebe-1 accepts poll request from Zeebe-0 because Zeebe-0's lastLogTerm > Zeebe-1's current term 1. This leads to inconsistency. \r\n\r\n**Impact**\r\nIf it happens immediately after restore before writing any new data, it should be safe to restart the brokers. Due to the above error, the new leader Zeebe-0 cannot commit anything. So, as a result there won't be any actual data inconsistency. If we wait until all nodes are healthy before restarting the traffic to the cluster, this will not lead to any actual data inconsistency because no user requests has been processed yet. However, if there are new user data that has been processed, then there is a possibility for partial data loss. **Note that the data recovered from the backup won't be lost if this happens. Only new data will be affected.**\r\n\r\n**To Reproduce**\r\n\r\nFollowing scenario can lead to this error:\r\n1. A, B, C restored from the same backup, where the term of the last entry in the log is > 1\r\n2. A, B forms the quorum and A became the leader\r\n3. A and B committed new entries \r\n4. C does not know about the new leader. So it starts new election.\r\n\r\n**Expected behavior**\r\n\r\nAfter, restoring from a backup raft can continue working in all scenarios.\r\n\r\n**Environment:**\r\n- Zeebe Version: Observed in 8.2.13\r\n\n",
    "title": "Potential inconsistency in raft after restoring from a backup"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14486",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nWhen writing to the checksum file of a snapshot fails, we silently ignore `IOException`s:\r\n\r\nhttps://github.com/camunda/zeebe/blob/6d3a703a34677809e08335e0f19c81814d59673c/snapshot/src/main/java/io/camunda/zeebe/snapshots/impl/SfvChecksumImpl.java#L74-L88\r\n\r\n`PrintWriter#flush` does not throw `IOException`s but only set's an internal error flag. The documentation recommends to use `PrintWriter#checkError` to flush and check for errors at the same time.\r\n\r\n**To Reproduce**\r\n\r\nProbably difficult to reproduce but can happen for any IO failure while writing the checksum file.\r\n\r\n**Expected behavior**\r\n\r\nThe exception is thrown so that the snapshot store can handle it as a failed attempt to commit a snapshot.\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\n<STACKTRACE>\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.3.0-alpha6, possibly all versions\r\n\r\n\n",
    "title": "Failure to write snapshot checksum is silently ignored"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14406",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nKudos to @romansmirnov for identifying this issue!\r\n\r\nWith https://github.com/camunda/zeebe/issues/13763, already executed migration tasks are marked as finished in the runtime state, to avoid rerunning already executed migrations with every restart.\r\n\r\nThis can lead to inconsistent states of snapshots across the brokers when a rolling update is performed. This is due to the fact that during the update brokers running an old version of zeebe are create a different state when replaying events than it would be the case when running the new version already.\r\n\r\n**To Reproduce/Scenario**\r\n\r\n_Given_\r\nWith 8.3 a new column family `PROCESS_INSTANCE_KEY_BY_DEFINITION_KEY` has been introduced. It maps a process instance key to its process definition. This new column family is used when trying to delete a process definition. However, in 8.2 that specific column family `PROCESS_INSTANCE_KEY_BY_DEFINITION_KEY` does not exist.\r\n\r\n_When_\r\nThe user upgrades from 8.2 to 8.3, and the upgrade happens in a rolling update manner, broker by broker.\r\n\r\nDuring that update, it may happen that a broker `x` (already updated to 8.3) will replicate a (already migrated) snapshot `s1` to a broker `y` which is still on version 8.2.\r\n\r\nAs the snapshot `s1` was already migrated by broker `x`, it contains an entry marking the migration task as finished.\r\n\r\nAfter broker `y` installed the received snapshot `s1`, it will start to replay the events.\r\n\r\nWhile replaying, it won't put anything into the column family `PROCESS_INSTANCE_KEY_BY_DEFINITION_KEY` because broker `y` is still running the previous version where this column family didn't exist.\r\n\r\nIn the meantime, broker `y` takes a new snapshot `s2`. Eventually, broker `y` gets updated to 8.3\r\n\r\n_Then_\r\n, it will recover from snapshot `s2` but it won't run the migration task anymore because the respective migration task was already marked as finished in snapshot `s1`. Thus the snapshot state of broker `y` will not be consistent with the one of broker `x`. If broker `y` eventually becomes leader, the corrupted state will be used going forward.\r\n\r\n**Expected behavior**\r\n\r\nMigrations must be executed in a way to ensure all brokers have eventually a consistent snapshot state.\r\n\r\n**Environment:**\r\n- OS: <!-- [e.g. Linux] -->\r\n- Zeebe Version: 8.3.x\r\n- Configuration: <!-- [e.g. exporters etc.] -->\r\n\n\n korthout: For completeness, I'll share [my thoughts](https://camunda.slack.com/archives/C037W9NMATG/p1695278745025669?thread_ts=1695219026.075349&cid=C037W9NMATG) here as well:\r\n\r\nI see two options:\r\n1. Followers should not replay events created by brokers on a higher version because we can't guarantee that these will be applied the same. This would result in slower failover during rolling update.\r\n2. Events should always be replayed in the same way, even between broker versions. This can be achieved by increasing the semantic `recordVersion` property of the event in the newer broker version. For backward compatibility, this requires separate event appliers (i.e. versioned event appliers).\r\n\r\nOption 2 would only work if we carefully create versioned event appliers for:\r\nevents that write to `PROCESS_INSTANCE_KEY_BY_DEFINITION_KEY`\r\nevents that interact with states that were migrated with migration tasks for multi-tenancy.\r\n\r\nIMO, option 1 is much easier to implement, and its negative effect may be limited.\n megglos: Thread with further discussions https://camunda.slack.com/archives/C037W9NMATG/p1695618000621469\n megglos: ZDP-Planning:\n- following-up with the ZPA team next if they have capacity in the next week\n megglos: ZPA-Planning outcome:\r\n\r\n@korthout & @remcowesterhoud  raised that removing https://github.com/camunda/zeebe/issues/13763 is the simplest solution as of now and it will be suffice for the 8.3 release, as all migrations now result in new CFs created.\r\n\r\nThus a migration will be run based on a condition if the deprecated CF still exists and contains data, if so the records in it will get migrated when a broker got updated eventually. This covers the edge case of an old broker receiving a snapshot from a new broker, as if it replays events it will write them to the deprecated CF , on update it will migrate those records to the new CF\r\n\r\nThank you 🙌 \n korthout: @remcowesterhoud I think we can close this issue right?\n remcowesterhoud: Yes we can",
    "title": "Migration could result in inconsistent data on rolling update"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14367",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nError group https://console.cloud.google.com/errors/detail/CLXT_uWi3KaNSQ;service=zeebe;time=P7D?project=camunda-saas-int-chaos\r\n\r\nWe recently reworked how we store replicated snapshots, and added some error cases that should fail when we for example receive a duplicate snapshot.\r\n\r\nThis seemed to happen in one of our recent chaos tests. The impact is quite high since due to the exception, which is thrown in this case, the node transitions to inactive. It will no longer take part of the corresponding partition.\r\n\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n**To Reproduce**\r\n\r\nNot clear yet.\r\n\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\n**Expected behavior**\r\n\r\nNo duplicate snapshot replication\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\nio.camunda.zeebe.snapshots.SnapshotException$SnapshotAlreadyExistsException: Expected to receive snapshot with id 225-1-225-240, but was already persisted. This shouldn't happen.\r\n\tat io.camunda.zeebe.snapshots.impl.FileBasedSnapshotStore.newReceivedSnapshot(FileBasedSnapshotStore.java:381) ~[zeebe-snapshots-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.snapshots.impl.FileBasedSnapshotStore.newReceivedSnapshot(FileBasedSnapshotStore.java:51) ~[zeebe-snapshots-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.atomix.raft.roles.PassiveRole.onInstall(PassiveRole.java:190) ~[zeebe-atomix-cluster-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.atomix.raft.roles.FollowerRole.onInstall(FollowerRole.java:83) ~[zeebe-atomix-cluster-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.atomix.raft.impl.RaftContext.lambda$registerHandlers$8(RaftContext.java:303) ~[zeebe-atomix-cluster-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.atomix.raft.impl.RaftContext.lambda$runOnContext$27(RaftContext.java:322) ~[zeebe-atomix-cluster-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.atomix.utils.concurrent.SingleThreadContext$WrappedRunnable.run(SingleThreadContext.java:178) [zeebe-atomix-utils-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) [?:?]\r\n\tat java.util.concurrent.FutureTask.run(Unknown Source) [?:?]\r\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source) [?:?]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]\r\n\tat java.lang.Thread.run(Unknown Source) [?:?]\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- OS: <!-- [e.g. Linux] -->\r\n- Zeebe Version: snapshot 8.3\r\n- Configuration: <!-- [e.g. exporters etc.] -->\r\n\n\n deepthidevaki: The threads are not running at the same time. Thread 40 logs are until `01:37` and Thread 39 logs are from `01:43`. Was there a restart in between?\n Zelldon: @deepthidevaki I think you are right, \r\n[Here](\r\nhttps://console.cloud.google.com/logs/query;query=logName:%22stdout%22%0Aresource.type%3D%22k8s_container%22%0Aresource.labels.location%3D%22europe-west1%22%0Aresource.labels.namespace_name%3D%222042c610-7a76-4f45-bfb3-b5bd2e8a9083-zeebe%22%0Aresource.labels.project_id%3D%22camunda-saas-int-chaos%22%0Aresource.labels.container_name%3D%22zeebe%22%0Aresource.labels.cluster_name%3D%22worker-chaos-1%22%0Aresource.labels.pod_name%3D%22zeebe-0%22;pinnedLogId=2023-09-20T01:43:33.274205286Z%2Fizrpouifho13c9fq;summaryFields=labels%252F%2522k8s-pod%252Fstatefulset_kubernetes_io%252Fpod-name%2522,jsonPayload%252Fcontext%252FthreadId:false:32:beginning;cursorTimestamp=2023-09-20T01:43:40.387554808Z;startTime=2023-09-20T01:08:25.576Z;endTime=2023-09-20T02:08:25.576Z?project=camunda-saas-int-chaos) is the restart\r\n\r\nSomehow overseen the timestamp gaps 👍🏼 \r\n\n Zelldon: Keeping it as critical for now since it is a regression and causing the node to transition to inactive when receiving duplicate snapshot\n npepinpe: From the logs, it looks like the leader tried sending it the same snapshot twice in a row.\r\n\r\n> 2023-09-20 03:37:52.583 CEST RaftServer{raft-partition-partition-2}{role=FOLLOWER} - Delete existing log (lastIndex '0') and replace with received snapshot (index '225'). First entry in the log will be at index 226\r\n>\r\n> 2023-09-20 03:37:55.560 CEST Committed new snapshot 225-1-225-240\r\n>\r\n> 2023-09-20 03:37:55.573 CEST RaftServer{raft-partition-partition-2}{role=FOLLOWER} - Committed snapshot FileBasedSnapshot{directory=/usr/local/zeebe/data/raft-partition/partitions/2/snapshots/225-1-225-240, checksumFile=/usr/local/zeebe/data/raft-partition/partitions/2/snapshots/225-1-225-240.checksum, checksum=1740324580, snapshotId=FileBasedSnapshotId{index=225, term=1, processedPosition=225, exporterPosition=240}, metadata=FileBasedSnapshotMetadata[version=1, processedPosition=225, exportedPosition=240, lastFollowupEventPosition=240]}\r\n>\r\n> 2023-09-20 03:37:55.576 CEST RaftServer{raft-partition-partition-2} - An uncaught exception occurred, transition to inactive role\r\n\r\nNow, the exception occurs is the following:\r\n\r\n```\r\nio.camunda.zeebe.snapshots.SnapshotException$SnapshotAlreadyExistsException: Expected to receive snapshot with id 225-1-225-240, but was already persisted. This shouldn't happen.\r\n\tat io.camunda.zeebe.snapshots.impl.FileBasedSnapshotStore.newReceivedSnapshot(FileBasedSnapshotStore.java:381) ~[zeebe-snapshots-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.snapshots.impl.FileBasedSnapshotStore.newReceivedSnapshot(FileBasedSnapshotStore.java:51) ~[zeebe-snapshots-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.atomix.raft.roles.PassiveRole.onInstall(PassiveRole.java:190) ~[zeebe-atomix-cluster-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.atomix.raft.roles.FollowerRole.onInstall(FollowerRole.java:83) ~[zeebe-atomix-cluster-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.atomix.raft.impl.RaftContext.lambda$registerHandlers$8(RaftContext.java:303) ~[zeebe-atomix-cluster-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.atomix.raft.impl.RaftContext.lambda$runOnContext$27(RaftContext.java:322) ~[zeebe-atomix-cluster-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.atomix.utils.concurrent.SingleThreadContext$WrappedRunnable.run(SingleThreadContext.java:178) [zeebe-atomix-utils-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) [?:?]\r\n\tat java.util.concurrent.FutureTask.run(Unknown Source) [?:?]\r\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source) [?:?]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]\r\n\tat java.lang.Thread.run(Unknown Source) [?:?]\r\n```\r\n\r\nAs we can see, it occurs because it received an install request from the leader. So nothing too weird is happening here - or rather, nothing that would lead to data loss or corruption or something :smile: \r\n\r\nThe question now is: did it make sense for the leader to send the snapshot install request again? If yes, then we should log a warning when this happens, but not fail, and return an appropriate response to the leader (i.e. we have this snapshot). If not, then we should fix it on the leader side.\r\n\r\nMy hypothesis is this is a race condition. Since the snapshot listener is invoked asynchronously after persisting, the snapshot reference of the follow may not be updated yet when the leader pushes out an append request (after it thinks the install operation is finished), which leads to it trying to send the same snapshot again.\n npepinpe: OK let's work through the hypothesis, since it's difficult to replicate this in a controlled way.\r\n\r\nThe follower's `onInstall` method will persist the snapshot. Meaning the Raft thread is blocked. And until we reply, the leader will not send another request to us. Now, in `onInstall`, we call the `persist()` method of the pending snapshot, and join on the resulting future. By the time this returns, the snapshot listeners have been invoked. This listener is asynchronous, so it won't execute immediately, but it will enqueue a job on the Raft thread. _Then we reply, and then the leader can send us another request_. \r\n\r\nSince all operations on the Raft thread are sequenced, the snapshot listener will be executed before the next leader request is processed. So the snapshot reference will be updated before the next request is processed, and thus the leader shouldn't be sending us the same snapshot again.\r\n\r\nOf course, this is all expected behavior, and maybe there's a bug somewhere in the code :smile: But according to that, my hypothesis is then wrong.\n npepinpe: Hm, I think the analysis above is wrong. In `onInstall`, we check if our current snapshot index is `>=` to the request's own index. So clearly, in this case, we decided this was false...\r\n\r\n```java\r\n    // If the snapshot already exists locally, do not overwrite it with a replicated snapshot.\r\n    // Simply reply to the request successfully.\r\n    final var latestIndex = raft.getCurrentSnapshotIndex();\r\n    if (latestIndex >= request.index()) {\r\n      abortPendingSnapshots();\r\n\r\n      return CompletableFuture.completedFuture(\r\n          logResponse(InstallResponse.builder().withStatus(RaftResponse.Status.OK).build()));\r\n    }\r\n```\r\n\r\nSo what might have happened is for some reason the leader decided to send another install request before the snapshot listener was handled. Are we possibly retrying install requests and ended up sending one too many? The default install request timeout is 2.5 seconds. If there's any I/O stalls, for example, and we take longer, then we'll retry the request. \r\n\r\nIndeed, on the leader, zeebe-1, we see the install request timed out.\r\n\r\n> 2023-09-20 03:37:54.894 CEST RaftServer{raft-partition-partition-2} - InstallRequest{currentTerm=1, leader=1, index=225, term=1, version=1, chunkId=HeapByteBuffer{position=0, remaining=14, limit=14, capacity=14, mark=java.nio.HeapByteBuffer[pos=0 lim=14 cap=14], hash=-1428579103}, nextChunkId=null, data=HeapByteBuffer{position=0, remaining=159, limit=159, capacity=159, mark=java.nio.HeapByteBuffer[pos=0 lim=159 cap=159], hash=559971236}, initial=false, complete=true} to 0 failed: java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException: Request ProtocolRequest{id=6017, subject=raft-partition-partition-2-install, sender=zeebe-1.zeebe-broker-service.2042c610-7a76-4f45-bfb3-b5bd2e8a9083-zeebe.svc.cluster.local:26502, payload=byte[]{length=308, hash=-186090835}} to zeebe-0.zeebe-broker-service.2042c610-7a76-4f45-bfb3-b5bd2e8a9083-zeebe.svc.cluster.local:26502 **timed out in PT2.5S**\r\n\r\nSo the request was simply retried, and it was then received and queued before the snapshot listener was processed.\r\n\r\n![mic drop](https://media.giphy.com/media/15BuyagtKucHm/giphy.gif)\r\n",
    "title": "Duplicate snapshot replication causing to fail raft follower"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14309",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n\r\n\r\nThe issue seemed to happen while processing. The exporter could not process the record as the DeployedProcess.getVersion() returns null, because ProcessState.getLatestProcessVersionByProcessId()\" is null. \r\n\r\n\r\n\r\n\r\n```\r\nExpected to process record \r\n'TypedRecordImpl{metadata=RecordMetadata{recordType=COMMAND, valueType=DEPLOYMENT, intent=CREATE, authorization=UNKNOWN}, value={\"resources\":[],\"processesMetadata\":[{\"bpmnProcessId\":\"Process_Connect_Intra\",\"version\":4,\"processDefinitionKey\":2251799813740831,\"resourceName\":\"diagram_connect_intra.bpmn\",\"checksum\":\"vE026arBq5Hh09ilkSj4Cw==\",\"isDuplicate\":false,\"tenantId\":\"<default>\"}],\"decisionRequirementsMetadata\":[],\"decisionsMetadata\":[],\"tenantId\":\"<default>\"}}' \r\nwithout errors, but exception occurred with message \r\n'Cannot invoke \"io.camunda.zeebe.engine.state.deployment.DeployedProcess.getVersion()\" \r\nbecause the return value of \"io.camunda.zeebe.engine.state.immutable.ProcessState.getLatestProcessVersionByProcessId(org.agrona.DirectBuffer)\" is null'.\r\n```\r\n\r\n**To Reproduce**\r\n\r\nDont know as of moment, the previous behavior in the [logs](https://console.cloud.google.com/logs/query%3Bquery=logName:%22stdout%22%0Aresource.type=%22k8s_container%22%0Aresource.labels.container_name=%22zeebe%22%0Aresource.labels.project_id=%22camunda-cloud-240911%22%0Aresource.labels.namespace_name=%22179a54e4-45b8-4850-8ff8-7caca6407a47-zeebe%22%0Aresource.labels.cluster_name=%22prod-worker-4%22%0Aresource.labels.location=%22australia-southeast1%22%0Aresource.labels.pod_name=%22zeebe-1%22%0A%3BpinnedLogId=2023-09-14T00:20:12.977565274Z/az5svd4vg7gfo3zz%3BcursorTimestamp=2023-09-14T00:21:43.373199577Z%3BstartTime=2023-09-13T23:50:42.977Z%3BendTime=2023-09-14T00:50:42.977Z?project=camunda-cloud-240911) seems normal at first glance.\r\n\r\n\r\n\r\n\r\n\r\n**Log/Stacktrace**\r\n\r\n```\r\njava.lang.NullPointerException: Cannot invoke \"io.camunda.zeebe.engine.state.deployment.DeployedProcess.getVersion()\" because the return value of \"io.camunda.zeebe.engine.state.immutable.ProcessState.getLatestProcessVersionByProcessId(org.agrona.DirectBuffer)\" is null\r\n\tat io.camunda.zeebe.engine.processing.deployment.StartEventSubscriptionManager.isLatestProcess(StartEventSubscriptionManager.java:68) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.deployment.StartEventSubscriptionManager.tryReOpenStartEventSubscription(StartEventSubscriptionManager.java:58) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.deployment.DeploymentCreateProcessor.processDistributedCommand(DeploymentCreateProcessor.java:108) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.streamprocessor.DistributedTypedRecordProcessor.processRecord(DistributedTypedRecordProcessor.java:31) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.Engine.process(Engine.java:127) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.batchProcessing(ProcessingStateMachine.java:353) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$2(ProcessingStateMachine.java:269) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run(ZeebeTransaction.java:98) ~[zeebe-db-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand(ProcessingStateMachine.java:269) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord(ProcessingStateMachine.java:228) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.readNextRecord(ProcessingStateMachine.java:204) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:109) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:204) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n```\r\n\r\n**Environment:**\r\n- OS: <!-- [e.g. Linux] -->\r\n- Zeebe Version: 8.3.0-alpha6\r\n- Configuration: <!-- [e.g. exporters etc.] -->\r\n\n\n megglos: ZDP-Triage:\n- happened on a trial cluster, the user deleted the cluster after\n- this seems to be a processing error that is gracefully handled and will result in a command rejection\n- it actually happened while processing not exporting\n- the causing error in the engine needs investigation as it is a potential regression\n- might be related to start event subscriptions\n- should be raised to ZPA for investigation on the actual cause, could be critical if it's indeed a regression\n megglos: @koevskinikola could this be a regression related to Multi Tenancy?\n koevskinikola: @megglos from the logs this doesn't seem related to multi-tenancy (MT) because:\r\n\r\nThe MT-related changes to the `ProcessState` haven't been merged with `main`, they are kept on the `12653_epic_multi-tenancy` branch. We refactored the signature of `ProcessState#getLatestProcessVersionByProcessId(DirectBuffer processId)` to include a `tenantId` input argument. The logs show the old version of this method.\r\n\r\nHowever,  this might be related to the resource deletion Epic, as there were some recent changes in the referenced code though [this PR](https://github.com/camunda/zeebe/pull/13935) (although nothing that significantly impacts the logic).\n korthout: ZPA Triage:\r\n- should be investigated before the 8.3 release\r\n- let's mark this as critical, because this appears to block deploying and can lead to partition inconsistency\r\n- #14366 appears to have the same root cause\n korthout: I had a look into this yesterday. I was not yet able to reproduce or even to understand how this could happen, but I'll share my findings so far.\r\n\r\n🧺 I started by collecting similar issues:\r\n- https://github.com/camunda/zeebe/issues/14366\r\n- https://github.com/camunda/zeebe/issues/14309\r\n- https://github.com/camunda/zeebe/issues/14055 (3 weeks ago)\r\n- https://github.com/camunda/zeebe/issues/11734 (Feb 17th)\r\n- https://github.com/camunda/zeebe/issues/11414 (Jan 16th)\r\n\r\n🔍 All these issues have `getLatestProcessVersionByProcessId` in their stacktrace, but they come from different places:\r\n- [Cancel process instance](https://github.com/camunda/zeebe/issues/14366) on 8.3.0-alpha6\r\n- [Create deployment](https://github.com/camunda/zeebe/issues/14055#issue-1873096848) (distributed to another partition using command distribution) on 8.3.0-alpha4\r\n- [Call activity](https://github.com/camunda/zeebe/issues/14055#issuecomment-1700454899) on 8.2.12\r\n- [DeploymentDistributeProcessor](https://github.com/camunda/zeebe/issues/14055#issuecomment-1700454899) on 8.2.12\r\n\r\n🤔 While similar, the last two on 8.2.12 may have a different root cause, as 8.3.0 introduces several related topics:\r\n- Create deployment over command distribution\r\n- Delete process using resource deletion\r\n\r\n🕵️ After collecting this, I wondered: could a newer process version be distributed to another partition before the older version is distributed to that same partition?\r\n- Yes, individual commands sent to another partition are unreliable, which is why we've built command distribution to provide a reliable mechanism for distributing commands\r\n- However, this does not lead to the failure\r\n  - I adjusted the engine not to send the deployment of the first version of a process to other partitions\r\n  - and wrote a multi-partition test case that deploys two versions of processes\r\n  - the exception is not thrown because version 2 that is distributed is [first persisted to state](https://github.com/camunda/zeebe/blob/09f444092d8f42e402449eb2cde0a546b781a4c0/engine/src/main/java/io/camunda/zeebe/engine/processing/deployment/DeploymentCreateProcessor.java#L109-L111) before it is looked up as being the last version, and so it doesn't return null\r\n\r\n🔬 Looking more closely [at the code](https://github.com/camunda/zeebe/blob/09f444092d8f42e402449eb2cde0a546b781a4c0/engine/src/main/java/io/camunda/zeebe/engine/processing/deployment/StartEventSubscriptionManager.java#L57-L58), I wondered: could it be that a non-executable process would be distributed but not stored in state and so could not be found?\r\n- No, even when pooled together with executable processes, the non-executable process is not distributed. It is not extracted into the process metadata.\r\n\r\nAnd this is where I stopped. 🔚 \r\n\r\n🤞 One idea is to look at the data from https://github.com/camunda/zeebe/issues/14055. Perhaps we can use it, together with a test and some debugging to see if we can reproduce that case.\r\n> Further data for analysis is [here](https://drive.google.com/drive/folders/1g6HlGdY3RwJO1yd1zl3Ld3T7r_CJUxto)\n Zelldon: @korthout happened on PROd again. Might be worth to investigate the running cluster.\r\n\r\nhttps://console.cloud.google.com/errors/detail/CMHx49mjl8qiRQ;service=zeebe;time=P7D?project=camunda-cloud-240911\n nicpuppa: @Zelldon can we get a snapshot of the state ?\n Zelldon: @nicpuppa see slack please handle [it their](https://app.slack.com/client/T0PM0P1SA/C05TBCABJUS) I have to other medic duties still to do\n nicpuppa: data directory can be found [here](https://drive.google.com/drive/folders/16VnPRGYisXY-0weDKmMLRIhyXlZd5TPw)\n remcowesterhoud: ## Root-cause\r\n\r\nIt's an issue with the caching of process versions. There is a scenario in which different keys in the cache reference the same value object. Because of this versions of different processes can influence each other.\r\n\r\n1. If we deploy a process we store it in the state using a shared object (nextVersion) (this name is still bad btw)\r\n2. If get the version info we search in the cache. If we can't find it we get it from the state and store this in the cache.\r\n3. Since getting it from the state wraps the values in the same object it means the entries in the cache all reference this same object. So if the object changes it changes for all entries.\r\n\r\nWe can fix it by making sure we copy the value from the state before storing it in the cache.\r\n\r\n**Reproducible test case**\r\n\r\n```java\r\n  @Test\r\n  public void x() {\r\n    // given\r\n    final var process1V1 = creatingProcessRecord(processingState, \"process1\").setVersion(1);\r\n    final var process1V2 = creatingProcessRecord(processingState, \"process1\").setVersion(2);\r\n    final var process2V1 = creatingProcessRecord(processingState, \"process2\").setVersion(1);\r\n    final var process2V2 = creatingProcessRecord(processingState, \"process2\").setVersion(2);\r\n    final var process2V3 = creatingProcessRecord(processingState, \"process2\").setVersion(3);\r\n\r\n    // when\r\n    processState.putProcess(process1V1.getKey(), process1V1);\r\n    processState.putProcess(process2V1.getKey(), process2V1);\r\n    processState.putProcess(process2V2.getKey(), process2V2);\r\n    processState.clearCache();\r\n    processState.putProcess(process1V2.getKey(), process1V2);\r\n    processState.putProcess(process2V3.getKey(), process2V3);\r\n\r\n    // then\r\n    assertThat(processState.getNextProcessVersion(\"process1\")).isEqualTo(2);\r\n  }\r\n```",
    "title": "Cannot process record due to process deployed version being null."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14275",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n\r\n- When we write something from the StreamProcessor we write it as a batch of records.\r\n- That is written to the Sequencer, wrapped as SequencedBatch. \r\n- The LogstorageAppender is writing this to the AtomixLogStorage.\r\n- The AtomixLogstorage writes to the LogAppender (LeaderRole).\r\n- The LeaderRole schedules[ a job to append](https://github.com/camunda/zeebe/blob/main/atomix/cluster/src/main/java/io/atomix/raft/roles/LeaderRole.java#L592), [the SequencedBatch is now wrapped as UnserializedApplicationEntry](https://github.com/camunda/zeebe/blob/main/atomix/cluster/src/main/java/io/atomix/raft/roles/LeaderRole.java#L595)\r\n- The LeaderRole appends the entry, [wrapped again as RaftLogEntry to the RaftLog.](https://github.com/camunda/zeebe/blob/main/atomix/cluster/src/main/java/io/atomix/raft/roles/LeaderRole.java#L617)\r\n- The RaftLog returns the IndexedRaftLogEntry (wrapped again)\r\n- LeaderRole [starts to replicate, with the IndexedRaftLogEntry](https://github.com/camunda/zeebe/blob/main/atomix/cluster/src/main/java/io/atomix/raft/roles/LeaderRole.java#L628)\r\n- LeaderAppender#appendEntries [creates futures for each index](https://github.com/camunda/zeebe/blob/main/atomix/cluster/src/main/java/io/atomix/raft/roles/LeaderAppender.java#L530)\r\n- LeaderRole registers async completion listeners (lambdas),[ which reference the IndexedRaftLogEntry](https://github.com/camunda/zeebe/blob/main/atomix/cluster/src/main/java/io/atomix/raft/roles/LeaderRole.java#L635)\r\n\r\nThe appendFutures and async lambda completion listeners in consequence reference the IndexedRaftEntries the whole lifetime. \r\n\r\nWhen a commit is not possible for a longer period of time, e.g. due to network issues, disk issues, etc. We accumulate more and more futures, and records. This is especially problematic if we have a lot of internal load, like scheduled timers that trigger all at once, and maybe even repeatedly since they are committed or processed not fast enough. \r\n\r\nThis can reach a certain point where the Broker goes out of memory.\r\n\r\nCame up in a support case: https://jira.camunda.com/browse/SUPPORT-18291\r\n\r\n\r\nRelated heap dump extract:\r\n\r\n![LeaderAppender-appendfutures](https://github.com/camunda/zeebe/assets/2758593/c309b99c-b715-4cdf-9310-1d099771513e)\r\n![sequencebatch](https://github.com/camunda/zeebe/assets/2758593/6c29b4dd-48d2-4fb2-9a28-7ddfe86384c8)\r\n![timerrecord-ref](https://github.com/camunda/zeebe/assets/2758593/057ae361-0d0a-49c2-b7d0-76249cae0d88)\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n**To Reproduce**\r\n\r\nCause some issue that commit is not possible have several timer due, which should be triggered (and triggered and triggered again) such that this causes a lot of load on the system. Eventually, the broker will go out of memory.\r\n\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\n**Expected behavior**\r\n\r\nWe don't keep the reference to the instances, we don't need all the information in the listeners. Mostly we only [need to commit indexes etc.](https://github.com/camunda/zeebe/blob/main/atomix/cluster/src/main/java/io/atomix/raft/roles/LeaderRole.java#L644-L648)\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n\r\n**Environment:**\r\n- OS: <!-- [e.g. Linux] -->\r\n- Zeebe Version: 8.2.x\r\n- Configuration: <!-- [e.g. exporters etc.] -->\r\n\n\n Zelldon: Related to https://github.com/camunda/zeebe/issues/13870, but instead of Jobs we had issues with timer triggers which caused this.",
    "title": "IndexedRaftLogEntry are kept in memory until committed, which can lead to OOM"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14055",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n```\r\nExpected to process record 'TypedRecordImpl{metadata=RecordMetadata{recordType=COMMAND, valueType=DEPLOYMENT, intent=CREATE}, value={...}' without errors, but exception occurred with message 'Expected to find executable process in persisted process with key '2251799813685388', but after transformation no such executable process could be found.'.\r\n\r\n\r\njava.util.NoSuchElementException: Expected to find executable process in persisted process with key '2251799813685388', but after transformation no such executable process could be found.\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.lambda$updateInMemoryState$2(DbProcessState.java:231) ~[zeebe-workflow-engine-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat java.util.Optional.orElseThrow(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.updateInMemoryState(DbProcessState.java:228) ~[zeebe-workflow-engine-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.lookupProcessByIdAndPersistedVersion(DbProcessState.java:375) ~[zeebe-workflow-engine-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.getLatestProcessVersionByProcessId(DbProcessState.java:271) ~[zeebe-workflow-engine-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.engine.processing.deployment.StartEventSubscriptionManager.isLatestProcess(StartEventSubscriptionManager.java:64) ~[zeebe-workflow-engine-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.engine.processing.deployment.StartEventSubscriptionManager.tryReOpenStartEventSubscription(StartEventSubscriptionManager.java:55) ~[zeebe-workflow-engine-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.engine.processing.deployment.DeploymentCreateProcessor.processDistributedCommand(DeploymentCreateProcessor.java:105) ~[zeebe-workflow-engine-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.engine.processing.streamprocessor.DistributedTypedRecordProcessor.processRecord(DistributedTypedRecordProcessor.java:31) ~[zeebe-workflow-engine-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.engine.Engine.process(Engine.java:126) ~[zeebe-workflow-engine-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.batchProcessing(ProcessingStateMachine.java:353) ~[zeebe-stream-platform-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$2(ProcessingStateMachine.java:269) ~[zeebe-stream-platform-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run(ZeebeTransaction.java:98) ~[zeebe-db-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand(ProcessingStateMachine.java:269) ~[zeebe-stream-platform-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord(ProcessingStateMachine.java:228) ~[zeebe-stream-platform-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.readNextRecord(ProcessingStateMachine.java:204) ~[zeebe-stream-platform-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) [zeebe-scheduler-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:109) [zeebe-scheduler-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:204) [zeebe-scheduler-8.3.0-alpha4.jar:8.3.0-alpha4]\r\n\r\n```\r\n\r\nThe error is repeatedly occurring in partitions 2 and 3. But didn't see it in Partition 1. The error is logged every time, partition 1 retries to distribute the deployment. It looks like the deployment was created successfully in partition 1, but it cannot distributed it to partitions 2 and 3. So the users would assume the deployment was successful, but they cannot create instances on other partitions.\r\n\r\n**To Reproduce**\r\n\r\nNot sure. Further data for analysis is [here](https://drive.google.com/drive/folders/1g6HlGdY3RwJO1yd1zl3Ld3T7r_CJUxto)\r\n\r\n**Expected behavior**\r\n\r\nA deployment successfully created in partition 1 should be accepted by other partitions.\r\n\r\n**Log/Stacktrace**\r\n\r\n[Link to logs](https://console.cloud.google.com/logs/query;cursorTimestamp=2023-08-29T12:56:18.601486031Z;endTime=2023-08-30T07:23:27.338991Z;pinnedLogId=2023-08-29T12:56:18.601486031Z%2Fenfqgfrxijsl5m8x;query=logName:%22stdout%22%0Aresource.type%3D%22k8s_container%22%0Aresource.labels.container_name%3D%22zeebe%22%0Aresource.labels.project_id%3D%22camunda-cloud-240911%22%0Aresource.labels.namespace_name%3D%2225697942-1bb8-47f8-9c2e-2d7e6b348a75-zeebe%22%0Aresource.labels.location%3D%22australia-southeast1%22%0Aresource.labels.cluster_name%3D%22prod-worker-4%22%0Atimestamp%3D%222023-08-29T12:56:18.601486031Z%22%0AinsertId%3D%22enfqgfrxijsl5m8x%22;startTime=2023-08-25T12:33:36.208Z;summaryFields=resource%252Flabels%252Fpod_name,jsonPayload%252Fcontext%252FpartitionId:false:32:beginning?project=camunda-cloud-240911)\r\n\r\n\r\n```\r\n\r\nDEBUG 2023-08-29T12:56:18.012401245Z [resource.labels.podName: zeebe-2] [jsonPayload.context.partitionId: 1] Ignoring unknown BPMN element 'http://camunda.org/schema/1.0/bpmn:inputOutput'\r\nDEBUG 2023-08-29T12:56:18.490833810Z [resource.labels.podName: zeebe-2] [jsonPayload.context.partitionId: 3] Ignoring unknown BPMN element 'http://camunda.org/schema/1.0/bpmn:inputOutput'\r\nDEBUG 2023-08-29T12:56:18.492924797Z [resource.labels.podName: zeebe-2] [jsonPayload.context.partitionId: 3] Ignoring unknown BPMN element 'http://camunda.org/schema/1.0/bpmn:inputOutput'\r\nDEBUG 2023-08-29T12:56:18.493531262Z [resource.labels.podName: zeebe-2] [jsonPayload.context.partitionId: 3] Ignoring unknown BPMN element 'http://camunda.org/schema/1.0/bpmn:inputOutput'\r\nDEBUG 2023-08-29T12:56:18.493878660Z [resource.labels.podName: zeebe-2] [jsonPayload.context.partitionId: 3] Ignoring unknown BPMN element 'http://camunda.org/schema/1.0/bpmn:inputOutput'\r\nERROR 2023-08-29T12:56:18.601486031Z [resource.labels.podName: zeebe-2] [jsonPayload.context.partitionId: 3] Expected to process record 'TypedRecordImpl{metadata=RecordMetadata{recordType=COMMAND, valueType=DEPLOYMENT, intent=CREATE}, value={\"resources\":...}' without errors, but exception occurred with message 'Expected to find executable process in persisted process with key '2251799813685388', but after transformation no such executable process could be found.'.\r\n\r\n\r\n```\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.3.0-alpha4\r\n\n\n korthout: ZPA Triage:\r\n- potential regression to 8.2 due to generalized distribution for deployments\r\n- possibly something with a race condition between different versions of a process\r\n- size medium, because unclear for now\r\n- priority `upcoming` because this could be quite severe\n deepthidevaki: There were two similar errors, for which I think the root cause might be the same even though it occurred when processing different records.\r\n\r\n\r\n[Logs](https://console.cloud.google.com/errors/detail/CL3C3rOOkMXdRw;service=zeebe;time=P7D?project=camunda-cloud-240911)\r\n```\r\nExpected to process record 'TypedRecordImpl{metadata=RecordMetadata{recordType=COMMAND, valueType=JOB, intent=COMPLETE}, value={\"deadline\":-1,\"worker\":\"\",\"retries\":-1,\"retryBackoff\":0,\"recurringTime\":-1,\"type\":\"\",\"customHeaders\":[packed value (length=1)],\"variables\":\"...\",\"errorMessage\":\"\",\"errorCode\":\"\",\"bpmnProcessId\":\"\",\"processDefinitionVersion\":-1,\"processDefinitionKey\":-1,\"processInstanceKey\":-1,\"elementId\":\"\",\"elementInstanceKey\":-1}}' without errors, but exception occurred with message 'No value present'.\r\n\r\njava.util.NoSuchElementException: No value present\r\n\tat java.util.Optional.orElseThrow(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.updateInMemoryState(DbProcessState.java:178) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.lookupProcessByIdAndPersistedVersion(DbProcessState.java:319) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.getLatestProcessVersionByProcessId(DbProcessState.java:215) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnStateBehavior.getLatestProcessVersion(BpmnStateBehavior.java:120) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.container.CallActivityProcessor.getProcessForProcessId(CallActivityProcessor.java:170) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.util.Either$Right.flatMap(Either.java:366) ~[zeebe-util-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.container.CallActivityProcessor.onActivate(CallActivityProcessor.java:66) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.container.CallActivityProcessor.onActivate(CallActivityProcessor.java:29) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.lambda$processEvent$2(BpmnStreamProcessor.java:144) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.util.Either$Right.ifRightOrLeft(Either.java:381) ~[zeebe-util-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.processEvent(BpmnStreamProcessor.java:143) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.lambda$processRecord$0(BpmnStreamProcessor.java:92) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.util.Either$Right.ifRightOrLeft(Either.java:381) ~[zeebe-util-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.processRecord(BpmnStreamProcessor.java:89) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.engine.Engine.process(Engine.java:140) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.batchProcessing(ProcessingStateMachine.java:352) ~[zeebe-stream-platform-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$2(ProcessingStateMachine.java:269) ~[zeebe-stream-platform-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run(ZeebeTransaction.java:84) ~[zeebe-db-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand(ProcessingStateMachine.java:269) ~[zeebe-stream-platform-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord(ProcessingStateMachine.java:228) ~[zeebe-stream-platform-8.2.12.jar:8.2.12]\r\n```\r\n\r\n[Logs](https://console.cloud.google.com/errors/detail/CLe7o6_LmPTh4AE;service=zeebe;time=P7D?project=camunda-cloud-240911)\r\n\r\n```\r\nExpected to process record 'TypedRecordImpl{metadata=RecordMetadata{recordType=COMMAND, valueType=DEPLOYMENT, intent=DISTRIBUTE}, value={\"resources\":[{\"resourceName\":\"xxx.bpmn\",\"resource\":\"...}' without errors, but exception occurred with message 'No value present'.\r\n\r\njava.util.NoSuchElementException: No value present\r\n\tat java.util.Optional.orElseThrow(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.updateInMemoryState(DbProcessState.java:178) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.lookupProcessByIdAndPersistedVersion(DbProcessState.java:319) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.getLatestProcessVersionByProcessId(DbProcessState.java:215) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.engine.processing.deployment.StartEventSubscriptionManager.isLatestProcess(StartEventSubscriptionManager.java:64) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.engine.processing.deployment.StartEventSubscriptionManager.tryReOpenStartEventSubscription(StartEventSubscriptionManager.java:55) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.engine.processing.deployment.distribute.DeploymentDistributeProcessor.processRecord(DeploymentDistributeProcessor.java:46) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.engine.Engine.process(Engine.java:140) ~[zeebe-workflow-engine-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.batchProcessing(ProcessingStateMachine.java:352) ~[zeebe-stream-platform-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$2(ProcessingStateMachine.java:269) ~[zeebe-stream-platform-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run(ZeebeTransaction.java:84) ~[zeebe-db-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand(ProcessingStateMachine.java:269) ~[zeebe-stream-platform-8.2.12.jar:8.2.12]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord(ProcessingStateMachine.java:228) ~[zeebe-stream-platform-8.2.12.jar:8.2.12]\r\n\r\n```\r\n\n korthout: Thanks @deepthidevaki That's useful input!\n nicpuppa: Started to look at it last week 👀 Didn't figured out yet how to reproduce.\r\n\r\n~~Could be related to this [PR](https://github.com/camunda/zeebe/pull/13505), but idk~~",
    "title": "No executable process found while processing Deployment:Create command"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14044",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nObserved 100% backpressure on one partition. On further investigation, we found that there was back-to-back role transition.\r\n\r\nThe node was leader for partition 2. It transitioned to follower. The transition was cancelled in between because it became leader again.\r\n\r\nBecause of [this fix](https://github.com/camunda/zeebe/pull/13541), command api is not notified that it became follower. So when it transitioned to leader again, it reuse the limiter from previous leader role because it was not removed. https://github.com/camunda/zeebe/blob/615c751216c3fdc99493792d8f1c19644633d275/broker/src/main/java/io/camunda/zeebe/broker/transport/backpressure/PartitionAwareRequestLimiter.java#L147\r\n\r\nThe partition started processing when a new leader was elected after a few hours.\r\n\r\n**To Reproduce**\r\n\r\nNot easy to reproduce. Leader -> Follower -> Leader transition should happen where the follower transition is cancelled.\r\n\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.2.12\r\n\n",
    "title": "Backpressure queue is not reset when back-to-back role transitions"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13936",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nIt seems that an Inclusive Gateway with a single outgoing Sequence Flow ignores the Condition\r\n\r\n<img width=\"974\" alt=\"Screenshot 2023-08-17 at 09 52 07\" src=\"https://github.com/camunda/zeebe/assets/3511026/57eb2d2d-8646-4947-b7c6-3aa05472521a\">\r\n\r\nThe BPMN spec is quite clear about the expected behavior:\r\n\r\n> A default path can optionally be identified, to be taken in the event that none of the conditional `Expressions` evaluate to `true`. If a default path is not specified and the **Process** is executed such that none of the conditional `Expressions` evaluates to `true`, a runtime exception occurs. - [10.6.3 Inclusive Gateway](https://www.omg.org/spec/BPMN/2.0.2/PDF#10.6.3%20Inclusive%20Gateway)\r\n\r\nFor C8, such a runtime exception should be represented by [an incident](https://docs.camunda.io/docs/next/components/concepts/incidents/) at the inclusive gateway.\r\n\r\nOriginally reported on the forums: https://forum.camunda.io/t/strange-behavior-with-only-one-sequence-flow-with-condition/46938\r\n\r\n**To Reproduce**\r\n\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\n- deploy a process with an inclusive gateway that has one outgoing sequence flow with the condition `= false`\r\n- create an instance of it\r\n- notice that the sequence flow is taken and that no incident is raised\r\n\r\n**Expected behavior**\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\nAn incident is raised at the inclusive gateway\n\n korthout: Mid severity, because there is a workaround: use two outgoing sequence flows - set the second one to `= false` and let it flow to a none end event\n lzgabel: 👋 Hi @korthout. Please assign this task to me. I'll take a look.",
    "title": "Condition ignored on Inclusive Gateway with singular outgoing Sequence Flow"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13233",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "We've introduced a regression on deploying large payloads on multi-partition clusters with\r\n- #11661 \r\n\r\nBefore that pull request, a deployed resource was written in two follow-up events:\r\n- `Deployment:CREATED`\r\n- `Process:CREATED` (or `DecisionRequirements:CREATED` depending on resource)\r\n\r\nThe specialized `DeploymentDistribution` would take the resource from the `Deployment:CREATED` to distribute it to the other partitions.\r\n\r\nWith #11456, a new event `CommandDistribution:STARTED` was introduced to store the command for distribution. For deployments, this contains the entire Deployment incl. the resource. This event is only appended on multi-partition clusters. But, when it is appended, it further reduces the maximum payload size because the follow-up events reach the `MAX_BATCH_SIZE` restriction with a lower payload size.\r\n\r\n> **Note** The `MAX_BATCH_SIZE` is a limitation that originated from the `MAX_MESSAGE_SIZE` configuration setting but nowadays is only defined by the `LogStreamBuilder`'s `MAX_FRAGMENT_SIZE`: 4MB.\r\n\r\nThis regression lowers the maximum payload size of deployments from ~2MB down to ~1.4MB.\r\n\r\nThe regression does not exist on single partition clusters as these do not require the distribution of the deployment.\r\n\r\n---\r\n\r\nSuggested solution:\r\n- don't write the resource in the `Deployment:CREATED` event, only in the `Process:CREATED` and the `DecisionRequirements:CREATED` events (estimate: x-small)\r\n\r\nOf course, we'd need to inform users that we're no longer writing the resource for that event in the Update Guide. As this would be breaking user space. The documentation should clarify that the resource is available in the `Process:CREATED` and the `DecisionRequirements:CREATED` instead.\n\n korthout: There are several solution ideas:\n- https://github.com/camunda/zeebe/issues/11513\n  - Does not seem trivial to implement\n  - Mostly a ZDP effort\n- don't write the resource in the `Deployment:CREATED` event, only in the `Process:CREATED` and the `DecisionRequirements:CREATED` events (estimate: x-small)\n  - would require alignment with Operate and Optimize to make sure they consume the resource from `Process:CREATED` \n korthout: @sdorokhova Can you verify for me that Operate consumes the `Process:CREATED` event to get the BPMN model and not the `Deployment:CREATED` event? Likewise, does it consume the `DecisionRequirements:CREATED` event for DMN models?\r\n\r\n@RomanJRW Can you verify the same as above, but for Optimize?\n sdorokhova: Hi @korthout ,\r\nwe read processes from `process` index and decision requirements from `decision-requirements` index. You can check [here](https://github.com/camunda/operate/blob/168d22f5352c66bfd56c47678e92655a82414194/common/src/main/java/io/camunda/operate/zeebe/ZeebeESConstants.java) all the indices we're reading from. Is this what you were asking about? \n korthout: @sdorokhova Perfect! That's exactly what I mean. I should've mentioned that it concerns the ES indices `zeebe-record-process` and `zeebe-record-decision-requirements`. \r\n\r\nI was hoping you weren't reading the BPMN/DMN resources from the `zeebe-record-deployment` index.\n megglos: @korthout do I understand this correctly that this regression is only present in 8.3? And the changes that happened in 8.2 like https://github.com/camunda/zeebe/issues/11660 have not caused a regression?\n korthout: @megglos Correct, the regression only exists on `main` and if left unfixed will only be present in the upcoming 8.3. The reason is that deployment distribution has been switched over only for 8.3(-alpha*), not for 8.2.\n RomanJRW: Hey @korthout - apologies for slow response, I have had FTO. I can confirm the same for Optimize, that we read from `zeebe-record-process` and not the deployment index\n korthout: Now that both Operate and Optimize have confirmed that they don't access the resource from the `zeebe-record-deployment` index, I think it's fine to take this solution:\r\n- don't write the resource in the `Deployment:CREATED` event, only in the `Process:CREATED` and the `DecisionRequirements:CREATED` events (estimate: x-small)\r\n\r\nOf course, we'd need to inform users that we're no longer writing the resource for that event in the Update Guide. As this would be breaking user space. The documentation should clarify that the resource is available in the `Process:CREATED` and the `DecisionRequirements:CREATED` instead.",
    "title": "Regression in deploying large payloads"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13093",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nRunning `RandomizedRaftTest.consistencyTestWithSnapshot` on 8.0.17 leads to the following exception:\r\n```\r\njava.lang.IllegalStateException: Expected to delete index after 404, but it is lower than the commit index 405. Deleting committed entries can lead to inconsistencies and is prohibited. at io.atomix.raft.storage.log.RaftLog.deleteAfter(RaftLog.java:186) at io.atomix.raft.roles.PassiveRole.replaceExistingEntry(PassiveRole.java:623) at io.atomix.raft.roles.PassiveRole.tryToAppend(PassiveRole.java:562) at io.atomix.raft.roles.PassiveRole.appendEntries(PassiveRole.java:514) at io.atomix.raft.roles.PassiveRole.handleAppend(PassiveRole.java:370) at io.atomix.raft.roles.ActiveRole.onAppend(ActiveRole.java:50) at io.atomix.raft.roles.FollowerRole.onAppend(FollowerRole.java:187) at io.atomix.raft.impl.RaftContext.lambda$registerHandlers$13(RaftContext.java:263) at \r\n...\r\n```\r\n\r\nSee [Test results for unit tests(1).zip](https://github.com/camunda/zeebe/files/11734753/Test.results.for.unit.tests.1.zip) from test run https://github.com/camunda/zeebe/actions/runs/5255172343/jobs/9494733250\r\n\r\n**To Reproduce**\r\n| jqwick | explanation |\r\n|--------|--------|\r\n| tries = 10                    | # of calls to property |\r\n| checks = 10                   | # of not rejected calls |\r\n| generation = RANDOMIZED       | parameters are randomly generated |\r\n| after-failure = PREVIOUS_SEED | use the previous seed |\r\n| when-fixed-seed = ALLOW       | fixing the random seed is allowed |\r\n| edge-cases#mode = NONE        | edge cases are not explicitly generated |\r\n| seed = 2349382887260487435    | random seed to reproduce generated values |\r\n\r\n**Expected behavior**\r\n\r\nTest does not fail.\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.lang.IllegalStateException: Expected to delete index after 404, but it is lower than the commit index 405. Deleting committed entries can lead to inconsistencies and is prohibited. at io.atomix.raft.storage.log.RaftLog.deleteAfter(RaftLog.java:186) at io.atomix.raft.roles.PassiveRole.replaceExistingEntry(PassiveRole.java:623) at io.atomix.raft.roles.PassiveRole.tryToAppend(PassiveRole.java:562) at io.atomix.raft.roles.PassiveRole.appendEntries(PassiveRole.java:514) at io.atomix.raft.roles.PassiveRole.handleAppend(PassiveRole.java:370) at io.atomix.raft.roles.ActiveRole.onAppend(ActiveRole.java:50) at io.atomix.raft.roles.FollowerRole.onAppend(FollowerRole.java:187) at io.atomix.raft.impl.RaftContext.lambda$registerHandlers$13(RaftContext.java:263) at io.atomix.raft.impl.RaftContext.lambda$runOnContext$20(RaftContext.java:274) at io.atomix.raft.DeterministicSingleThreadContext$WrappedRunnable.run(DeterministicSingleThreadContext.java:129) at org.jmock.lib.concurrent.DeterministicScheduler$CallableRunnableAdapter.call(DeterministicScheduler.java:176) at org.jmock.lib.concurrent.DeterministicScheduler$ScheduledTask.run(DeterministicScheduler.java:251) at org.jmock.lib.concurrent.DeterministicScheduler.runNextPendingCommand(DeterministicScheduler.java:66) at io.atomix.raft.ControllableRaftContexts.runNextTask(ControllableRaftContexts.java:246) at io.atomix.raft.RaftOperation.lambda$getDefaultRaftOperations$0(RaftOperation.java:62) at io.atomix.raft.RaftOperation.run(RaftOperation.java:42) at io.atomix.raft.RandomizedRaftTest.consistencyTest(RandomizedRaftTest.java:123) at io.atomix.raft.RandomizedRaftTest.consistencyTestWithSnapshot(RandomizedRaftTest.java:88) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:568) at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725) at org.junit.platform.commons.support.ReflectionSupport.invokeMethod(ReflectionSupport.java:198) at net.jqwik.engine.execution.CheckedPropertyFactory.lambda$createRawFunction$1(CheckedPropertyFactory.java:84) at net.jqwik.engine.execution.CheckedPropertyFactory.lambda$createRawFunction$2(CheckedPropertyFactory.java:91) at net.jqwik.engine.properties.CheckedFunction.execute(CheckedFunction.java:17) at net.jqwik.api.lifecycle.AroundTryHook.lambda$static$0(AroundTryHook.java:57) at net.jqwik.engine.execution.lifecycle.HookSupport.lambda$wrap$2(HookSupport.java:48) at net.jqwik.engine.hooks.lifecycle.TryLifecycleMethodsHook.aroundTry(TryLifecycleMethodsHook.java:57) at net.jqwik.engine.execution.lifecycle.HookSupport.lambda$wrap$3(HookSupport.java:53) at net.jqwik.engine.execution.CheckedPropertyFactory.lambda$createTryExecutor$0(CheckedPropertyFactory.java:60) at net.jqwik.engine.execution.lifecycle.AroundTryLifecycle.execute(AroundTryLifecycle.java:23) at net.jqwik.engine.properties.GenericProperty.testPredicate(GenericProperty.java:166) at net.jqwik.engine.properties.GenericProperty.check(GenericProperty.java:68) at net.jqwik.engine.execution.CheckedProperty.check(CheckedProperty.java:67) at net.jqwik.engine.execution.PropertyMethodExecutor.executeProperty(PropertyMethodExecutor.java:90) at net.jqwik.engine.execution.PropertyMethodExecutor.executeMethod(PropertyMethodExecutor.java:69) at net.jqwik.engine.execution.PropertyMethodExecutor.lambda$execute$0(PropertyMethodExecutor.java:49) at net.jqwik.api.lifecycle.AroundPropertyHook.lambda$static$0(AroundPropertyHook.java:46) at net.jqwik.engine.execution.lifecycle.HookSupport.lambda$wrap$0(HookSupport.java:26) at net.jqwik.engine.hooks.lifecycle.PropertyLifecycleMethodsHook.aroundProperty(PropertyLifecycleMethodsHook.java:57) at net.jqwik.engine.execution.lifecycle.HookSupport.lambda$wrap$1(HookSupport.java:31) at net.jqwik.engine.execution.lifecycle.HookSupport.lambda$wrap$0(HookSupport.java:26) at net.jqwik.engine.hooks.statistics.StatisticsHook.aroundProperty(StatisticsHook.java:37) at net.jqwik.engine.execution.lifecycle.HookSupport.lambda$wrap$1(HookSupport.java:31) at net.jqwik.engine.execution.lifecycle.HookSupport.lambda$wrap$0(HookSupport.java:26) at net.jqwik.engine.hooks.lifecycle.AutoCloseableHook.aroundProperty(AutoCloseableHook.java:13) at net.jqwik.engine.execution.lifecycle.HookSupport.lambda$wrap$1(HookSupport.java:31) at net.jqwik.engine.execution.PropertyMethodExecutor.execute(PropertyMethodExecutor.java:47) at net.jqwik.engine.execution.PropertyTaskCreator.executeTestMethod(PropertyTaskCreator.java:166) at net.jqwik.engine.execution.PropertyTaskCreator.lambda$createTask$1(PropertyTaskCreator.java:51) at net.jqwik.engine.execution.lifecycle.CurrentDomainContext.runWithContext(CurrentDomainContext.java:28) at net.jqwik.engine.execution.PropertyTaskCreator.lambda$createTask$2(PropertyTaskCreator.java:50) at net.jqwik.engine.execution.pipeline.ExecutionTask$1.lambda$execute$0(ExecutionTask.java:31) at net.jqwik.engine.execution.lifecycle.CurrentTestDescriptor.runWithDescriptor(CurrentTestDescriptor.java:17) at net.jqwik.engine.execution.pipeline.ExecutionTask$1.execute(ExecutionTask.java:31) at net.jqwik.engine.execution.pipeline.ExecutionPipeline.runToTermination(ExecutionPipeline.java:82) at net.jqwik.engine.execution.JqwikExecutor.execute(JqwikExecutor.java:46) at net.jqwik.engine.JqwikTestEngine.executeTests(JqwikTestEngine.java:70) at net.jqwik.engine.JqwikTestEngine.execute(JqwikTestEngine.java:53) at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107) at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88) at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54) at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67) at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52) at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114) at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86) at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86) at org.apache.maven.surefire.junitplatform.LazyLauncher.execute(LazyLauncher.java:55) at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:234) at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133) at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:228) at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:175) at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:131) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:456) at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:169) at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:595) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:581) \r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.0.17\r\n\r\n\r\n\n\n megglos: ZDP-Triage:\n- unexpected breaking behavior\n- needs to be investigated\n megglos: ZDP-Planning:\n- this indicates a high severity bug and we need to investigate this asap\n deepthidevaki: @npepinpe You might not be able to reproduce this easily. When I was investigating sometime ago, I was wondering why it is not reproducible with the seed, and it looks like https://github.com/camunda/zeebe/blob/bcfe82dcb3be17b6f4a1ee732d40574d374cc1d8/atomix/cluster/src/main/java/io/atomix/raft/cluster/impl/RaftMemberContext.java#L137 this is the reason. I didn't debug further, but relying on System time will be non-deterministic. So most likely this is the reason. This doesn't help in your investigation, but just a hint in case you failed to reproduce it  :smile: \n npepinpe: We have `System.currentTimeMillis()` in other parts of our system as well :sweat: For example, with heartbeat related code, member context, quorum timeout calculation, etc.\r\n\r\nI guess the next step would be to use some clock for full reproducibility :+1: \n npepinpe: Observations:\r\n\r\n1. Expected to delete all entries after 404, but the commit index is 405.\r\n1. We tried to delete after 404 because the replicated entry did not have the same term as the existing entry\r\n1. The mismatched entries index was 405, hence why we want to delete it and anything after (so we say after 404, exclusive)\r\n1. The existing entry was committed\r\n\r\nSo the issue is either:\r\n\r\n1. The entry should not have been committed\r\n1. The terms should not have been mismatched\n npepinpe: A little above, we see the following line:\r\n\r\n```\r\n12:08:19.021 [] WARN  io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - AppendRequest{term=23, leader=1, prevLogIndex=404, prevLogTerm=19, entries=145, commitIndex=839} to 0 failed: java.util.concurrent.TimeoutException\r\n```\r\n\r\nHad to scroll way up higher to find when the commit index was not 405 on any of these nodes. So 405 was appended on 2 as:\r\n\r\n```\r\n12:08:18.230 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=405, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@5d2e7a6a}\r\n```\r\n\r\nIts term was 19.\r\n\r\nIt was appended on 0 as:\r\n\r\n```\r\n12:08:18.229 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=405, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@2dc97eb8}\r\n```\r\n\r\nAlso term 19. Alright. Let's check then when we try to re-append it, what the term was. Cool, we don't have that in the logs :)\r\n\r\nThis is the request we get which triggers the error:\r\n\r\n```\r\n12:08:19.023 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received AppendRequest{term=23, leader=1, prevLogIndex=404, prevLogTerm=19, entries=145, commitIndex=839}\r\n```\r\n\r\nSo the prevLogTerm seems correct, but unfortunately we don't log the mismatched entries, so I can't check it.\r\n\r\nBy the time the error occurs, the term is 23.\r\n\r\nOne weird thing, it seems the leader sends this request to 0:\r\n\r\n```\r\n12:08:19.021 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Sending AppendRequest{term=23, leader=1, prevLogIndex=873, prevLogTerm=23, entries=0, commitIndex=839} to 0\r\n```\r\n\r\nThere is no log for when the response is received from 0, no time out or anything; and then it suddenly tries to send starting at 405 :thinking: Unclear to me what caused it to do that. Possibly it's backed up and has many messages buffered?\r\n\r\nCorrect. Way above, we see:\r\n\r\n```\r\n12:08:19.021 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Received AppendResponse{status=OK, term=23, succeeded=false, lastLogIndex=578, lastSnapshotIndex=404} from 0\r\n12:08:19.021 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Reset next index for RaftMemberContext{member=0, term=23, configIndex=0, snapshotIndex=404, nextSnapshotIndex=0, nextSnapshotChunk=null, matchIndex=0, heartbeatTime=1686658099013, appending=0, appendSucceeded=false, appendTime=1686658099018, configuring=false, installing=false, failures=0} to 579\r\n12:08:19.021 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Sending AppendRequest{term=23, leader=1, prevLogIndex=404, prevLogTerm=19, entries=145, commitIndex=839} to 0\r\n```\r\n\r\nBut if the last log index on 0 is 578, why aren't we sending starting at 579?? Why are we sending starting at 405?\r\n\r\nAh, then we get \r\n\r\n```\r\n12:08:19.021 [] WARN  io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - AppendRequest{term=23, leader=1, prevLogIndex=404, prevLogTerm=19, entries=145, commitIndex=839} to 0 failed: java.util.concurrent.TimeoutException\r\n```\r\n\r\nFollowed quickly by:\r\n\r\n```\r\n12:08:19.021 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Sending AppendRequest{term=23, leader=1, prevLogIndex=873, prevLogTerm=23, entries=0, commitIndex=839} to 0\r\n```\r\n\r\nUnclear why a timeout would lead to the prevLogIndex to be reset...\r\n\r\nAnyway, that second one is never really processed by 0.\n npepinpe: If we keep going further back in time, we see the last append that was processed by 0 was:\r\n\r\n```\r\n12:08:19.018 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Sending AppendRequest{term=23, leader=1, prevLogIndex=860, prevLogTerm=23, entries=0, commitIndex=405} to 0\r\n12:08:19.020 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received AppendRequest{term=23, leader=1, prevLogIndex=860, prevLogTerm=23, entries=0, commitIndex=405}\r\n12:08:19.020 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Rejected AppendRequest{term=23, leader=1, prevLogIndex=860, prevLogTerm=23, entries=0, commitIndex=405}: Previous index (860) is greater than the local log's last index (578)\r\n12:08:19.021 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending AppendResponse{status=OK, term=23, succeeded=false, lastLogIndex=578, lastSnapshotIndex=404}\r\n```\r\n\r\nThis correlates with the reset we saw on 1 earlier, when it reset to 579 (presumably - apparently not!).\r\n\r\nIn fact, we see many of these. Due to time outs, 1 keeps trying to send the same append request, and 0 keeps rejecting it because the previous index is greater than the local log's last index. So let's try to find the last successful append (579).\r\n\r\n```\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received AppendRequest{term=19, leader=1, prevLogIndex=560, prevLogTerm=19, entries=18, commitIndex=405}\r\n12:08:18.284 [] INFO  io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Found leader 1\r\n12:08:18.284 [] TRACE io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Set leader 1\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=561, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@7a75a78e}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=562, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@5058a8e2}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=563, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@1187855a}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=564, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@1f7952dd}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=565, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@4867cf6b}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=566, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@5d6d613d}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=567, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@706d929f}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=568, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@2557d9f5}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=569, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@533d56a3}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=570, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@656fb170}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=571, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@790a2e28}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=572, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@6209d88d}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=573, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@5b381221}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=574, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@43c4dc07}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=575, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@6b17d139}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=576, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@47f6a166}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=577, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@5c441aa3}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=578, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@3985d1c8}\r\n12:08:18.284 [] TRACE io.atomix.raft.storage.system.MetaStore - Store last flushed index 578\r\n12:08:18.284 [] TRACE io.atomix.raft.storage.system.MetaStore - Skip storing same last flushed index 578\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending AppendResponse{status=OK, term=19, succeeded=true, lastLogIndex=578, lastSnapshotIndex=404}\r\n12:08:18.284 [] TRACE io.atomix.raft.impl.RandomizedElectionTimer - Election timeout scheduled for PT3.733S\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received AppendRequest{term=19, leader=1, prevLogIndex=578, prevLogTerm=19, entries=0, commitIndex=405}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending AppendResponse{status=OK, term=19, succeeded=true, lastLogIndex=578, lastSnapshotIndex=404}\r\n```\r\n\r\nSo we can see all the other entries were also during term 19. So it looks like after term 19, 0 never appended anything.\r\n\r\nThen we see 0 soon after trying to get elected for term 20:\r\n\r\n```\r\n12:08:18.285 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 0\r\n12:08:18.285 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 0\r\n12:08:18.285 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - No heartbeat from 1 since 1ms\r\n12:08:18.285 [] TRACE io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Set leader null\r\n12:08:18.285 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending poll requests to all active members: [DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.453Z}, DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.453Z}]\r\n12:08:18.285 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.453Z} for next term 20\r\n12:08:18.285 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.453Z} for next term 20\r\n12:08:18.285 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick 50ms on 0\r\n12:08:18.285 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 0\r\n12:08:18.285 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Failed to poll a majority of the cluster in PT2.5S\r\n```\r\n\r\nThis fails. So let's try and find all the successful elections between 19 and 23.\r\n\r\nThere seems to be no successful election for term 20:\r\n\r\n```\r\n12:08:18.289 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Rejected AppendRequest{term=19, leader=1, prevLogIndex=578, prevLogTerm=19, entries=0, commitIndex=405}: request term is less than the current term (20)\r\n12:08:18.289 [] TRACE io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Sending AppendResponse{status=OK, term=20, succeeded=false, lastLogIndex=578, lastSnapshotIndex=404}\r\n12:08:18.290 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 0\r\n12:08:18.290 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - No heartbeat from a known leader since 6ms\r\n```\r\n\r\nBy the way, during this time, 1 still thinks it's the leader and tries to send append requests.\r\n\r\n```\r\n12:08:18.289 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Received AppendRequest{term=19, leader=1, prevLogIndex=559, prevLogTerm=19, entries=0, commitIndex=405}\r\n12:08:18.289 [] INFO  io.atomix.raft.impl.RaftContext - RaftServer{2-partition-1} - Found leader 1\r\n12:08:18.289 [] TRACE io.atomix.raft.impl.RaftContext - RaftServer{2-partition-1} - Set leader 1\r\n12:08:18.289 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Sending AppendResponse{status=OK, term=19, succeeded=true, lastLogIndex=559, lastSnapshotIndex=404}\r\n12:08:18.289 [] TRACE io.atomix.raft.impl.RandomizedElectionTimer - Election timeout scheduled for PT2.743S\r\n```\r\n\r\nThen later on:\r\n\r\n```\r\n12:08:18.290 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received PollRequest{term=19, candidate=2, lastLogIndex=559, lastLogTerm=19}\r\n12:08:18.290 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Rejected PollRequest{term=19, candidate=2, lastLogIndex=559, lastLogTerm=19}: candidate's term is less than the current term\r\n12:08:18.290 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending PollResponse{status=OK, term=20, accepted=false}\r\n12:08:18.290 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 1\r\n12:08:18.290 [] WARN  io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - AppendRequest{term=19, leader=1, prevLogIndex=559, prevLogTerm=19, entries=0, commitIndex=405} to 2 failed: java.util.concurrent.TimeoutException\r\n12:08:18.290 [] WARN  io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - AppendRequest{term=19, leader=1, prevLogIndex=578, prevLogTerm=19, entries=0, commitIndex=405} to 0 failed: java.util.concurrent.TimeoutException\r\n12:08:18.290 [] TRACE io.atomix.raft.roles.LeaderRole - RaftServer{1-partition-1}{role=LEADER} - Appended IndexedRaftLogEntryImpl{index=613, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=PersistedJournalRecord[metadata=RecordMetadata[checksum=3177859851, length=77], record=RecordData[index=613, asqn=0, data=UnsafeBuffer{addressOffset=140583371101078, capacity=49, byteArray=null, byteBuffer=java.nio.DirectByteBuffer[pos=9159 lim=10240 cap=10240]}]]}\r\n12:08:18.290 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Sending AppendRequest{term=19, leader=1, prevLogIndex=559, prevLogTerm=19, entries=0, commitIndex=405} to 2\r\n12:08:18.290 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Sending AppendRequest{term=19, leader=1, prevLogIndex=578, prevLogTerm=19, entries=0, commitIndex=405} to 0\r\n12:08:18.290 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 0\r\n12:08:18.290 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - No heartbeat from a known leader since 6ms\r\n12:08:18.290 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending poll requests to all active members: [DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.453Z}, DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.453Z}]\r\n12:08:18.290 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.453Z} for next term 21\r\n12:08:18.290 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.453Z} for next term 21\r\n```\r\n\r\nThen we get this weird one about term 1??\r\n\r\n```\r\n12:08:18.290 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received AppendRequest{term=19, leader=1, prevLogIndex=578, prevLogTerm=19, entries=0, commitIndex=405}\r\n12:08:18.290 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Rejected AppendRequest{term=19, leader=1, prevLogIndex=578, prevLogTerm=19, entries=0, commitIndex=405}: request term is less than the current term (20)\r\n12:08:18.290 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending AppendResponse{status=OK, term=20, succeeded=false, lastLogIndex=578, lastSnapshotIndex=404}\r\n12:08:18.290 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Expected heartbeat from null in term 20, but received one from 1 in term 1, ignoring it\r\n```\r\n\r\nThen rapidly 0 will keep doing elections, bumping the term from 20 to 21, then 22:\r\n\r\n```\r\n12:08:18.291 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Received PollRequest{term=20, candidate=0, lastLogIndex=578, lastLogTerm=19}\r\n12:08:18.291 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Accepted PollRequest{term=20, candidate=0, lastLogIndex=578, lastLogTerm=19}: candidate's log is up-to-date\r\n12:08:18.291 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Sending PollResponse{status=OK, term=20, accepted=true}\r\n12:08:18.291 [] INFO  io.atomix.raft.RandomizedRaftTest - Receive next message on 2\r\n[...]\r\n12:08:18.292 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received accepted poll from DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.453Z}\r\n12:08:18.292 [] INFO  io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Transitioning to CANDIDATE\r\n12:08:18.292 [] INFO  io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Starting election\r\n12:08:18.292 [] TRACE io.atomix.raft.storage.system.MetaStore - Store term 21\r\n12:08:18.292 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote null\r\n12:08:18.292 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Set term 21\r\n12:08:18.292 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote 0\r\n12:08:18.292 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Voted for 0\r\n12:08:18.292 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Requesting votes for term 21\r\n12:08:18.292 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Requesting vote from DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.453Z} for term 21\r\n12:08:18.292 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Requesting vote from DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.453Z} for term 21\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 1\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Run next task on 2\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Run next task on 2\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Run next task on 0\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Run next task on 2\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick heartbeatTimeout on 2\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 0\r\n12:08:18.292 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Election timed out. Restarting election.\r\n12:08:18.292 [] TRACE io.atomix.raft.storage.system.MetaStore - Store term 22\r\n12:08:18.292 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote null\r\n12:08:18.292 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Set term 22\r\n12:08:18.292 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote 0\r\n12:08:18.292 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Voted for 0\r\n12:08:18.292 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Requesting votes for term 22\r\n12:08:18.292 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Requesting vote from DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.453Z} for term 22\r\n12:08:18.292 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Requesting vote from DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.453Z} for term 22\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Run next task on 2\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Append on leader on 0\r\n12:08:18.292 [] INFO  TEST - Appending on leader 1\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Append on leader on 0\r\n12:08:18.292 [] INFO  TEST - Appending on leader 1\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Take snapshot on 2\r\n12:08:18.292 [] INFO  TEST - Snapshot taken at index 404. Current commit index is 405\r\n12:08:18.292 [] DEBUG io.camunda.zeebe.journal.file.SegmentsManager - No segments can be deleted with index < 404 (first log index: 313)\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Receive next message on 1\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Run next task on 0\r\n12:08:18.292 [] WARN  io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - null\r\n```\r\n\r\nThen we get this cool warning:\r\n\r\n```\r\n12:08:18.292 [] WARN  io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - null\r\n```\r\n\r\n:smile:\r\n\r\n2 will vote for 0 for term 22:\r\n\r\n```\r\n12:08:18.292 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Received VoteRequest{term=22, candidate=0, lastLogIndex=578, lastLogTerm=19}\r\n12:08:18.292 [] TRACE io.atomix.raft.storage.system.MetaStore - Store term 22\r\n12:08:18.292 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote null\r\n12:08:18.292 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{2-partition-1} - Set term 22\r\n12:08:18.292 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Accepted VoteRequest{term=22, candidate=0, lastLogIndex=578, lastLogTerm=19}: candidate's log is up-to-date\r\n12:08:18.292 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote 0\r\n12:08:18.292 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{2-partition-1} - Voted for 0\r\n12:08:18.292 [] TRACE io.atomix.raft.impl.RandomizedElectionTimer - Election timeout scheduled for PT3.103S\r\n12:08:18.292 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Sending VoteResponse{status=OK, term=22, voted=true}\r\n```\r\n\r\nStill, no election succeeded yet. Then 2 bumps term to 23:\r\n\r\n```\r\n12:08:18.293 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 2\r\n12:08:18.293 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - No heartbeat from a known leader since 1ms\r\n12:08:18.293 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Sending poll requests to all active members: [DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.455Z}, DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.455Z}]\r\n12:08:18.293 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.455Z} for next term 23\r\n12:08:18.293 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.455Z} for next term 23\r\n```\r\n\r\nThen after all this time, 1 processes the message from 0 about term increasing to 21 (yes, 1 was still on term 19 trying to append stuff to the followers):\r\n\r\n```\r\n12:08:18.293 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 1\r\n12:08:18.293 [] TRACE io.atomix.raft.storage.system.MetaStore - Store term 21\r\n12:08:18.293 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote null\r\n12:08:18.293 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{1-partition-1} - Set term 21\r\n12:08:18.293 [] INFO  io.atomix.raft.roles.LeaderRole - RaftServer{1-partition-1}{role=LEADER} - Received greater term from 0\r\n12:08:18.293 [] INFO  io.atomix.raft.impl.RaftContext - RaftServer{1-partition-1} - Transitioning to FOLLOWER\r\n12:08:18.296 [] TRACE io.atomix.raft.roles.LeaderRole - RaftServer{1-partition-1}{role=LEADER} - Cancelling append timer\r\n12:08:18.296 [] TRACE io.atomix.raft.impl.RandomizedElectionTimer - Election timeout scheduled for PT4.57S\r\n12:08:18.296 [] TRACE io.atomix.raft.roles.LeaderRole - RaftServer{1-partition-1}{role=LEADER} - Received VoteRequest{term=21, candidate=0, lastLogIndex=578, lastLogTerm=19}\r\n12:08:18.296 [] DEBUG io.atomix.raft.roles.LeaderRole - RaftServer{1-partition-1}{role=LEADER} - Rejected VoteRequest{term=21, candidate=0, lastLogIndex=578, lastLogTerm=19}: candidate's last log entry (578) is at a lower index than the local log (621)\r\n12:08:18.296 [] TRACE io.atomix.raft.roles.LeaderRole - RaftServer{1-partition-1}{role=LEADER} - Sending VoteResponse{status=OK, term=21, voted=false}\r\n[...]\r\n12:08:18.297 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Second round of election timed out. Transitioning to follower.\r\n12:08:18.297 [] INFO  io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Transitioning to FOLLOWER\r\n12:08:18.297 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Cancelling election\r\n12:08:18.297 [] TRACE io.atomix.raft.impl.RandomizedElectionTimer - Election timeout scheduled for PT3.95S\r\n12:08:18.297 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 1\r\n12:08:18.297 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - No heartbeat from a known leader since 4ms\r\n12:08:18.297 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Sending poll requests to all active members: [DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.454Z}, DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.454Z}]\r\n12:08:18.297 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.454Z} for next term 22\r\n12:08:18.297 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.454Z} for next term 22\r\n[...]\r\n12:08:18.298 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Failed to poll a majority of the cluster in PT2.5S\r\n12:08:18.298 [] TRACE io.atomix.raft.impl.RandomizedElectionTimer - Election timeout scheduled for PT3.769S\r\n12:08:18.298 [] INFO  io.atomix.raft.RandomizedRaftTest - Drop next message on 2\r\n12:08:18.298 [] INFO  TEST: - Dropped a message to 2\r\n12:08:18.298 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick heartbeatTimeout on 2\r\n12:08:18.298 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick 50ms on 1\r\n12:08:18.298 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 1\r\n12:08:18.298 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Failed to poll a majority of the cluster in PT2.5S\r\n12:08:18.298 [] TRACE io.atomix.raft.impl.RandomizedElectionTimer - Election timeout scheduled for PT4.267S\r\n[...]\r\n12:08:18.298 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Received VoteRequest{term=22, candidate=0, lastLogIndex=578, lastLogTerm=19}\r\n12:08:18.298 [] TRACE io.atomix.raft.storage.system.MetaStore - Store term 22\r\n12:08:18.298 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote null\r\n12:08:18.298 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{1-partition-1} - Set term 22\r\n12:08:18.298 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Rejected VoteRequest{term=22, candidate=0, lastLogIndex=578, lastLogTerm=19}: candidate's last log entry (578) is at a lower index than the local log (621)\r\n12:08:18.298 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Sending VoteResponse{status=OK, term=22, voted=false}\r\n[...]\r\n12:08:18.298 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Expected heartbeat from null in term 22, but received one from 1 in term 1, ignoring it\r\n12:08:18.298 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received PollRequest{term=22, candidate=2, lastLogIndex=559, lastLogTerm=19}\r\n12:08:18.298 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Rejected PollRequest{term=22, candidate=2, lastLogIndex=559, lastLogTerm=19}: candidate's last log entry (559) is at a lower index than the local log (578)\r\n12:08:18.298 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending PollResponse{status=OK, term=22, accepted=false}\r\n[...]\r\n12:08:18.299 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Sending poll requests to all active members: [DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.455Z}, DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.455Z}]\r\n12:08:18.299 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.455Z} for next term 23\r\n12:08:18.299 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.455Z} for next term 23\r\n[...]\r\n12:08:18.299 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received PollRequest{term=22, candidate=2, lastLogIndex=559, lastLogTerm=19}\r\n12:08:18.299 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Rejected PollRequest{term=22, candidate=2, lastLogIndex=559, lastLogTerm=19}: candidate's last log entry (559) is at a lower index than the local log (578)\r\n12:08:18.299 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending PollResponse{status=OK, term=22, accepted=false}\r\n[...]\r\n12:08:18.299 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Received PollRequest{term=22, candidate=2, lastLogIndex=559, lastLogTerm=19}\r\n12:08:18.299 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Rejected PollRequest{term=22, candidate=2, lastLogIndex=559, lastLogTerm=19}: candidate's last log entry (559) is at a lower index than the local log (621)\r\n12:08:18.299 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Sending PollResponse{status=OK, term=22, accepted=false}\r\n[...]\r\n12:08:18.299 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - No heartbeat from a known leader since 6ms\r\n12:08:18.299 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Sending poll requests to all active members: [DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.454Z}, DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.454Z}]\r\n12:08:18.299 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.454Z} for next term 23\r\n12:08:18.299 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.454Z} for next term 23\r\n```\r\n\r\nSo we see both 0 and 1 keep rejecting 2 as a candidate, and since 1 has the longest log, it should become elected eventually. Last we see it tries an election on term 23.\r\n\r\n```\r\n12:08:18.299 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received PollRequest{term=22, candidate=1, lastLogIndex=621, lastLogTerm=19}\r\n12:08:18.299 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Accepted PollRequest{term=22, candidate=1, lastLogIndex=621, lastLogTerm=19}: candidate's log is up-to-date\r\n12:08:18.299 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending PollResponse{status=OK, term=22, accepted=true}\r\n[...]\r\n12:08:18.299 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{1-partition-1} - Set term 23\r\n12:08:18.299 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote 1\r\n12:08:18.299 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{1-partition-1} - Voted for 1\r\n12:08:18.299 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{1-partition-1}{role=CANDIDATE} - Requesting votes for term 23\r\n12:08:18.299 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{1-partition-1}{role=CANDIDATE} - Requesting vote from DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.454Z} for term 23\r\n12:08:18.299 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{1-partition-1}{role=CANDIDATE} - Requesting vote from DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.454Z} for term 23\r\n[...]\r\n12:08:18.299 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received VoteRequest{term=23, candidate=1, lastLogIndex=621, lastLogTerm=19}\r\n12:08:18.299 [] TRACE io.atomix.raft.storage.system.MetaStore - Store term 23\r\n12:08:18.299 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote null\r\n12:08:18.299 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Set term 23\r\n12:08:18.300 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Accepted VoteRequest{term=23, candidate=1, lastLogIndex=621, lastLogTerm=19}: candidate's log is up-to-date\r\n12:08:18.300 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote 1\r\n12:08:18.300 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Voted for 1\r\n12:08:18.300 [] TRACE io.atomix.raft.impl.RandomizedElectionTimer - Election timeout scheduled for PT2.939S\r\n12:08:18.300 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending VoteResponse{status=OK, term=23, voted=true}\r\n[...]\r\n12:08:18.300 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - No heartbeat from a known leader since 8ms\r\n12:08:18.300 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Sending poll requests to all active members: [DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.455Z}, DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.455Z}]\r\n12:08:18.300 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.455Z} for next term 23\r\n12:08:18.300 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.455Z} for next term 23\r\n12:08:18.300 [] INFO  io.atomix.raft.RandomizedRaftTest - Append on leader on 0\r\n12:08:18.300 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick heartbeatTimeout on 0\r\n12:08:18.300 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 1\r\n12:08:18.300 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{1-partition-1}{role=CANDIDATE} - Received successful vote from DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.454Z}\r\n12:08:18.300 [] INFO  io.atomix.raft.impl.RaftContext - RaftServer{1-partition-1} - Transitioning to LEADER\r\n12:08:18.300 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{1-partition-1}{role=CANDIDATE} - Cancelling election\r\n```\r\n\r\nSo now we get to term 23. After that is a long time of just sending append requests trying to keep appending on the nodes, the log of which I already put above.\r\n\n npepinpe: So that the commit index is 405 is fine, and it would make sense that the entry is committed. We saw that both nodes received further appends (2 up to 559, and 0 up to 578), both requests which started at 406. So 405 should be committed, with term 19.\r\n\r\nSo possibly the problem is that during replication, 1 should have started sending entries from 579, but instead it started at 405? Or some other weird thing happened...but it's likely a replication issue.\n npepinpe: One thing to note, the index we delete after does not come from the entry we're trying to replace, but instead from the AppendRequest's prevLogIndex. It then gets incremented as we append...\r\n\r\nIs this correct? We saw that 0 replies that it's prevLogIndex is not 404, but instead 578.\r\n\r\nSee:\r\n\r\n```\r\n12:08:19.021 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Received AppendResponse{status=OK, term=23, succeeded=false, lastLogIndex=578, lastSnapshotIndex=404} from 0\r\n12:08:19.021 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Reset next index for RaftMemberContext{member=0, term=23, configIndex=0, snapshotIndex=404, nextSnapshotIndex=0, nextSnapshotChunk=null, matchIndex=0, heartbeatTime=1686658099013, appending=0, appendSucceeded=false, appendTime=1686658099018, configuring=false, installing=false, failures=0} to 579\r\n12:08:19.021 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Sending AppendRequest{term=23, leader=1, prevLogIndex=404, prevLogTerm=19, entries=145, commitIndex=839} to 0\r\n```\r\n\r\n\n npepinpe: From what I can see, it reset to 578, but then when building the next AppendRequest, it builds it from the previous entry which is...404? Is it because the snapshot is at 404?\r\n\r\nSo on the leader, if the previous entry (previous from what we should send) is not found, then we will send the prevLogEntry is the snapshot. Since 0 reports its last entry as 405, we try to get the prevLogEntry - which is not there, because 405 is the first entry in 1's log. So we send the snapshot - 404 - with term 19. But the first entry in the request is 579.\r\n\r\nSo on the follower (0), our last log entry is 578, greater than the reported `prevLogEntry` (404), so we think we have to replace some existing entries, starting at 405 (even if the first entry in the request is 578).\r\n\r\nI still don't get why the terms were mismatched though - it seems to me they should be the same, 19, so we shouldn't even have caught this bug :scream: \n npepinpe: @deepthidevaki - let's sync tomorrow on this\r\n\r\nI don't remember why we use the prevLogIndex as the start index instead of just using the entry's index themselves.\n deepthidevaki: > Is this correct? We saw that 0 replies that it's prevLogIndex is not 404, but instead 578.\r\n\r\nThis doesn't look like the expected behavior. Were there other AppendRequests in between, which would have truncated the entries in 0 back to 404? \n npepinpe: So it did compact right before:\r\n\r\n```\r\n12:08:19.021 [] INFO  io.atomix.raft.RandomizedRaftTest - Take snapshot on 1\r\n12:08:19.021 [] INFO  TEST - Snapshot taken at index 791. Current commit index is 839\r\n12:08:19.021 [] DEBUG io.camunda.zeebe.journal.file.SegmentsManager - atomix - Deleting log up from 313 up to 625 (removing 4 segments)\r\n```\n npepinpe: Alright, so the bug is that we have two snapshot listeners: one triggers compaction, and one updates the locale reference of the persisted snapshot in the Raft context. While both execute on the Raft thread, they may be executed in any order (since the listeners on the snapshot store are a set, iteration is not ordered).\r\n\r\nSo what happened here is:\r\n\r\n1. A new snapshot was taken up to entry 839\r\n2. The log is compacted up to 839\r\n3. 1 sent an append request for 839 to 0\r\n4. 0 rejected saying its last log index is 578\r\n5. 1 tries to reset 0 to 578, but the entry does not exist\r\n6. 1 sends a new append request using the local snapshot reference for prevLogIndex and prevLogTerm\r\n7. The snapshot reference is updated to 839\r\n\r\nSolution is to ensure that all the changes to the Raft context (updating the reference, compacting, etc.) are done in a single listener so we can control the ordering: update snapshot ref, then compact.\n npepinpe: This likely affect all versions. The severity is high in this case, as the partition will go inactive due to an uncaught exception. Workaround is to restart your broker.",
    "title": "`RandomizedRaftTest.consistencyTestWithSnapshot` fails with unexpected exception"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12780",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n```\r\nERROR 2023-05-16T00:30:04.104402170Z [resource.labels.containerName: zeebe] Unexpected error on writing CREATE command Failed to write request to logstream\r\n```\r\n\r\nThe error is from CommandAPIHandler when it tries to write a user request to the leader's logstream. This happened while the leader is transition to follower, and the logstream has already closed. Before this error we see that Sequencer rejects the record because it is closed. \r\n\r\nThis is a new error message introduced in https://github.com/camunda/zeebe/pull/12676. Previously this error was ignored. So we never got the error message.\r\n\r\n[logs](https://console.cloud.google.com/errors/detail/CNy21ZD_7pnW3AE;service=zeebe;time=P7D?project=zeebe-io)\r\n\r\n**Expected behavior**\r\n\r\n- Reduce the log level to warn/debug\r\n- logstream#tryWrite should return specific error code instead of -1, and use that to log more meaningful message. \r\n- If we can recognize that this is during the leader transition we can chose to not log the error. Instead return a PARTITION_LEADER_MISMATCH code back to the gateway so that it can retry the command with the new leader before sending an error to the client.\r\n\n\n megglos: ZDP-Triage:\n- mostly noise\n- it's expected and shouldn't be logged as error in the particular scenario\n- as it's new (last or next patch) it can be considered a regression => could be confusing after update\n megglos: ZDP-Planning:\n- we will look into it before the next release\n- also affects 8.2,8.1,8.0 due to a backporrt\n Zelldon: I feel this is not 100% resolved. [We see a lot of errors messages also in the gateway](https://console.cloud.google.com/logs/query;query=resource.type%3D%22k8s_container%22%0Aresource.labels.project_id%3D%22zeebe-io%22%0Aresource.labels.location%3D%22europe-west1-b%22%0Aresource.labels.cluster_name%3D%22zeebe-cluster%22%0Aresource.labels.namespace_name%3D%22medic-y-2023-cw-20-d2345cc-benchmark%22%0Alabels.k8s-pod%2Fapp%3D%22camunda-platform%22%0Alabels.k8s-pod%2Fapp_kubernetes_io%2Fcomponent%3D%22zeebe-gateway%22%0Alabels.k8s-pod%2Fapp_kubernetes_io%2Finstance%3D%22medic-y-2023-cw-20-d2345cc-benchmark%22%0Alabels.k8s-pod%2Fapp_kubernetes_io%2Fmanaged-by%3D%22Helm%22%0Alabels.k8s-pod%2Fapp_kubernetes_io%2Fname%3D%22zeebe-gateway%22%0Alabels.k8s-pod%2Fapp_kubernetes_io%2Fpart-of%3D%22camunda-platform%22;cursorTimestamp=2023-06-07T10:45:04.209515605Z?project=zeebe-io), which is also in this case a lot of noise.\r\n\r\nExample of a current medic benchmark\r\n```\r\nio.camunda.zeebe.gateway.cmd.BrokerErrorException: Received error from broker (INTERNAL_ERROR): Failed writing request: Failed to write request to logstream\r\n\tat io.camunda.zeebe.gateway.impl.broker.BrokerRequestManager.handleResponse(BrokerRequestManager.java:194) ~[zeebe-gateway-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.gateway.impl.broker.BrokerRequestManager.lambda$sendRequestInternal$2(BrokerRequestManager.java:143) ~[zeebe-gateway-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.future.FutureContinuationRunnable.run(FutureContinuationRunnable.java:28) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\"\r\n```\r\n\r\n[Where at this time a role change happens](https://grafana.dev.zeebe.io/d/NzsO1mUnk/zeebe-overview?orgId=1&var-DS_PROMETHEUS=Prometheus&var-cluster=All&var-namespace=medic-y-2023-cw-20-d2345cc-benchmark&var-pod=All&var-partition=All&from=1686132065352&to=1686136683285)\r\n![role](https://github.com/camunda/zeebe/assets/2758593/66f672f3-3af8-44f0-8cb3-7d3cb5029dd5)\r\n\n deepthidevaki: @Zelldon That is an old benchmark before the bug fix.\n Zelldon: Ups thanks @deepthidevaki you're right :+1: ",
    "title": "Failing to write to logstream during stepdown is logged as error"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/7855",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nIt seems we are closing the resources not in the correct order. We still accept or want to send an response but the message service is closed concurrently and the sending fails and causes on error on closing the Broker.\r\n\r\nOccurred 10 times in two error groups within `1.2.0-alpha2`\r\n\r\n\r\nError groups:\r\n\r\n * https://console.cloud.google.com/errors/CJeh5tGzv8X1Rg?service=zeebe&time=P7D&refresh=off&project=camunda-cloud-240911\r\n * https://console.cloud.google.com/errors/CKKJ762u3J3ZUw?service=zeebe&time=P7D&refresh=off&project=camunda-cloud-240911\r\n\r\n\r\n**To Reproduce**\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n - when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\nI assume run the service and close the broker.\r\n\r\n**Expected behavior**\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nNo error, correct sequence of closing resources.\r\n\r\n**Log/Stacktrace**\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.lang.IllegalStateException: Messaging service is not running.\r\nat io.camunda.zeebe.transport.impl.AtomixClientTransportAdapter.tryToSend (AtomixClientTransportAdapter.java:105)\r\nat io.camunda.zeebe.transport.impl.AtomixClientTransportAdapter.lambda$handleResponse$7 (AtomixClientTransportAdapter.java:206)\r\nat io.camunda.zeebe.util.sched.ActorJob.invoke (ActorJob.java:76)\r\nat io.camunda.zeebe.util.sched.ActorJob.execute (ActorJob.java:39)\r\nat io.camunda.zeebe.util.sched.ActorTask.execute (ActorTask.java:122)\r\nat io.camunda.zeebe.util.sched.ActorThread.executeCurrentTask (ActorThread.java:94)\r\nat io.camunda.zeebe.util.sched.ActorThread.doWork (ActorThread.java:78)\r\nat io.camunda.zeebe.util.sched.ActorThread.run (ActorThread.java:191)\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n\r\n**Environment:**\r\n\r\n - Zeebe Version: 1.2.0-alpha2\r\n\r\n\n\n Zelldon: Error message introduce via https://github.com/camunda-cloud/zeebe/pull/7568\n Zelldon: related https://github.com/camunda-cloud/zeebe/issues/5521\n Zelldon: @pihme do you think this is solved due to recent refactorings?\n pihme: @Zelldon No I don't. The order of atomix and embedded gateway has not changed by the refactoring. One thing I wonder is whether this occurred in the Broker or in a Standalone Gateway. Do you know where it occurred?\r\n\r\nIn terms of macro-order of shutdown steps this should not happen.\r\n \r\nIt could happen if either shutdown of the actor does not cancel all subsequent planned tasks. Or it could happen if it is possible to schedule new tasks after an actor has been shutdown.\n pihme: Reoccurred recently:\r\n\r\nLogs:\r\n```\r\nD 2022-01-16T09:29:16.098476Z Closing Broker-0 [6/10]: embedded gateway closed in 15 ms \r\nI 2022-01-16T09:29:16.098659Z Closing Broker-0 [7/10]: cluster services \r\nD 2022-01-16T09:29:16.098802Z Closing Broker-0 [7/10]: cluster services closed in 0 ms \r\nI 2022-01-16T09:29:16.098978Z Closing Broker-0 [8/10]: subscription api \r\nD 2022-01-16T09:29:16.102018Z Closing Broker-0 [8/10]: subscription api closed in 2 ms \r\nI 2022-01-16T09:29:16.102259Z Closing Broker-0 [9/10]: command api transport and handler \r\nI 2022-01-16T09:29:16.232428Z Stopped \r\nD 2022-01-16T09:29:16.233107Z Closing Broker-0 [9/10]: command api transport and handler closed in 131 ms \r\nI 2022-01-16T09:29:16.233481Z Closing Broker-0 [10/10]: Migrated Startup Steps \r\nD 2022-01-16T09:29:16.235563Z Shutdown was called with context: io.camunda.zeebe.broker.bootstrap.BrokerStartupContextImpl@36539fa \r\nI 2022-01-16T09:29:16.235975Z Shutdown Cluster Services (Creation) \r\nI 2022-01-16T09:29:16.238984Z Stopped \r\nI 2022-01-16T09:29:16.240144Z Stopped \r\nI 2022-01-16T09:29:16.241539Z 0 - Member deactivated: Member{id=0, address=zeebe-0.zeebe-broker-service.0bbb3676-e3ad-445b-891e-0e1350800e41-zeebe.svc.cluster.local:26502, properties={brokerInfo=EADJAAAAAwAAAAAAAgAAAAMAAAADAAAAAAABCgAAAGNvbW1hbmRBcGlfAAAAemVlYmUtMC56ZWViZS1icm9rZXItc2VydmljZS4wYmJiMzY3Ni1lM2FkLTQ0NWItODkxZS0wZTEzNTA4MDBlNDEtemVlYmUuc3ZjLmNsdXN0ZXIubG9jYWw6MjY1MDEFAAIBAAAAAQIAAAABDAAABQAAADEuMi45BQACAQAAAAECAAAAAQ==, event-service-topics-subscribed=KIIDAGpvYnNBdmFpbGFibOU=}} \r\nI 2022-01-16T09:29:16.241947Z Stopped \r\nI 2022-01-16T09:29:16.242539Z Local node Member{id=0, address=zeebe-0.zeebe-broker-service.0bbb3676-e3ad-445b-891e-0e1350800e41-zeebe.svc.cluster.local:26502, properties={brokerInfo=EADJAAAAAwAAAAAAAgAAAAMAAAADAAAAAAABCgAAAGNvbW1hbmRBcGlfAAAAemVlYmUtMC56ZWViZS1icm9rZXItc2VydmljZS4wYmJiMzY3Ni1lM2FkLTQ0NWItODkxZS0wZTEzNTA4MDBlNDEtemVlYmUuc3ZjLmNsdXN0ZXIubG9jYWw6MjY1MDEFAAIBAAAAAQIAAAABDAAABQAAADEuMi45BQACAQAAAAECAAAAAQ==, event-service-topics-subscribed=KIIDAGpvYnNBdmFpbGFibOU=}} left the bootstrap servide \r\nI 2022-01-16T09:29:16.243398Z Stopped cluster membership service for member Member{id=0, address=zeebe-0.zeebe-broker-service.0bbb3676-e3ad-445b-891e-0e1350800e41-zeebe.svc.cluster.local:26502, properties={brokerInfo=EADJAAAAAwAAAAAAAgAAAAMAAAADAAAAAAABCgAAAGNvbW1hbmRBcGlfAAAAemVlYmUtMC56ZWViZS1icm9rZXItc2VydmljZS4wYmJiMzY3Ni1lM2FkLTQ0NWItODkxZS0wZTEzNTA4MDBlNDEtemVlYmUuc3ZjLmNsdXN0ZXIubG9jYWw6MjY1MDEFAAIBAAAAAQIAAAABDAAABQAAADEuMi45BQACAQAAAAECAAAAAQ==, event-service-topics-subscribed=KIIDAGpvYnNBdmFpbGFibOU=}} \r\nE 2022-01-16T09:29:18.270930Z Expected to handle gRPC request, but an unexpected error occurred \r\nE 2022-01-16T09:29:18.272130Z Expected to handle gRPC request, but an unexpected error occurred \r\nE 2022-01-16T09:29:18.273074Z Expected to handle gRPC request, but an unexpected error occurred \r\nI 2022-01-16T09:29:18.362961Z Stopped \r\nI 2022-01-16T09:29:18.363819Z Stopped \r\nI 2022-01-16T09:29:18.365121Z Shutdown monitoring services \r\nD 2022-01-16T09:29:18.366707Z Finished shutdown process \r\nD 2022-01-16T09:29:18.367117Z Closing Broker-0 [10/10]: Migrated Startup Steps closed in 2134 ms \r\nI 2022-01-16T09:29:18.367404Z Closing Broker-0 succeeded. Closed 10 steps in 2419 ms. \r\nI 2022-01-16T09:29:18.367629Z Broker shut down. \r\nD 2022-01-16T09:29:18.367920Z Closing actor thread ground 'Broker-0-zb-fs-workers' \r\nD 2022-01-16T09:29:18.368650Z Closing actor thread ground 'Broker-0-zb-actors' \r\nD 2022-01-16T09:29:18.369899Z Closing actor thread ground 'Broker-0-zb-actors': closed successfully \r\nD 2022-01-16T09:29:18.369957Z Closing actor thread ground 'Broker-0-zb-fs-workers': closed successfully \r\n```\n Zelldon: This happened again on our benchmark week 27\r\n\r\nhttps://console.cloud.google.com/errors/detail/CJ-difGB-LXsAg;service=zeebe;version=medic-cw-27-56ad2b36c8-benchmark;time=P7D?project=zeebe-io\n korthout: Happened again on benchmark week 27 ([newly reported error](https://console.cloud.google.com/errors/detail/CK6Khe-c7YzlFw;service=zeebe;version=medic-cw-27-56ad2b36c8-benchmark;time=P7D?project=zeebe-io)) with a different stacktrace. \r\n\r\nThis time because of `AtomixClientTransportAdapter.lambda$sendRequestInternal$2` instead of `AtomixClientTransportAdapter.lambda$handleResponse$7`.\n Zelldon: Last seen: 1 day ago.\n oleschoenburg: Happened again on 8.1.8: https://console.cloud.google.com/errors/detail/CK6Khe-c7YzlFw;service=zeebe;time=P7D?project=camunda-cloud-240911\n Zelldon: Happened in [zeebe:8.0.19-SNAPSHOT-stable-8.0-c326e93b](https://console.cloud.google.com/errors/detail/CKKJ762u3J3ZUw;service=zeebe;version=8.0.19-SNAPSHOT-stable-8.0-c326e93b;time=P7D?project=camunda-saas-int-chaos)\r\n\r\nhttps://console.cloud.google.com/errors/detail/CKKJ762u3J3ZUw;service=zeebe;time=P7D?project=camunda-saas-int-chaos\n deepthidevaki: Observed in 8.2.12\r\n\r\nRoot cause analysis:\r\n\r\nThe error message originates in `AtomixClientTransportAdapter`. This is created in Gateway BrokerClient\r\n\r\nhttps://github.com/camunda/zeebe/blob/18657c586a1974f6bb3ce3f86c1e16359458050d/gateway/src/main/java/io/camunda/zeebe/gateway/impl/broker/BrokerClientImpl.java#L64\r\n\r\nBut never closed by it\r\nhttps://github.com/camunda/zeebe/blob/0945c3088629e5a72b260778c104418afb113b08/gateway/src/main/java/io/camunda/zeebe/gateway/impl/broker/BrokerClientImpl.java#L70-L87\r\n\r\nFix:\r\nMessaging service is only closed after BrokerClient (I guess so because the lifecycle is managed by Spring, so the dependency order should be preserved). So closing `AtomixClientTransportAdapter` when closing `BrokerClientImpl` should ensure that no request are send after it is closed.\n oleschoenburg: Observed again on 8.1.16: https://console.cloud.google.com/errors/detail/CK6Khe-c7YzlFw;service=zeebe;time=P7D?project=camunda-saas-int-chaos\n megglos: ZDP-Triage:\n- mostly noise during shutdown\n- maybe the close is not sync",
    "title": "Messaging service is not running"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/5209",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Description**\r\nCurrently we start zeebe partition services only after atomix is fully started. That means if a node has 3 partitions, we wait until all 3 partitions has successfully started their raft servers. In some benchmarks it was observed that sometimes 2 partitions in a node succesfuly starts and become the raft leaders. But the third partition did not join, or took long (hours) to join (due to #5208 ). During this time, this node is the raft leader for 2 partitions, but no processing is happening because we don't install stream processor and other leader services until the atomix start is completed. As a result, the service is unavailable or only partially available even though there are leaders for all partitions.\r\n\n\n Zelldon: I think this goes in hand with removing the complete atomix wrapper and bootstrap logic, which wanted to do anyway at some point. I think it makes sense to remove it and have independent starters for each partition.\n deepthidevaki: Is there an issue for it already? If so, let's link it here.\n Zelldon: Unfortunately I haven't found it in the backlog. That's why I just commented it :smile: \n npepinpe: Do we see any blockers to starting partitions in parallel without waiting for the others to be started?\n Zelldon: I see no blockers. Maybe @deepthidevaki has some thoughts?\r\n\n deepthidevaki: No real blockers that I know. We would have to refactor atomix interface. There should be a way to detect when an individual partition is started and ready. For a single partition, the steps in bootstrap are the following in order.\r\n1. RaftPartitionServer is created\r\n2. Become follower\r\n3. Leader election happens\r\n4. Join completes\r\n5. Catch up if needed\r\n6. Marked as Ready -> Startup complete.\r\n\r\nWe have to ensure that zeebe services are installed only after step 6. Currently, we do `atomix.start().join()`, this is guaranteed.\r\n  \n Zelldon: What we could to instead is:\r\n\r\n * start atomix transport\r\n * membership\r\n * start topology\r\n * start monitoring \r\n * start disk space\r\n * split up in partition starts with sub steps for installing processors etc.\n npepinpe: Would love to improve the bootstrapping logic next quarter - let's see if we can make it happen.\n deepthidevaki: We observed an e2e test failure as a consequence of this.\r\n\r\nBroker 2 was not becoming ready, because partition 1 was not able to receive heartbeats from leader (due to another bug). But it was receiving events for partitions 2 and 3. Since raft partition 1 is not ready, Zeebe services  (processing and snapshoting) were not started for any partitions. So partitions 2 and 3 starts accumulating events in the log, but no snapshot were taken.\r\n\r\nLater, the root issue was resolved, partition 1 startup was unblocked and broker 2 became ready. Immediately after that, broker 2 became the leader for partition 3. Since it was not replaying and taking snapshots before, it's last snapshot was from 12 hours ago. As a result, partition 3 had to replay events generated in the last 12 hours before it can start processing. The replay took almost 1 hour, and during that time partition 3 was essentially unavailable for processing. All instances that were created before the leader change, was completed only after 1 hour.\n megglos: planning: affects higher partition count setups more heavily, risk of incidents,\nlet's time-box the investigation to 2h\n oleschoenburg: I looked into this, mostly to understand the startup process.\r\nTo me it looks like we have two \"barriers\" that wait for all partitions before continuing the startup process:\r\n1. `RaftPartitionGroup#join` waits for all raft partitions [to become `READY`](https://github.com/camunda/zeebe/blob/f574c79929f01e51f33a3d5902fc651aa15c3d46/atomix/cluster/src/main/java/io/atomix/raft/impl/DefaultRaftServer.java#L191-L197). This is [called by the PartitionManger](https://github.com/camunda/zeebe/blob/6a454eedc03e14d4f7fa56e5560c4febfbe57f89/broker/src/main/java/io/camunda/zeebe/broker/partitioning/PartitionManagerImpl.java#L128-L129) before scheduling `ZeebePartition`s. \r\n2.  `PartitionManagerImpl#start` waits for all `ZeebePartition`s to start. This blocks at least the installation of `AdminApiRequestHandler`\r\n\r\nThe first one is the crux I think, we don't want to wait for the partition _group_ to start, we just want to wait for each partition individually.\r\n\r\nThe second one appears to be trivial to solve:\r\n\r\n```patch\r\ndiff --git a/broker/src/main/java/io/camunda/zeebe/broker/partitioning/PartitionManagerImpl.java b/broker/src/main/java/io/camunda/zeebe/broker/partitioning/PartitionManagerImpl.java\r\n--- a/broker/src/main/java/io/camunda/zeebe/broker/partitioning/PartitionManagerImpl.java\t(revision 4fa6c0dc0009f5faedb987b09a055f9135629495)\r\n+++ b/broker/src/main/java/io/camunda/zeebe/broker/partitioning/PartitionManagerImpl.java\t(revision 6a454eedc03e14d4f7fa56e5560c4febfbe57f89)\r\n@@ -127,7 +127,7 @@\r\n \r\n     return partitionService\r\n         .start()\r\n-        .thenApply(\r\n+        .thenApplyAsync(\r\n             ps -> {\r\n               LOGGER.info(\"Registering Partition Manager\");\r\n \r\n@@ -158,12 +158,10 @@\r\n                       topologyManager,\r\n                       brokerCfg.getExperimental().getFeatures().toFeatureFlags()));\r\n \r\n-              final var futures =\r\n-                  partitions.stream()\r\n-                      .map(partition -> CompletableFuture.runAsync(() -> startPartition(partition)))\r\n-                      .toArray(CompletableFuture[]::new);\r\n+              for (final var partition : partitions) {\r\n+                CompletableFuture.runAsync(() -> startPartition(partition));\r\n+              }\r\n \r\n-              CompletableFuture.allOf(futures).join();\r\n               return null;\r\n             });\r\n   }\r\n```\r\n\r\n",
    "title": "Startup failure of one raft partition server affects the availability of other healthy partition"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/2890",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nAt a multi-instance activity, I can define the input collection to iterate over. If the collection holds a large number of elements, the broker might fail to spawn the inner instances. Such a case can only be fixed, by manually decreasing the collection variable.\r\n\r\n**Describe the solution you'd like**\r\n* I can spawn as many instances as defined by the input collection\r\n* instead of spawning the instances all at once on activating the body (current behavior), spawn the instances step-wise until all instances are created (e.g. spawn 10 instances and write a record to spawn the next 10)\r\n* the instance spawning can be interrupted by an event or a terminate command\r\n\r\n**Describe alternatives you've considered**\r\n* increasing buffers and record max length - see #2880 \r\n* splitting the creation into multiple steps (e.g. loop or nested multi-instance) or control it externally -  \r\n\r\nRelates to\r\nhttps://jira.camunda.com/browse/SUPPORT-16653\r\nhttps://jira.camunda.com/browse/SUPPORT-16549\r\n\n\n saig0: Using Zeebe 0.21.0, we can execute a multi-instance activity with an input collection of ~ 5.000 elements (depending on the variables). By increasing the max message size, we could process even larger collections. \r\n\r\nSince this is ok for now, we will postpone the issue.\n npepinpe: Just a side note, let's postpone at least until the new engine is done (i.e. at least Q2 2021). I can see us working on reducing the max message size next year, so we will need to implement this kind of \"chunking\" - obviously can't guarantee, but it sounds plausible to me.\n saig0: **Update:** I tested it again with version `1.0.0-alpha7`. We can now iterate over an input collection with up to ~12.000 elements :tada: \n korthout: Marking priority as `later` because 'multi-instance for large collections' is not the main concern for the process automation team right now. However, we should probably work on this when we work on\r\n- #8687\r\n\r\nNote that the impact of this bug is reduced once we've resolved\r\n- #5221\r\n\r\nPlease comment if you think this should have a higher priority.\n epollum: Hello team, I want to mention that we have a customer who requires the ability to use large multi-instance activities. Please see https://jira.camunda.com/browse/SUPPORT-16499\r\nAnd https://github.com/camunda/zeebe/issues/11355\r\n\r\nThank you!\n daniel-ewing: Hi team, here is another one: https://jira.camunda.com/browse/SUPPORT-16653\n npepinpe: And another: https://jira.camunda.com/browse/SUPPORT-16549\n abbasadel: Team meeting: we changed the priority to \"Upcoming\" to pick this up when we have time\n felix-mueller: Hey @remcowesterhoud \r\nI saw you worked on this item, there is one more item in the backlog: https://github.com/camunda/zeebe/issues/8687\r\n\r\nCould you perhaps elaborate if this is fixed now as well or is there a case which is not covered? \r\nCould you perhaps explain which case is not covered for #8687?\r\n\r\nThanks\r\nFelix\n remcowesterhoud: @felix-mueller from what I understand after reading #8687 it is a different issue. This one had to do with instance banning and the original author of the issue [explicitly mentioned](https://github.com/camunda/zeebe/issues/8687#issuecomment-1026023644) his instances weren't getting banned.\r\n\r\nWhether this is fixed, I am not sure as I'm struggling to understand what is happening in the other issue 😅 @korthout do you have any ideas? It reads to me like it's more about job activation than multi-instance input collections.\r\n\r\nIf it's stale we can consider closing it and seeing if it ever occurs again.\n korthout: @remcowesterhoud I've had a look. IMO, that issue is not specifically related to multi-instance, but rather to activating many jobs with large variables in the Go client.\r\n\r\nI've provided my thoughts on that issue [here](https://github.com/camunda/zeebe/issues/8687#issuecomment-1665950474)",
    "title": "I can spawn inner instances for a large input collection"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14236",
      "component": "Zeebe",
      "subcomponent": "Gateway",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nWhen calling any of the `StreamObserver` methods (including `#onNext`) concurrently, the underlying stream will send garbled messages, and you end up with errors like \r\n\r\n```\r\njava.lang.IllegalStateException: sendHeaders has already been called \r\n```\r\n\r\nOr \r\n\r\n```\r\nio.grpc.StatusRuntimeException: INTERNAL: Connection closed after GOAWAY. HTTP/2 error code: INTERNAL_ERROR, debug data: Stream 7 sent too many headers EOS: false\r\n```\r\n\r\nReading the documentation, it's quite clearly stated in `StreamObserver.java`:\r\n\r\n> Separate StreamObservers do not need to be synchronized together; incoming and outgoing directions are independent. Since individual StreamObservers are not thread-safe, if multiple threads will be writing to a StreamObserver concurrently, the application must synchronize calls.\r\n\r\nIn the `ClientStreamAdapter`, we call `onNext` using a thread pool executor, so concurrent calls are possible.\r\n\r\n**To Reproduce**\r\n\r\nEasily reproducible with [Camunda 8 Benchmark](https://github.com/camunda-community-hub/camunda-8-benchmark), since there we have a single client, so it's more likely for us to receive all the jobs.\r\n\r\n**Expected behavior**\r\n\r\nCalls to a `StreamObserver` are serialized. Either all consumers become an actor (what is the overhead there?), or we use gRPC's `SerializingExecutor` to wrap around our thread pool.\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.3.0-alpha5, 8.3.0-alpha6\r\n\n\n npepinpe: Open questions:\r\n\r\n- Can we make all `ClientStreamConsumerImpl` just be actors? I mean, that's our synchronization/serialization mechanism, and it lends itself quite well here. What's the overhead of creating potentially so many actors? Just that they're all on the same pool? That they will be performing I/O?\r\n- As a quick fix, we could use gRPC's `SerializingExecutor`, but that seems to be an internal utility, so maintenance wise it could be an issue in the end. In fact, they have a TODO comment: `// TODO(madongfly): figure out a way to not expose it or move it to transport package.`\n npepinpe: For job push to be usable in 8.3, this needs to be fixed by then\n Zelldon: I think this is related to https://github.com/camunda/zeebe/issues/14047\n\nWe have also observed the go away errors when testing with Immi.\n npepinpe: So maybe for 8.3 (in a few weeks) we quickly fix it using `SerializingExecutor` (literally a one line fix), but commit to a better solution using actors which could take a bit longer (definitely not small :smile:)",
    "title": "Prevent concurrent StreamObserver calls on the same instance"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14176",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nThe constructor of the `ActivateJobsCommandImpl` is trying to set the `workerName` during construction and expects the `config.getDefaultJobWorkerName())` to be non null. This leads to a `NullPointerException` as the underlying builder doesnt support a null value. This is an issue as clients are able to set the `workerName` using the builder pattern after constructing the command.\r\n\r\n```\r\npublic ActivateJobsCommandImpl(\r\n      final GatewayStub asyncStub,\r\n      final ZeebeClientConfiguration config,\r\n      final JsonMapper jsonMapper,\r\n      final Predicate<Throwable> retryPredicate) {\r\n    this.asyncStub = asyncStub;\r\n    this.jsonMapper = jsonMapper;\r\n    this.retryPredicate = retryPredicate;\r\n    builder = ActivateJobsRequest.newBuilder();\r\n    requestTimeout(config.getDefaultRequestTimeout());\r\n    timeout(config.getDefaultJobTimeout());\r\n    workerName(config.getDefaultJobWorkerName());\r\n  }\r\n```\r\n\r\n**To Reproduce**\r\n\r\nCreate an implementation of the `ZeebeClientProperties` that returns `null` for the `getDefaultJobWorkerName`\r\n\r\n**Expected behavior**\r\n\r\nThe builder should fallback to the default worker name only if it wasnt set by a client when constructing the final command.\r\n\r\n**Log/Stacktrace**\r\n\r\n```\r\njava.lang.NullPointerException: null\r\n\tat io.camunda.zeebe.gateway.protocol.GatewayOuterClass$ActivateJobsRequest$Builder.setWorker(GatewayOuterClass.java:2169)\r\n\tat io.camunda.zeebe.client.impl.command.ActivateJobsCommandImpl.workerName(ActivateJobsCommandImpl.java:83)\r\n\tat io.camunda.zeebe.client.impl.command.ActivateJobsCommandImpl.<init>(ActivateJobsCommandImpl.java:60)\r\n```\r\n\r\n**Environment:**\r\n- Zeebe Version:8.3.0-alpha4\r\n\r\nLooks like it happens as the command builder is used instead of the previously used request builder since: https://github.com/camunda/zeebe/commit/359a402b5bbee7247749385a458c5b3f3aba7e78\n\n korthout: @npepinpe This sounds like a regression introduced with #12888. Could you please have a look?\n npepinpe: We had a look together, and AFAIK this is the same behavior as ever. We were always calling `workerName` in the constructor of the command, and it makes sense to me to follow Simon's suggestion and only apply defaults in the build method, not in the constructor.\r\n\r\nEDIT: here's the commit which introduced setting the worker name as the default configuration: https://github.com/camunda/zeebe/blob/06f4e463efd3ca1be67056660f12abd4ad7867d8/clients/java/src/main/java/io/zeebe/client/impl/job/ActivateJobsCommandImpl.java#L46\r\n\r\nThat's from 2018, so it looks to me like this was always the behavior. The only thing I can imagine would be that _maybe_ the gRPC behavior changed, where they simply don't allow null values anymore, but I doubt it. I suspect if we check, it was always like this.\n npepinpe: > Looks like it happens as the command builder is used instead of the previously used request builder since: https://github.com/camunda/zeebe/commit/359a402b5bbee7247749385a458c5b3f3aba7e78\r\n\r\n@sbuettner - can you point out to me where the change from request to command builder happened? From the commit, the code is the same - we extracted an interface out of a class, but the implementation was not changed.\r\n\r\nFor example, in the \"old\" implementation:\r\n\r\n```java\r\n    final ActivateJobsCommandStep3 activateCommand =\r\n        jobClient\r\n            .newActivateJobsCommand()\r\n            .jobType(jobType)\r\n            .maxJobsToActivate(maxJobsToActivate)\r\n            .timeout(timeout)\r\n            .workerName(workerName);\r\n```\r\n\r\nAnd it was simply moved in the \"new\" one, but remains the same:\r\n\r\n```java\r\n    final ActivateJobsCommandStep3 activateCommand =\r\n        jobClient\r\n            .newActivateJobsCommand()\r\n            .jobType(jobType)\r\n            .maxJobsToActivate(maxJobsToActivate)\r\n            .timeout(timeout)\r\n            .workerName(workerName);\r\n```\r\n\r\nI'm also not sure what you mean by request vs command builder :thinking: \n sbuettner: @npepinpe It looks like it was changed here: https://github.com/camunda/zeebe/commit/11c548dd4d46f62119a600da0dc1abad7231c417#diff-7101968ae21254c317a1606e4a1969ec3864b538403541ecc7161351b2bd38ddL86\n npepinpe: Good catch! Seems unrelated to job push, but it is indeed a regression in that sense for 8.3.0. I'll defer to the ZPA team on what the priority for it is.\n korthout: Thanks @npepinpe and @sbuettner for the extra input.\r\n\r\n- We'll need to fix this before 8.3 as it regressed recently.\r\n- We consider this mid severity, as a workaround is available (don't set null as the default job worker name)\r\n- We wonder how quickly a user would run into this, do you have to set `null` explicitly, or does it also occur when the client doesn't define any default?\r\n- This would change the impact dramatically, and so our priority\r\n- We want to timebox whether that is the case\r\n- It might be worth it to fix immediately then (but please timebox it)",
    "title": "ActivateJobsCommandImpl throws NullPointerException when ZeebeClientProperties.getDefaultJobWorkerName is null"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14496",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nRestoring a backup taken with `clusterSize=3` from a broker with `clusterSize=1` fails because not all backups can be found.\r\n\r\nThe `PartitionRestoreService`  builds known broker ids based on `clusterSize` and then attempts to find a partition backup by going through all broker ids. When `clusterSize=1`, it will only search for backups taken by broker 1, thus missing backups taken by broker 2 and 3.\r\n\r\n**To Reproduce**\r\n\r\n1. Take a backup with `clusterSize=3` where broker 1 is _not_ leader for all partitions\r\n2. Restore with `clusterSize=1`\r\n\r\n**Expected behavior**\r\n\r\nRestore finds backups from all brokers, not only those in the current configuration.\r\n\r\n**Context**\r\nSupport case: https://jira.camunda.com/browse/SUPPORT-18562\n",
    "title": "Restore with fewer brokers fails to find all backups"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14418",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nWe got alerted in SaaS, an incident was created which contains several information to it https://app.slack.com/client/T0PM0P1SA/C05TG4DLD5H/thread/C05TG4DLD5H-1695364924.679279\r\n\r\nWe can see that a role change happened and on the new leader the corruption was detected. The partition was marked as dead afterward!\r\n\r\n\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n**To Reproduce**\r\nNot clear yet.\r\n\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\n**Expected behavior**\r\n\r\nNot detection\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n**Log/Stacktrace**\r\nError group https://console.cloud.google.com/errors/detail/CKLTyeONkqLZCg;service=zeebe;time=P7D?project=camunda-cloud-240911\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\nio.camunda.zeebe.db.ZeebeDbInconsistentException: Key DbCompositeKey{first=DbLong{2251799813994826}, second=optimus_entity_created_message} in ColumnFamily PROCESS_SUBSCRIPTION_BY_KEY already exists\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.assertKeyDoesNotExist(TransactionalColumnFamily.java:286) ~[zeebe-db-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.lambda$insert$0(TransactionalColumnFamily.java:80) ~[zeebe-db-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.lambda$ensureInOpenTransaction$19(TransactionalColumnFamily.java:314) ~[zeebe-db-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.DefaultTransactionContext.runInTransaction(DefaultTransactionContext.java:31) ~[zeebe-db-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.ensureInOpenTransaction(TransactionalColumnFamily.java:313) ~[zeebe-db-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.insert(TransactionalColumnFamily.java:75) ~[zeebe-db-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.state.message.DbProcessMessageSubscriptionState.put(DbProcessMessageSubscriptionState.java:82) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.state.appliers.ProcessMessageSubscriptionCreatingApplier.applyState(ProcessMessageSubscriptionCreatingApplier.java:28) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.state.appliers.ProcessMessageSubscriptionCreatingApplier.applyState(ProcessMessageSubscriptionCreatingApplier.java:15) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.state.appliers.EventAppliers.applyState(EventAppliers.java:327) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.streamprocessor.writers.ResultBuilderBackedEventApplyingStateWriter.appendFollowUpEvent(ResultBuilderBackedEventApplyingStateWriter.java:56) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.streamprocessor.writers.ResultBuilderBackedEventApplyingStateWriter.appendFollowUpEvent(ResultBuilderBackedEventApplyingStateWriter.java:42) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.common.CatchEventBehavior.subscribeToMessageEvent(CatchEventBehavior.java:271) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.common.CatchEventBehavior.lambda$subscribeToMessageEvents$7(CatchEventBehavior.java:246) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat java.util.stream.ForEachOps$ForEachOp$OfRef.accept(Unknown Source) ~[?:?]\r\n\tat java.util.stream.ReferencePipeline$2$1.accept(Unknown Source) ~[?:?]\r\n\tat java.util.ArrayList$ArrayListSpliterator.forEachRemaining(Unknown Source) ~[?:?]\r\n\tat java.util.stream.AbstractPipeline.copyInto(Unknown Source) ~[?:?]\r\n\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(Unknown Source) ~[?:?]\r\n\tat java.util.stream.ForEachOps$ForEachOp.evaluateSequential(Unknown Source) ~[?:?]\r\n\tat java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(Unknown Source) ~[?:?]\r\n\tat java.util.stream.AbstractPipeline.evaluate(Unknown Source) ~[?:?]\r\n\tat java.util.stream.ReferencePipeline.forEach(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.engine.processing.common.CatchEventBehavior.subscribeToMessageEvents(CatchEventBehavior.java:246) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.common.CatchEventBehavior.lambda$subscribeToEvents$4(CatchEventBehavior.java:154) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.util.Either$Right.ifRight(Either.java:371) ~[zeebe-util-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.common.CatchEventBehavior.subscribeToEvents(CatchEventBehavior.java:152) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnEventSubscriptionBehavior.subscribeToEvents(BpmnEventSubscriptionBehavior.java:48) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.container.CallActivityProcessor.lambda$onActivate$0(CallActivityProcessor.java:64) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.util.Either$Right.flatMap(Either.java:366) ~[zeebe-util-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.container.CallActivityProcessor.onActivate(CallActivityProcessor.java:64) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.container.CallActivityProcessor.onActivate(CallActivityProcessor.java:29) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.lambda$processEvent$2(BpmnStreamProcessor.java:144) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.util.Either$Right.ifRightOrLeft(Either.java:381) ~[zeebe-util-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.processEvent(BpmnStreamProcessor.java:143) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.lambda$processRecord$0(BpmnStreamProcessor.java:92) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.util.Either$Right.ifRightOrLeft(Either.java:381) ~[zeebe-util-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.processRecord(BpmnStreamProcessor.java:89) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.incident.ResolveIncidentProcessor.lambda$attemptToContinueProcessProcessing$0(ResolveIncidentProcessor.java:105) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.util.Either$Right.ifRightOrLeft(Either.java:381) ~[zeebe-util-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.incident.ResolveIncidentProcessor.attemptToContinueProcessProcessing(ResolveIncidentProcessor.java:103) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.incident.ResolveIncidentProcessor.processRecord(ResolveIncidentProcessor.java:81) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.Engine.process(Engine.java:127) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.batchProcessing(ProcessingStateMachine.java:353) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$2(ProcessingStateMachine.java:269) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run(ZeebeTransaction.java:98) ~[zeebe-db-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand(ProcessingStateMachine.java:269) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord(ProcessingStateMachine.java:228) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.readNextRecord(ProcessingStateMachine.java:204) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) ~[zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:109) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:204) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- OS: <!-- [e.g. Linux] -->\r\n- Zeebe Version: 8.3.0-alpha6\r\n- Configuration: <!-- [e.g. exporters etc.] -->\r\n\n\n korthout: >Based on the metrics we can see that around 23:11:30 a leader change happened.\r\n>- Before Zeebe-1 was leader for all a partitions (also 1)\r\n>- Later Zeebe-0 becomes leader for partition one (Zeebe-1 is still leader!)\r\n>- Both leaders become unhealthy\r\n\r\nThis sounds like a raft issue. If both are leader they would be handing out the same key and inconsistency is expected.\r\n\r\nThis is a platform issue, so moving it to Zeebe Distributed Platform team board.\n Zelldon: @korthout please write in the incident channel\n korthout: After more research, we found out that this inconsistency was the cause of leader election and that there were not two leaders at the same time as initially thought.\r\n\r\nFurther investigation showed that the inconsistency occurred due to the following:\r\n\r\n- A message subscription and process message subscription are both created along with the raised incident on activating a call activity where the called process is not found\r\n- When the incident is resolved, the call activity is attempted to be activated again.\r\n- This time, the message subscription and process message subscription are recreated as part of the call activity activation.\r\n- This causes the inconsistency failure\r\n\r\nIt should be possible to reproduce this in a test case\n korthout: I've added a reproducing test case in https://github.com/camunda/zeebe/compare/8.3.0-alpha6...korthout-14418-test-inconsistency",
    "title": "Inconsitency detected in PROCESS_SUBSCRIPTION_BY_KEY"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14366",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nError group: https://console.cloud.google.com/errors/detail/CJLK28eniZi1JA;service=zeebe;time=P7D?project=camunda-cloud-240911 (occurred 12 times yesterday on alpha6)\r\n\r\nThe cancelation of a process fails because the process model can't be found or is not available and the processing then fails with a NPE.\r\n\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n**To Reproduce**\r\n\r\nCancel a process I guess? I'm not 100% sure how this could happen in the log we also don't see the reference key, unfortunately. \r\n\r\nI tried with zbctl to cancel negative or non-existing but it didn't worked\r\n\r\n```\r\n$ zbctl cancel instance \"-1\"\r\nError: unknown shorthand flag: '1' in -1\r\n$ zbctl cancel instance \"1\" --insecure\r\nError: rpc error: code = Unavailable desc = Expected to execute command on partition 0, but either it does not exist, or the gateway is not yet aware of it\r\n```\r\n\r\nFor me it looks like it has to be a real key otherwise it will not be forwarded from the gateway to the right partition and written to the log.\r\n🤷🏼 \r\n\r\n\r\nI tried to reproduce this on SaaS:\r\n\r\n1. Created a cluster with alpha6\r\n2. Deployed a simple model with a user task\r\n3. Created an instance\r\n4. Canceled the instance in operate\r\n5. Cancelation was successful\r\n\r\nNot sure how to reproduce this, but I think this is clearly a regression.\r\n\r\n\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\n**Expected behavior**\r\n\r\nWe can cancel a process instance without issues.\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n<pre>\r\njava.lang.NullPointerException: Cannot invoke \"io.camunda.zeebe.engine.state.deployment.DeployedProcess.getKey()\" because \"process\" is null\r\n\tat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnBufferedMessageStartEventBehavior.findNextMessageToCorrelate(BpmnBufferedMessageStartEventBehavior.java:95) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnBufferedMessageStartEventBehavior.correlateNextBufferedMessage(BpmnBufferedMessageStartEventBehavior.java:76) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnBufferedMessageStartEventBehavior.correlateMessage(BpmnBufferedMessageStartEventBehavior.java:66) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.container.ProcessProcessor.lambda$getPostTransitionAction$10(ProcessProcessor.java:212) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat java.util.Optional.ifPresent(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.container.ProcessProcessor.lambda$getPostTransitionAction$11(ProcessProcessor.java:210) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.util.Either$Right.ifRightOrLeft(Either.java:381) ~[zeebe-util-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.container.ProcessProcessor.transitionTo(ProcessProcessor.java:182) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.container.ProcessProcessor.onChildTerminated(ProcessProcessor.java:167) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.container.ProcessProcessor.onChildTerminated(ProcessProcessor.java:27) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnStateTransitionBehavior.lambda$onElementTerminated$5(BpmnStateTransitionBehavior.java:424) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnStateTransitionBehavior.invokeElementContainerIfPresent(BpmnStateTransitionBehavior.java:474) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnStateTransitionBehavior.onElementTerminated(BpmnStateTransitionBehavior.java:419) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.task.JobWorkerTaskProcessor.lambda$onTerminate$10(JobWorkerTaskProcessor.java:97) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat java.util.Optional.ifPresentOrElse(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.task.JobWorkerTaskProcessor.onTerminate(JobWorkerTaskProcessor.java:86) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.task.JobWorkerTaskProcessor.onTerminate(JobWorkerTaskProcessor.java:25) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.processEvent(BpmnStreamProcessor.java:153) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.lambda$processRecord$0(BpmnStreamProcessor.java:92) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.util.Either$Right.ifRightOrLeft(Either.java:381) ~[zeebe-util-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.processRecord(BpmnStreamProcessor.java:89) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.engine.Engine.process(Engine.java:127) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.batchProcessing(ProcessingStateMachine.java:353) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$2(ProcessingStateMachine.java:269) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run(ZeebeTransaction.java:98) ~[zeebe-db-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand(ProcessingStateMachine.java:269) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord(ProcessingStateMachine.java:228) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.readNextRecord(ProcessingStateMachine.java:204) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:109) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:204) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\r\n\"\r\n</pre>\r\n</details>\r\n\r\n**Environment:**\r\n- OS: <!-- [e.g. Linux] -->\r\n- Zeebe Version: 8.3.0-alpha6\r\n- Configuration: <!-- [e.g. exporters etc.] -->\r\n\n\n Zelldon: I put the severity to high since I think we should take a look asap since it seems to be a regression due to our latest changes. I was not putting it as critical since I was not able to reproduce it easily, so there might be some weird circumstances that this happens not sure yet.\n nicpuppa: This error seems to be related to this method `processState.getLatestProcessVersionByProcessId()`. This method is somehow implicated also in those bugs #14309 and #14055 \r\n\r\n@Zelldon why do you think is related to process cancellation ?\n Zelldon: @nicpuppa because of the stacktrace see above and the related [log](https://console.cloud.google.com/logs/query;query=error_group%2528%22CJLK28eniZi1JA%22%2529%0AlogName:%22stdout%22%0Aresource.type%3D%22k8s_container%22%0Aresource.labels.namespace_name%3D%227832669d-388f-4728-8e2d-d4426c350cd4-zeebe%22%0Aresource.labels.cluster_name%3D%22prod-worker-4%22%0Aresource.labels.pod_name%3D%22zeebe-2%22%0Aresource.labels.container_name%3D%22zeebe%22%0Aresource.labels.location%3D%22australia-southeast1%22%0Aresource.labels.project_id%3D%22camunda-cloud-240911%22;cursorTimestamp=2023-09-19T11:42:39.114714745Z;startTime=2023-09-19T11:14:14.453Z;endTime=2023-09-19T12:14:14.453Z?project=camunda-cloud-240911)\r\n\r\n```\r\nERROR 2023-09-19T11:42:39.114714745Z [resource.labels.containerName: zeebe] Expected to process record 'TypedRecordImpl{metadata=RecordMetadata{recordType=COMMAND, valueType=PROCESS_INSTANCE, intent=CANCEL, authorization=UNKNOWN}, value={\"bpmnElementType\":\"UNSPECIFIED\",\"elementId\":\"\",\"bpmnProcessId\":\"\",\"version\":-1,\"processDefinitionKey\":-1,\"processInstanceKey\":-1,\"flowScopeKey\":-1,\"bpmnEventType\":\"UNSPECIFIED\",\"parentProcessInstanceKey\":-1,\"parentElementInstanceKey\":-1}}' without errors, but exception occurred with message 'Cannot invoke \"io.camunda.zeebe.engine.state.deployment.DeployedProcess.getKey()\" because \"process\" is null'.\r\n  {\r\n    \"insertId\": \"0d4cq88vzwi3t76q\",\r\n    \"jsonPayload\": {\r\n      \"serviceContext\": {\r\n        \"version\": \"8.3.0-alpha6\",\r\n        \"service\": \"zeebe\"\r\n      },\r\n      \"message\": \"Expected to process record 'TypedRecordImpl{metadata=RecordMetadata{recordType=COMMAND, valueType=PROCESS_INSTANCE, intent=CANCEL, authorization=UNKNOWN}, value={\\\"bpmnElementType\\\":\\\"UNSPECIFIED\\\",\\\"elementId\\\":\\\"\\\",\\\"bpmnProcessId\\\":\\\"\\\",\\\"version\\\":-1,\\\"processDefinitionKey\\\":-1,\\\"processInstanceKey\\\":-1,\\\"flowScopeKey\\\":-1,\\\"bpmnEventType\\\":\\\"UNSPECIFIED\\\",\\\"parentProcessInstanceKey\\\":-1,\\\"parentElementInstanceKey\\\":-1}}' without errors, but exception occurred with message 'Cannot invoke \\\"io.camunda.zeebe.engine.state.deployment.DeployedProcess.getKey()\\\" because \\\"process\\\" is null'.\",\r\n      \"@type\": \"type.googleapis.com/google.devtools.clouderrorreporting.v1beta1.ReportedErrorEvent\",\r\n      \"exception\": \"java.lang.NullPointerException: Cannot invoke \\\"io.camunda.zeebe.engine.state.deployment.DeployedProcess.getKey()\\\" because \\\"process\\\" is null\\n\\tat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnBufferedMessageStartEventBehavior.findNextMessageToCorrelate(BpmnBufferedMessageStartEventBehavior.java:95) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnBufferedMessageStartEventBehavior.correlateNextBufferedMessage(BpmnBufferedMessageStartEventBehavior.java:76) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnBufferedMessageStartEventBehavior.correlateMessage(BpmnBufferedMessageStartEventBehavior.java:66) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.engine.processing.bpmn.container.ProcessProcessor.lambda$getPostTransitionAction$10(ProcessProcessor.java:212) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat java.util.Optional.ifPresent(Unknown Source) ~[?:?]\\n\\tat io.camunda.zeebe.engine.processing.bpmn.container.ProcessProcessor.lambda$getPostTransitionAction$11(ProcessProcessor.java:210) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.util.Either$Right.ifRightOrLeft(Either.java:381) ~[zeebe-util-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.engine.processing.bpmn.container.ProcessProcessor.transitionTo(ProcessProcessor.java:182) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.engine.processing.bpmn.container.ProcessProcessor.onChildTerminated(ProcessProcessor.java:167) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.engine.processing.bpmn.container.ProcessProcessor.onChildTerminated(ProcessProcessor.java:27) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnStateTransitionBehavior.lambda$onElementTerminated$5(BpmnStateTransitionBehavior.java:424) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnStateTransitionBehavior.invokeElementContainerIfPresent(BpmnStateTransitionBehavior.java:474) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnStateTransitionBehavior.onElementTerminated(BpmnStateTransitionBehavior.java:419) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.engine.processing.bpmn.task.JobWorkerTaskProcessor.lambda$onTerminate$10(JobWorkerTaskProcessor.java:97) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat java.util.Optional.ifPresentOrElse(Unknown Source) ~[?:?]\\n\\tat io.camunda.zeebe.engine.processing.bpmn.task.JobWorkerTaskProcessor.onTerminate(JobWorkerTaskProcessor.java:86) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.engine.processing.bpmn.task.JobWorkerTaskProcessor.onTerminate(JobWorkerTaskProcessor.java:25) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.processEvent(BpmnStreamProcessor.java:153) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.lambda$processRecord$0(BpmnStreamProcessor.java:92) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.util.Either$Right.ifRightOrLeft(Either.java:381) ~[zeebe-util-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.processRecord(BpmnStreamProcessor.java:89) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.engine.Engine.process(Engine.java:127) ~[zeebe-workflow-engine-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.batchProcessing(ProcessingStateMachine.java:353) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$2(ProcessingStateMachine.java:269) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run(ZeebeTransaction.java:98) ~[zeebe-db-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand(ProcessingStateMachine.java:269) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord(ProcessingStateMachine.java:228) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.readNextRecord(ProcessingStateMachine.java:204) ~[zeebe-stream-platform-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:109) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:204) [zeebe-scheduler-8.3.0-alpha6.jar:8.3.0-alpha6]\\n\",\r\n      \"context\": {\r\n        \"threadName\": \"zb-actors-1\",\r\n        \"loggerName\": \"io.camunda.zeebe.broker.process\",\r\n        \"threadId\": 16,\r\n        \"partitionId\": \"2\",\r\n        \"threadPriority\": 5,\r\n        \"actor-name\": \"StreamProcessor-2\",\r\n        \"actor-scheduler\": \"Broker-2\"\r\n      }\r\n    },\r\n    \"resource\": {\r\n      \"type\": \"k8s_container\",\r\n      \"labels\": {\r\n        \"container_name\": \"zeebe\",\r\n        \"pod_name\": \"zeebe-2\",\r\n        \"location\": \"australia-southeast1\",\r\n        \"namespace_name\": \"7832669d-388f-4728-8e2d-d4426c350cd4-zeebe\",\r\n        \"cluster_name\": \"prod-worker-4\",\r\n        \"project_id\": \"camunda-cloud-240911\"\r\n      }\r\n    },\r\n    \"timestamp\": \"2023-09-19T11:42:39.114714745Z\",\r\n    \"severity\": \"ERROR\",\r\n    \"labels\": {\r\n      \"k8s-pod/app_kubernetes_io/app\": \"zeebe\",\r\n      \"k8s-pod/controller-revision-hash\": \"zeebe-5549d9794b\",\r\n      \"k8s-pod/app_kubernetes_io/component\": \"gateway\",\r\n      \"compute.googleapis.com/resource_name\": \"gke-prod-worker-4-spot-v1-fdaca0c3-n1r2\",\r\n      \"k8s-pod/cloud_camunda_io/channel\": \"Alpha\",\r\n      \"k8s-pod/cloud_camunda_io/salesPlan\": \"Trial_with_Low_Hardware\",\r\n      \"k8s-pod/cloud_camunda_io/clusterPlan\": \"G3_Trial\",\r\n      \"k8s-pod/statefulset_kubernetes_io/pod-name\": \"zeebe-2\",\r\n      \"k8s-pod/cloud_camunda_io/orgId\": \"5b5e6afa_7ba5_455e_9aa1_4b2ad66d5cd2\",\r\n      \"k8s-pod/cloud_camunda_io/generation\": \"Camunda_Platform_8_3_0_alpha6\",\r\n      \"k8s-pod/cloud_camunda_io/internalSalesPlan\": \"false\",\r\n      \"k8s-pod/cloud_camunda_io/clusterPlanType\": \"Trial_Cluster\",\r\n      \"k8s-pod/cloud_camunda_io/salesPlanType\": \"trial\"\r\n    },\r\n    \"logName\": \"projects/camunda-cloud-240911/logs/stdout\",\r\n    \"sourceLocation\": {\r\n      \"file\": \"Engine.java\",\r\n      \"line\": \"170\",\r\n      \"function\": \"handleUnexpectedError\"\r\n    },\r\n    \"receiveTimestamp\": \"2023-09-19T11:42:44.167981019Z\"\r\n  }\r\n```\r\n\r\n\n nicpuppa: @Zelldon thank you for clarifying\n remcowesterhoud: I wonder if this could have to do with deleting a process definition. I checked the related cluster and saw that one of the definition was deleted. The process version management was modified for this topic, so that aligns with the `processState.getLatestProcessVersionByProcessId()` being involved.\r\n\r\nI tried reproducing it this morning but had no luck.\n nicpuppa: @remcowesterhoud I noticed that the record value is a bit strange \r\n\r\n```\r\nvalue{\"bpmnElementType\":\"UNSPECIFIED\",\"elementId\":\"\",\"bpmnProcessId\":\"\",\"version\":-1,\"processDefinitionKey\":-1,\"processInstanceKey\":-1,\"flowScopeKey\":-1,\"bpmnEventType\":\"UNSPECIFIED\",\"parentProcessInstanceKey\":-1,\"parentElementInstanceKey\":-1}\r\n```\r\n\r\n Seems like an empty record\n remcowesterhoud: > @remcowesterhoud I noticed that the record value is a bit strange\r\n> \r\n> ```\r\n> value{\"bpmnElementType\":\"UNSPECIFIED\",\"elementId\":\"\",\"bpmnProcessId\":\"\",\"version\":-1,\"processDefinitionKey\":-1,\"processInstanceKey\":-1,\"flowScopeKey\":-1,\"bpmnEventType\":\"UNSPECIFIED\",\"parentProcessInstanceKey\":-1,\"parentElementInstanceKey\":-1}\r\n> ```\r\n> \r\n> Seems like an empty record\r\n\r\nYea I noticed that, but this is a red herring. It appears when cancelling a process instance we set process instance key as the command key. All the details of the record are left empty.\n korthout: ZPA Triage:\n- should be investigated before the 8.3 release\n- shares a potential root cause with #14309 ",
    "title": "Cancel command fails because process is null (NPE)"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14146",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nWhen we delete a DRG it is not removed from the cache. This means it remains available until the broker is restarted. We must make sure we delete the DRG from the cache.\r\n\r\nThe unit test supposed to verify this contains a bug:\r\n\r\n```java\r\n    assertThat(\r\n        decisionState\r\n            .findDecisionsByDecisionRequirementsKey(drg1.getDecisionRequirementsKey())\r\n            .isEmpty());\r\n```\r\n\r\nThe parentheses are off. This means this assertion doesn't actually assert anything. It's the equivalent of `assertThat(false)` at the moment.\r\n\r\n\n",
    "title": "Remove DRG from cache upon deletion"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14047",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nIt looks like our current benchmark starter and worker don't work anymore with SaaS clusters. \r\n\r\n@hisImminence tried to use them for benchmarking cluster plans and ran into several issues. I was able to reproduce the same as well. We were only able to overcome this by using an old version.\r\n\r\n```\r\n          image: gcr.io/zeebe-io/starter:8.1.8\r\n```\r\n\r\nWould be great if we could fix this since we also need these applications for upcoming game days etc.\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n**To Reproduce**\r\n\r\n1. Create a cluster in SaaS\r\n2. Setup [a cloud benchmark ](https://github.com/camunda/zeebe/blob/main/benchmarks/setup/newCloudBenchmark.sh)\r\n3. Create credentials\r\n4. Update the credentials file\r\n5. Deploy benchmark via `make secret starter worker` \r\n6. Observe\r\n\r\nAsk @hisImminence for more input\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\n**Expected behavior**\r\n\r\nThe starter and worker can connect without issues.\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\nSee example [log](https://console.cloud.google.com/logs/query;query=resource.type%3D%22k8s_container%22%0Aresource.labels.project_id%3D%22zeebe-io%22%0Aresource.labels.location%3D%22europe-west1-b%22%0Aresource.labels.cluster_name%3D%22zeebe-cluster%22%0Aresource.labels.namespace_name%3D%22ck-immi-test%22%0Alabels.k8s-pod%2Fapp%3D%22starter%22;pinnedLogId=2023-08-29T13:10:00.033238529Z%2F103mjwp1vy2i8imu;cursorTimestamp=2023-08-29T13:10:00.033238529Z;startTime=2023-08-29T12:58:16.638597Z;endTime=2023-08-29T13:29:19.104Z?project=zeebe-io) \r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\n\r\nio.camunda.zeebe.client.api.command.ClientStatusException: http2 exception\r\n\tat io.camunda.zeebe.client.impl.ZeebeClientFutureImpl.transformExecutionException(ZeebeClientFutureImpl.java:116) ~[zeebe-client-java-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:54) ~[zeebe-client-java-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.App.printTopology(App.java:87) [classes/:?]\r\n\tat io.camunda.zeebe.Starter.run(Starter.java:57) [classes/:?]\r\n\tat io.camunda.zeebe.App.createApp(App.java:55) [classes/:?]\r\n\tat io.camunda.zeebe.Starter.main(Starter.java:227) [classes/:?]\r\nCaused by: java.util.concurrent.ExecutionException: io.grpc.StatusRuntimeException: INTERNAL: http2 exception\r\n\tat java.util.concurrent.CompletableFuture.reportGet(Unknown Source) ~[?:?]\r\n\tat java.util.concurrent.CompletableFuture.get(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.client.impl.ZeebeClientFutureImpl.join(ZeebeClientFutureImpl.java:52) ~[zeebe-client-java-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\t... 4 more\r\nCaused by: io.grpc.StatusRuntimeException: INTERNAL: http2 exception\r\n\tat io.grpc.Status.asRuntimeException(Status.java:537) ~[grpc-api-1.57.2.jar:1.57.2]\r\n\tat io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:491) ~[grpc-stub-1.57.2.jar:1.57.2]\r\n\tat io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39) ~[grpc-api-1.57.2.jar:1.57.2]\r\n\tat io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23) ~[grpc-api-1.57.2.jar:1.57.2]\r\n\tat io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40) ~[grpc-api-1.57.2.jar:1.57.2]\r\n\tat io.micrometer.core.instrument.binder.grpc.MetricCollectingClientCallListener.onClose(MetricCollectingClientCallListener.java:57) ~[micrometer-core-1.11.3.jar:1.11.3]\r\n\tat io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:567) ~[grpc-core-1.57.2.jar:1.57.2]\r\n\tat io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:71) ~[grpc-core-1.57.2.jar:1.57.2]\r\n\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:735) ~[grpc-core-1.57.2.jar:1.57.2]\r\n\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:716) ~[grpc-core-1.57.2.jar:1.57.2]\r\n\tat io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37) ~[grpc-core-1.57.2.jar:1.57.2]\r\n\tat io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133) ~[grpc-core-1.57.2.jar:1.57.2]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]\r\n\tat java.lang.Thread.run(Unknown Source) ~[?:?]\r\nCaused by: io.netty.handler.codec.http2.Http2Exception: First received frame was not SETTINGS. Hex dump for first 5 bytes: 485454502f\r\n\tat io.netty.handler.codec.http2.Http2Exception.connectionError(Http2Exception.java:109) ~[netty-codec-http2-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.handler.codec.http2.Http2ConnectionHandler$PrefaceDecoder.verifyFirstFrameIsSettings(Http2ConnectionHandler.java:353) ~[netty-codec-http2-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.handler.codec.http2.Http2ConnectionHandler$PrefaceDecoder.decode(Http2ConnectionHandler.java:247) ~[netty-codec-http2-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.handler.codec.http2.Http2ConnectionHandler.decode(Http2ConnectionHandler.java:453) ~[netty-codec-http2-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:529) ~[netty-codec-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:468) ~[netty-codec-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290) ~[netty-codec-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]\r\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]\r\n\tat java.lang.Thread.run(Unknown Source) ~[?:?]\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n\r\n- Zeebe Version: SNAPSHOT?\r\n- Configuration: SaaS + benchmark apps\r\n\n\n Zelldon: BTW searching for that error it looks like we use HTTP instead of HTTPS? Did we changed anything in the clients in this regard?\r\n\r\n\r\nMight be an issue for the upcoming version, which we should check rather soon.\r\n\r\nRelated findings: \r\n\r\n* https://github.com/grpc/grpc-java/issues/2905#issuecomment-293625616\r\n* https://stackoverflow.com/a/73515627/2165134\r\n\r\n\n npepinpe: It looks like the client builder is fixed to use plaintext instead of properly connecting to SaaS. This you configure the worker/starter to use TLS or not?\n Zelldon: Before it just worked when setting the credentials, so this changed?\n npepinpe: From what I can tell, the benchmark apps haven't changed since 8.1.8, so maybe it's the client? :shrug: \r\n\r\nDid we use to enable TLS automatically if you had credentials maybe? I'd have to check.\n npepinpe: I'd be curious if you just tried setting the credentials/env vars, and then also the TLS application config if it works.\n Zelldon: > Did we use to enable TLS automatically if you had credentials maybe? I'd have to check.\r\n\r\nThis is how it worked before afaik.\n npepinpe: While this may be a regression if that previous behavior has changed, I'm setting this as low severity since it's just a matter of properly configuring it. Could you confirm that with proper configuration it works @Zelldon ? If not, then we can treat it as higher severity.\r\n\r\nSo for now I'm putting this in the backlog until then.\n Zelldon: TBH that is quite problematic, since it is totally unclear to the user what is the problem it took me quite a long time to understand and find the root cause.\n megglos: there is actually a config [for tls which is set to enabled](https://github.com/camunda/zeebe/blob/main/benchmarks/setup/cloud-default/starter.yaml#L26) I will check why it isn't used properly.\r\n\r\nThis impacted QA when they wanted to make use of the benchmark for generating datasets.",
    "title": "Snapshot version of  benchmark application (starter/worker) doesn't work with SaaS"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14028",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n@philippfromme discovered that a link throw event can activate a link catch event inside an (event) subprocess. See the following process.  \r\n\r\n![Screenshot from 2023-08-28 09-41-57](https://github.com/camunda/zeebe/assets/4305769/58753ebb-d565-4a25-86ab-e39e30023139)\r\n\r\nAccording to the BPMN specification and the Camunda documentation, it should not be possible to activate a link catch event that is not in the same scope as the link throw event. Both events must be in the same scope.\r\n\r\nThis issue is kind of related to https://github.com/camunda/zeebe/issues/10854.\r\n\r\n**To Reproduce**\r\n\r\n1. Deploy the following process \r\n[link-event-subprocess.bpmn](https://github.com/camunda/zeebe/files/12452163/link-event-subprocess.bpmn.txt)\r\n2. Create a new instance of the process\r\n3. Verify that the link catch event is activated\r\n\r\n**Expected behavior**\r\n\r\nThe link catch event inside the (event) subprocess is not activated.\r\n\r\nThe deployment of the process is rejected because there is no link catch event in the same scope.\r\n\r\n**Log/Stacktrace**\r\n\r\n<details><summary>Output from Zeebe-Play</summary>\r\n <p>\r\n\r\n![Screenshot from 2023-08-28 10-22-54](https://github.com/camunda/zeebe/assets/4305769/cc3cbd5d-9e0e-4e7b-97b7-7645e9663265)\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- OS: <!-- [e.g. Linux] -->\r\n- Zeebe Version: `8.2.0`\r\n- Configuration: <!-- [e.g. exporters etc.] -->\r\n\n\n lzgabel: Hi @saig0. You can assign this task to me and I'll take a look. :bow:\n saig0: @lzgabel awesome. :rocket: Thank you for your engagement. :bow: ",
    "title": "Should not activate link catch event in subprocess"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13881",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nUpgrading a Zeebe cluster to a new version that includes fixes for https://github.com/camunda/zeebe/issues/12797 and https://github.com/camunda/zeebe/issues/13041, entries in the job deadline column family might be leaked, causing repeated error logs that a corresponding job cannot be found.\r\nThe cause is a change in behavior where we now expect no duplicate deadline entries and don't clean up duplicated or orphaned entries. \r\n\r\n**To Reproduce**\r\n\r\nUse 8.2.8, force the creation of duplicated deadline entries (probably happens with a long processing queue) and then upgrade to 8.2.9 or later.\r\n\r\n**Expected behavior**\r\n\r\nA migration cleans up orphaned entries so that they are removed from the state and do not cause error logs forever.\r\n\r\n**Environment:**\r\nZeebe <=8.2.8 upgrades to > 8.2.8\r\nZeebe <= 8.1.? upgrades to > 8.1.?\r\nZeebe <= 8.0.? upgrades to > 8.0.? \n",
    "title": "Upgrading leaves deadline entries without jobs"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13867",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nA `ConcurrentModificationException` occurs when clearing obsolete job activation (long polling) requests. The exception is thrown when clearing the `activeRequests` LinkedList which isn't thread-safe ([code](https://github.com/camunda/zeebe/blob/8ef7c6ffd293ab11c9d2f2ee2f79f98b47941d1d/gateway/src/main/java/io/camunda/zeebe/gateway/impl/job/InFlightLongPollingActivateJobsRequestsState.java#L77)).\r\n\r\n**To Reproduce**\r\n\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\n**Expected behavior**\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nA `ConcurrentModificationException` doesn't occur. \r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.util.ConcurrentModificationException: null\r\n\r\nat java.util.LinkedList$ListItr.checkForComodification\r\nat java.util.LinkedList$ListItr.remove\r\nat java.util.Collection.removeIf\r\nat io.camunda.zeebe.gateway.impl.job.InFlightLongPollingActivateJobsRequestsState.removeObsoleteRequestsAndUpdateMetrics ( io/camunda.zeebe.gateway.impl.job/InFlightLongPollingActivateJobsRequestsState.java:77 )\r\nat io.camunda.zeebe.gateway.impl.job.InFlightLongPollingActivateJobsRequestsState.getPendingRequests ( io/camunda.zeebe.gateway.impl.job/InFlightLongPollingActivateJobsRequestsState.java:71 )\r\nat io.camunda.zeebe.gateway.impl.job.LongPollingActivateJobsHandler.resetFailedAttemptsAndHandlePendingRequests ( io/camunda.zeebe.gateway.impl.job/LongPollingActivateJobsHandler.java:245 )\r\nat io.camunda.zeebe.gateway.impl.job.LongPollingActivateJobsHandler.lambda$onNotification$6 ( io/camunda.zeebe.gateway.impl.job/LongPollingActivateJobsHandler.java:181 )\r\nat io.camunda.zeebe.scheduler.ActorJob.invoke ( io/camunda.zeebe.scheduler/ActorJob.java:92 )\r\nat io.camunda.zeebe.scheduler.ActorJob.execute ( io/camunda.zeebe.scheduler/ActorJob.java:45 )\r\nat io.camunda.zeebe.scheduler.ActorTask.execute ( io/camunda.zeebe.scheduler/ActorTask.java:119 )\r\nat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask ( io/camunda.zeebe.scheduler/ActorThread.java:109 )\r\nat io.camunda.zeebe.scheduler.ActorThread.doWork ( io/camunda.zeebe.scheduler/ActorThread.java:87 )\r\nat io.camunda.zeebe.scheduler.ActorThread.run ( io/camunda.zeebe.scheduler/ActorThread.java:204 )\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- OS: <!-- [e.g. Linux] --> Linux\r\n- Zeebe Version: <!-- [e.g. 0.20.0] --> 8.3.0-SNAPSHOT\r\n- Configuration: <!-- [e.g. exporters etc.] -->\r\n- Error occured in CW32 benchmark ([src](https://console.cloud.google.com/logs/query;query=error_group%2528%22CKD6icXrpN_oLA%22%2529%0AlogName:%22stdout%22%0Aresource.type%3D%22k8s_container%22%0Aresource.labels.pod_name%3D%22medic-y-2023-cw-32-1f55f22-benchmark-zeebe-gateway-5fbb459ctcwr%22%0Aresource.labels.container_name%3D%22zeebe-gateway%22%0Aresource.labels.location%3D%22europe-west1-b%22%0Aresource.labels.project_id%3D%22zeebe-io%22%0Aresource.labels.cluster_name%3D%22zeebe-cluster%22%0Aresource.labels.namespace_name%3D%22medic-y-2023-cw-32-1f55f22-benchmark%22;cursorTimestamp=2023-08-10T20:21:27.752805343Z;startTime=2023-08-10T19:51:57.752Z;endTime=2023-08-10T20:51:57.752Z?project=zeebe-io)).\r\n\n\n Zelldon: This seems to be something new introduced, maybe due to some changes to the job stuff ? @npepinpe \r\n\r\nI can see a trial cluster which has more than 1k occurrences of this. I think if this happens the client can't activate jobs.\r\n\r\n\r\n![occur2](https://github.com/camunda/zeebe/assets/2758593/ece83e92-2157-4bdc-b77f-90e0d3b5fdd6)\r\n![occur1](https://github.com/camunda/zeebe/assets/2758593/bb9d1679-aa39-4707-bb7c-4a435e5d5c98)\r\n![activate](https://github.com/camunda/zeebe/assets/2758593/a71587c3-4180-4512-92e2-eafbc59bac34)\r\n\r\n\r\n* https://console.cloud.google.com/errors/detail/CLu0t_7q98T1pQE;service=zeebe;time=P7D?project=camunda-cloud-240911\r\n* https://console.cloud.google.com/errors/detail/CM-6h67j1ejk0AE;service=zeebe;time=P7D?project=camunda-cloud-240911\r\n\r\n\r\nI would mark it as critical so we take a look asap (since I think this bug has newly introduced and blocks the client to make progress). If we find out and think it is less of a problem we can also decrease the severity again.\n npepinpe: Yes, my bad, we have a state modification in a callback that gets executed on the gRPC executor and not the actor itself =/\r\n\r\nThis was merged 11th of July though, and AFAIK not back ported, so I'm unsure how a trial cluster would have it. Can they use alpha versions?\n Zelldon: > This was merged 11th of July though, and AFAIK not back ported, so I'm unsure how a trial cluster would have it. Can they use alpha versions?\r\n\r\nYes I think there was a decision to get them used alpha versions.\n npepinpe: Fix: https://github.com/camunda/zeebe/pull/13875\r\n\r\nNot really sure how to write a good regression test for that =/\n Zelldon: Unfortunately, this is part of alpha4 https://console.cloud.google.com/errors/detail/CKaE2PeurPD7Ng;service=zeebe;time=P7D?project=camunda-cloud-240911. Luckily this is already fixed thanks to @npepinpe \r\n\n Zelldon: @npepinpe wondering whether bot are fixed not the ConcurrentModification and the index out of bounce, but I think so there were related right?\n npepinpe: Most likely, since we were modifying non-thread-safe structures concurrently :+1: ",
    "title": "ConcurrentModificationException when clearing obsolete job activation requests"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13814",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nIt seems right now job streaming may unnecessary trigger job polling when a job is handled/completed. This is because we call `JobWorkerImpl#handleJobFinished`, which then decrements `remainingJob` (wrong) and then might trigger polling.\r\n\r\n**To Reproduce**\r\n\r\nJust use job streaming :upside_down_face: \r\n\r\n**Expected behavior**\r\n\r\nWe do not trigger unnecessary polling and do not touch the `remainingJobs` counter.\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.3.0-alpha4\r\n\n",
    "title": "Job streaming may trigger unnecessary polling"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13796",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nAn exception is thrown whenever the `JobStreamRemover` tries to remove a stream in the gateway. This is due to the future completing without an executor, thus completing within the actor context, and then calling `Actor.call`. As we already wanted to ensure that an executor was used, we should do that as well.\r\n\r\n**To Reproduce**\r\n\r\nRegister a stream via the command. Cancel it. An exception is thrown and the stream is not removed from the gateway nor the broker (even if the client has gone away).\r\n\r\n**Expected behavior**\r\n\r\nThe stream is removed and no error is thrown.\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.3.0-alpha4\r\n\n",
    "title": "IllegalStateArgument when removing job stream"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13787",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Description**\r\n\r\nI noticed this during the release, on 8.0/8.1 building starter & worker fails as e.g. the maven wrapper is not present on 8.1\r\n```\r\n/home/runner/work/_temp/79d164a7-e6b4-4e02-9e4e-89a3d8a459e8.sh: line 1: ./mvnw: No such file or directory\r\nError: Process completed with exit code 127.\r\n```\r\nhttps://github.com/camunda/zeebe/actions/runs/5738456998/job/15552373143\r\n\r\nOn 8.0 also build zeebe fails as there is no DIST=`build` setup in the Dockerfile https://github.com/camunda/zeebe/actions/runs/5202088373/job/14079846461\r\n\r\n\r\n```[tasklist]\n### Tasks\n- [x] backport maven wrapper to 8.0/1 - to avoid workflow merge conflicts from main to stable\n- [x] Add a `benchmark.yaml` workflow to each stable branch - to maintain a stable setup\n- [x] trigger the workflows via [workflow dispatch](https://docs.github.com/en/free-pro-team@latest/rest/actions/workflows?apiVersion=2022-11-28#create-a-workflow-dispatch-event) from the https://github.com/zeebe-io/zeebe-engineering-processes referencing the release_branch\n- [x] delete the `dispatch-benchmark.yaml` workflow from main\n```\r\n\r\n\r\n\n\n megglos: ZDP-Triage:\n- will look into it asap to resolve\n remcowesterhoud: Individual triage:\r\n- Seems to be actively worked on by ZDP at this time. It's not sensible to work on this simultaneously so I'll move it to the ZPA backlog. This also allows us to focus multi-tenancy and migration.",
    "title": "Release: `Repo dispatch Benchmark` fails on 8.0/8.1"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13715",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "See https://github.com/camunda/zeebe/actions/runs/5709567673\r\n```\r\nThe workflow is not valid. In .github/workflows/release-main-dry-run.yml (Line: 11, Col: 11): Error from called workflow camunda/zeebe/.github/workflows/release.yml@bc32ea937f8d3650b657431c70d22bae9339ba3c (Line: 329, Col: 12): Unrecognized named-value: 'env'. Located at position 1 within expression: env.RELEASE_BRANCH\r\n```\n\n megglos: https://github.com/actions/runner/issues/2372",
    "title": "Release Dry fails because of unrecognized argument"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13650",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n`ZeebePartitionHealth` repeatedly calls listeners and logs a change of health status when the health has not actually changed.\r\nThis has existed for a long time but was made worse with #13042 where we now update the health status more frequently.\r\n\r\nThis is the bug: https://github.com/camunda/zeebe/blob/c49d14b7eb8052895aa813c9884850ae9f590a2f/broker/src/main/java/io/camunda/zeebe/broker/system/partitions/ZeebePartitionHealth.java#L60\r\n\r\nHere we should actually compare the health reports and not check for identity.\r\n\r\n**Environment:**\r\nZeebe Version: 8.2.9\n",
    "title": "`ZeebePartitionHealth` repeatedly reports change of health status"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13521",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nIf activating a call activity with large variables causes processing to exceed the batch size, the complete process instance is banned and left in limbo.\r\n\r\nThe current workaround is to ensure that we don't run into this case; however, once you hit it, there's nothing you can do but recreate the instance with smaller payloads.\r\n\r\nSupport:\r\n-  https://jira.camunda.com/browse/SUPPORT-17661\r\n- https://jira.camunda.com/browse/SUPPORT-17882\r\n\r\n**To Reproduce**\r\n\r\nCreate a process with a dummy service task which leads to a call activity. Once the task is activated, separately as to not exceed the batch size, create 4 large variables, e.g. of 1MB each (using the set variable command, one at a time). Then complete the task. Activating the call activity will fail and result in a banned instance.\r\n\r\n**Expected behavior**\r\n\r\nAn incident is raised, such that I can modify my variables to save this instance.\r\n\r\n**Log/Stacktrace**\r\n\r\nI've anonymized the stacktrace below. You can find more logs here: https://cloudlogging.app.goo.gl/aarrs1J5TY22X5gXA\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\nExpected to process record 'TypedRecordImpl{metadata=RecordMetadata{recordType=COMMAND, valueType=PROCESS_INSTANCE, intent=ACTIVATE_ELEMENT}, value={\"bpmnElementType\":\"CALL_ACTIVITY\",\"elementId\":\"Activity_1aba60i\",\"bpmnProcessId\":\"process\",\"version\":8,\"processDefinitionKey\":2251800414371946,\"processInstanceKey\":4503599979491273,\"flowScopeKey\":4503599979491273,\"bpmnEventType\":\"UNSPECIFIED\",\"parentProcessInstanceKey\":-1,\"parentElementInstanceKey\":-1}}' without errors, but exception occurred with message 'Can't append entry: 'RecordBatchEntry[recordMetadata=RecordMetadata{recordType=EVENT, valueType=VARIABLE, intent=CREATED}, key=4503599980117234, sourceIndex=-1, unifiedRecordValue={\"name\":\"foo\",\"value\":\"U2VkIHV0IHBlcnNwaWNpYXRpcyB1bmRlIG9tbmlzIGlzdGUgbmF0dXMgZXJyb3Igc2l0IHZvbHVwdGF0ZW0gYWNjdXNhbnRpdW0gZG9sb3JlbXF1ZSBsYXVkYW50aXVtLCB0b3RhbSByZW0gYXBlcmlhbSwgZWFxdWUgaXBzYSBxdWFlIGFiIGlsbG8gaW52ZW50b3JlIHZlcml0YXRpcyBldCBxdWFzaSBhcmNoaXRlY3RvIGJlYXRhZSB2aXRhZSBkaWN0YSBzdW50IGV4cGxpY2Fiby4gTmVtbyBlbmltIGlwc2FtIHZvbHVwdGF0ZW0gcXVpYSB2b2x1cHRhcyBzaXQgYXNwZXJuYXR1ciBhdXQgb2RpdCBhdXQgZnVnaXQsIHNlZCBxdWlhIGNvbnNlcXV1bnR1ciBtYWduaSBkb2xvcmVzIGVvcyBxdWkgcmF0aW9uZSB2b2x1cHRhdGVtIHNlcXVpIG5lc2NpdW50LiBOZXF1ZSBwb3JybyBxdWlzcXVhbSBlc3QsIHF1aSBkb2xvcmVtIGlwc3VtIHF1aWEgZG9sb3Igc2l0IGFtZXQsIGNvbnNlY3RldHVyLCBhZGlwaXNjaSB2ZWxpdCwgc2VkIHF1aWEgbm9uIG51bXF1YW0gZWl1cyBtb2RpIHRlbXBvcmEgaW5jaWR1bnQgdXQgbGFib3JlIGV0IGRvbG9yZSBtYWduYW0gYWxpcXVhbSBxdWFlcmF0IHZvbHVwdGF0ZW0uIFV0IGVuaW0gYWQgbWluaW1hIHZlbmlhbSwgcXVpcyBub3N0cnVtIGV4ZXJjaXRhdGlvbmVtIHVsbGFtIGNvcnBvcmlzIHN1c2NpcGl0IGxhYm9yaW9zYW0sIG5...' with size: 1725121 this would exceed the maximum batch size. [ currentBatchEntryCount: 18, currentBatchSize: 2874115]'.\r\n\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- OS: Ubuntu Focal\r\n- Zeebe Version: 8.2.8\r\n- Configuration: SaaS\r\n\n\n Zelldon: Related https://github.com/camunda/zeebe/issues/13016\n korthout: Thanks for raising this @npepinpe \n\nAs there is no good workaround available and the chance to run into this is reasonable, I'm prioritizing this as `upcoming`.\n\n>**Expected behavior**\n>An incident is raised, such that I can modify my variables to save this instance.\n\nTo resolve this, we'll need to handle errors in the `BpmnStreamProcessor`. However, we're unable to raise an incident for every process instance element type yet. So, we either need to pass the error handling along to the respective `BpmnElementProcessor` (i.e. `CallActivityProcessor`) or have element type-specific code in the BpmnStreamProcessor.\n\nSince this is not entirely trivial, I'm sizing this as `Medium`.\n oleschoenburg: This has come up in another support case: https://jira.camunda.com/browse/SUPPORT-17882\r\nIn this case, the variables weren't even that large - just around 300KiB. IMO this bug is pretty bad, effectively causing data loss. When we ban an instance, we can't recover it. Depending on the use case, this can be really severe. It's also hard to predict, I wouldn't have expected 300KiB variables to cause this.\r\n\r\nI've raised the priority to critical. IMO all cases where we ban instances have to be considered critical.",
    "title": "Process instances are banned when trying to activate a call activity with large variables"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13471",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n```\r\nINFO 2023-07-10T18:18:50.941085479Z [resource.labels.containerName: zeebe] Preparing transition from FOLLOWER on term 31 completed\r\nINFO 2023-07-10T18:18:50.941163945Z [resource.labels.containerName: zeebe] Transition to LEADER on term 31 starting\r\nINFO 2023-07-10T18:18:50.941238114Z [resource.labels.containerName: zeebe] Cancelling transition to LEADER on term 31\r\nWARNING 2023-07-10T18:18:50.941635348Z [resource.labels.containerName: zeebe] Uncaught exception in Broker-1-ZeebePartition-2.\r\n\"java.lang.NullPointerException: must specify a log stream\r\n\tat java.util.Objects.requireNonNull(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.broker.jobstream.RemoteJobStreamErrorHandlerService.onBecomingLeader(RemoteJobStreamErrorHandlerService.java:56) ~[zeebe-broker-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.broker.system.partitions.PartitionStartupAndTransitionContextImpl.lambda$notifyListenersOfBecomingLeader$4(PartitionStartupAndTransitionContextImpl.java:164) ~[zeebe-broker-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat java.util.stream.ReferencePipeline$3$1.accept(Unknown Source) ~[?:?]\r\n\tat java.util.ArrayList$ArrayListSpliterator.forEachRemaining(Unknown Source) ~[?:?]\r\n\tat java.util.stream.AbstractPipeline.copyInto(Unknown Source) ~[?:?]\r\n\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(Unknown Source) ~[?:?]\r\n\tat java.util.stream.ReduceOps$ReduceOp.evaluateSequential(Unknown Source) ~[?:?]\r\n\tat java.util.stream.AbstractPipeline.evaluate(Unknown Source) ~[?:?]\r\n\tat java.util.stream.ReferencePipeline.collect(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.broker.system.partitions.PartitionStartupAndTransitionContextImpl.notifyListenersOfBecomingLeader(PartitionStartupAndTransitionContextImpl.java:165) ~[zeebe-broker-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.broker.system.partitions.ZeebePartition.lambda$leaderTransition$6(ZeebePartition.java:243) ~[zeebe-broker-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.future.FutureContinuationRunnable.run(FutureContinuationRunnable.java:28) ~[zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94) ~[zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT] ... \r\nERROR 2023-07-10T18:18:50.942738393Z [resource.labels.containerName: zeebe] Failed to install partition 2\r\n```\r\n\r\nTransition to leader is cancelled (probably because there is a new leader already). But the `PartitionStartupAndTransitionContextImpl.lambda$notifyListenersOfBecomingLeader` is still invoked. Since the services were not installed, logstream is null and this causes NPE in the listener. \r\n\r\nPartition is inactive after that. However, the partition is still marked as healthy (See #13401) \r\n\r\n**Expected behavior**\r\n\r\nPartition listeners are only invoked if the transition is successfully completed.\r\n\r\n\r\n**Environment:**\r\n- Zeebe Version: Observed in 8.3.0-SNAPSHOT (medic-cw-27 benchmark)\r\n\n\n megglos: ZDP-Triage:\r\n- likelihood seems high - happened a couple of times in benchmarks already\r\n- bug may exist for way longer already - it was not as visible before though\r\n- 3-4 already existing issues might be caused by this\r\n- workaround would be a pod restart - but hard to identify given partition is still marked as healthy\n oleschoenburg: The issue is that listeners are called when the transition completes normally and that we complete the transition normally when it is actually cancelled: https://github.com/camunda/zeebe/blob/a9d810011935b0538ed6a97cab342b72681c54a4/broker/src/main/java/io/camunda/zeebe/broker/system/partitions/impl/PartitionTransitionProcess.java#L66-L71",
    "title": "PartitionListeners are notified even if the transition is cancelled causing NPE"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13431",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nWhen tls enabled on gateway for [secure client communication](https://docs.camunda.io/docs/next/self-managed/zeebe-deployment/security/secure-client-communication/), readiness check fails. \r\n\r\n<details><summary>Failed health status</summary>\r\n <p>\r\n\r\n```\r\n{\r\n   \"components\" : {\r\n      \"diskSpace\" : {\r\n         \"details\" : {\r\n           ....\r\n         },\r\n         \"status\" : \"UP\"\r\n      },\r\n      \"gatewayClusterAwareness\" : {\r\n         \"status\" : \"UP\"\r\n      },\r\n      \"gatewayPartitionLeaderAwareness\" : {\r\n         \"status\" : \"UP\"\r\n      },\r\n      \"gatewayResponsive\" : {\r\n         \"details\" : {\r\n            \"error\" : \"java.util.concurrent.ExecutionException: io.grpc.StatusRuntimeException: UNAVAILABLE: io exception\\nChannel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]\",\r\n            \"healthZeebeClientProperties\" : {\r\n               \"requestTimeout\" : \"PT0.5S\",\r\n               \"securityProperties\" : {}\r\n            },\r\n            \"timeOut\" : \"PT0.5S\"\r\n         },\r\n         \"status\" : \"DOWN\"\r\n      },\r\n      \"gatewayStarted\" : {\r\n         \"status\" : \"UP\"\r\n      },\r\n      \"livenessGatewayClusterAwareness\" : {\r\n         \"details\" : {\r\n            \"derivedFrom\" : \"ClusterAwarenessHealthIndicator\",\r\n            \"lastSeenDelegateHealthStatus\" : {\r\n               \"status\" : \"UP\"\r\n            },\r\n            \"maxDowntime\" : \"PT5M\",\r\n            \"wasEverUp\" : true\r\n         },\r\n         \"status\" : \"UP\"\r\n      },\r\n      \"livenessGatewayPartitionLeaderAwareness\" : {\r\n         \"details\" : {\r\n            \"derivedFrom\" : \"PartitionLeaderAwarenessHealthIndicator\",\r\n            \"lastSeenDelegateHealthStatus\" : {\r\n               \"status\" : \"UP\"\r\n            },\r\n            \"maxDowntime\" : \"PT5M\",\r\n            \"wasEverUp\" : true\r\n         },\r\n         \"status\" : \"UP\"\r\n      },\r\n      \"livenessGatewayResponsive\" : {\r\n         \"details\" : {\r\n            \"derivedFrom\" : \"ResponsiveHealthIndicator\",\r\n            \"lastSeenDelegateHealthStatus\" : {\r\n               \"details\" : {\r\n                  \"error\" : \"java.util.concurrent.ExecutionException: io.grpc.StatusRuntimeException: UNAVAILABLE: io exception\\nChannel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]\",\r\n                  \"healthZeebeClientProperties\" : {\r\n                     \"requestTimeout\" : \"PT5S\",\r\n                     \"securityProperties\" : {}\r\n                  },\r\n                  \"timeOut\" : \"PT5S\"\r\n               },\r\n               \"status\" : \"DOWN\"\r\n            },\r\n            \"maxDowntime\" : \"PT10M\",\r\n            \"wasEverUp\" : false\r\n         },\r\n         \"status\" : \"DOWN\"\r\n      },\r\n      \"livenessMemory\" : {\r\n         \"details\" : {\r\n            \"threshold\" : 0.01\r\n         },\r\n         \"status\" : \"UP\"\r\n      },\r\n      \"livenessState\" : {\r\n         \"status\" : \"UP\"\r\n      },\r\n      \"ping\" : {\r\n         \"status\" : \"UP\"\r\n      },\r\n      \"readinessState\" : {\r\n         \"status\" : \"UP\"\r\n      }\r\n   },\r\n   \"groups\" : [\r\n      \"liveness\",\r\n      \"readiness\",\r\n      \"startup\"\r\n   ],\r\n   \"status\" : \"DOWN\"\r\n}\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\nBoth `gatewayResponsive` and `livenssGatewayResponsive` fails because of some SSL error.   \r\n\r\nThis was unnoticed before because readiness check was disabled by default in camunda platform helm until 8.2. From 8.2 it is enabled by default. \r\n\r\n**To Reproduce**\r\n\r\nStart a single gateway with TLS enabled as documented [here](https://docs.camunda.io/docs/next/self-managed/zeebe-deployment/security/secure-client-communication/). And then query `http://gateway-host:9600/actuator/health`.\r\n\r\n**Expected behavior**\r\n\r\nHealth and readiness check should work with TLS enabled.\r\n\r\n**Environment:**\r\n- Zeebe Version: All versions (reproduced with 8.0.x, 8.1.5, and 8.2.7)\r\n\r\nrelated to https://jira.camunda.com/browse/SUPPORT-17529 \r\nhttps://jira.camunda.com/browse/SUPPORT-16945\n\n npepinpe: See https://github.com/camunda/zeebe/issues/11799#issuecomment-1558773952\r\n\r\nI guess it was already linked to a support issue. Workaround is to disable the check by setting the following env var:\r\n\r\n```\r\nMANAGEMENT_HEALTH_GATEWAY-RESPONSIVE_ENABLED=false\r\n```\n deepthidevaki: I will leave it open for easy visibility. I searched for open bugs for this, but couldn't find any.\r\n\r\nPlease close both issues when it is fixed. \n megglos: ZDP-Triage:\n- no functional impact on gateway itself\n- breaks the helm chart update as readiness check was recently enabled there\n- workaround is just disabling the specific check https://github.com/camunda/zeebe/issues/13431#issuecomment-1630928379\n- relates to https://github.com/camunda/zeebe/issues/11799\n- quick win could be removing this indicator from the readiness\n- may be better to execute a topologyrequest from k8s instead\n- let's revisit readiness and liveness\n- we need to check-in with the distribution team on this => we may release the workaround disabling this check asap @megglos \n\n megglos: Hey @deepthidevaki ,\r\n\r\nas I try to follow-up on this with the distribution team, the only official config exposed relating to TLS on the helm chart is enabling it on the ingress https://github.com/camunda/camunda-platform-helm/blob/main/charts/camunda-platform/values.yaml#L595-L600 .\r\n\r\nIn the linked support case TLS was enabled on the zeebe pod by the customer via setting `ZEEBE_GATEWAY_SECURITY_ENABLED` etc., so it’s kind of special and not a general issue/breaking change with the helm charts intended way to enable TLS. Where if tls is enabled on the ingress the check should still work as TLS terminates at the ingress and the health check directly accesses the pod endpoint, do I understand this correctly?\r\n\r\nI would then conclude no change to the helm chart is needed and assume likelihood of others hitting that regression is on the lower end ^^ If someone sets up TLS termination manually on the pod, they can make use of the workaround until this issue is resolved on the zeebe side.\n deepthidevaki: > the only official config exposed relating to TLS on the helm chart is enabling it on the ingress\r\n\r\n@megglos \r\nIs that the recommended way to do it? If so, we can also recommend it to the customer. Is there any documentation on it? The only [documentation](https://docs.camunda.io/docs/next/self-managed/zeebe-deployment/security/secure-client-communication/) I found is to configure it on Zeebe gateway. \r\n\r\n> I would then conclude no change to the helm chart is needed and assume likelihood of others hitting that regression is on the lower end ^^ If someone sets up TLS termination manually on the pod, they can make use of the workaround until this issue is resolved on the zeebe side.\r\n\r\n:+1:\n npepinpe: Can we just get rid of this `ResponsiveHealthIndicator`? I don't think it really purposefully demonstrates that it's responsive:\r\n\r\n1. The user can query the topology themselves to see if the service is working\r\n2. The user queries the health endpoint, which queries the topology, to see if the service is working\r\n\r\nIt just seems like extra steps. I understand that the idea is for things like load balancers to not send requests to an unresponsive gateway, but I would argue that it really doesn't bring that much value for the added complexity, since the gateway client has to be configured for all possible gateway configuration (TLS, authentication, etc.) Plus it only checks that the gRPC server serving the right service, but not things like \"can you actually send requests to brokers?\" or the likes. It doesn't even do anything with the topology itself :shrug: \n oleschoenburg: Maybe we should just implement the gRPC [health checking protocol](https://grpc.github.io/grpc/core/md_doc_health-checking.html) and then recommend [gRPC probes](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#probe-check-methods)? \n npepinpe: Surprise surprise, we already do serve the gRPC health check protocol! But it's very basic, only reporting whether the gateway started or is shutting down. But it'd be cool to extend it :+1: \n oleschoenburg: Ah TIL :+1: \r\nI guess this would not 100% replace the `ResponsiveHealthIndicator` anyway because we'd not do a topology request that way. \r\n\r\nI'm not attached to the `ResponsiveHealthIndicator` so just removing it for now also sounds good to me.\n megglos: :information_source: What determines the liveness/readiness of the Gateway as of now?\r\n\r\nreadiness: `StartedHealthIndicator`\r\n\r\n[liveness](https://github.com/camunda/zeebe/blob/main/dist/src/main/resources/application-gateway.properties#L9):\r\n`livenessGatewayResponsive` ==> ResponsiveHealthIndicator  (doing a topologyRequest on the grpc endpoint of the pod)\r\n`livenessGatewayClusterAwareness`  ==> gateway::getStatus\r\n`livenessGatewayPartitionLeaderAwareness`  ==> doing topologyManager.getTopology and checking if there is a leader known for every partition\r\n`livenessMemory`  => checking if JVM has at least 10% free memory (actually intended was >1%, according to the initial task https://github.com/camunda/zeebe/issues/4339 is that a bug? :smile: )\r\n\r\n**I see the following options going forward:**\r\n\r\n:one: Make sure users can configure the internal zeebe client to work properly in such scenarios (there is actually dedicated config present for tls and oauth config)\r\n:heavy_plus_sign: Pro:\r\n- the check’s semantics stay as they are\r\n\r\n:heavy_minus_sign: Con:\r\n- it seems still like a weird UX, I may have to configure a client for my gateway within the Gateway? Likely to be missed and causing questions and support cases\r\n- the gateway being able to connect to it’s own GRPC port does not mean that port is properly reachable from outside the pod\r\n\r\n:wrench: Effort:\r\n- improve docs, hoping users will pick that up without needing help :crossed_fingers:\r\n- potentially extending the helm chart for allowing to config the client\r\n\r\n:two: We remove the ResponsiveHealthIndicator as suggested by Nicolas \r\n\r\n:heavy_plus_sign: Pro:\r\n- less complexity in our liveness check, no side-effects of auth or transport encryption setups\r\n- grpc readiness/connectivity can be checked via a grpc_health_probe via the existing GRPC health endpoint which exists since this https://github.com/camunda/zeebe/pull/7737 (thanks to @npepinpe for raising that!)\r\n  - as of now there is no real readiness probe implemented, that was decided [here](https://github.com/camunda/zeebe/issues/4339#issuecomment-627277532), the ready endpoint is still used by e.g. the helm chart but it equals the startup probe semantics, we may follow-up using the grpc health check as readiness endpoint\r\n- cluster awareness and connectivity is already covered by the other checks\r\n\r\n:heavy_minus_sign: Con:\r\n- semantics of the actuator liveness probe changes, needs announcement\r\n- breaking change which should not be backported, we may still need to advice people to disable the check via `MANAGEMENT_HEALTH_GATEWAY-RESPONSIVE_ENABLED` if they encounter issues\r\n\r\n:wrench: Effort:\r\n- 💥 ResponsiveHealthIndicator\r\n- docs announcement + update guide\r\n- (follow-up) with controller and distribution team of using the grpc_probe as readiness\r\n",
    "title": "Gateway readiness fails when tls enabled"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13254",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nWhen the command to deploy processes failed with, for example, `EXCEEDED_BATCH_RECORD_SIZE`, any processes - that have been cached during the deployment process - are still kept in cache afterward. That way, clients can start process instances of these processes. Once a leader change happened (or the broker restarted), the Stream Process fails to replay these started process instances:\r\n\r\n```\r\njava.lang.RuntimeException: java.lang.IllegalStateException: Expected to find a process deployed with key '2251799813685405' but not found.\r\n\tat io.camunda.zeebe.stream.impl.ReplayStateMachine.lambda$replayNextEvent$4(ReplayStateMachine.java:169) ~[zeebe-stream-platform-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.future.FutureContinuationRunnable.run(FutureContinuationRunnable.java:33) ~[zeebe-scheduler-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94) ~[zeebe-scheduler-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) [zeebe-scheduler-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) [zeebe-scheduler-8.2.4.jar:8.2.4]\r\nCaused by: java.lang.IllegalStateException: Expected to find a process deployed with key '2251799813685405' but not found.\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.getFlowElement(DbProcessState.java:288) ~[zeebe-workflow-engine-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.engine.state.appliers.ProcessInstanceElementActivatingApplier.createEventScope(ProcessInstanceElementActivatingApplier.java:261) ~[zeebe-workflow-engine-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.engine.state.appliers.ProcessInstanceElementActivatingApplier.applyState(ProcessInstanceElementActivatingApplier.java:51) ~[zeebe-workflow-engine-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.engine.state.appliers.ProcessInstanceElementActivatingApplier.applyState(ProcessInstanceElementActivatingApplier.java:32) ~[zeebe-workflow-engine-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.engine.state.appliers.EventAppliers.applyState(EventAppliers.java:302) ~[zeebe-workflow-engine-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.engine.Engine.replay(Engine.java:110) ~[zeebe-workflow-engine-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.stream.impl.ReplayStateMachine.lambda$replayEvent$7(ReplayStateMachine.java:230) ~[zeebe-stream-platform-8.2.4.jar:8.2.4]\r\n\tat java.util.Optional.ifPresent(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.stream.impl.ReplayStateMachine.replayEvent(ReplayStateMachine.java:230) ~[zeebe-stream-platform-8.2.4.jar:8.2.4]\r\n\tat java.util.Iterator.forEachRemaining(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.stream.impl.ReplayStateMachine.lambda$tryToReplayBatch$5(ReplayStateMachine.java:208) ~[zeebe-stream-platform-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run(ZeebeTransaction.java:84) ~[zeebe-db-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.stream.impl.ReplayStateMachine.tryToReplayBatch(ReplayStateMachine.java:206) ~[zeebe-stream-platform-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.stream.impl.ReplayStateMachine.lambda$replayNextEvent$3(ReplayStateMachine.java:165) ~[zeebe-stream-platform-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.retry.ActorRetryMechanism.run(ActorRetryMechanism.java:28) ~[zeebe-scheduler-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.retry.RecoverableRetryStrategy.run(RecoverableRetryStrategy.java:48) ~[zeebe-scheduler-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) ~[zeebe-scheduler-8.2.4.jar:8.2.4]\r\n\t... 5 more\r\n```\r\n\r\nBasically, when processing a deployment command the following happens\r\n\r\n1. `DeploymentCreateProcessor#processRecord()` transforms the given resources accordingly\r\nhttps://github.com/camunda/zeebe/blob/e4c88b2a20f48e764118caefa5052199e12ad15e/engine/src/main/java/io/camunda/zeebe/engine/processing/deployment/DeploymentCreateProcessor.java#L129-L135\r\n2. If succeeded, the engine will create timers for any timer start event\r\nhttps://github.com/camunda/zeebe/blob/e4c88b2a20f48e764118caefa5052199e12ad15e/engine/src/main/java/io/camunda/zeebe/engine/processing/deployment/DeploymentCreateProcessor.java#L137-L142\r\n3. When executing the `#createTimerIfTimerStartEvent()` it will try to get the process definition by key:\r\nhttps://github.com/camunda/zeebe/blob/e4c88b2a20f48e764118caefa5052199e12ad15e/engine/src/main/java/io/camunda/zeebe/engine/processing/deployment/DeploymentCreateProcessor.java#L193-L203\r\n4. Getting the process definition by key will eventually put the process definition into a map `processesByProcessIdAndVersion` for caching purposes:\r\nhttps://github.com/camunda/zeebe/blob/e4c88b2a20f48e764118caefa5052199e12ad15e/engine/src/main/java/io/camunda/zeebe/engine/state/deployment/DbProcessState.java#L201-L211\r\n5. Once `#createTimerIfTimerStartEvent()` finishes, the engine will write a follow-up event to indicate the deployment has been created\r\nhttps://github.com/camunda/zeebe/blob/e4c88b2a20f48e764118caefa5052199e12ad15e/engine/src/main/java/io/camunda/zeebe/engine/processing/deployment/DeploymentCreateProcessor.java#L146\r\n6. Writing this follow-up event might fail with `EXCEEDED_BATCH_RECORD_SIZE`\r\n\r\nAs a consequence, the RocksDB transaction gets rolled back, meaning, the process definitions are not stored in RocksDB. But the not successful deployed process definitions are still present in the map `processesByProcessIdAndVersion`. That way, when a client starts a process by `bpmnProcessId`, the engine will get the actually non-existing process definition from the map `processesByProcessIdAndVersion` and start a process instance eventually. And as long as the leader keeps the same and Zeebe does not restart, process instances of the non-existing process definition can get started.\r\n\r\nAdditionally, the `ProcessVersionManager#versionCache` caches the latest version of a process definition. The same applies here, when the deployment fails, the cached latest version has been already increased and stays present in the cache. Basically, since `processesByProcessIdAndVersion` contains a non-existing process definition (for example, with version `12`) and the `versionCache` points to the non-existing version `12` of that process definition, the engine will return the process definition from `processesByProcessIdAndVersion`\r\n\r\nhttps://github.com/camunda/zeebe/blob/e4c88b2a20f48e764118caefa5052199e12ad15e/engine/src/main/java/io/camunda/zeebe/engine/state/deployment/DbProcessState.java#L214-L231\r\n\r\nBefore `versionCache` have been introduced, the engine would get the latest version from RocksDB which would not be `12` for sure. That way, the engine ensured that it always returns a process definition that actually exists.\r\n\r\n**To Reproduce**\r\n1. Deploy many processes at once (containing a process with `bpmnProcessId=foo` )\r\n2. Ensure that the deployment fails with `EXCEEDED_BATCH_RECORD_SIZE`\r\n3. Start a process instance by `bpmnProcessId=foo`  => The process instance gets started accordingly.\r\n4. Stop the broker\r\n5. Start the broker\r\n\r\n**Expected behavior**\r\n* Whenever a deployment fails, the client cannot start any process instance of any of those process definitions.\r\n* To be more concrete, any process definitions that get cached when processing the deployment command must be removed when the deployment failed.\r\n* Or as an alternative, don't cache any process definitions during deployment.\r\n* The same applies to `ProcessVersionManager#versionCache`\r\n\r\n**What is the impact of the issue?**\r\n* Any follower cannot replay any events of process instances belonging to a non-existing process definition. That way, the stream processor will stop replaying anything. The follower won't create any new snapshots, and so on.\r\n* If there is a failover, the new leader will fail to replay the events when recovering from the last snapshot. It will again stop the stream processor. No snapshots are taken anymore. The back pressure goes up to 100%. The partition gets unhealthy.\r\n* It may result in process definitions that are available on partition >= 2 but not on partition 1. For example:\r\n  1. Deploy the first version of a process `foo` that succeeds\r\n  2. Deploy a new version of the process `foo` that fails with `EXCEEDED_BATCH_RECORD_SIZE` (try doing multiple times)\r\n  3. Then redeploy the first version once again (and let it succeed)\r\n  4. => It will result in a process definition with version `x` (`x > 1`) which gets distributed to other partitions but on partition 1 this process definition won't get persisted in RocksDB.\r\n  5. So on partition > 1, there are running process instances referencing process definition with version `x`. But these process instances cannot be opened in Operate, because the process definition was never exported and imported.\r\n* There is no way to recover from that. Only purging the clusters resolves the issue.\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.2.4\r\n\r\n---\r\n\r\nrelated to #5723 \r\nrelated to [SUPPORT-17483](https://jira.camunda.com/browse/SUPPORT-17483)\r\nrelated to #12873\n\n Zelldon: **Sidenote**: this might affect also other places where we have a transient state (cached data in the state) since we never roll back such cached data on errors. \n Zelldon: Small assessment from my side:\n\nRight now, it is hard to detect from the engine perspective when a transaction is rolled back. \n\nWe could better handle the exceeding batch size, of course, but even then, it might be already part of our cache.\n\nI haven't checked the code yet. It might make sense to avoid the cache here, but I'm not sure whether this is possible.\n\nAlternatively, we could split the caches and store it in a second cache during processing, and when we are at the end of the command processing, we can put everything in the actual one.\n\nSimilarly, we could do this on the stream platform side. We have a primary cache object and a secondary cache object which is a copy which we give as an argument to the engine. On rollback, the secondary cache is cleared on success we persist the cache in our primary cache. \n\nThis involves a lot of copying which might be suboptimal if you have cached a lot of data but would resolve our issue in a more general fashion which we can use also for other places.\n\nRight now, i have no other or better idea yet. Except remove all caches.\n\n korthout: So far, we know the following solutions may work:\r\n1. only cache the processes when deployment successful\r\n  - requires reworking the logic\r\n2. when an exception is thrown, clear cache\r\n  - may slow down regular processing slightly as cache needs to be rebuild\r\n  - this also happens on leader changes\r\n3. allowing the stream processor to invalidate new cache entries similar to state transaction rollbacks\r\n  - more involved, but likely the most elegant solution\r\n  - will allow us to add caches more easily in the future\r\n\r\nAs a quick solution to overcome this problem fast, I suggest using option 2 for the next patch release. \r\n\r\n@Zelldon @romansmirnov What do you think?\n romansmirnov: @korthout, option 2 sounds like a good approach to get it fixed quickly. I share your assessment.\n Zelldon: Agree :+1: \n korthout: I was thinking a bit about how this case could've happened. We have several guarantees on our persisted state (e.g. all state changes must be covered by an event on the log), including some relational consistency checks. We do not have a foreign key relation from process instance to the process definition it belongs to. \r\n\r\nIf we had such a check, the corrupted state would've been avoided. The affected partition would've been `DEAD` but recoverable after a patch. Such a check would avoid all errors (including the unknown ones) related to process instance creation where the process definition is actually not stored in state.\r\n\r\nKeeping track of this foreign key relation would come at the cost of performance (two additional IO operations on process instance creation: one to store the additional entry in a new column family, and one to check the foreign key relation when the instance is being created in the state). \r\n\r\nIn the past, we've made decisions to go for safety over speed, but here we'd have to consider that decision again.\n korthout: Closing as fixed by:\r\n- #13256 \r\n- #13327 \r\n- #13280 \r\n- #13328 \r\n\r\nHowever, we should follow up with:\r\n- https://github.com/camunda/zeebe/issues/13259\r\n- https://github.com/camunda/zeebe/issues/13254#issuecomment-1617550522",
    "title": "When the deployment of a process definition failed, clients can still start new process instances of the actually non-existing process definition"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13164",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nA segfault can occur when one of the experimental feature flags is enabled and the other disabled:\r\n- `ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLEMESSAGETTLCHECKERASYNC`\r\n- `ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLETIMERDUEDATECHECKERASYNC`\r\n\r\nThis same error does not occur when both are enabled, or both are disabled.\r\n\r\n**To Reproduce**\r\n\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\nEnable either of, and disable the other:\r\n- `ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLEMESSAGETTLCHECKERASYNC`\r\n- `ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLETIMERDUEDATECHECKERASYNC`\r\n\r\n**Expected behavior**\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\nNo segfault occurs.\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\nCurrent thread (0x00007f4dbd280db0):  JavaThread \"-zb-actors-7\" [_thread_in_native, id=15747, stack(0x00007f4d1222d000,0x00007f4d1232e000)]\r\n\r\nStack: [0x00007f4d1222d000,0x00007f4d1232e000],  sp=0x00007f4d1232c340,  free space=1020k\r\nNative frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)\r\nC  [librocksdbjni14514272448621065586.so+0x5e2a98]  rocksdb::Arena::~Arena()+0x188\r\nC  [librocksdbjni14514272448621065586.so+0x877aa6]  rocksdb::WriteBatchWithIndex::Rep::ClearIndex()+0x16\r\nC  [librocksdbjni14514272448621065586.so+0x851e59]  rocksdb::TransactionBaseImpl::Clear()+0x349\r\nC  [librocksdbjni14514272448621065586.so+0x83afca]  rocksdb::OptimisticTransaction::Rollback()+0x2a\r\nC  [librocksdbjni14514272448621065586.so+0x2def2e]  Java_org_rocksdb_Transaction_rollback+0x1e\r\nJ 13789  org.rocksdb.Transaction.rollback(J)V (0 bytes) @ 0x00007f4dad4fa05b [0x00007f4dad4f9fa0+0x00000000000000bb]\r\nJ 14913 c1 org.rocksdb.Transaction.rollback()V (60 bytes) @ 0x00007f4da5bf2fbc [0x00007f4da5bf2e00+0x00000000000001bc]\r\nJ 14285 c2 io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.ensureInOpenTransaction(Lio/camunda/zeebe/db/impl/rocksdb/transaction/TransactionConsumer;)V (29 bytes) @ 0x00007f4dad398614 [0x00007f4dad398280+0x0000000000000394]\r\nJ 21410 c1 io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.forEach(Ljava/util/function/BiConsumer;)V (24 bytes) @ 0x00007f4da5fb5a94 [0x00007f4da5fb56a0+0x00000000000003f4]\r\nj  io.camunda.zeebe.engine.state.distribution.DbDistributionState.foreachPendingDistribution(Lio/camunda/zeebe/engine/state/immutable/DistributionState$PendingDistributionVisitor;)V+49\r\nj  io.camunda.zeebe.engine.processing.distribution.CommandRedistributor.runRetryCycle()V+30\r\nj  io.camunda.zeebe.engine.processing.distribution.CommandRedistributor$$Lambda$727+0x00000008015c5c08.run()V+4\r\nJ 18846 c1 io.camunda.zeebe.stream.api.scheduling.SimpleProcessingScheduleService.lambda$runAtFixedRate$0(Ljava/lang/Runnable;Ljava/time/Duration;)V (49 bytes) @ 0x00007f4da57d5274 [0x00007f4da57d5160+0x0000000000000114]\r\nJ 18845 c1 io.camunda.zeebe.stream.api.scheduling.SimpleProcessingScheduleService$$Lambda$720+0x00000008015c4698.run()V (18 bytes) @ 0x00007f4da5f4ecac [0x00007f4da5f4eb40+0x000000000000016c]\r\nJ 15127 c2 io.camunda.zeebe.scheduler.ActorJob.execute(Lio/camunda/zeebe/scheduler/ActorThread;)V (351 bytes) @ 0x00007f4dad5606ac [0x00007f4dad560540+0x000000000000016c]\r\nJ 18627 c2 io.camunda.zeebe.scheduler.ActorTask.execute(Lio/camunda/zeebe/scheduler/ActorThread;)Z (350 bytes) @ 0x00007f4dad8730f4 [0x00007f4dad872840+0x00000000000008b4]\r\nJ 22040 c2 io.camunda.zeebe.scheduler.ActorThread.doWork()V (285 bytes) @ 0x00007f4dadbb264c [0x00007f4dadbb2240+0x000000000000040c]\r\nJ 11375 c2 io.camunda.zeebe.scheduler.ActorThread.run()V (103 bytes) @ 0x00007f4dacf579e0 [0x00007f4dacf578e0+0x0000000000000100]\r\nv  ~StubRoutines::call_stub\r\nV  [libjvm.so+0x825045]  JavaCalls::call_helper(JavaValue*, methodHandle const&, JavaCallArguments*, JavaThread*)+0x315\r\nV  [libjvm.so+0x82683b]  JavaCalls::call_virtual(JavaValue*, Handle, Klass*, Symbol*, Symbol*, JavaThread*)+0x1cb\r\nV  [libjvm.so+0x8f14f3]  thread_entry(JavaThread*, JavaThread*)+0xa3\r\nV  [libjvm.so+0xe62564]  JavaThread::thread_main_inner()+0x184\r\nV  [libjvm.so+0xe65c10]  Thread::call_run()+0xc0\r\nV  [libjvm.so+0xc1aa21]  thread_native_entry(Thread*)+0xe1\r\n```\r\n\r\n</p>\r\n</details>\n\n korthout: ❗ Since merging #13008, this segfault can also occur when both feature flags are enabled. We should fix this soon! Luckily the fix is easy and already coming up.\r\n\r\nThe segfault occurs because of the following situation:\r\n- the stream processor has its own transaction context\r\n- the scheduled tasks have their own shared transaction context\r\n- the scheduled tasks can be run on the stream processor's actor, or async on their scheduled tasks actor\r\n- the problem occurs when two scheduled tasks run on different actors while reusing the shared transaction context\n korthout: Since this issue only exists in a few places and those don't affect users, we'll look into it after the company retreat:\r\n- on released versions (`8.2.7-` and `8.1.13-`, but not on any `8.3` alphas) when only one of these experimental feature flags are enabled and the other is disabled:\r\n  - `ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLEMESSAGETTLCHECKERASYNC`\r\n  - `ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLETIMERDUEDATECHECKERASYNC`\r\n- on `main` branch when one or both feature flags are enabled\r\n- on `main` branch in tests (because both feature flags are enabled)\r\n\r\nWe should fix this before the next alpha release: `8.3.0-alpha3`.",
    "title": "Segfault on enabling async scheduled task"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13123",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nCurrently, the `JobTimeOutProcessor` assumes that the job it is supposed to time out is in the state. This is wrong, as it's possible to have the following sequence on the log: Job.COMPLETE, Job.TIME_OUT. The first command will remove the job from the state, and when processing the time out command, the state will not return any job.\r\n\r\n**To Reproduce**\r\n\r\nNew test in `JobTimeOutTest`:\r\n\r\n```java\r\n@Test\r\n  public void shouldRejectIfJobDoesNotExist() {\r\n    // given\r\n    final var jobKey = ENGINE.createJob(jobType, PROCESS_ID).getKey();\r\n    final var job = ENGINE.getProcessingState().getJobState().getJob(jobKey);\r\n    final var partitionId = Protocol.decodePartitionId(jobKey);\r\n\r\n    // when\r\n    ENGINE.pauseProcessing(partitionId);\r\n    ENGINE.writeRecords(\r\n        RecordToWrite.command().job(JobIntent.COMPLETE, job),\r\n        RecordToWrite.command().job(JobIntent.TIME_OUT, job));\r\n    ENGINE.resumeProcessing(partitionId);\r\n    Awaitility.await(\"until everything processed\").until(ENGINE::hasReachedEnd);\r\n\r\n    // then activated again\r\n    final List<Record<JobRecordValue>> jobEvents =\r\n        jobRecords()\r\n            .withRecordKey(jobKey)\r\n            .withIntent(JobIntent.TIME_OUT)\r\n            .withRecordType(RecordType.COMMAND_REJECTION)\r\n            .limit(1)\r\n            .toList();\r\n    assertThat(jobEvents).isNotEmpty();\r\n  }\r\n```\r\n\r\n**Expected behavior**\r\n\r\nThe command is rejected when the job to time out does not exist.\r\n\r\n**Log/Stacktrace**\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.lang.NullPointerException: Cannot invoke \"io.camunda.zeebe.protocol.impl.record.value.job.JobRecord.getDeadline()\" because \"job\" is null\r\n\tat io.camunda.zeebe.engine.processing.job.JobTimeOutProcessor.processRecord(JobTimeOutProcessor.java:53) ~[zeebe-workflow-engine-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.engine.Engine.process(Engine.java:143) ~[zeebe-workflow-engine-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.batchProcessing(ProcessingStateMachine.java:347) ~[zeebe-stream-platform-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$2(ProcessingStateMachine.java:268) ~[zeebe-stream-platform-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run(ZeebeTransaction.java:84) ~[zeebe-db-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand(ProcessingStateMachine.java:268) ~[zeebe-stream-platform-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord(ProcessingStateMachine.java:227) ~[zeebe-stream-platform-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.readNextRecord(ProcessingStateMachine.java:203) ~[zeebe-stream-platform-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:203) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n```\r\n\r\n</p>\r\n</details>\r\n\n\n koevskinikola: Just adding for transparency:\r\n\r\n*Observed behavior*:\r\nThe process instance related to the job is banned, making it unrecoverable.\r\n\r\n*Workarounds*:\r\n- There are no workarounds to fix this issue if it has already happened. As a result, this bug has a high severity.",
    "title": "NPE when processing Job.TIME_OUT command"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13061",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nThere is currently a potential race condition which would result in a remote stream existing server side, even though the client stream has gone away.\r\n\r\nSince we register remote streams asynchronously, a remove request may be submitted client side, which will immediately remove it there. Then asynchronous removal requests are sent to the server. However, this can be interleaved with the asynchronous registration, resulting in a stream existing server side.\r\n\r\nThe impact is additional latency during a push, or possible unnecessary job activation if it was the last stream for this type. However, the stream will eventually get removed appropriately.\r\n\r\n**Expected behavior**\r\n\r\nRegistration/removal of remote streams is sequenced, such that a removal request would cancel registration attempts, and queue the removal after whatever in-flight requests were sent are finished.\r\n\r\nThere is still a slight edge case around time outs, of course, but I think this is acceptable for now. The other option would be introducing even more coordination in the protocol, and I'd rather avoid this.\n\n npepinpe: The idea is to associate a `ClientStreamRegistration` state machine to each stream, which manages the current state of the remote registration. States would be:\r\n\r\n```mermaid\r\ngraph LR;\r\n    Initial-->Adding;\r\n    Adding-->Added;\r\n    Initial-->Removing;\r\n    Adding-->Removing;\r\n    Added-->Removing;\r\n    Removing-->Removed;\r\n```\r\n\r\nWhen a client first connects, it creates a registration state machine. The initial state is `INITIAL`, before any request is sent. In this state, nothing is happening until the first request occurs. Once an add request is submitted, it transitions to `ADDING`. It may also transition to `REMOVING` immediately; for simplicity, we'll send a REMOVE request anyway at the moment. \r\n\r\nIn `ADDING`, it will send registration requests to all known brokers. It transitions to `ADDED` when it has been registered to all known brokers. If the client disconnects in this stage, it transitions to `REMOVING`.\r\n\r\nIn the `ADDED` state, it does nothing. When a new broker is discovered, it transitions to `ADDING`, and again only transitions back to `ADDED` when it is registered to all known brokers. If, however, a client disconnects in the `ADDED` state, it transitions to `REMOVING`.\r\n\r\nIn the `REMOVING` state, it first cancels any in-flight registration requests, if possible. It will also discard any responses from previous registration requests at that point. Then it sends a request to all known brokers to remove this stream. When it is disconnected from all brokers, it transitions to `REMOVED`. If a new broker is discovered in this state, nothing should happen.\r\n\r\nIn the `REMOVED` state, nothing happens, and ideally the registration is garbage collected. Any incoming responses should be discarded.\n npepinpe: The above is not quite enough to catch all edge cases however. Since requests can be processed out of order, for every request to a broker per stream, we have to wait until previous requests are finished before sending the next one. Now, we still have the time out edge case - when a request to a broker times out, we can't tell if it never made it, or if it will eventually be processed. But the likelihood of a request being processed out of order in this case is very low I think. Say you send an ADD request, which times out, then a REMOVE request. It's very unlikely that the REMOVE request will somehow be received and processed before the ADD by the remote server. Whereas if you send an ADD, then immediately a REMOVE (without waiting for the ADD), the likelihood is much higher.\n deepthidevaki: If we are not able to prevent the edge case due to REMOVE processed before ADD, we can handle it safely by ensuring that gateway sends a `NoStreamExistsException`, and the broker proactively closes the stream if it gets this error response. \n npepinpe: I think that's already the case. In a way, this issue is generally an optimization one, to avoid unnecessarily activating and pushing a job when there is nothing to receive it. It's a bit hard for me to evaluate the likelihood of this happening. I see two cases:\r\n\r\n1. A client disconnects quickly while registration is still on-going.\r\n2. A client disconnects around the same time a new broker was added to the topology.\r\n\r\nI think for case 1 this is not too likely, so the second case is probably going to be the \"common\" one (and also how I initially observed it).\r\n\r\nMy current approach is to replace the set of servers an `AggregatedClientStream` is connected to with a map of server -> registration state. The registration state will keep track of the current state (e.g. `INITIAL`, `ADDING`, etc.), transition rules, and the single in-flight request (as we don't allow more than one at a time). Note that registrations are scoped per server.\r\n\r\nThe request manager now takes in registrations and manipulates those based on their state.\r\n\r\nAn alternative was to push the communication into the registration itself as well :shrug: Another one was to keep track of registrations in the request manager, not in the client stream. The drawback was we'd have to keep track of yet another source truth, in yet another map of maps (stream -> [server -> [registration]]).\n deepthidevaki: > I think that's already the case.\r\n\r\n:+1: \r\n\r\n> My current approach is to replace the set of servers an AggregatedClientStream is connected to with a map of server -> registration state. The registration state will keep track of the current state (e.g. INITIAL, ADDING, etc.), transition rules, and the single in-flight request (as we don't allow more than one at a time). Note that registrations are scoped per server.\r\n\r\nThis sounds good. Will AggregatedClientStream also marks if the client is closed, so that it can actively close registrations in case of out-of-order Add/Remove requests? i.e if a broker moves to ADDED state after the client is closed, how do we detect this and send a REMOVE request? \n npepinpe: Correct, I ended up adding a CLOSED state to the registration as a short-circuit way to close any pending operations.",
    "title": "Cancel on-going remote stream registration on stream removal"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13046",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nIn #11460 we've implemented a truncation on error messages. This was done to prevent error messages from exceeding the maximum message size. Currently the messages are limited to 500 characters.\r\n\r\nWe've had a support case asking us where the remainder of the error message has gone. 500 characters may be too strict. A regular stack trace will quickly exceed this limit.\r\n\r\nGenerally users would have logging on their workers which should be able to show them the full message. Obviously this isn't visible in Operate and is an extra step for users. If we could provide a better UX by increasing this limit we should consider this.\r\n\r\nWe could also think about making this limit configurable.\r\n\r\n**To Reproduce**\r\n\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\nThrow an error on a Job with > 500 characters.\r\n\r\n**Expected behavior**\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nI can see more than just the first 500 characters of my error.\r\n\r\n-------\r\n\r\nhttps://jira.camunda.com/browse/SUPPORT-17090\n\n korthout: ZPA triage:\n- request makes sense\n- we could easily increase the limit much higher (e.g. 10k) without exceeding MAX_MESSAGE_SIZE\n- we could also consider ways to make this more dynamic, but this is more complex\n- the simple fix has our preference and could be backported to all supported versions easily\n- we'll mark this a bug\n- could be a good first issue for new onboarders\n akkie: The support also pointed me to this issue. I work for a consulting company and we helped a large German corporation to implement Camunda Platform 8 for their processes. In all our presentations, we showed how easy it is for the DevOps guys to see the errors directly in Operate instead of searching through the logs. Of course this was not the only selling point, but it was a feature that everyone really liked.\r\n\r\nI understand the problem for truncating the message, but as @korthout mentioned, if it's ok to increase the limit, it would be really appreciated, because currently with the 500 chars limit, it's not really useful. For us 10k would be more as enough.\n korthout: Thanks for sharing @akkie. It's very helpful to understand such perspectives 🙇 \n aleksander-dytko: Another Support issue about this: https://jira.camunda.com/browse/SUPPORT-17503\r\n\r\nIt would be great to finish this soon. Thank @nicpuppa for taking care of this one 🚀 ",
    "title": "Error message truncation may be too strict"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13041",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "Similar to https://github.com/camunda/zeebe/issues/12797, the `JobBackoffChecker` also tries to cleanup backoffs:\r\n\r\nhttps://github.com/camunda/zeebe/blob/d166007d8fee3fa6f112367ea595d35199807f4f/engine/src/main/java/io/camunda/zeebe/engine/state/instance/DbJobState.java#L308-L317\n",
    "title": "Don't mutate state through `JobBackoffChecker`"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13038",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nWhen trying/retrying a push from the broker side, we every time pick the next random stream consumer to push to. However, it's possible that during a retry all consumers are removed. Right now, this throws an error because we try to generate a random index from 0 to 0 :smile: \r\n\r\n**Expected behavior**\r\n\r\nWe should cope with streams having no consumers (on both the gateway/broker) even during retries, and bail out early.\r\n\n\n npepinpe: Alright, so `AggregatedRemoteStream` is not thread safe, but it's also not immutable right now.\r\n\r\nSo we have two options: making it thread-safe and handle mutation, or making it immutable and potentially handle more errors.\r\n\r\nMaking it thread safe means turning the consumer list into a `CopyOnWriteArrayList` (as we likely are reading from it more often than writing to it), and handling mutations when picking a random stream by first grabbing a copy of the list before doing any operations on it.\r\n\r\nMaking it immutable for now would simply be copying the `AggregatedRemoteStream` record when picking the target, and passing this around downstream. I would lean towards the second option as it's simpler to handle, and in the common case we still wouldn't be handling more errors than usual.",
    "title": "Handle stream consumers changes during push"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13036",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nWhen registering a new client job stream from the gateway to any broker, the brokers are successfully registering the streams, but not responding to the gateway, causing the gateway to retry over and over.\r\n\r\nWhile this still allows pushing, it creates a lot of noise and the impression that the streams are not successfully registered.\r\n\r\nThe underlying cause is the usage of the `ClusterCommunicationService#subscribe(String, Function<byte[], M>, BiConsumer<MemberId, M>, Executor executor)` - any subscriber which is a consumer will never send a response back. However, the client in this case expects a response.\r\n\r\n**Expected behavior**\r\n\r\nClient streams are properly registered on both sides.\r\n\n\n npepinpe: One hurdle here is there's no easy way to introspect the registered client streams. So our tests were passing because we just check if the server has registered the stream (which it has), but we have no real way to catch that the client is in an endless loop.",
    "title": "Endless client job stream registration"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12957",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nSince 8.2, it is possible to deploy processes with [Undefined Task](https://docs.camunda.io/docs/next/components/modeler/bpmn/undefined-tasks/), which the workflow engine processes as a straight-through activity. When placed in a loop without a wait state (i.e. element in the process that requires the engine to wait), the workflow engine may process faster than the exporters can export the produced records.\r\n\r\nWhen the workflow engine is faster than the exporters, the log grows, and Zeebe's disk space usage increases. Eventually, Zeebe Brokers may take too long to start up because the log is too large. This may lead to the unavailability of Zeebe.\r\n\r\nWe've encountered this on the Elasticsearch exporter, where ES may reject exported records. In that case, it can even result in the unavailability of Operate and Zeebe.\r\n\r\nAs a temporary workaround:\r\n- increase the CPU/memory resources of Elasticsearch to support the higher influx of records\r\n- increase the failure threshold Zeebe Brokers are allowed as start-up duration\r\n- cancel the process instance with the straight-through processing loop\r\n\r\n**To Reproduce**\r\n\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\n> The following only works on clusters exporting to Elasticsearch with specific resource allocations. You may need to reduce the available Elasticsearch resources.\r\n\r\n- deploy a process with a loop of undefined tasks\r\n- create an instance of this process\r\n- notice the number of not exported records increasing\r\n\r\n**Expected behavior**\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\nProcess instances of processes with a straight-through processing loop should not cause problems.\r\n\r\n**Solution proposals**\r\n\r\nA solution has already been proposed to resolve this issue:\r\n- #12696 \r\n\r\nAdditionally, we suggest to:\r\n- add a feature flag to allow/disallow undefined tasks and/or processes without wait states\r\n  - allows Play to use them in the Modeling/Play iteration; while it can be disabled on SaaS production-like clusters\r\n  - flag could also disallow creating instance of such processes if they are already deployed\r\n  - flag could also stop execution of process instances of such processes if already created\r\n- raise an [Incident](https://docs.camunda.io/docs/next/components/concepts/incidents/) when a straight-through processing loop is detected during process execution\r\n\r\n**Considered alternatives**\r\n\r\nI also considered interrupting a process instance (and its child process instances) immediately when canceling it instead of the current recursive and async termination flow. However, that solution would be better fitting to kill fork bombs. A simple straight-through processing loop does not produce a log of unprocessed records.\r\n\n\n Zelldon: > IMO, we should not reject deployments of processes without wait states because these may be valuable in Modeling/Play iteration when first designing a process.\r\n\r\nI think this can be differentiated. We consider Play our new tool for users to play around with there models and here I agree it makes sense to allow it, but not on production-like clusters.\r\n\r\nWe could have a feature flag that allows us to disable certain features, like undefined tasks or other activities. We could then reject models (with a well-written message) which contain such elements on SaaS clusters, and reference to Play to try out processes. This would also allow us to prevent problematic scenarios in production environments.\r\n\r\nIn my opinion, this would be straightforward and with minimal effort to implement. \r\n\r\nAlternative and much more complex would be of course detection mechanism, and in general better handling of suspending/cancelation of bad behavior instances.\r\n\r\n\r\n\n korthout: Good point @Zelldon! I'll adjust the description accordingly.\n megglos: Thanks for raising this @korthout! I wanted to follow-up on this topic this week. I would love us to have a short-term mitigation, like e.g. disabling undefined tasks until we have a proper way of at least recovering or even better detecting these situations and suspend such instances.\r\n\r\nFor short term I see these options:\r\n\r\n1. disable undefined task processing by default on SaaS, only offer a flag to opt-in => assuming the use case is covered by  Zeebe Play (for which it is enabled)\r\nPro:\r\nno such incidents on SaaS anymore\r\noptional: we can make this transparent to the user by the engine rejecting processes that use undefined tasks\r\nCon:\r\nthe feature is effectively removed from SaaS\r\n\r\n2. option 1 but only for all non-trial clusters => prevent an incident where a real production cluster gets impacted by such a process\r\nPro:\r\nwe prevent this from happening on a production cluster\r\nCon:\r\nthe noise caused by trials will not disappear\r\n\r\n3. Assess a fist iteration to detect loops of undefined tasks and reject their deployment => even if it may not cover all potential edge-cases may a first iteration be feasible that already is good-enough to prevent a majority of such processes to cause trouble?\r\nPro:\r\nmay be suffice to reduce the frequency of such incidents heavily already\r\nerror message can make this transparent to the user\r\nmay better fit a 8.2 patch scope => prevent users from running into this issue while keeping the previous default of undefined tasks being allowed\r\nCon:\r\neffort seems higher\r\nstill a risk of an edge-case not covered causing such loop (a call-activity of a process that contains undefined tasks only 🙃 ?)\r\n\r\nMy personal favorite is option 3 and if that's too complicated option 1 asap. (I'm fine doing an out of schedule patch, for the sake of preventing this issue asap)\r\nOption 2 is not really helping on the trial noise, meaning it is not good enough of a proper mitigation from that perspective,  and introduced feature imbalance for trial that we didn't have yet. right?\r\n\r\nwdyt?\r\n\r\n\n Zelldon: \r\n![giphy](https://github.com/camunda/zeebe/assets/2758593/16eeb955-6c44-4f78-831f-9071bb92e104)\r\n\r\nStop the bleeding with one, and work on a better solution e.g. three. \r\n\r\nExamples: Reject deployments which contain only no wait states, or detect loops which contains no wait states (https://stackoverflow.com/questions/261573/best-algorithm-for-detecting-cycles-in-a-directed-graph) \r\n\n megglos: The issue we have with 1 & 2 is that it effectively disables a feature of 8.2, that's why I would like to favor 3 first and take 1 as last resort + the communication overhead to announce this breaking change.\n megglos: @korthout as a mitigation to this would help us to avoid repeated incidents with manual effort I would suggest to bump severity to high, wdyt?\n korthout: @megglos The `severity` label is clearly defined. This bug does not have high severity because a workaround is available. \r\n\r\nI do see the need to prioritize it as `🚧 upcoming` (intend to work on it soon) or even as `⛔ blocker` (stop-the-world) due to the impact it has on us (incidents, alert noise, etc).\n megglos: another option for a potential mitigation that was raised by @oleschoenburg \r\n\r\n4. What about having an artificial delay for undefined and manual tasks, e.g. 1s?\r\nThat would throttle such loops significantly by introducing a wait state and thus offsetting disk resource issues significantly.\r\nFrom a user perspective that might be acceptable to unnoticeable. We could make it configurable to cover cases where a self-managed user cares about low latency and still wants to keep and undefined/manual task in the process\r\nPro:\r\nmight it be more straightforward to implement compared to option 3 => basically applying timer event behavior\r\nCon:\r\nDoesn't prevent this situation but offsets any resulting resource problems\r\n\r\nSo we could decide on doing either 4 or 3 first. While 3 would prevent such situations (but not all of them) 4 would offset any resulting issues of such processes by a significant amount of time. Both together appear already good enough to mitigate the pain of such looping processes.\r\n\r\nWith these in place we may be able to close this issue and eventually shift towards https://github.com/camunda/zeebe/issues/12696 that would enable us to at least manually intervene with any rogue process.\r\n\r\nwdyt @korthout @Zelldon ?\r\n\r\n\n korthout: Interesting idea\r\n\r\n>artificial delay for undefined and manual tasks\r\n\r\nI assume we mean non-blocking for the stream processor. So schedule a delayed task that appends the command to complete the task.\n Zelldon: Not sure how this should look like, since if you start to process it you need to commit all changes to it before starting on the next command, the processing of commands is in a serial order. I can only imagine you put it back to the end of the log, other than that I see no way to do that ?\r\n\n oleschoenburg: > Not sure how this should look like, since if you start to process it you need to commit all changes to it before starting on the next command\r\n\r\nSimilar to how job timeouts works for example: after the manual task is `ACTIVATED` and after some period of time has passed we write a `COMPLETE` command to the log.\r\nThat's how I thought it could work anyway, maybe that doesn't work?\n korthout: @Zelldon is right; it wouldn't be safe to do so. Unless we add a way to write the `COMPLETE` command for any currently `ACTIVATED` undefined/manual tasks, on recovery.\n oleschoenburg: But every `ACTIVATED` undefined/manual task should already have a `COMPLETE` follow-up command on the log. Otherwise, how would this task ever complete?\n korthout: >But every ACTIVATED undefined/manual task should already have a COMPLETE follow-up command on the log. \r\n\r\nThat's not correct. The proposal was to add the delay async, so when processing `ACTIVATE_ELEMENT` we append `ELEMENT_ACTIVATING` and `ELEMENT_ACTIVATED`, and also schedule a post-commit task to append `COMPLETE_ELEMENT` after a delay (this may be lost).\n oleschoenburg: Ah sorry, I misunderstood and thought you were talking about the upgrade from old to new behavior.\r\n\r\nOf course you are right and this would require persistence of activated manual/undefined tasks so that they can be completed eventually. Again, similar to job timeouts.\n Zelldon: So to summarize the proposal (just that I get it right):\r\n\r\n * There will be another checker/consumer which is scheduled via the ProcessingScheduleService (in order to be decoupled of the processing). Lets call it U-COMPLETER\r\n * The processing of undefined tasks is split up, it will only produce ACTIVATE_ACTIVATED on processing\r\n * During activation we need to store the record and all necessary data in a new column family (for the U-COMPLETER)\r\n * The U-COMPLETER will write based on the data in the column family the complete commands, in order to continue the processing of the instance.\n korthout: ZPA triage:\n- let's split up this issue into the different solution proposals\n- we like the proposal to reject deployments of processes with an undefined task loop but there are simpler solutions, so we'll focus on those first\n- we like the proposal to slow down undefined task, but it's unclear to us whether we should do this sync or async.\n  - sync: this blocks the stream processor a bit but is a simpler solution\n  - async: adds additional complexity over sync because we have to care about the unreliability of scheduled post-commit tasks\n  - @camunda/zeebe-distributed-platform, do you have an opinion about the performance loss?\n- we should check what impact our solutions have on Zeebe Play\n- we will focus on the quick fix in the upcoming iteration\n megglos: > sync: this blocks the stream processor a bit but is a simpler solution\r\n> async: adds additional complexity over sync because we have to care about the unreliability of scheduled post-commit tasks\r\n\r\nsync would not be an option due to the impact on overall processing latency/throughput\r\n\n megglos: I'm also in all in for rolling out a 8.2.X patch & a 8.2.0-alpha2.1 (did we ever do an out of cadence alpha before?) as soon as a mitigation is available\n korthout: >sync would not be an option due to the impact on overall processing latency/throughput\r\n\r\nIs Undefined Task really an element used in production where performance (latency/throughput matters?\r\n\r\nI imagine Undefined Task only has use in Zeebe Play, or as a lonely element in a production process where a small latency (~10-50ms) doesn't really matter.\n remcowesterhoud: I've created https://github.com/camunda/zeebe/issues/12993 as a separate issue to delay undefined tasks.\n korthout: Thanks @remcowesterhoud, marking this issue as `later` priority, as #12993 is split out with higher prio.\n oleschoenburg: > I imagine Undefined Task only has use in Zeebe Play, or as a lonely element in a production process where a small latency (~10-50ms) doesn't really matter.\r\n\r\nI disagree that it's fine for production workloads. If a user \"accidentally\" deploys and starts such as process that loops on an undefined task, it could seriously impact the performance of an entire partition because the partition will repeatedly sleep. Even worse with batch processing enabled where it's not just one sleep but many (like 10 or 100).\n megglos: ZDP-Triage:\n- moving to backlog now as @remcowesterhoud is looking into this\n remcowesterhoud: As the quick-fix is done and getting released as I type this, I'm going to remove my assignment from this issue. We'll still need to make a proper solution, so I will move this back to the teams inbox to triage it again.\n Zelldon: Great work @remcowesterhoud \n korthout: ZPA triage:\n- with a quick fix available, we think other issues have higher priority than this\n- we'll increase priority if this does occur again\n- marking as `later`\n megglos: ZDP-Triage:\n- loop detection was a first band-aid and we still lack measures to handle situations not covered by validation\n- measures from the steam platform might be possible, e.g. detect high processing load on the same key, it needs to be discussed if this is the right direction though\n- most solutions likely require collab between both teams\n megglos: https://github.com/camunda/zeebe/issues/12696 is related to this as well, will put focus on that one first\n megglos: ZDP-Planning:\n- is related to #12560 and would be mitigated if processing is rate limited\n- we have alerts on noop loops => we can cancel such instances proactively",
    "title": "Straight-through processing loop may cause problems"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12933",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nWith job push enabled, when a job is failed with remaining retries and no backoff, the job is first activated and then immediately marked as failed again. This results in pushing out jobs that are in failed state and presumably some data inconsistencies because the job lifecycle is not followed. \r\n\r\n**To Reproduce**\r\n\r\nRun `ActivatableJobsPushTest#shouldPushAfterJobFailed` and inspect the log.\r\n\r\n**Expected behavior**\r\n\r\nWhen a job is failed with remaining retries and no backoff, the job should first transition to failed before being activated again.\r\n\r\n**Log/Stacktrace**\r\n\r\n<details><summary>Compact log representation</summary>\r\n <p>\r\n\r\n```\r\nC DPLY      CREATE            - #01-> -1  -1 - \r\nE PROC      CREATED           - #02->#01 K01 - process.bpmn -> \"process\" (version:1)\r\nE DPLY      CREATED           - #03->#01 K02 - process.bpmn\r\nE DPLY      FULLY_DISTRIBUTED - #04->#01 K02 - \r\nC CREA      CREATE            - #05-> -1  -1 - new <process \"process\"> (default start)  with variables: {a=valA, b=valB, c=valC}\r\nE VAR       CREATED           - #06->#05 K04 - b->\"valB\" in <process [K03]>\r\nE VAR       CREATED           - #07->#05 K05 - a->\"valA\" in <process [K03]>\r\nE VAR       CREATED           - #08->#05 K06 - c->\"valC\" in <process [K03]>\r\nC PI        ACTIVATE          - #09->#05 K03 - PROCESS \"process\" in <process \"process\"[K03]>\r\nE CREA      CREATED           - #10->#05 K07 - new <process \"process\"> (default start)  with variables: {a=valA, b=valB, c=valC}\r\nE PI        ACTIVATING        - #11->#05 K03 - PROCESS \"process\" in <process \"process\"[K03]>\r\nE PI        ACTIVATED         - #12->#05 K03 - PROCESS \"process\" in <process \"process\"[K03]>\r\nC PI        ACTIVATE          - #13->#05  -1 - START_EVENT \"start\" in <process \"process\"[K03]>\r\nE PI        ACTIVATING        - #14->#05 K08 - START_EVENT \"start\" in <process \"process\"[K03]>\r\nE PI        ACTIVATED         - #15->#05 K08 - START_EVENT \"start\" in <process \"process\"[K03]>\r\nC PI        COMPLETE          - #16->#05 K08 - START_EVENT \"start\" in <process \"process\"[K03]>\r\nE PI        COMPLETING        - #17->#05 K08 - START_EVENT \"start\" in <process \"process\"[K03]>\r\nE PI        COMPLETED         - #18->#05 K08 - START_EVENT \"start\" in <process \"process\"[K03]>\r\nE PI        SEQ_FLOW_TAKEN    - #19->#05 K09 - SEQUENCE_FLOW \"sequenc..7fb902a\" in <process \"process\"[K03]>\r\nC PI        ACTIVATE          - #20->#05 K10 - SERVICE_TASK \"task\" in <process \"process\"[K03]>\r\nE PI        ACTIVATING        - #21->#05 K10 - SERVICE_TASK \"task\" in <process \"process\"[K03]>\r\nE JOB       CREATED           - #22->#05 K11 - K11 \"id-ae6f81e8-dfdb-436e-a86a-d681b855a326\" @\"task\"[K10] 3 retries, in <process \"process\"[K03]> (no vars)\r\nE JOB_BATCH ACTIVATED         - #23->#05 K12 - \"id-ae6f81e8-dfdb-436e-a86a-d681b855a326\" 1/-1\r\n                K11 \"id-ae6f81e8-dfdb-436e-a86a-d681b855a326\" @\"task\"[K10] 3 retries, in <process \"process\"[K03]> (no vars)\r\nE PI        ACTIVATED         - #24->#05 K10 - SERVICE_TASK \"task\" in <process \"process\"[K03]>\r\nC JOB       FAIL              - #25-> -1 K11 - K11 5 retries, in <process ?[?]> (no vars)\r\nE JOB_BATCH ACTIVATED         - #26->#25 K13 - \"id-ae6f81e8-dfdb-436e-a86a-d681b855a326\" 1/-1\r\n                K11 \"id-ae6f81e8-dfdb-436e-a86a-d681b855a326\" @\"task\"[K10] 5 retries, in <process \"process\"[K03]> (no vars)\r\nE JOB       FAILED            - #27->#25 K11 - K11 \"id-ae6f81e8-dfdb-436e-a86a-d681b855a326\" @\"task\"[K10] 5 retries, in <process \"process\"[K03]> (no vars)\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n\r\nThis is currently a (soft) blocker for https://github.com/camunda/zeebe/issues/12797\n\n oleschoenburg: I think the issue is related to the _very confusing_ interplay between the `JobFailProcessor` (a `CommandProcessor`), the `CommandControl` and the `BpmnJobActivationBehavior`.\r\nThe code _seemingly_ does the right thing, first transitioning to `FAILED` and then handing over to `jobActivationBehavior` which should transition to `ACTIVATED`:\r\nhttps://github.com/camunda/zeebe/blob/5bd2e1f830d8de2efb6869dc0a29d47896b51800/engine/src/main/java/io/camunda/zeebe/engine/processing/job/JobFailProcessor.java#L137-L143\r\n\r\nIf I understand correctly, the mistakes is that the commandControl does not immediately write the follow-up event but only later when the `ACTIVATED` event is already written.\n npepinpe: Do we really need to mark the job as failed? Could we skip it entirely?\r\n\r\nAt any rate, if we have to, the log would ideally look like: ACTIVATED -> FAIL -> FAILED -> ACTIVATED. While the push has to happen as a side effect, the new ACTIVATED record should be the last one written in terms of state changes.\n koevskinikola: This bug will also be present when pushing jobs on timeout and recur, as the corresponding processors implement the `CommandProcessor` interface as well.\r\n\r\n1. The quick fix would be to move the job push to the `CommandProcessor#afterAccept` method.\r\n1. The nicer fix, which would make these Processors more readable, would be to refactor them to use the `TypedRecordProcessor` interface, where it's more clear when the events are written.\n npepinpe: :+1: for the second one\n koevskinikola: @npepinpe sorry for not replying to:\r\n> Do we really need to mark the job as failed? Could we skip it entirely?\r\n\r\nI think it would be good to have the following sequence of job events on the log: ACTIVATED -> FAIL -> FAILED -> ACTIVATED.\r\n\r\nIt's clearer to users that the `FAIL` command resulted in a `FAILED` event, and the job was then `ACTIVATED`.\r\n\r\nHaving ACTIVATED -> FAIL -> ACTIVATED is a bit confusing.",
    "title": "Failing jobs with job push enabled breaks the job lifecycle"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12915",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "While working on #12839, it was observed that it is not possible to change the journal record schema without breaking backward compatability. (See [comment](https://github.com/camunda/zeebe/pull/12839#issuecomment-1568545921))\r\n\r\nWhen journal record schema was moved to SBE, one of the main motivator was to allow extending it without breaking changes. \r\n\r\nProblems:\r\n- A broker on newer version cannot receive any events via raft replication, if the event was written by a leader at older version.\r\n   - This is a deal breaker. As we cannot upgrade a running system to a new version.\r\n- A broker on older version cannot receive any events via raft replication, of the event was written by a leader at newer version.\r\n   - This is less problematic, as it might cause some unavailability during rolling update.\r\n\r\nGoal:\r\nEnsure we can extend journal and raft record schema with out breaking compatibility. \r\n\r\nTo limit the scope, it might be ok if we could ensure backward compatibility. That is, ensure brokers at newer versions can work with record written with old version. It is ok, if old versions cannot read records from new version as long as it can detect it and do not cause any inconsistency. \r\n\r\n\r\nblocking #12839 \r\n\n\n deepthidevaki: Summary of discussion with @npepinpe \r\n\r\nIt is better to send the serialized journal record in AppendRequest. This would require some changes in the journal api and raft replication handling. We have to figure out how to change this, without breaking compatibility. During rolling update, it might be acceptible if follower's on older version cannot receive any events from a leader on newer version. But we should ensure that, followers on newer version can receive events from leaders on older version. Otherwise this can block both rolling update and recreating with new version.\r\n\r\nOne idea is to check the version of the sender, and interpret the request accordingly. Right now, we don't have a concept of raft-protocol-version. But we can probably add that, and not having it can be interpreted as old version. When raft follower on new version, receives a request from old leader, it writes the record using the old sbe version and the checksum calculated will match that of the original one. If the request is from the new leader, it uses the new logic to handle the serialized journal record.\n megglos: ZDP-Triage:\n- the checksum is affected by the schema version\n- we need a general mechanism to handle such changes\n- blocks any changes to the journal schema => makes sense to work on it asap\n- also relates to properly supporting updates in general\n- the solution will have an effect on rolling updates going forward https://github.com/camunda/product-hub/issues/256",
    "title": "Allow extending journal record format with out breaking compatibility"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12886",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\n\r\nZeebe offers a [Stackdriver](https://github.com/camunda/zeebe/tree/main/util/src/main/java/io/camunda/zeebe/util/logging/stackdriver) logging format that displays logs as JSON, formatted for Google Cloud Logging.\r\n\r\nNot all the logs are formatted according to this format though:\r\n\r\n* At startup, Zeebe displays a huge ASCII banner showing  `Zeebe`\r\n* Some logs (Tomcat logs?) are not formatted as JSON:\r\n  ```\r\n  May 30, 2023 6:51:02 AM org.apache.coyote.AbstractProtocol init\r\n  INFO: Initializing ProtocolHandler [\"http-nio-0.0.0.0-9600\"]\r\n  May 30, 2023 6:51:02 AM org.apache.catalina.core.StandardService startInternal\r\n  INFO: Starting service [Tomcat]\r\n  May 30, 2023 6:51:02 AM org.apache.catalina.core.StandardEngine startInternal\r\n  INFO: Starting Servlet engine: [Apache Tomcat/10.1.7]\r\n  May 30, 2023 6:51:02 AM org.apache.catalina.core.ApplicationContext log\r\n  INFO: Initializing Spring embedded WebApplicationContext\r\n  ```\r\n\r\n**Describe the solution you'd like**\r\n\r\nIdeally:\r\n\r\n* All the logs are properly formatted as JSON, instead of a mix of plain text/JSON.\r\n* The banner can be completely removed.\r\n\r\nThis would make all the logs uniform, help to classify them correctly, and would prevent spurious parsing of plain-text only logs.\r\n\r\n**Describe alternatives you've considered**\r\n\r\nNone\r\n\r\n**Additional context**\r\n\r\nA typical startup looks like this:\r\n\r\n```\r\nPicked up JAVA_TOOL_OPTIONS: -XX:MaxRAMPercentage=50.0 -XX:InitialRAMPercentage=25.0 -XX:+ExitOnOutOfMemoryError -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/usr/local/zeebe/data/java_started_1685429455.hprof\r\n  ______  ______   ______   ____    ______     ____    _____     ____    _  __  ______   _____  \r\n |___  / |  ____| |  ____| |  _ \\  |  ____|   |  _ \\  |  __ \\   / __ \\  | |/ / |  ____| |  __ \\ \r\n    / /  | |__    | |__    | |_) | | |__      | |_) | | |__) | | |  | | | ' /  | |__    | |__) |\r\n   / /   |  __|   |  __|   |  _ <  |  __|     |  _ <  |  _  /  | |  | | |  <   |  __|   |  _  / \r\n  / /__  | |____  | |____  | |_) | | |____    | |_) | | | \\ \\  | |__| | | . \\  | |____  | | \\ \\ \r\n /_____| |______| |______| |____/  |______|   |____/  |_|  \\_\\  \\____/  |_|\\_\\ |______| |_|  \\_\\\r\n                                                                                                \r\n{\"severity\":\"INFO\",\"logging.googleapis.com/sourceLocation\":{\"function\":\"logStarting\",\"file\":\"StartupInfoLogger.java\",\"line\":51},\"message\":\"Starting StandaloneBroker v8.2.3 using Java 17.0.6 with PID 7 (/usr/local/zeebe/lib/camunda-zeebe-8.2.3.jar started by root in /usr/local/zeebe)\",\"serviceContext\":{\"service\":\"zeebe\",\"version\":\"8.2.3\"},\"context\":{\"threadId\":1,\"threadPriority\":5,\"loggerName\":\"io.camunda.zeebe.broker.StandaloneBroker\",\"threadName\":\"main\"},\"timestampSeconds\":1685429459,\"timestampNanos\":42397893}\r\n{\"severity\":\"DEBUG\",\"logging.googleapis.com/sourceLocation\":{\"function\":\"logStarting\",\"file\":\"StartupInfoLogger.java\",\"line\":52},\"message\":\"Running with Spring Boot v3.0.5, Spring v6.0.8\",\"serviceContext\":{\"service\":\"zeebe\",\"version\":\"8.2.3\"},\"context\":{\"threadId\":1,\"threadPriority\":5,\"loggerName\":\"io.camunda.zeebe.broker.StandaloneBroker\",\"threadName\":\"main\"},\"timestampSeconds\":1685429459,\"timestampNanos\":128377708}\r\n{\"severity\":\"INFO\",\"logging.googleapis.com/sourceLocation\":{\"function\":\"logStartupProfileInfo\",\"file\":\"SpringApplication.java\",\"line\":638},\"message\":\"The following 1 profile is active: \\\"broker\\\"\",\"serviceContext\":{\"service\":\"zeebe\",\"version\":\"8.2.3\"},\"context\":{\"threadId\":1,\"threadPriority\":5,\"loggerName\":\"io.camunda.zeebe.broker.StandaloneBroker\",\"threadName\":\"main\"},\"timestampSeconds\":1685429459,\"timestampNanos\":130430841}\r\n{\"severity\":\"INFO\",\"logging.googleapis.com/sourceLocation\":{\"function\":\"initialize\",\"file\":\"TomcatWebServer.java\",\"line\":108},\"message\":\"Tomcat initialized with port(s): 9600 (http)\",\"serviceContext\":{\"service\":\"zeebe\",\"version\":\"8.2.3\"},\"context\":{\"threadId\":1,\"threadPriority\":5,\"loggerName\":\"org.springframework.boot.web.embedded.tomcat.TomcatWebServer\",\"threadName\":\"main\"},\"timestampSeconds\":1685429462,\"timestampNanos\":567312120}\r\nMay 30, 2023 6:51:02 AM org.apache.coyote.AbstractProtocol init\r\nINFO: Initializing ProtocolHandler [\"http-nio-0.0.0.0-9600\"]\r\nMay 30, 2023 6:51:02 AM org.apache.catalina.core.StandardService startInternal\r\nINFO: Starting service [Tomcat]\r\nMay 30, 2023 6:51:02 AM org.apache.catalina.core.StandardEngine startInternal\r\nINFO: Starting Servlet engine: [Apache Tomcat/10.1.7]\r\nMay 30, 2023 6:51:02 AM org.apache.catalina.core.ApplicationContext log\r\nINFO: Initializing Spring embedded WebApplicationContext\r\n{\"severity\":\"INFO\",\"logging.googleapis.com/sourceLocation\":{\"function\":\"prepareWebApplicationContext\",\"file\":\"ServletWebServerApplicationContext.java\",\"line\":291},\"message\":\"Root WebApplicationContext: initialization completed in 3603 ms\",\"serviceContext\":{\"service\":\"zeebe\",\"version\":\"8.2.3\"},\"context\":{\"threadId\":1,\"threadPriority\":5,\"loggerName\":\"org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext\",\"threadName\":\"main\"},\"timestampSeconds\":1685429462,\"timestampNanos\":856072455}\r\n{\"severity\":\"WARNING\",\"logging.googleapis.com/sourceLocation\":{\"function\":\"overrideDiskConfig\",\"file\":\"DataCfg.java\",\"line\":69},\"message\":\"Configuration parameter data.diskUsageCommandWatermark is deprecated. Use data.disk.freeSpace.processing instead.\",\"serviceContext\":{\"service\":\"zeebe\",\"version\":\"8.2.3\"},\"context\":{\"threadId\":1,\"threadPriority\":5,\"loggerName\":\"io.camunda.zeebe.broker.system\",\"threadName\":\"main\"},\"timestampSeconds\":1685429463,\"timestampNanos\":631599523}\r\n{\"severity\":\"WARNING\",\"logging.googleapis.com/sourceLocation\":{\"function\":\"overrideDiskConfig\",\"file\":\"DataCfg.java\",\"line\":75},\"message\":\"Configuration parameter data.diskUsageReplicationWatermark is deprecated. Use data.disk.freeSpace.replication instead.\",\"serviceContext\":{\"service\":\"zeebe\",\"version\":\"8.2.3\"},\"context\":{\"threadId\":1,\"threadPriority\":5,\"loggerName\":\"io.camunda.zeebe.broker.system\",\"threadName\":\"main\"},\"timestampSeconds\":1685429463,\"timestampNanos\":632382123}\r\n{\"severity\":\"INFO\",\"logging.googleapis.com/sourceLocation\":{\"function\":\"<init>\",\"file\":\"EndpointLinksResolver.java\",\"line\":58},\"message\":\"Exposing 8 endpoint(s) beneath base path '/actuator'\",\"serviceContext\":{\"service\":\"zeebe\",\"version\":\"8.2.3\"},\"context\":{\"threadId\":1,\"threadPriority\":5,\"loggerName\":\"org.springframework.boot.actuate.endpoint.web.EndpointLinksResolver\",\"threadName\":\"main\"},\"timestampSeconds\":1685429465,\"timestampNanos\":427691130}\r\nMay 30, 2023 6:51:05 AM org.apache.coyote.AbstractProtocol start\r\nINFO: Starting ProtocolHandler [\"http-nio-0.0.0.0-9600\"]\r\n\r\n[...]\r\n```\r\n\r\n* The banner should be removed\r\n* Non-JSON logs should be turned to JSON\r\n* Ideally, the `JAVA_TOOL_OPTIONS` should also be turned into JSON (or removed)\n\n korthout: ZPA triage:\n- relevant issue, but not a priority for us (`later`)\n- good first issue for newcomers\n megglos: ZDP-Triage:\n- spring/tomcat seems to use some different logger setup or sysout\n- JAVA_TOOL_OPTIONS output seems to come from the JVM => it cannot be suppressed => would require sout redirect, alternatively we could use other ways to pass these options\n- @multani what's the actual impact for the logging infrastructure, I would assume these logs are ignored or not properly categorized?\n- assigning later as well\n multani: > * @multani what's the actual impact for the logging infrastructure, I would assume these logs are ignored or not properly categorized?\r\n\r\nImpact is not huge: logs are analyzed as plain text without any extra metadata and are not really filterable / analyzable.\r\n\r\nFor instance: I never saw Tomcat errors in our systems, but it would be difficult to look them up as the log level is not parsed. If they were parsed correctly, errors would clearly stand out.\r\n\r\nThe banner is only displays as garbage into the logs and should be simply removed.\n multani: This is definitely not critical, but consider it as a good practice for production.\n npepinpe: re Tomcat, tbh I've been thinking we should switch to WebFlux, but there wasn't much pressure for it. but it would remove unnecessary dependencies, since webflux is built on top of netty and we already bundle netty for our own usage (and would not do this weird logging to STDERR as well).\n npepinpe: My proposal:\r\n\r\n- [x] Since we can't suppress `JAVA_TOOL_OPTIONS`, use `JAVA_OPTS` to pass options instead of `JAVA_TOOL_OPTIONS`. This would require some changes on the Helm chart and the controller though.\r\n- [x] [Switch from Tomcat to WebFlux or Undertow (preference to WebFlux). WebFlux is Spring's reactive web server based on Netty, and as our whole system is based on Netty anyway, might as well go for that](https://github.com/camunda/zeebe/pull/13539)\r\n- [x] The huge banner can be disabled via `SPRING_MAIN_BANNER-MODE=off` - again this is a change in the Helm chart/controller.\r\n\r\nSo from the Zeebe side, it would be just the second part. The others would be done in the controller.\n multani: If you can't remove the `Picked up JAVA_TOOL_OPTIONS` line, would it be possible to output that line to stdout instead of stderr?\n npepinpe: We have no control over that, that's the JVM directly writing this =/\r\n\r\nThe only way I can think of would be to redirect STDOUT to STDERR, but then that's all logs =/\r\n\r\nI think the best option is to simply not use `JAVA_TOOL_OPTIONS`. Our scripts already support `JAVA_OPTS`, so we can just make sure the controller and Helm chars both use that instead.\n multani: > We have no control over that, that's the JVM directly writing this =/\r\n\r\nAh yes, you are right, I forgot about that :facepalm: \n npepinpe: Done via https://github.com/camunda-cloud/camunda-operator/pull/1753\r\n\r\nThe Helm charts were not updated, so they will still print non JSON logs, but that can be fixed by essentially doing the same we did in https://github.com/camunda-cloud/camunda-operator/pull/1753.",
    "title": "Zeebe should only log as JSON, if configured to do so"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12875",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Summary**\r\n\r\nRun `ModifyProcessInstanceRejectionTest#shouldRejectActivationWhenAncestorScopeIsNotFlowScope` test failed on Windows 10.\r\n\r\n**Logs**\r\n\r\n\r\n<details><summary>Logs</summary>\r\n<pre>\r\nE PI   SEQ_FLOW_TAKEN    - #63->#05 K21 - SEQUENCE_FLOW \"sequenc..b588006\" in <process \"process\"[K03]>\r\nC PI   ACTIVATE          - #64->#05 K22 - PARALLEL_GATEWAY \"join\" in <process \"process\"[K03]>\r\nR PI   ACTIVATE          - #65->#05 K22 - PARALLEL_GATEWAY \"join\" in <process \"process\"[K03]> !INVALID_STATE (Expected to be able to activate parallel gateway 'join', but not all sequence flows have been taken.)\r\nC MOD  MODIFY            - #66-> -1 K03 - <activate \"A\" no vars> <activate \"B\" no vars> <activate \"A\" no vars> \r\nR MOD  MODIFY            - #67->#66 K03 - <activate \"A\" no vars> <activate \"B\" no vars> <activate \"A\" no vars>  !INVALID_ARGUMENT (Expected to modify instance of process 'process' but it contains one or more activate instructions with an ancestor scope key that is not an ancestor of the element to activate:\r\n- instance '2251799813685257' of element 'subProcess2' is not an ancestor of element 'A'\r\n- instance '2251799813685263' of element 'B' is not an ancestor of element 'A')\r\n\r\n\r\n\r\njava.lang.AssertionError: [Expect that subProcess2 cannot be selected as ancestor of task A] \r\nExpecting rejectionReason of:\r\n  <{\"valueType\":\"PROCESS_INSTANCE_MODIFICATION\",\"key\":2251799813685251,\"position\":67,\"timestamp\":1685115125924,\"recordType\":\"COMMAND_REJECTION\",\"intent\":\"MODIFY\",\"partitionId\":1,\"rejectionType\":\"INVALID_ARGUMENT\",\"rejectionReason\":\"Expected to modify instance of process 'process' but it contains one or more activate instructions with an ancestor scope key that is not an ancestor of the element to activate:\\r\\n- instance '2251799813685257' of element 'subProcess2' is not an ancestor of element 'A'\\r\\n- instance '2251799813685263' of element 'B' is not an ancestor of element 'A'\",\"brokerVersion\":\"8.3.0\",\"value\":{\"processInstanceKey\":2251799813685251,\"ancestorScopeKeys\":[],\"terminateInstructions\":[],\"activateInstructions\":[{\"ancestorScopeKeys\":[],\"elementId\":\"A\",\"ancestorScopeKey\":2251799813685257,\"variableInstructions\":[]},{\"ancestorScopeKeys\":[],\"elementId\":\"B\",\"ancestorScopeKey\":2251799813685257,\"variableInstructions\":[]},{\"ancestorScopeKeys\":[],\"elementId\":\"A\",\"ancestorScopeKey\":2251799813685263,\"variableInstru...>\r\nto be:\r\n  <Expected to modify instance of process 'process' but it contains one or more activate instructions with an ancestor scope key that is not an ancestor of the element to activate:\r\n- instance '2251799813685257' of element 'subProcess2' is not an ancestor of element 'A'\r\n- instance '2251799813685263' of element 'B' is not an ancestor of element 'A'>\r\nbut was:\r\n  <Expected to modify instance of process 'process' but it contains one or more activate instructions with an ancestor scope key that is not an ancestor of the element to activate:\r\n- instance '2251799813685257' of element 'subProcess2' is not an ancestor of element 'A'\r\n- instance '2251799813685263' of element 'B' is not an ancestor of element 'A'>\r\n\r\n\tat io.camunda.zeebe.engine.processing.processinstance.ModifyProcessInstanceRejectionTest.shouldRejectActivationWhenAncestorScopeIsNotFlowScope(ModifyProcessInstanceRejectionTest.java:664)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n</pre>\r\n</details>\r\n\r\n```\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Detecting the operating system and CPU architecture\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] os.detected.name: windows\r\n[INFO] os.detected.arch: x86_64\r\n[INFO] os.detected.bitness: 64\r\n[INFO] os.detected.version: 10.0\r\n[INFO] os.detected.version.major: 10\r\n[INFO] os.detected.version.minor: 0\r\n[INFO] os.detected.classifier: windows-x86_64\r\n\r\n```\r\n\r\n\n\n remcowesterhoud: @skayliu I assume this test always fails for you and is not flaky? I believe it has to do with the line breaks I can see in your error message: `\\r\\n`\n skayliu: @remcowesterhoud, Yes, It always failed not flaky.\n remcowesterhoud: Perfect, thanks! Time to dust off my old Windows laptop 😄 ",
    "title": "`ModifyProcessInstanceRejectionTest#shouldRejectActivationWhenAncestorScopeIsNotFlowScope` test fails on Windows"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12837",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\nUsing an error catch event (boundary or start event in Subprocess) with an empty `errorCode` results in incident with `errorType=UNHANDLED_ERROR_EVENT`. \r\nAlso only error events with a non-empty errorCode will be reported like this: `Available error events are [boundary]`.\r\n\r\n\r\n**To Reproduce**\r\n\r\n1. Create a BPMN process with error catch event as boundary event or as start event in subprocess.\r\n2. Deploy, start the process. In a job client throw an error command with an unhandled errorCode.\r\n3. Zeebe creates an incident instead of catching the error \r\n\r\nHere an example process [errorProcessCatchAll.bpmn.txt](https://github.com/camunda/zeebe/files/11544062/errorProcessCatchAll.bpmn.txt)\r\n\r\n[Slack thread](https://camunda.slack.com/archives/CSQ2E3BT4/p1684841521468159)\r\n\r\n**Expected behavior**\r\n\r\nThe error is caught and no incident is created.\r\n\r\n**Log/Stacktrace**\r\n\r\nExample error message from Zeebe:\r\n```\r\nerrorType=UNHANDLED_ERROR_EVENT, errorMessage='Expected to throw an error event with the code 'unknown' with message 'Process error', but it was not caught. Available error events are [boundary]\r\n```\r\n\r\n**Environment:**\r\n- OS: MacOS\r\n- Zeebe Version: `8.2.0-alpha3` and later\r\n- Configuration: ElasticsearchExporter\r\n\r\n\n\n lzgabel: Hi @korthout. I think I found the problem, please assign this task to  me, I will submit a PR to fix this problem soon. Thanks :heart:\n lzgabel: BTW. As a workaround, user can catch all error events by removing the `errorEventDefinition`.\r\n",
    "title": "Catch all error events by using empty 'errorCode' does not work"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12833",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nThere was an incident in one of the chaos test models, where a none end event had an output mapping referencing a non-existent variable `source` to be mapped to `target`. I resolved the incident by adding a dummy variable (the mapping should not have been there anyway), but then the process was stuck on the none end event.\r\n\r\nSee https://bru-2.operate.camunda.io/eeef5734-cfd6-47a5-a2ed-5fe13269e589/processes/2251799815205221\r\n\r\n**To Reproduce**\r\n\r\nAdd the following test case to `io.camunda.zeebe.engine.processing.incidentOutputMappingIncidentTest`:\r\n\r\n```java\r\n          {\r\n            \"None end event\",\r\n            ENGINE\r\n                .deployment()\r\n                .withXmlResource(\r\n                    Bpmn.createExecutableProcess(PROCESS_ID)\r\n                        .startEvent()\r\n                        .endEvent(\"endEventId\", b -> b.zeebeOutputExpression(\"foo\", \"bar\"))\r\n                        .done()),\r\n            \"endEventId\",\r\n            false\r\n          },\r\n```\r\n\r\n**Expected behavior**\r\n\r\nI can resolve incidents on a none end event.\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.2.3\r\n- Configuration: SaaS - G3 S\r\n\n\n korthout: As a workaround, users can use [Process Instance Modification](https://docs.camunda.io/docs/next/components/concepts/process-instance-modification/) to re-activate the end event\n korthout: ZPA triage:\r\n- should be simple to resolve `good-first-issue` (just implement the `onComplete` method on `NoneEndEventBehavior` in the `EndEventProcessor`)\r\n- since it's low-hanging fruit and may effect users we would like to tackle it sooner than later.",
    "title": "Cannot resolve output mapping incident on a none end event"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12754",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nRelated to #12374 \r\n\r\nA customer observed another case where the startup failed with the error:\r\n```\r\n\"Expected to find a snapshot at index >= log's first index X, but found snapshot Y. A previous snapshot is most likely corrupted.\"\r\n```\r\nI could not verify it, but our assumption currently is that this is a false positive. The following might have happened:\r\n1. Follower received snapshot\r\n2. Before committing the snapshot, it reset segments\r\n3. While deleting the segments, the node was shutdown.\r\n4. After restart, it has the old snapshot plus partially deleted segments.\r\n\r\nI think this is plausible as the reset/deleting segments is not and atomic operation. We might not be handling it correctly.\r\n\r\n**Expected behavior**\r\n\r\nThe reset or snapshot commit process should not result in an invalid intermediate state which is detected as corruption.\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.1.*\r\n\r\nRelated to [support](https://jira.camunda.com/browse/SUPPORT-16905) \r\n\n\n deepthidevaki: A potential solution might be to delete the segments in the reverse order, combined with the PR that fixed #12374.\r\n\r\nIn a related issue, I discussed with @npepinpe a different approach. I was planning to create a separate issue for it, but I'm just dumping my thoughts here, as I think it is also relevant for this bug.\r\n\r\n------\r\nTerminologies\r\n\r\nIn-Sync replicas - The followers that are in sync with leaders and participating in the commit. In-sync replicas forms the quorum.\r\n\r\nOut-of-sync follower - The follower which is receiving the event potentially after it is already committed.\r\n\r\nIn a setup with replicationFactor 3, there will be 2 in-sync replicas, out of which one will be the leader. The third one can be an out-of-sync follower.\r\n\r\nRaft guarantees that any committed event is available in all in-sync replicas. The events are replicated to the out-of-sync follower eventually. But there is no guarantee when they are replicated. Ideally, they are replicated immediately. But it is possible the follower is slow, or the network is slow, resulting the out-of-sync follower to lag behind the leader by a large amount. \r\n\r\nAt some point, the leader sends a snapshot instead of the events, because the follower is lagging behind. At this point, the current behavior is as follows:\r\n\r\nFollower resets the log (delete all segments)\r\nReceives and commits the new snapshot\r\nDelete the old snapshot\r\nContinue receiving the events after the snapshot index\r\n\r\nThe above process can lead to the following scenarios:\r\n\r\nStep 1 and 2 are not atomic. If the replica crashes in between, then it restarts with an empty state.\r\nIf it restarts before step 4, then it starts with a snapshot  that is not use-able. Snapshot is valid only if the event corresponding to lastFollowUpEventPosition is in the log.\r\n\r\nIn a normal scenario, the above situation is not problematic because the in-sync replicas are healthy and it is guaranteed that only one of the in-sync replica becomes the leader.\r\n\r\nHowever, in a disaster scenario where all in-sync replicas are gone, it might be acceptable to continue functioning with whatever data is available in the out-of-sync replica. It might not have up-to-date state, but for some use-cases it is ok to lose last X amount of data. In such cases, we want to enable users to recover the cluster from the state of the out-of-sync replica. However, in the above scenario the out-of-sync replica might be in a state where it's state is empty or the snapshot is not useable. \r\n\r\nSo, it would be good if we could ensure that the state in the out-of-sync follower is always in a valid state. To acheive  that we can\r\n\r\nDo not delete old snapshot and reset log immediately when it receives a snapshot.\r\nInstead keep the old snapshot and the logs until the new snapshot is committed and its follow up event is received.\r\n\r\nSolution 1:\r\n\r\nWe keep the old snapshot + logs in a different folder, the new snapshot and new logs in a different folder. On restarts, it attempts to use the new snapshot + log and fall back to the old state if necessary. \r\n\r\nSolution 2:\r\n\r\nDo not reset the logs, when a new snapshot is received. Instead, add a marker record in the journal to indicate that there is a snapshot at the position. The readers must know how to handle these “gaps”. For example, if stream processor reader hits this marker record, it has to throw away its state and replace with the snapshot. If the raft leader reads this record, it should replicate the snapshot instead of the event etc.\r\n\r\nThe compaction logic should take this into account, and compact only if there is one valid snapshot with its followup event in the log. So the follower can rollback to an old valid state if necessary.\r\n\r\nThis would also help in some issue that we observed recently where restarting the node during resetting the log can incorrectly lead to detecting it as corrupted log.\n npepinpe: With #12868, we implemented a quick fix to prevent the specific case we're aware of. By deleting the segments in reverse order during reset, we can ensure that there are no gaps in the log/snapshot, and thus no corruption. Data loss is acceptable here as it was the desired outcome of the reset operation.\r\n\r\nHowever, we agreed not to close the issue - this quick fix only solves a specific thing, and not the root cause, which is that the install operation on the follower is not atomic, specifically persisting the snapshot/clearing the log/updating the meta store/getting the first record for the snapshot/etc. are all independent operations and the node may stop at any point in between, leaving us in a weird state.\n megglos: ZDP-Planning:\r\n- while we are at it, it would be worth to assess potential solutions to get an understanding of how much effort it is to change\r\n- needs prototyping\r\n- back to backlog as upcoming for now to be picked up going forward\r\n- relates to upcoming resilience epic",
    "title": "Journal reset resulting in an intermediate state which is detected as corruption"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12699",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "In https://github.com/camunda/zeebe/issues/12591, we found that when engine rejects the deployment request it cannot write the rejection record because it was attempting to write the whole deployment which is greater than maxMessageSize. This resulted in a loop in StreamProcessor, where it repeatedly fails to write the rejection record leading to the partition being fully blocked and not making any progress. https://github.com/camunda/zeebe/issues/12591#issuecomment-1535879364 \r\n\r\nTo fix this, now we reject the request in CommandAPI before writing to the logstream https://github.com/camunda/zeebe/pull/12676. But, to be safer we should also ensure that we are not writing rejection records that are too large, to prevent such accidental cases.\r\n\r\nEnsure that rejections records can be written reliably, even if the original command is rejected due to ExceededBatchSize. Ideally, rejection record should only have a reference to the original command, but not the full command. If possible, do not write the entire command for any type of rejections. Or atleast, trim rejection record in case of ExceededBatchSize exception.\r\n\r\n\r\n\n\n berkaycanbc: ZPA Triage:\n\nWe think this is a bug. We are planning to process this in the next mob-programming session.\n korthout: ~~Mob~~Pair programming with @koevskinikola results:\r\n- Run `io.camunda.zeebe.it.client.command.CreateDeploymentTest::shouldRejectDeployIfResourceIsTooLarge`\r\n- Notice that an error is logged, this should not be the case\r\n- Make sure that [rejection is written safely](https://github.com/camunda/zeebe/blob/main/engine/src/main/java/io/camunda/zeebe/engine/Engine.java#L186-L194)\n abbasadel: ZPA planning: \n- Moving back to the backlog since we don't have time for it in the next iteration.",
    "title": "Do not write the entire command for rejection"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12623",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nAfter restoring from backup, if the leader that took the original backup is no more the leader of the partition, this can result in a duplicate backup of the partition with the same backup id. The new backup is logically equivalent to the old backup. So it doesn't matter which backup we use later. However, re-taking the backup is unnecessary as it wastes resources.\r\n\r\n**To Reproduce**\r\nRestore zeebe from a backup, and observe the logs or inspect the backup store.\r\n\r\n**Expected behavior**\r\nA partition should not re-take a backup after restore.\r\n\n",
    "title": "After restoring from backup, a partition re-takes the backup"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12622",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nSometime, a partition may attempt to take a backup again when a backup for the same id already exists. This case is rare, but can happen sometimes if there is a leader change while taking a backup. If this happens, list backup fails with an error message:\r\n```\r\n{\r\n    \"message\": \"Duplicate key 1 (attempted merging values BackupStatus[backupId=1, partitionId=1, status=COMPLETED, failureReason=, brokerVersion=8.2.2, createdAt=...])\"\r\n}\r\n```\r\n\r\n**Expected behavior**\r\nList backup should be able to handle duplicate backup ids for a partition.\r\n\n",
    "title": "List backup fails when a partition has same backup taken by multiple nodes"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12597",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nTrying to list all available backups will always fail without a useful error message.\r\nThe gateway distributes a list request to all brokers which then list all of their backups and try to respond with a `BackupListResponse`:\r\n\r\nhttps://github.com/camunda/zeebe/blob/4854b6606a803926ed9cadabfc2edb4aede18cb4/protocol/src/main/resources/cluster-management-protocol.xml#L64-L76\r\n\r\nThe `groupSizeEncoding` is defined by us:\r\n\r\nhttps://github.com/camunda/zeebe/blob/c861aac736376e1cc20aa558979c6d9c289b4a1f/protocol/src/main/resources/common-types.xml#L16-L19\r\n\r\nIt uses a `unit8` to represent the number of entries. When trying to write a `BackupListResponse` with more than 255 entries, the encoder rejects it:\r\n```\r\njava.lang.IllegalArgumentException: count outside allowed range: count=774\r\n\tat io.camunda.zeebe.protocol.management.BackupListResponseEncoder$BackupsEncoder.wrap(BackupListResponseEncoder.java:137) ~[zeebe-protocol-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.protocol.management.BackupListResponseEncoder.backupsCount(BackupListResponseEncoder.java:114) ~[zeebe-protocol-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.protocol.impl.encoding.BackupListResponse.write(BackupListResponse.java:100) ~[zeebe-protocol-impl-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.broker.transport.backupapi.BackupApiResponseWriter.write(BackupApiResponseWriter.java:71) ~[zeebe-broker-8.2.2.jar:8.2.2]\r\n....\r\n\r\n```\r\n**To Reproduce**\r\n\r\nTake 256 backups, then query all backups via `GET actuator/backups`.\r\n\r\n**Expected behavior**\r\n\r\n1. Zeebe supports much more backups than 255 (for example by using a `uint16`, thus supporting 65535 backups)\r\n2. The number of listed backups should be limited to a reasonable number. Querying the backup store to list, say 1000, available backups is likely to result in timeouts and makes the backup API unusable.\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.lang.IllegalArgumentException: count outside allowed range: count=774\r\n\tat io.camunda.zeebe.protocol.management.BackupListResponseEncoder$BackupsEncoder.wrap(BackupListResponseEncoder.java:137) ~[zeebe-protocol-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.protocol.management.BackupListResponseEncoder.backupsCount(BackupListResponseEncoder.java:114) ~[zeebe-protocol-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.protocol.impl.encoding.BackupListResponse.write(BackupListResponse.java:100) ~[zeebe-protocol-impl-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.broker.transport.backupapi.BackupApiResponseWriter.write(BackupApiResponseWriter.java:71) ~[zeebe-broker-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.transport.impl.ServerResponseImpl.write(ServerResponseImpl.java:50) ~[zeebe-transport-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.transport.impl.AtomixServerTransport.sendResponse(AtomixServerTransport.java:154) ~[zeebe-transport-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.broker.transport.backupapi.BackupApiResponseWriter.tryWriteResponse(BackupApiResponseWriter.java:51) ~[zeebe-broker-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.broker.transport.AsyncApiRequestHandler.lambda$handleRequest$1(AsyncApiRequestHandler.java:123) ~[zeebe-broker-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.future.FutureContinuationRunnable.run(FutureContinuationRunnable.java:28) ~[zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94) ~[zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) [zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) [zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- Zeebe Version: >= 8.1\r\n\n",
    "title": "Listing backups fails if more than 255 backups are available"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12591",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nAn attempt to upload oversized BPMN (other the segment limit) causes unrecoverable failure of Zeebe:\r\n* further BPMN upload of proper sizes are not possible\r\n* eventually partitions becomes unhealthy and not recovered\r\n\r\nIt is degradation from version 8.1.6 that just rejected incorrect BPMN without further problems.\r\n\r\n**To Reproduce**\r\n\r\nUpload the BMPN bigger than configured \r\n      maxMessageSize: 64KB\r\n\r\n\r\n**Expected behavior**\r\n\r\nBMPN Upload is rejected\r\n\r\n**Log/Stacktrace**\r\n```\r\n2023-04-27 17:03:37.989 [Broker-0-StreamProcessor-1] [Broker-0-zb-actors-1] ERROR\r\n      io.camunda.zeebe.broker.process - Unexpected error while processing resource 'f6c0b39d-8357-40ea-8d79-7e2611a89677.bpmn'\r\nio.camunda.zeebe.stream.api.records.ExceededBatchRecordSizeException: Can't append entry: 'RecordBatchEntry[recordMetadata=RecordMetadata{recordType=EVENT, valueType=PROCESS, intent=CREATED}, key=2251799852104669, sourceIndex=-1, unifiedRecordValue={\"bpmnProcessId\":\"id_f6c0b39d-8357-40ea-8d79-7e2611a89677\",\"version\":1,\"processDefinitionKey\":2251799852104669,\"resourceName\":\"f6c0b39d-8357-40ea-8d79-7e2611a89677.bpmn\",\"checksum\":\"uG1QH8XcklrgFGYZI3HhPg==\",\"resource\":\"PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0idXRmLTgiPz4NCjxicG1uOmRlZmluaXRpb25zIHhtbG5zOmJwbW5kaT0iaHR0cDovL3d3dy5vbWcub3JnL3NwZWMvQlBNTi8yMDEwMDUyNC9ESSIgeG1sbnM6ZGM9Imh0dHA6Ly93d3cub21nLm9yZy9zcGVjL0RELzIwMTAwNTI0L0RDIiB4bWxuczp6ZWViZT0iaHR0cDovL2NhbXVuZGEub3JnL3NjaGVtYS96ZWViZS8xLjAiIHhtbG5zOmRpPSJodHRwOi8vd3d3Lm9tZy5vcmcvc3BlYy9ERC8yMDEwMDUyNC9ESSIgeG1sbnM6eHNpPSJodHRwOi8vd3d3LnczLm9yZy8yMDAxL1hNTFNjaGVtYS1pbnN0YW5jZSIgaWQ9ImlkXzZmNTk2NTlmLWE3OGMtNDMyMy04M2VmLTdlODMwYmJlNTUwNSIgdGFyZ2V0TmFtZXNwYWNlPSJodHRwOi8vYnBtbi5pby9zY2hlbWEvYnBtbiIgZXhwb3J0ZXI9IkNvbmZpcm1pdCBCUE1OIEJ1aWxkZXIiIGV4cG9ydGVyVmVyc2lvbj0iMS4wLjAuMCIgeG1sbnM6YnBtbj0iaHR0cD...' with size: 1010867 this would exceed the maximum batch size. [ currentBatchEntryCount: 0, currentBatchSize: 0]\r\n\tat io.camunda.zeebe.stream.impl.records.RecordBatch.appendRecord(RecordBatch.java:67) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.BufferedProcessingResultBuilder.appendRecordReturnEither(BufferedProcessingResultBuilder.java:62) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.api.ProcessingResultBuilder.appendRecord(ProcessingResultBuilder.java:38) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.streamprocessor.writers.ResultBuilderBackedEventApplyingStateWriter.appendFollowUpEvent(ResultBuilderBackedEventApplyingStateWriter.java:40) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.transformProcessResource(BpmnResourceTransformer.java:162) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.lambda$transformResource$0(BpmnResourceTransformer.java:77) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.util.Either$Right.map(Either.java:355) ~[zeebe-util-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.lambda$transformResource$1(BpmnResourceTransformer.java:75) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.util.Either$Right.flatMap(Either.java:366) ~[zeebe-util-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.transformResource(BpmnResourceTransformer.java:65) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.DeploymentTransformer.transformResource(DeploymentTransformer.java:122) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.DeploymentTransformer.transform(DeploymentTransformer.java:98) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.DeploymentCreateProcessor.processRecord(DeploymentCreateProcessor.java:87) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.Engine.process(Engine.java:142) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.batchProcessing(ProcessingStateMachine.java:346) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$2(ProcessingStateMachine.java:268) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run(ZeebeTransaction.java:84) ~[zeebe-db-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand(ProcessingStateMachine.java:268) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord(ProcessingStateMachine.java:227) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.readNextRecord(ProcessingStateMachine.java:203) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\r\n\r\n```\r\n\r\n**Environment:**\r\n- OS: Windows\r\n- Zeebe Version: 8.2.3\r\n\r\nIn version 8.1.6 the behavior is correct. Here's stack traces from this version:\r\n\r\n```\r\n2023-04-27 17:52:05.506 [io.camunda.zeebe.broker.transport.commandapi.CommandApiRequestHandler] [Broker-0-zb-actors-1] ERROR\r\n      io.camunda.zeebe.broker.transport - Unexpected error on writing CREATE command\r\njava.lang.IllegalArgumentException: Expected to claim segment of size 1010866, but can't claim more than 65536 bytes.\r\n\tat io.camunda.zeebe.dispatcher.Dispatcher.offer(Dispatcher.java:207) ~[zeebe-dispatcher-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.dispatcher.Dispatcher.claimSingleFragment(Dispatcher.java:143) ~[zeebe-dispatcher-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.logstreams.impl.log.LogStreamWriterImpl.claimLogEntry(LogStreamWriterImpl.java:165) ~[zeebe-logstreams-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.logstreams.impl.log.LogStreamWriterImpl.tryWrite(LogStreamWriterImpl.java:124) ~[zeebe-logstreams-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.broker.transport.commandapi.CommandApiRequestHandler.writeCommand(CommandApiRequestHandler.java:141) ~[zeebe-broker-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.broker.transport.commandapi.CommandApiRequestHandler.handleExecuteCommandRequest(CommandApiRequestHandler.java:114) ~[zeebe-broker-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.broker.transport.commandapi.CommandApiRequestHandler.handle(CommandApiRequestHandler.java:58) ~[zeebe-broker-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.broker.transport.commandapi.CommandApiRequestHandler.handleAsync(CommandApiRequestHandler.java:49) ~[zeebe-broker-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.broker.transport.commandapi.CommandApiRequestHandler.handleAsync(CommandApiRequestHandler.java:27) ~[zeebe-broker-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.broker.transport.AsyncApiRequestHandler.handleRequest(AsyncApiRequestHandler.java:110) ~[zeebe-broker-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.broker.transport.AsyncApiRequestHandler.lambda$onRequest$0(AsyncApiRequestHandler.java:75) ~[zeebe-broker-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n2023-04-27 17:52:05.508 [io.camunda.zeebe.gateway.impl.broker.BrokerRequestManager] [Broker-0-zb-actors-0] ERROR\r\n      io.camunda.zeebe.gateway - Expected to handle gRPC request, but received an internal error from broker: BrokerError{code=INTERNAL_ERROR, message='Failed writing response: java.lang.IllegalArgumentException: Expected to claim segment of size 1010866, but can't claim more than 65536 bytes.'}\r\nio.camunda.zeebe.gateway.cmd.BrokerErrorException: Received error from broker (INTERNAL_ERROR): Failed writing response: java.lang.IllegalArgumentException: Expected to claim segment of size 1010866, but can't claim more than 65536 bytes.\r\n\tat io.camunda.zeebe.gateway.impl.broker.BrokerRequestManager.handleResponse(BrokerRequestManager.java:194) ~[zeebe-gateway-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.gateway.impl.broker.BrokerRequestManager.lambda$sendRequestInternal$2(BrokerRequestManager.java:143) ~[zeebe-gateway-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.future.FutureContinuationRunnable.run(FutureContinuationRunnable.java:28) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\r\n```\r\n\r\n\r\n\r\n\r\n\n\n Zelldon: Hey @sergeylebed thanks for reporting this!\r\n\r\nLooks like a regression @megglos \n Zelldon: @sergeylebed can you confirm that you didn't get an error response in the client? :thinking: \r\n\r\nIt looks like, based on the stacktrace that it just failed on a different place before. \r\n\r\n```\r\nat io.camunda.zeebe.broker.transport.commandapi.CommandApiRequestHandler.writeCommand(CommandApiRequestHandler.java:141) ~[zeebe-broker-8.1.6.jar:8.1.6]\r\n```\r\n\r\nIs at the CommandAPI, when receiving the Command and writing to the dispatcher (before replicating and processing). We replaced the dispatcher in 8.2. Meaning it is now possible to write larger entries, but as you see processing is still not possible. But I think at least I would expect you get an error response. Did you?\n sergeylebed: I got an error on the client but \r\na) it is a generic error about timeout\r\nb) the system becomes inoperable \r\n\r\n```\r\nGrpc: 'DeadlineExceeded' 'Status(StatusCode=\\\"DeadlineExceeded\\\", Detail=\\\"Time out between gateway and broker: Request timed out after PT15S\\\", DebugException=\\\"Grpc.Core.Internal.CoreErrorDetailException: {\\\"created\\\":\\\"@1682543232.485000000\\\",\\\"description\\\":\\\"Error received from peer ipv6:[::1]:26500\\\",\\\"file\\\":\\\"..\\\\..\\\\..\\\\src\\\\core\\\\lib\\\\surface\\\\call.cc\\\",\\\"file_line\\\":953,\\\"grpc_message\\\":\\\"Time out between gateway and broker: Request timed out after PT15S\\\",\\\"grpc_status\\\":4}\\\")'\",\"Exception\":\"Grpc.Core.RpcException: Status(StatusCode=\\\"DeadlineExceeded\\\", Detail=\\\"Time out between gateway and broker: Request timed out after PT15S\\\", DebugException=\\\"Grpc.Core.Internal.CoreErrorDetailException: {\\\"created\\\":\\\"@1682543232.485000000\\\",\\\"description\\\":\\\"Error received from peer ipv6:[::1]:26500\\\",\\\"file\\\":\\\"..\\\\..\\\\..\\\\src\\\\core\\\\lib\\\\surface\\\\call.cc\\\",\\\"file_line\\\":953,\\\"grpc_message\\\":\\\"Time out between gateway and broker: Request timed out after PT15S\\\",\\\"grpc_status\\\":4}\\\")\\r\\n   at Zeebe.Client.Impl.Commands.DeployProcessCommand.Send(Nullable`1 timeout, CancellationToken token)\\r\\n   at  \r\n```\n sergeylebed: The very first error in the log:\r\n\r\n```\r\n2023-04-26 12:49:17.323 [Broker-0-StreamProcessor-1] [Broker-0-zb-actors-1] ERROR\r\n      io.camunda.zeebe.broker.process - Unexpected error while processing resource 'f6c0b39d-8357-40ea-8d79-7e2611a89677.bpmn'\r\nio.camunda.zeebe.stream.api.records.ExceededBatchRecordSizeException: Can't append entry: 'RecordBatchEntry[recordMetadata=RecordMetadata{recordType=EVENT, valueType=PROCESS, intent=CREATED}, key=2251799852104669, sourceIndex=-1, unifiedRecordValue={\"bpmnProcessId\":\"id_f6c0b39d-8357-40ea-8d79-7e2611a89677\",\"version\":1,\"processDefinitionKey\":2251799852104669,\"resourceName\":\"f6c0b39d-8357-40ea-8d79-7e2611a89677.bpmn\",\"checksum\":\"uG1QH8XcklrgFGYZI3HhPg==\",\"resource\":\"PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0idXRmLTgiPz4NCjxicG1uOmRlZmluaXRpb25zIHhtbG5zOmJwbW5kaT0iaHR0cDovL3d3dy5vbWcub3JnL3NwZWMvQlBNTi8yMDEwMDUyNC9ESSIgeG1sbnM6ZGM9Imh0dHA6Ly93d3cub21nLm9yZy9zcGVjL0RELzIwMTAwNTI0L0RDIiB4bWxuczp6ZWViZT0iaHR0cDovL2NhbXVuZGEub3JnL3NjaGVtYS96ZWViZS8xLjAiIHhtbG5zOmRpPSJodHRwOi8vd3d3Lm9tZy5vcmcvc3BlYy9ERC8yMDEwMDUyNC9ESSIgeG1sbnM6eHNpPSJodHRwOi8vd3d3LnczLm9yZy8yMDAxL1hNTFNjaGVtYS1pbnN0YW5jZSIgaWQ9ImlkXzZmNTk2NTlmLWE3OGMtNDMyMy04M2VmLTdlODMwYmJlNTUwNSIgdGFyZ2V0TmFtZXNwYWNlPSJodHRwOi8vYnBtbi5pby9zY2hlbWEvYnBtbiIgZXhwb3J0ZXI9IkNvbmZpcm1pdCBCUE1OIEJ1aWxkZXIiIGV4cG9ydGVyVmVyc2lvbj0iMS4wLjAuMCIgeG1sbnM6YnBtbj0iaHR0cD...' with size: 1010867 this would exceed the maximum batch size. [ currentBatchEntryCount: 0, currentBatchSize: 0]\r\n\tat io.camunda.zeebe.stream.impl.records.RecordBatch.appendRecord(RecordBatch.java:67) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.BufferedProcessingResultBuilder.appendRecordReturnEither(BufferedProcessingResultBuilder.java:62) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.api.ProcessingResultBuilder.appendRecord(ProcessingResultBuilder.java:38) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.streamprocessor.writers.ResultBuilderBackedEventApplyingStateWriter.appendFollowUpEvent(ResultBuilderBackedEventApplyingStateWriter.java:40) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.transformProcessResource(BpmnResourceTransformer.java:162) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.lambda$transformResource$0(BpmnResourceTransformer.java:77) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.util.Either$Right.map(Either.java:355) ~[zeebe-util-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.lambda$transformResource$1(BpmnResourceTransformer.java:75) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.util.Either$Right.flatMap(Either.java:366) ~[zeebe-util-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.transformResource(BpmnResourceTransformer.java:65) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.DeploymentTransformer.transformResource(DeploymentTransformer.java:122) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.DeploymentTransformer.transform(DeploymentTransformer.java:98) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.DeploymentCreateProcessor.processRecord(DeploymentCreateProcessor.java:87) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.Engine.process(Engine.java:142) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.batchProcessing(ProcessingStateMachine.java:346) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$2(ProcessingStateMachine.java:268) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run(ZeebeTransaction.java:84) ~[zeebe-db-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand(ProcessingStateMachine.java:268) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord(ProcessingStateMachine.java:227) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.readNextRecord(ProcessingStateMachine.java:203) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n```\n sergeylebed: In version 8.1.6 the client error is different\r\n\r\n```\r\nStatus(StatusCode=\\\"Internal\\\", Detail=\\\"Unexpected error occurred between gateway and broker (code: INTERNAL_ERROR)\\\", DebugException=\\\"Grpc.Core.Internal.CoreErrorDetailException: {\\\"created\\\":\\\"@1682632476.120000000\\\",\\\"description\\\":\\\"Error received from peer ipv6:[::1]:26500\\\",\\\"file\\\":\\\"..\\\\..\\\\..\\\\src\\\\core\\\\lib\\\\surface\\\\call.cc\\\",\\\"file_line\\\":953,\\\"grpc_message\\\":\\\"Unexpected error occurred between gateway and broker (code: INTERNAL_ERROR)\\\",\\\"grpc_status\\\":13}\\\")\",\"Exception\":\"Grpc.Core.RpcException: Status(StatusCode=\\\"Internal\\\", Detail=\\\"Unexpected error occurred between gateway and broker (code: INTERNAL_ERROR)\\\", DebugException=\\\"Grpc.Core.Internal.CoreErrorDetailException: {\\\"created\\\":\\\"@1682632476.120000000\\\",\\\"description\\\":\\\"Error received from peer ipv6:[::1]:26500\\\",\\\"file\\\":\\\"..\\\\..\\\\..\\\\src\\\\core\\\\lib\\\\surface\\\\call.cc\\\",\\\"file_line\\\":953,\\\"grpc_message\\\":\\\"Unexpected error occurred between gateway and broker (code: INTERNAL_ERROR)\\\",\\\"grpc_status\\\":13}\\\")\\r\\n   at Zeebe.Client.Impl.Commands.DeployProcessCommand.Send(Nullable`1 timeout, CancellationToken token)\r\n```\n npepinpe: Under the assumption that doing so bricks your partition unrecoverably, we'll prioritize it as a blocker/critical issue.\n deepthidevaki: I was able to reproduce this by setting maxMessageSize to 1MB in the test `CreateDeploymentTest::shouldRejectDeployIfResourceIsTooLarge()` https://github.com/camunda/zeebe/blob/main/qa/integration-tests/src/test/java/io/camunda/zeebe/it/client/command/CreateDeploymentTest.java#L28 \r\n\r\nI see multiple issues here:\r\n\r\n1. CommandAPI is not rejecting requests which exceeds maxMessageSize. This result in oversized command to be written to the log stream.\r\n2. When engine cannot write the follow up event because it is above batch size limit, it attempts to write a rejection record which contains the whole command. Since the command is already above batch size it cannot write rejection to the log stream. This result in a loop in the processing machine where it tries to handle this error endlessly.\r\n\r\nFor fixing this issue, I propose to reject the request in CommandAPI. So it is never written to the logstream. However, I would also suggest to revisit if we really have to write the whole command in the rejection record.\n sergeylebed: It does not seem to be fixed in 8.2.5. Do you think it can be merged into the latest versions?\n deepthidevaki: @sergeylebed The fix will be included in 8.2.6.",
    "title": "8.2.3 Degradation: Creating an oversized BPMN causes unrecoverable failure "
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12509",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nMessageTTL checking fails with deserialization errors.\r\n\r\n```\r\njava.lang.RuntimeException: Could not deserialize object [MessageRecord]. Deserialization stuck at offset 24 of length 69\r\n    at io.camunda.zeebe.msgpack.UnpackedObject.wrap(UnpackedObject.java:38)\r\n    at io.camunda.zeebe.engine.api.records.RecordBatchEntry.createEntry(RecordBatchEntry.java:73)\r\n    at io.camunda.zeebe.engine.api.records.RecordBatch.appendRecord(RecordBatch.java:48)\r\n    at io.camunda.zeebe.streamprocessor.BufferedTaskResultBuilder.appendCommandRecord(BufferedTaskResultBuilder.java:47)\r\n    at io.camunda.zeebe.engine.processing.message.MessageTimeToLiveChecker.lambda$execute$0(MessageTimeToLiveChecker.java:90)\r\n```\r\n\r\nThis will fail the processing actor and prevent processing on this partition. When the (experimental) feature flag `enableMessageTtlCheckerAsync` is used, a different actor fails and only prevents further MessageTTL checking but not processing.\r\n\r\nThis is a regression, introduced in https://github.com/camunda/zeebe/commit/e1a6cae69c17325fc71a8ee92022a70d969bd0da\r\nIt affects versions >= 8.1.9 and >= 8.2.0.\r\n\r\n\r\n**To Reproduce**\r\n\r\nRun a broker that is leader for multiple partitions. When messages from two different partitions are expired at the same time, there is unsafe concurrent access to the writer of a shared record value. \r\n\r\n**Expected behavior**\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.lang.RuntimeException: Could not deserialize object [MessageRecord]. Deserialization stuck at offset 24 of length 69\r\n    at io.camunda.zeebe.msgpack.UnpackedObject.wrap(UnpackedObject.java:38)\r\n    at io.camunda.zeebe.engine.api.records.RecordBatchEntry.createEntry(RecordBatchEntry.java:73)\r\n    at io.camunda.zeebe.engine.api.records.RecordBatch.appendRecord(RecordBatch.java:48)\r\n    at io.camunda.zeebe.streamprocessor.BufferedTaskResultBuilder.appendCommandRecord(BufferedTaskResultBuilder.java:47)\r\n    at io.camunda.zeebe.engine.processing.message.MessageTimeToLiveChecker.lambda$execute$0(MessageTimeToLiveChecker.java:90)\r\n    at io.camunda.zeebe.engine.state.message.DbMessageState.lambda$visitMessagesWithDeadlineBeforeTimestamp$2(DbMessageState.java:358)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.visit(TransactionalColumnFamily.java:390)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.lambda$forEachInPrefix$19(TransactionalColumnFamily.java:369)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.ColumnFamilyContext.withPrefixKey(ColumnFamilyContext.java:112)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.forEachInPrefix(TransactionalColumnFamily.java:353)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.lambda$whileTrue$8(TransactionalColumnFamily.java:174)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.lambda$ensureInOpenTransaction$18(TransactionalColumnFamily.java:308)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.DefaultTransactionContext.runInNewTransaction(DefaultTransactionContext.java:61)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.DefaultTransactionContext.runInTransaction(DefaultTransactionContext.java:33)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.ensureInOpenTransaction(TransactionalColumnFamily.java:307)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.whileTrue(TransactionalColumnFamily.java:174)\r\n    at io.camunda.zeebe.engine.state.message.DbMessageState.visitMessagesWithDeadlineBeforeTimestamp(DbMessageState.java:351)\r\n    at io.camunda.zeebe.engine.processing.message.MessageTimeToLiveChecker.execute(MessageTimeToLiveChecker.java:76)\r\n    at io.camunda.zeebe.streamprocessor.ProcessingScheduleServiceImpl.lambda$toRunnable$6(ProcessingScheduleServiceImpl.java:137)\r\n    at io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94)\r\n    at io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45)\r\n    at io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119)\r\n    at io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106)\r\n    at io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87)\r\n    at io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198)\r\nCaused by: io.camunda.zeebe.msgpack.spec.MsgpackReaderException: Unable to determine string type, found unknown header byte 0x00 at reader offset 23\r\n    at io.camunda.zeebe.msgpack.spec.MsgPackReader.exceptionOnUnknownHeader(MsgPackReader.java:474)\r\n    at io.camunda.zeebe.msgpack.spec.MsgPackReader.readStringLength(MsgPackReader.java:140)\r\n    at io.camunda.zeebe.msgpack.value.StringValue.read(StringValue.java:96)\r\n    at io.camunda.zeebe.msgpack.value.ObjectValue.read(ObjectValue.java:91)\r\n    at io.camunda.zeebe.msgpack.UnpackedObject.wrap(UnpackedObject.java:32)\r\n    ... 24 more\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- OS: <!-- [e.g. Linux] -->\r\n- Zeebe Version: <!-- [e.g. 0.20.0] -->\r\n- Configuration: <!-- [e.g. exporters etc.] -->\r\n- \r\nRelates to https://jira.camunda.com/browse/SUPPORT-16711\r\n\n",
    "title": "MessageTTL checking fails with deserialization errors"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12433",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Description**\r\nWe are configuring brokers with the S3 backup properties as follows:\r\n- ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLEBACKUP: true\r\n- ZEEBE_BROKER_DATA_BACKUP_STORE: S3\r\n- ZEEBE_BROKER_DATA_BACKUP_S3_BUCKETNAME: <s3_bucket_name>\r\n- ZEEBE_BROKER_DATA_BACKUP_S3_REGION: us-east-1\r\n- ZEEBE_BROKER_DATA_BACKUP_S3_ENDPOINT: \"https://s3.us-east-1.amazonaws.com\"\r\n\r\nWe don't supply credentials through environment variables as they can be pulled from aws credentials provider chain. These properties worked on old version 8.1.9 but we got errors when switching to latest version 8.2.1 and broker cannot start due to the error:\r\n\r\n> \r\n> 2023-04-14T11:21:51.313-04:00 | org.springframework.boot.SpringApplication - Application run failed\r\n> 2023-04-14T11:21:51.313-04:00 | java.lang.IllegalStateException: Failed to execute CommandLineRunner\r\n> 2023-04-14T11:21:51.313-04:00 | at org.springframework.boot.SpringApplication.callRunner(SpringApplication.java:772) [spring-boot-3.0.5.jar:3.0.5]\r\n> 2023-04-14T11:21:51.313-04:00 | at org.springframework.boot.SpringApplication.callRunners(SpringApplication.java:753) [spring-boot-3.0.5.jar:3.0.5]\r\n> 2023-04-14T11:21:51.313-04:00 | at org.springframework.boot.SpringApplication.run(SpringApplication.java:317) [spring-boot-3.0.5.jar:3.0.5]\r\n> 2023-04-14T11:21:51.313-04:00 | at io.camunda.zeebe.broker.StandaloneBroker.main(StandaloneBroker.java:82) [camunda-zeebe-8.2.1.jar:8.2.1]\r\n> 2023-04-14T11:21:51.313-04:00 | Caused by: io.camunda.zeebe.broker.system.InvalidConfigurationException: Failed configuring backup store S3\r\n> 2023-04-14T11:21:51.313-04:00 | at io.camunda.zeebe.broker.system.SystemContext.validateBackupCfg(SystemContext.java:132) ~[zeebe-broker-8.2.1.jar:8.2.1]\r\n> 2023-04-14T11:21:51.313-04:00 | at io.camunda.zeebe.broker.system.SystemContext.validateDataConfig(SystemContext.java:116) ~[zeebe-broker-8.2.1.jar:8.2.1]\r\n> 2023-04-14T11:21:51.313-04:00 | at io.camunda.zeebe.broker.system.SystemContext.validateConfiguration(SystemContext.java:71) ~[zeebe-broker-8.2.1.jar:8.2.1]\r\n> 2023-04-14T11:21:51.313-04:00 | at io.camunda.zeebe.broker.system.SystemContext.initSystemContext(SystemContext.java:60) ~[zeebe-broker-8.2.1.jar:8.2.1]\r\n> 2023-04-14T11:21:51.313-04:00 | at io.camunda.zeebe.broker.system.SystemContext.<init>(SystemContext.java:56) ~[zeebe-broker-8.2.1.jar:8.2.1]\r\n> 2023-04-14T11:21:51.313-04:00 | at io.camunda.zeebe.broker.StandaloneBroker.run(StandaloneBroker.java:87) ~[camunda-zeebe-8.2.1.jar:8.2.1]\r\n> 2023-04-14T11:21:51.313-04:00 | at org.springframework.boot.SpringApplication.callRunner(SpringApplication.java:769) ~[spring-boot-3.0.5.jar:3.0.5]\r\n> 2023-04-14T11:21:51.313-04:00 | ... 3 more\r\n> 2023-04-14T11:21:51.313-04:00 | Caused by: java.lang.NullPointerException: Access key ID cannot be blank.\r\n\r\nIt looks like io.camunda.zeebe.backup.s3.S3BackupStore.buildClient still calls AwsBasicCredentials.create(credentials.accessKey(), credentials.secretKey() even we don't pass in any credentials. It might be related to this commit: https://github.com/camunda/zeebe/commit/dfd3b9e1034365b3fc1859d5e35353826e1222b3#diff-1b91d4d4e6875c0169ea56f9c35382f5f194a1af782db4879a26971fe32e6daeL72-L74\r\nWe also noticed that none of your unit or integration tests are checking for cases where ACCESS_KEY, SECRET_KEY are not passed. \r\n\r\nCould you please look into this issue? Thank you!\r\n\n\n oleschoenburg: Thanks for reporting @NingyuanZhang, you are right that https://github.com/camunda/zeebe/commit/dfd3b9e1034365b3fc1859d5e35353826e1222b3#diff-1b91d4d4e6875c0169ea56f9c35382f5f194a1af782db4879a26971fe32e6daeL72-L74 broke this use case.\r\n\r\nI'll provide a fix for this and see if we can test this too.",
    "title": "Broker cannot start with S3 accessKey and secretKey not supplied "
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12374",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nA Zeebe broker is crash looping. The broker tries to start up but failed with the following error message. \r\n\r\n```\r\nio.camunda.zeebe.journal.CorruptedJournalException: Expected to read the version byte from segment 'raft-partition-partition-1-1.log' but got EOF instead\r\n```\r\n\r\nThe broker was part of a cluster with three brokers. The other brokers were healthy and continued processing. \r\n\r\nTo mitigate the issue, we did a fresh restart of the broker. We removed all data and restarted the broker. After the restart, the broker was healthy again and joined the cluster.\r\n\r\n**To Reproduce**\r\n\r\nUnknown. \r\n\r\nThe broker was forced to shut down (`Shutdown was called with context: ...`). The broker created a new snapshot 30 seconds before. The log contains no warnings or suspicious behavior before or after the shutdown.\r\n\r\n_EDIT:_\r\n\r\nIt seems `zeebe-2` was restarted while receiving a snapshot and resetting the log:\r\n```\r\nINFO 2023-04-06T15:37:07.317296306Z [jsonPayload.context.partitionId: 1] [resource.labels.containerName: zeebe] RaftServer{raft-partition-partition-1}{role=FOLLOWER} - Started receiving new snapshot FileBasedReceivedSnapshot{directory=/usr/local/zeebe/data/raft-partition/partitions/1/pending/14080296-246-15599729-15599732-1, snapshotStore=Broker-2-SnapshotStore-1, metadata=FileBasedSnapshotId{index=14080296, term=246, processedPosition=15599729, exporterPosition=15599732}} from 1\r\nINFO 2023-04-06T15:37:07.490944632Z [jsonPayload.context.partitionId: 1] [resource.labels.containerName: zeebe] RaftServer{raft-partition-partition-1}{role=FOLLOWER} - Delete existing log (lastIndex '14080107') and replace with received snapshot (index '14080296'). First entry in the log will be at index 14080297\r\nERROR 2023-04-06T15:39:35.994311846Z [resource.labels.containerName: zeebe] + export SPRING_CONFIG_LOCATION=classpath:/,file:./config/zeebe.cfg.yaml\r\n```\r\nThese are the last logs from that broker, and immediately it was restarted.\r\n\r\nMost likely, the log segment on the disk is an intermediate state where the previous segments have been deleted, the new one is only partially created - which is detected as corruption after the restart. \r\n\r\n**Expected behavior**\r\n\r\nThe broker can handle this corruption failure. For example, by removing (or archiving) the corrupted data and fetching the latest data from the cluster.\r\n\r\n**Log/Stacktrace**\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.util.concurrent.ExecutionException: Startup failed in the following steps: [Partition Manager]. See suppressed exceptions for details.\r\n\tat io.camunda.zeebe.scheduler.future.CompletableActorFuture.get(CompletableActorFuture.java:142) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.future.CompletableActorFuture.get(CompletableActorFuture.java:109) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.FutureUtil.join(FutureUtil.java:21) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.future.CompletableActorFuture.join(CompletableActorFuture.java:198) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.broker.Broker.internalStart(Broker.java:101) ~[zeebe-broker-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.util.LogUtil.doWithMDC(LogUtil.java:23) ~[zeebe-util-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.broker.Broker.start(Broker.java:83) ~[zeebe-broker-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.broker.StandaloneBroker.run(StandaloneBroker.java:92) ~[camunda-zeebe-8.1.3.jar:8.1.3]\r\n\tat org.springframework.boot.SpringApplication.callRunner(SpringApplication.java:771) ~[spring-boot-2.7.4.jar:2.7.4]\r\n\tat org.springframework.boot.SpringApplication.callRunners(SpringApplication.java:755) ~[spring-boot-2.7.4.jar:2.7.4]\r\n\tat org.springframework.boot.SpringApplication.run(SpringApplication.java:315) ~[spring-boot-2.7.4.jar:2.7.4]\r\n\tat io.camunda.zeebe.broker.StandaloneBroker.main(StandaloneBroker.java:82) ~[camunda-zeebe-8.1.3.jar:8.1.3]\r\nCaused by: io.camunda.zeebe.scheduler.startup.StartupProcessException: Startup failed in the following steps: [Partition Manager]. See suppressed exceptions for details.\r\n\tat io.camunda.zeebe.scheduler.startup.StartupProcess.aggregateExceptionsSynchronized(StartupProcess.java:282) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.startup.StartupProcess.completeStartupFutureExceptionallySynchronized(StartupProcess.java:183) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.startup.StartupProcess.lambda$proceedWithStartupSynchronized$3(StartupProcess.java:167) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.future.FutureContinuationRunnable.run(FutureContinuationRunnable.java:33) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tSuppressed: io.camunda.zeebe.scheduler.startup.StartupProcessStepException: Bootstrap step Partition Manager failed\r\n\t\tat io.camunda.zeebe.scheduler.startup.StartupProcess.completeStartupFutureExceptionallySynchronized(StartupProcess.java:185) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.scheduler.startup.StartupProcess.lambda$proceedWithStartupSynchronized$3(StartupProcess.java:167) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.scheduler.future.FutureContinuationRunnable.run(FutureContinuationRunnable.java:33) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tCaused by: java.util.concurrent.CompletionException: io.camunda.zeebe.journal.CorruptedJournalException: Expected to read the version byte from segment 'raft-partition-partition-1-1.log' but got EOF instead.\r\n\t\tat java.util.concurrent.CompletableFuture.encodeThrowable(Unknown Source) ~[?:?]\r\n\t\tat java.util.concurrent.CompletableFuture.uniApplyNow(Unknown Source) ~[?:?]\r\n\t\tat java.util.concurrent.CompletableFuture.uniApplyStage(Unknown Source) ~[?:?]\r\n\t\tat java.util.concurrent.CompletableFuture.thenApply(Unknown Source) ~[?:?]\r\n\t\tat io.atomix.raft.partition.RaftPartition.open(RaftPartition.java:104) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.partition.RaftPartitionGroup.lambda$join$7(RaftPartitionGroup.java:201) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat java.util.stream.ReferencePipeline$3$1.accept(Unknown Source) ~[?:?]\r\n\t\tat java.util.HashMap$KeySpliterator.forEachRemaining(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.AbstractPipeline.copyInto(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.AbstractPipeline.evaluate(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.AbstractPipeline.evaluateToArrayNode(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.ReferencePipeline.toArray(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.ReferencePipeline.toArray(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.ReferencePipeline.toList(Unknown Source) ~[?:?]\r\n\t\tat io.atomix.raft.partition.RaftPartitionGroup.join(RaftPartitionGroup.java:203) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.primitive.partition.impl.DefaultPartitionService.start(DefaultPartitionService.java:63) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.broker.partitioning.PartitionManagerImpl.start(PartitionManagerImpl.java:125) ~[zeebe-broker-8.1.3.jar:8.1.3]\r\n\t\tat java.util.concurrent.CompletableFuture$AsyncSupply.run(Unknown Source) ~[?:?]\r\n\t\tat java.lang.Thread.run(Unknown Source) ~[?:?]\r\n\tCaused by: io.camunda.zeebe.journal.CorruptedJournalException: Expected to read the version byte from segment 'raft-partition-partition-1-1.log' but got EOF instead.\r\n\t\tat io.camunda.zeebe.journal.file.SegmentLoader.readVersion(SegmentLoader.java:173) ~[zeebe-journal-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.journal.file.SegmentLoader.readDescriptor(SegmentLoader.java:128) ~[zeebe-journal-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.journal.file.SegmentLoader.loadExistingSegment(SegmentLoader.java:87) ~[zeebe-journal-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.journal.file.SegmentsManager.loadSegments(SegmentsManager.java:292) ~[zeebe-journal-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.journal.file.SegmentsManager.open(SegmentsManager.java:238) ~[zeebe-journal-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.journal.file.SegmentedJournal.<init>(SegmentedJournal.java:61) ~[zeebe-journal-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.journal.file.SegmentedJournalBuilder.build(SegmentedJournalBuilder.java:158) ~[zeebe-journal-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.storage.log.RaftLogBuilder.build(RaftLogBuilder.java:136) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.storage.RaftStorage.openLog(RaftStorage.java:194) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.impl.RaftContext.<init>(RaftContext.java:194) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.impl.DefaultRaftServer$Builder.build(DefaultRaftServer.java:258) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.impl.DefaultRaftServer$Builder.build(DefaultRaftServer.java:232) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.partition.impl.RaftPartitionServer.buildServer(RaftPartitionServer.java:189) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.partition.impl.RaftPartitionServer.initServer(RaftPartitionServer.java:155) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.partition.impl.RaftPartitionServer.start(RaftPartitionServer.java:114) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.partition.RaftPartition.open(RaftPartition.java:104) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.partition.RaftPartitionGroup.lambda$join$7(RaftPartitionGroup.java:201) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat java.util.stream.ReferencePipeline$3$1.accept(Unknown Source) ~[?:?]\r\n\t\tat java.util.HashMap$KeySpliterator.forEachRemaining(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.AbstractPipeline.copyInto(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.AbstractPipeline.evaluate(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.AbstractPipeline.evaluateToArrayNode(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.ReferencePipeline.toArray(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.ReferencePipeline.toArray(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.ReferencePipeline.toList(Unknown Source) ~[?:?]\r\n\t\tat io.atomix.raft.partition.RaftPartitionGroup.join(RaftPartitionGroup.java:203) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.primitive.partition.impl.DefaultPartitionService.start(DefaultPartitionService.java:63) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.broker.partitioning.PartitionManagerImpl.start(PartitionManagerImpl.java:125) ~[zeebe-broker-8.1.3.jar:8.1.3]\r\n\t\tat java.util.concurrent.CompletableFuture$AsyncSupply.run(Unknown Source) ~[?:?]\r\n\t\tat java.lang.Thread.run(Unknown Source) ~[?:?]\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\nSee more in the downloaded [log file](https://drive.google.com/file/d/1V89vBHeXytmm_lviRhMjojBhdeoz7mJs/view?usp=share_link).\r\n\r\n**Environment:**\r\n- OS: Camunda SaaS\r\n- Zeebe Version: `8.1.3`\r\n- Configuration: `prod-worker-3`\r\n\n\n SeanAda: I have the same exeption on a self hosted Zeebe (Version 8.1.4).\r\nThe file `raft-partition-partition-1-1.log` is empty.\r\n\r\nIs there any chance to recover the data or copy it from other nodes?\n saig0: > Is there any chance to recover the data or copy it from other nodes?\r\n\r\n@SeanAda it depends on your setup. With a replication factor of 3, there is a good chance that the other two nodes takes over and continue processing. \r\n\r\nIn this case, you can mitigate the issue by doing a fresh restart of the broker. So, remove all data and restart the broker. After the restart, the broker should become healthy again and join the cluster. The broker should receive the missing data from the other nodes of the cluster.\n deepthidevaki: I had a quick look at the logs of the affected cluster. It seems `zeebe-2` was restarted while receiving a snapshot and resetting the log:\r\n```\r\nINFO 2023-04-06T15:37:07.317296306Z [jsonPayload.context.partitionId: 1] [resource.labels.containerName: zeebe] RaftServer{raft-partition-partition-1}{role=FOLLOWER} - Started receiving new snapshot FileBasedReceivedSnapshot{directory=/usr/local/zeebe/data/raft-partition/partitions/1/pending/14080296-246-15599729-15599732-1, snapshotStore=Broker-2-SnapshotStore-1, metadata=FileBasedSnapshotId{index=14080296, term=246, processedPosition=15599729, exporterPosition=15599732}} from 1\r\nINFO 2023-04-06T15:37:07.490944632Z [jsonPayload.context.partitionId: 1] [resource.labels.containerName: zeebe] RaftServer{raft-partition-partition-1}{role=FOLLOWER} - Delete existing log (lastIndex '14080107') and replace with received snapshot (index '14080296'). First entry in the log will be at index 14080297\r\nERROR 2023-04-06T15:39:35.994311846Z [resource.labels.containerName: zeebe] + export SPRING_CONFIG_LOCATION=classpath:/,file:./config/zeebe.cfg.yaml\r\n```\r\nThese are the last logs from that broker, and immediately it was restarted.\r\n\r\nMost likely, the log segment on the disk is an intermediate state where the previous segments have been deleted, the new one is only partially created - which is detected as corruption after the restart. We should handle this case better as it is not an actual corruption.\r\n\n SeanAda: Removing all the data and restarting the broker did work.\r\n\r\nThank you!\n saig0: @deepthidevaki great finding. :rocket: I added your comment to the description. \n codingman1990: But our the other two node become unhealthy too.How should i do something to recover it?\n npepinpe: Hi @codingman1990, what is the state of your two nodes? Are all brokers reporting the same error for the same partition?\n codingman1990: camunda-zeebe-0 and camunda-zeebe-1 be unhealthy.And camunda-zeebe-2 can't restart.\r\nBut camunda-zeebe-1 zeebe.log like this:\r\n\r\n```\r\n2023-04-24 15:44:14.259 [] [raft-server-0-raft-partition-partition-3] WARN \r\n      io.atomix.raft.roles.FollowerRole - RaftServer{raft-partition-partition-3}{role=FOLLOWER} - Poll request to 2 failed: java.net.ConnectException: Expected to send a message with subject 'raft-partition-partition-3-poll' to member '2', but member is not known. Known members are '[Member{id=1, address=camunda-zeebe-1.camunda-zeebe.default.svc:26502, properties={brokerInfo=EADJAAAAAwABAAAAAwAAAAMAAAADAAAAAAABCgAAAGNvbW1hbmRBcGkvAAAAY2FtdW5kYS16ZWViZS0xLmNhbXVuZGEtemVlYmUuZGVmYXVsdC5zdmM6MjY1MDEFAAMBAAAAAQIAAAABAwAAAAEMAAAFAAAAOC4xLjkFAAMBAAAAAAIAAAAAAwAAAAA=}}, Member{id=camunda-zeebe-gateway-7db56f85d6-v2jb7, address=172.20.8.149:26502, properties={event-service-topics-subscribed=KIIDAGpvYnNBdmFpbGFibOU=}}, Member{id=camunda-zeebe-gateway-7db56f85d6-z69qr, address=172.20.3.169:26502, properties={event-service-topics-subscribed=KIIDAGpvYnNBdmFpbGFibOU=}}, Member{id=0, address=camunda-zeebe-0.camunda-zeebe.default.svc:26502, properties={brokerInfo=EADJAAAAAwAAAAAAAwAAAAMAAAADAAAAAAABCgAAAGNvbW1hbmRBcGkvAAAAY2FtdW5kYS16ZWViZS0wLmNhbXVuZGEtemVlYmUuZGVmYXVsdC5zdmM6MjY1MDEFAAMBAAAAAQIAAAABAwAAAAEMAAAFAAAAOC4xLjkFAAMBAAAAAQIAAAABAwAAAAE=}}]'\r\n```\r\n\r\nAnd one zeebe_error6.log like this:\r\n\r\n```\r\n#\r\n# A fatal error has been detected by the Java Runtime Environment:\r\n#\r\n#  SIGBUS (0x7) at pc=0x00007f38d75053cc, pid=6, tid=64\r\n#\r\n# JRE version: OpenJDK Runtime Environment Temurin-17.0.6+10 (17.0.6+10) (build 17.0.6+10)\r\n# Java VM: OpenJDK 64-Bit Server VM Temurin-17.0.6+10 (17.0.6+10, mixed mode, sharing, tiered, compressed oops, compressed class ptrs, g1 gc, linux-amd64)\r\n# Problematic frame:\r\n# v  ~StubRoutines::updateBytesCRC32C\r\n#\r\n# Core dump will be written. Default location: /usr/local/zeebe/core.6\r\n#\r\n# If you would like to submit a bug report, please visit:\r\n#   https://github.com/adoptium/adoptium-support/issues\r\n#\r\n\r\n---------------  S U M M A R Y ------------\r\n\r\nCommand Line: -XX:+HeapDumpOnOutOfMemoryError -XX:InitialHeapSize=2147483648 -XX:MaxHeapSize=2147483648 -XX:HeapDumpPath=/usr/local/zeebe/data -XX:ErrorFile=/usr/local/zeebe/data/zeebe_error%p.log -XX:+ExitOnOutOfMemoryError -Xms128m -XX:+ExitOnOutOfMemoryError -Dfile.encoding=UTF-8 -Dapp.name=broker -Dapp.pid=6 -Dapp.repo=/usr/local/zeebe/lib -Dapp.home=/usr/local/zeebe -Dbasedir=/usr/local/zeebe io.camunda.zeebe.broker.StandaloneBroker\r\n\r\nHost: Intel(R) Xeon(R) CPU E5-2682 v4 @ 2.50GHz, 8 cores, 7G, Ubuntu 20.04.5 LTS\r\nTime: Mon Apr 24 05:25:16 2023 CST elapsed time: 151584.214926 seconds (1d 18h 6m 24s)\r\n\r\n---------------  T H R E A D  ---------------\r\n\r\nCurrent thread (0x00007f38080ce950):  JavaThread \"raft-server-0-raft-partition-partition-2\" [_thread_in_Java, id=64, stack(0x00007f38746dd000,0x00007f38747de000)]\r\n\r\nStack: [0x00007f38746dd000,0x00007f38747de000],  sp=0x00007f38747dc060,  free space=1020k\r\nNative frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)\r\nv  ~StubRoutines::updateBytesCRC32C\r\n\r\n\r\nsiginfo: si_signo: 7 (SIGBUS), si_code: 2 (BUS_ADRERR), si_addr: 0x00007f3744000011\r\n```\r\ncamunda-zeebe-1 zeebe.log no log content has been for 2 hours.\n codingman1990: ![image](https://user-images.githubusercontent.com/12196018/233933546-e37706d0-17a9-4573-b44c-98f7a99849cc.png)\r\nThe grafana page shows we have last camunda-zeebe-2 promethus data.\n codingman1990: After 1 day,camunda be unhealthy again.\r\n![image](https://user-images.githubusercontent.com/12196018/234449827-d98659b8-4f49-4947-adb9-afac4a8bcc83.png)\r\nHere is the  zeebe log.\r\n`2023-04-26 09:23:16.742 [] [raft-server-0-raft-partition-partition-1] ERROR\r\n      io.atomix.utils.concurrent.SingleThreadContext - Shutting down because we can't recover from JVM errors. Consider restarting this broker if it is a temporary issue.\r\njava.lang.InternalError: a fault occurred in a recent unsafe memory access operation in compiled Java code\r\n        at io.camunda.zeebe.journal.file.MessageHeaderEncoder.blockLength(MessageHeaderEncoder.java:85) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.camunda.zeebe.journal.file.SegmentDescriptorEncoder.wrapAndApplyHeader(SegmentDescriptorEncoder.java:80) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.camunda.zeebe.journal.file.SegmentDescriptor.copyTo(SegmentDescriptor.java:289) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.camunda.zeebe.journal.file.UninitializedSegment.initializeForUse(UninitializedSegment.java:35) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.camunda.zeebe.journal.file.SegmentsManager.getNextSegment(SegmentsManager.java:133) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.camunda.zeebe.journal.file.SegmentedJournal.getNextSegment(SegmentedJournal.java:203) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.camunda.zeebe.journal.file.SegmentedJournalWriter.createNewSegment(SegmentedJournalWriter.java:110) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.prometheus.client.Histogram$Child.timeWithExemplar(Histogram.java:273) ~[simpleclient-0.16.0.jar:?]\r\n        at io.prometheus.client.Histogram$Child.time(Histogram.java:260) ~[simpleclient-0.16.0.jar:?]\r\n        at io.camunda.zeebe.journal.file.JournalMetrics.observeSegmentCreation(JournalMetrics.java:71) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.camunda.zeebe.journal.file.SegmentedJournalWriter.append(SegmentedJournalWriter.java:58) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.camunda.zeebe.journal.file.SegmentedJournal.append(SegmentedJournal.java:76) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.atomix.raft.storage.log.RaftLog.append(RaftLog.java:139) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n        at io.atomix.raft.roles.LeaderRole.tryToAppend(LeaderRole.java:515) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n        at io.atomix.raft.roles.LeaderRole.append(LeaderRole.java:487) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n        at io.atomix.raft.roles.LeaderRole.safeAppendEntry(LeaderRole.java:564) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n        at io.atomix.raft.roles.LeaderRole.lambda$appendEntry$8(LeaderRole.java:541) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n        at io.atomix.utils.concurrent.SingleThreadContext$WrappedRunnable.run(SingleThreadContext.java:171) ~[zeebe-atomix-utils-8.1.9.jar:8.1.9]\r\n        at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) ~[?:?]\r\n        at java.util.concurrent.FutureTask.run(Unknown Source) ~[?:?]\r\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source) ~[?:?]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]\r\n        at java.lang.Thread.run(Unknown Source) ~[?:?]\r\n2023-04-26 09:23:16.758 [Broker-0-Startup] [Broker-0-zb-actors-0] INFO \r\n      io.camunda.zeebe.broker.system - Shutdown Admin API\r\n2023-04-26 09:23:16.759 [Broker-0-Startup] [Broker-0-zb-actors-0] INFO \r\n      io.camunda.zeebe.broker.system - Shutdown Partition Manager`\r\nCan anyone help solve this problem.\n npepinpe: I will first focus on the original issue, as I think your issue @codingman1990 is not quite the same. It seems like you have some memory issues (crashing due to a SIGBUS), so it looks like an issue with the underlying storage medium and our usage of mmap. OTOH, I would say, if you're using network storage that would explain the increased likelihood, and I would advise against using network storage (e.g. NFS, Samba) with Zeebe for now.\r\n\r\nGoing back to the original issue. It's unclear what the solution would be. In this state, we've already updated the lastFlushedIndex to be the snapshot's index, but we possibly have no snapshot. So we have no snapshot, we have a segment with possibly no descriptor or a partial descriptor, and we have a lastFlushedIndex which is likely quite high.\r\n\r\nOne case with no descriptor, since we pre-allocate the segments, we could simply check if the descriptor portion is all zeros. Then we know we never wrote anything in there, so it's not really corrupted.\r\n\r\nThat leaves us with a partial descriptor case. How do we distinguish partially written descriptor from a corrupted one? Right now, we rely on the last position of the previous segment to determine this, but what if we have no segments?\r\n\r\nOne option would be to initially write a magic byte signifying that we're about to write the descriptor. Flush. Then write the descriptor. Flush.\r\n\r\nSo we have the following lifecycle: \r\n\r\n- Descriptor is all zero (including initial magic byte) => no descriptor, can be deleted\r\n- First byte is the magic WILL_WRITE_DESCRIPTOR byte => no descriptor, can be deleted\r\n- Descriptor is present but corrupted => corruption, cannot be deleted automatically\r\n\r\nOf course, this solution is prone to bitrot, where some bit flip turns the first byte of the descriptor into the WILL_WRITE_DESCRIPTOR magic byte and we erroneously detect it as safe to delete.\r\n\r\nOne other option is piggyback on top of our meta store, and also keep track of the last initialized segment. So whenever we write the descriptor, our segment is now ready for use, and we update the meta store. Additionally, on load, we may need to update the last initialized segment with the latest value in case we pick up a segment which was initialized but we had crashed/shutdown before updating the meta store.\r\n\r\nSo if we have an initialized segment index of 5, and we cannot read the descriptor of 5, then we know this is real corruption. If we have an initialized segment index of 5, and we cannot read the descriptor of 6, then we know this is segment was never actually used.\n npepinpe: So, it came to my mind that this is an over-engineered solution. A simple fix would be to reset the last flushed index to a null value whenever the log is reset, but before the next segment is created.\r\n\r\nThe error only affects the case where we have no previous segment, i.e. when we've reset the log. In all other cases - e.g. a new segment is created but we crash before writing its descriptor - we have a previous segment and a lastFlushedIndex to compare to. When a new segment is created, it will not be written to until initialized, so we can use the lastFlushedIndex to distinguish that.\n saig0: :information_source: I saw a similar error message that seems to be related: \r\n\r\n```\r\njava.util.concurrent.CompletionException: io.camunda.zeebe.journal.CorruptedJournalException: Couldn't read or recognize version of segment 'raft-partition-partition-1-1.log'.\r\n\tat java.util.concurrent.CompletableFuture.encodeThrowable(Unknown Source) ~[?:?]\r\n\tat java.util.concurrent.CompletableFuture.uniApplyNow(Unknown Source) ~[?:?]\r\n\tat java.util.concurrent.CompletableFuture.uniApplyStage(Unknown Source) ~[?:?]\r\n\tat java.util.concurrent.CompletableFuture.thenApply(Unknown Source) ~[?:?]\r\n\tat io.atomix.raft.partition.RaftPartition.open(RaftPartition.java:91) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.partition.RaftPartitionGroup.lambda$join$2(RaftPartitionGroup.java:176) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat java.util.stream.ReferencePipeline$3$1.accept(Unknown Source) ~[?:?]\r\n\tat java.util.HashMap$KeySpliterator.forEachRemaining(Unknown Source) ~[?:?]\r\n\tat java.util.stream.AbstractPipeline.copyInto(Unknown Source) ~[?:?]\r\n\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(Unknown Source) ~[?:?]\r\n\tat java.util.stream.AbstractPipeline.evaluate(Unknown Source) ~[?:?]\r\n\tat java.util.stream.AbstractPipeline.evaluateToArrayNode(Unknown Source) ~[?:?]\r\n\tat java.util.stream.ReferencePipeline.toArray(Unknown Source) ~[?:?]\r\n\tat io.atomix.raft.partition.RaftPartitionGroup.join(RaftPartitionGroup.java:177) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.primitive.partition.impl.DefaultPartitionService.start(DefaultPartitionService.java:63) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.broker.partitioning.PartitionManagerImpl.start(PartitionManagerImpl.java:129) ~[zeebe-broker-8.2.3.jar:8.2.3]\r\n\tat java.util.concurrent.CompletableFuture$AsyncSupply.run(Unknown Source) ~[?:?]\r\n\tat java.lang.Thread.run(Unknown Source) ~[?:?]\r\nCaused by: io.camunda.zeebe.journal.CorruptedJournalException: Couldn't read or recognize version of segment 'raft-partition-partition-1-1.log'.\r\n\tat io.camunda.zeebe.journal.file.SegmentLoader.readDescriptor(SegmentLoader.java:182) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentLoader.loadExistingSegment(SegmentLoader.java:116) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentsManager.loadSegments(SegmentsManager.java:346) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentsManager.open(SegmentsManager.java:267) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentedJournal.<init>(SegmentedJournal.java:56) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentedJournalBuilder.build(SegmentedJournalBuilder.java:174) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.storage.log.RaftLogBuilder.build(RaftLogBuilder.java:151) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.storage.RaftStorage.openLog(RaftStorage.java:197) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.impl.RaftContext.<init>(RaftContext.java:194) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.impl.DefaultRaftServer$Builder.build(DefaultRaftServer.java:243) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.impl.DefaultRaftServer$Builder.build(DefaultRaftServer.java:217) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.partition.impl.RaftPartitionServer.buildServer(RaftPartitionServer.java:184) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.partition.impl.RaftPartitionServer.initServer(RaftPartitionServer.java:150) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.partition.impl.RaftPartitionServer.start(RaftPartitionServer.java:109) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\t... 14 more\r\nCaused by: io.camunda.zeebe.journal.file.UnknownVersionException: Expected version byte to be one [1 2] but got 0 instead.\r\n\tat io.camunda.zeebe.journal.file.SegmentDescriptor.getEncodingLengthForVersion(SegmentDescriptor.java:225) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentLoader.readDescriptor(SegmentLoader.java:155) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentLoader.loadExistingSegment(SegmentLoader.java:116) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentsManager.loadSegments(SegmentsManager.java:346) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentsManager.open(SegmentsManager.java:267) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentedJournal.<init>(SegmentedJournal.java:56) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentedJournalBuilder.build(SegmentedJournalBuilder.java:174) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.storage.log.RaftLogBuilder.build(RaftLogBuilder.java:151) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.storage.RaftStorage.openLog(RaftStorage.java:197) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.impl.RaftContext.<init>(RaftContext.java:194) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.impl.DefaultRaftServer$Builder.build(DefaultRaftServer.java:243) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.impl.DefaultRaftServer$Builder.build(DefaultRaftServer.java:217) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.partition.impl.RaftPartitionServer.buildServer(RaftPartitionServer.java:184) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.partition.impl.RaftPartitionServer.initServer(RaftPartitionServer.java:150) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.partition.impl.RaftPartitionServer.start(RaftPartitionServer.java:109) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\t... 14 more\r\n```\r\nEnvironment: SaaS, version `8.2.3`\n codingman1990: > \r\n\r\nAs you mentioned network storage problem,is there any chance we can recover it?We are using k8s and helm to deploy camunda in production.If the error mentioned above occurs,we can't restart camunda zeebe any more.\n npepinpe: So SIGBUS errors can happen of course even without network storage, but typically that will be due to a bug or misuse, e.g. a file gets truncated while it was mmap'd. With network storage, they can happen simply due to network issues, which makes them unsuitable for any Java program using memory mapped files, where it's not possible to trap and handle such signals.\r\n\r\nIf you're not running on network storage however, this is likely a bug, and I'd be interested in how this happened. Do you perhaps have programs which try to reap `.log` files indiscriminately? `.log` files in Zeebe aren't traditional log files (e.g. for logging), but represent the actual application data, and shouldn't never be modified externally.\r\n\r\nAs for recovery, if at least one node has the right data, the simplest way is to delete the data of the other nodes, and manually copy over the \"good\" data to them. If however all nodes fail to start up, I'm afraid you'll have to rely [on your backups (if any) to restore](https://docs.camunda.io/docs/self-managed/backup-restore/backup-and-restore/).\n codingman1990: > So SIGBUS errors can happen of course even without network storage, but typically that will be due to a bug or misuse, e.g. a file gets truncated while it was mmap'd. With network storage, they can happen simply due to network issues, which makes them unsuitable for any Java program using memory mapped files, where it's not possible to trap and handle such signals.\r\n> \r\n> If you're not running on network storage however, this is likely a bug, and I'd be interested in how this happened. Do you perhaps have programs which try to reap `.log` files indiscriminately? `.log` files in Zeebe aren't traditional log files (e.g. for logging), but represent the actual application data, and shouldn't never be modified externally.\r\n> \r\n> As for recovery, if at least one node has the right data, the simplest way is to delete the data of the other nodes, and manually copy over the \"good\" data to them. If however all nodes fail to start up, I'm afraid you'll have to rely [on your backups (if any) to restore](https://docs.camunda.io/docs/self-managed/backup-restore/backup-and-restore/).\r\n\r\nThanks for your tips about `.log` file.Our production machine really has a timing clearing old and `big` files task,maybe that's the root cause.We will trying close it.",
    "title": "CorruptedJournalException: Fail to read version byte from segment"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12328",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nWith 8.2.0, it's not possible to disable the Raft flush without specifying a delay. This is mostly due to how Spring deserializes its configuration. As we use a `record` internally for the configuration, it will try to pass both properties, and the second one will be null. It's not really possible to rely on \"default\" values as we used to with simple classes.\r\n\r\n**To Reproduce**\r\n\r\nStart a 8.2.0 broker with the following environment variable: `ZEEBE_BROKER_CLUSTER_RAFT_FLUSH_ENABLED=false`\r\n\r\nThe broker will fail to start. It will however start if you specify:\r\n\r\n```yaml\r\nZEEBE_BROKER_CLUSTER_RAFT_FLUSH_ENABLED=false\r\nZEEBE_BROKER_CLUSTER_RAFT_FLUSH_DELAYTIME=0s\r\n```\r\n\r\nWith this configuration, you can pass whatever as delay time, it will simply be ignored. The behavior after is correct - it's just a matter of deserializing the configuration.\r\n\r\n**Expected behavior**\r\n\r\nI can disable the flush without having to specify a delay time.\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.2.0\r\n\n",
    "title": "Cannot disable Raft flush without specifying a delay"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12326",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "Hi Zeebe Team, please consider the following bug\r\n\r\n**Describe the bug**\r\n\r\nGiven the [following code snippet](https://github.com/camunda/connector-sdk/blob/main/runtime-util/src/main/java/io/camunda/connector/runtime/util/outbound/ConnectorJobHandler.java#L145-L150).\r\n\r\nThe `newThrowErrorCommand` cannot be handled via BPMN.\r\n\r\n**To Reproduce**\r\n\r\n1. Clone [camunda platform docker compose repo](https://github.com/camunda/camunda-platform).\r\n2. Keep `CAMUNDA_PLATFORM_VERSION` as `SNAPSHOT` or `8.2.0` in the `.env` [file](https://github.com/camunda/camunda-platform/blob/main/.env#L4).\r\n3. Run any variant, e.g. core as `docker-compose -f docker-compose-core.yaml up`.\r\n4. Deploy and start a BPMN diagram attached to this ticket: [error-handling.bpmn.txt](https://github.com/camunda/zeebe/files/11178216/error-handling.bpmn.txt).\r\n5. See an error: `Expected to throw an error event with the code '500' with message 'Got a 500', but it was not caught. No error events are available in the scope.`\r\n\r\n![Screenshot 2023-04-07 at 13 25 51](https://user-images.githubusercontent.com/108870003/230594417-d92ba764-6387-49a7-a9c7-b6773b66e863.png)\r\n\r\n![Screenshot 2023-04-07 at 13 26 02](https://user-images.githubusercontent.com/108870003/230594436-9703f853-5e96-4d4f-8676-3509789e59e9.png)\r\n\r\n\r\n**Expected behavior**\r\n\r\nDo the same as in **To Reproduce** except change the `CAMUNDA_PLATFORM_VERSION` to `8.1.10` or `8.1.9`.\r\n\r\nSee the process works correctly.\r\n\r\n![Screenshot 2023-04-07 at 13 23 19](https://user-images.githubusercontent.com/108870003/230594734-ef38e7b0-48ec-4632-bd62-4378e34a6fef.png)\r\n\r\n**Log/Stacktrace**\r\n\r\nNo valuable stacktraces.\r\n\r\n**Environment:**\r\n- OS:  MacOS 13.3 with M1 chip; Docker Engine version 20.10.23\r\n- Zeebe Version: `8.2.0`, `SNAPSHOT` - reproducible; `8.1.10`, `8.1.9` - not reproducible.\r\n- Configuration: [camunda-platform docker compose repo](https://github.com/camunda/camunda-platform/blob/main/docker-compose-core.yaml).\r\n\n\n remcowesterhoud: I had a first look. This was introduced when [adding support for FEEL expressions in error codes](https://github.com/camunda/zeebe/pull/10972). \r\n\r\nIn this PR we changed the way transform errors, by either setting the `errorCodeExpression` (not support by catch events) or by setting the `errorCode` directly. We _always_ parse the error code using the FEEL engine. In this case this will result in a static expression.\r\nWhere we're going wrong is that we make the assumption that this static expression can only be of the type `String`. In this case this is not true, it is a `Number` instead.\r\n\r\nRelevant code:\r\n\r\n```java\r\n  @Override\r\n  public void transform(final Error element, final TransformContext context) {\r\n\r\n    final var error = new ExecutableError(element.getId());\r\n    final var expressionLanguage = context.getExpressionLanguage();\r\n\r\n    // ignore error events that are not references by the process\r\n    Optional.ofNullable(element.getErrorCode())\r\n        .ifPresent(\r\n            errorCode -> {\r\n              final Expression errorCodeExpression = expressionLanguage.parseExpression(errorCode);\r\n\r\n              error.setErrorCodeExpression(errorCodeExpression);\r\n              if (errorCodeExpression.isStatic()) {\r\n                final EvaluationResult errorCodeResult =\r\n                    expressionLanguage.evaluateExpression(errorCodeExpression, variable -> null);\r\n\r\n                if (errorCodeResult.getType() == ResultType.STRING) {\r\n                  error.setErrorCode(BufferUtil.wrapString(errorCodeResult.getString()));\r\n                }\r\n              }\r\n\r\n              context.addError(error);\r\n            });\r\n  }\r\n```\r\n\r\nLooking into the `StaticExpression`  can only be of type `String` or `Number`\r\n```java\r\n  public StaticExpression(final String expression) {\r\n    this.expression = expression;\r\n\r\n    try {\r\n      treatAsNumber(expression);\r\n    } catch (final NumberFormatException e) {\r\n      treatAsString(expression);\r\n    }\r\n  }\r\n```\r\n\r\n------\r\n\r\nOptions to fix this:\r\n- Support `Number` in our transformers\r\n- Add a `getAsString` method to the `StaticExpression` which takes care of parsing the `Number` to a `String` (not sure how viable this is)\r\n\r\n------\r\n\r\nPlease note we do a similar thing in:\r\n- `EscalationTransformer`\r\n- `MessageTransformer` - Caught with deployment rejection or creating an incident. We should not change this behaviour\r\n- `SignalTransformer` - 8.2 only supports start event. Deployments with a signal as a number get rejected already\r\n- `StartEventTransformer` - Only for messages and signals which we should not change",
    "title": "The `newThrowErrorCommand` incorrectly handled in `8.2.0`"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12173",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nObserved in a test cluster in `ultrachaos`.\r\n`zeebe-2` is restarted. But it can never gets ready because it is not receiving heartbeats from the leader zeebe-0.\r\n We can see repeated logs in zeebe-2  `No heartbeat from a known leader .. Sending poll requests to all active members`\r\nHowever, in zeebe-0 we see that it is indeed sending messages to zeebe-2 and getting acknowledgments.\r\n```\r\nDEBUG 2023-03-28T11:13:29.998615141Z [resource.labels.podName: zeebe-0] [resource.labels.containerName: zeebe] RaftServer{raft-partition-partition-3} - Sending AppendRequest{term=2, leader=0, prevLogIndex=32433723, prevLogTerm=2, entries=1, commitIndex=32433722} to 2\r\nDEBUG 2023-03-28T11:13:29.998929396Z [resource.labels.podName: zeebe-0] [resource.labels.containerName: zeebe] RaftServer{raft-partition-partition-1} - Received AppendResponse{status=OK, term=1, succeeded=true, lastLogIndex=32442689, lastSnapshotIndex=32240013} from 2\r\n``` \r\nHowever, zeebe-2 is not receiving any of these messages. Confirmed from the logs and zeebe-2 journal segments.\r\n\r\nOn investigating futher, it seems zeebe-1 is receiving messages for zeebe-2. \r\n\r\nFor every single request send from zeebe-0 to zeebe-1, zeebe-1 is receiving two requests. It seems the duplicate request is for zeebe-2. (the logs that shows requests to 2 is omitted in the following logs).\r\n```\r\nDEBUG 2023-03-28T11:11:08.699639973Z [ zeebe-0] - Sending AppendRequest{term=1, leader=0, prevLogIndex=32402784, prevLogTerm=1, entries=1, commitIndex=32402784} to 1\r\n\r\nDEBUG 2023-03-28T11:11:08.701492523Z [ zeebe-1]{role=FOLLOWER} - Sending AppendResponse{status=OK, term=1, succeeded=true, lastLogIndex=32402785, lastSnapshotIndex=32240013}\r\nDEBUG 2023-03-28T11:11:08.701775314Z [ zeebe-1]{role=FOLLOWER} - Received AppendRequest{term=1, leader=0, prevLogIndex=32402784, prevLogTerm=1, entries=1, commitIndex=32402784}\r\nDEBUG 2023-03-28T11:11:08.702472045Z [ zeebe-1]{role=FOLLOWER} - Sending AppendResponse{status=OK, term=1, succeeded=true, lastLogIndex=32402785, lastSnapshotIndex=32240013}\r\nDEBUG 2023-03-28T11:11:08.702963186Z [ zeebe-0] - Received AppendResponse{status=OK, term=1, succeeded=true, lastLogIndex=32402785, lastSnapshotIndex=32240013} from 1\r\n\r\n\r\nDEBUG 2023-03-28T11:11:08.706279292Z [ zeebe-0] - Sending AppendRequest{term=1, leader=0, prevLogIndex=32402785, prevLogTerm=1, entries=1, commitIndex=32402785} to 1\r\nDEBUG 2023-03-28T11:11:08.706659278Z [ zeebe-1]{role=FOLLOWER} - Received AppendRequest{term=1, leader=0, prevLogIndex=32402785, prevLogTerm=1, entries=1, commitIndex=32402785}\r\nDEBUG 2023-03-28T11:11:08.708798386Z [ zeebe-1]{role=FOLLOWER} - Sending AppendResponse{status=OK, term=1, succeeded=true, lastLogIndex=32402786, lastSnapshotIndex=32240013}\r\nDEBUG 2023-03-28T11:11:08.709046684Z [ zeebe-1]{role=FOLLOWER} - Received AppendRequest{term=1, leader=0, prevLogIndex=32402785, prevLogTerm=1, entries=1, commitIndex=32402785}\r\nDEBUG 2023-03-28T11:11:08.709334419Z [ zeebe-1]{role=FOLLOWER} - Sending AppendResponse{status=OK, term=1, succeeded=true, lastLogIndex=32402786, lastSnapshotIndex=32240013}\r\n```\r\n\r\n**To Reproduce**\r\n\r\n\r\n**Expected behavior**\r\n\r\n\r\n\r\n**Log/Stacktrace**\r\n[logs](https://console.cloud.google.com/logs/query;cursorTimestamp=2023-03-28T11:13:29.999108007Z;pinnedLogId=2023-03-28T11:13:29.998615141Z%2Fvx3s55brpp9of4ho;query=resource.type%3D%22k8s_container%22%0Aresource.labels.project_id%3D%22camunda-cloud-240911%22%0Aresource.labels.location%3D%22europe-west1-d%22%0Aresource.labels.cluster_name%3D%22ultrachaos%22%0Aresource.labels.namespace_name%3D%22fea132ff-1e4e-4c7b-b278-8059264f9efa-zeebe%22%0Aresource.labels.container_name%3D%22zeebe%22%0Aresource.labels.pod_name%3D%22zeebe-0%22%0Atimestamp%3D%222023-03-28T11:13:29.998615141Z%22%0AinsertId%3D%22vx3s55brpp9of4ho%22;summaryFields=resource%252Flabels%252Fpod_name:false:32:beginning;timeRange=2023-03-28T10:49:15.000Z%2F2023-03-28T11:50:45.000Z?project=camunda-cloud-240911)\r\n\r\n\r\n[Heap dump of zeebe-0](https://github.com/camunda/zeebe/files/11089429/zeebe-0-13-30.zip)\r\n\r\n\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.2.0-SNAPSHOT \r\n\n\n deepthidevaki: On analyzing heap dump, the channel state in NettyMessaginService is inconsistent.\r\n\r\nAccording to state in SwimMembershipProtocol, the IP of the brokers are as follows:\r\n```\r\nzeebe-1: 10.56.46.232\r\nzeebe-2: 10.56.52.2\r\n```\r\n\r\nThere is a channel pool for `10.56.52.2`\r\n![image](https://user-images.githubusercontent.com/1997478/228247889-53407fd1-7e67-4982-861f-6fd54833f6e3.png)\r\n\r\nThe channel for `raft-partition-partition-1-append` will be at offset 4. But the channel at offset 4 has a remote connection to zeebe-1: 10.56.46.232\r\n![image](https://user-images.githubusercontent.com/1997478/228248235-fb298026-3159-4c6b-9e72-7b6b2d9b1239.png)\r\n\r\n\r\nSo it seems AppendRequest to zeebe-2 (10.56.52.2) is being sent to zeebe-1(10.56.46.232) instead.\n npepinpe: Good catch!\r\n\r\nThe first image is showing the channel pool for 10.56.52.2 on the `zeebe-0` node? Funnily enough, the key, though pointing to 10.56.52.2, has an InetAddress of `zeebe-1`...\r\n\r\nAny thoughts on solutions?\r\n\r\nJust thinking out loud:\r\n\r\n1. When a node is removed from the cluster, it's channels should be removed from the pool (not sure if this is already done)\r\n1. When a node is added to the cluster, channels for its IP should be reset in the pool (or just removed? Is it even possible to have something in the pool for that IP if the member was not in the cluster?)\r\n\r\nOr encoding the recipient concept in the protocol. Have the sender send both the member ID and the cluster ID, so the receiver can reject with a special message, forcing the sender to evict all channels for the wrong IP.\r\n\r\nBoth of these rely on the membership protocol providing the right IPs. I think this may break down with advertised addresses, since the advertised IP is not that of the receiver, and the receiver may not even be able to resolve that one. So perhaps coupling the member ID to the messaging service is necessary? :shrug: \n deepthidevaki: > The first image is showing the channel pool for 10.56.52.2 on the zeebe-0 node? Funnily enough, the key, though pointing to 10.56.52.2, has an InetAddress of zeebe-1...\r\n\r\nI also observed this. And I think this also points to one of the root cause. Here, zeebe-0 re-used old zeebe-1 IP. The channel pool use InetSocketAddress as key, which used IP address in `equals` if IP is available. So when finding channel for zeebe-0, it finds channel pool created for zeebe-1 with it's old address. However, I don't know yet how this channel pool contains a channel to new zeebe-1 address. \r\n\r\nI haven't thought about a solution. But we should see if we can combine hostname+ip to find the channels. So when IP is re-assigned we don't accidentally chose the wrong channel.\r\n\r\n> Or encoding the recipient concept in the protocol. Have the sender send both the member ID and the cluster ID, so the receiver can reject with a special message, forcing the sender to evict all channels for the wrong IP.\r\n\r\n:+1: This would be useful as a early detection/prevention of such cases.\r\n\r\n\n deepthidevaki: Summary of discussion with @npepinpe \r\n- Use address + inetAddress to find the channel pool. This will prevent using the wrong channel, in case ip is reassigned.\r\n- We could encode recipient host with all messages. On the receiver we can check and return a error response. Looks like this can be done in a backward compatible way. Server and client negotiates the protocol version during the handshake. So we can add a new protocol version with the recipient address. https://github.com/camunda/zeebe/issues/12309\r\n- We discarded the other idea to clear channelpool when membership of a node changed. This requires messaging layer to have knowledge about membership service, which is not ideal.",
    "title": "Zeebe node sends messages to wrong node"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12007",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n```\r\nio.camunda.zeebe.stream.api.records.ExceededBatchRecordSizeException: Can't append entry: \r\n'RecordBatchEntry[recordMetadata=RecordMetadata{recordType=EVENT, valueType=JOB_BATCH, intent=ACTIVATED}, \r\nkey=4503599628576248, sourceIndex=-1, unifiedRecordValue=\r\n\r\n\"maxJobsToActivate\":13822\r\n```\r\n\r\nPartition is unhealthy. \r\n```\r\nBroker-1-StreamProcessor-2{status=UNHEALTHY, issue=HealthIssue[message=not making progress, \r\n\r\nBroker-1-StreamProcessor-1{status=UNHEALTHY, issue=HealthIssue[message=actor appears blocked, \r\n```\r\nBut StreamProcessor is not stuck. Eventhough there is high backpressure, it is not 100%. Meaning that StreamProcessor is accepting new commands and processing them. But it seems no job can be activated.\r\n\r\n**To Reproduce**\r\n\r\n\r\n\r\n**Expected behavior**\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n**Log/Stacktrace**\r\n\r\n[Link to logs](https://console.cloud.google.com/errors/detail/CIeFtI729PW2Cw;service=zeebe;time=P7D?project=camunda-cloud-240911)\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\nio.camunda.zeebe.stream.api.records.ExceededBatchRecordSizeException: Can't append entry: 'RecordBatchEntry[recordMetadata=RecordMetadata{recordType=EVENT, valueType=JOB_BATCH, intent=ACTIVATED}, key=4503599628576248, sourceIndex=-1, unifiedRecordValue={\"type\":\"***\",\"worker\":\"***\",\"timeout\":300000,\"maxJobsToActivate\":13822,\"jobKeys\":[4503599628420203,4503599628420225,4503599628420247,4503599628420269,4503599628420291,4503599628420314,4503599628420318,4503599628420340,4503599628420344,4503599628420348,4503599628420352,4503599628420356,4503599628420360,4503599628420364,4503599628420446,4503599628420450,4503599628420454,4503599628420458,4503599628420462,4503599628420466,4503599628420516,4503599628420520,4503599628420524,4503599628420528,4503599628420532,4503599628420629,4503599628420633,4503599628420650,4503599628420702,4503599628420706,4503599628420716,4503599628420726,4503599628420730,4503599628420734,4503599628420770,4503599628420790,4503599628420794,4503599628420810,4503599628420818,4503599628420822,4503599628420834,4503599628420842,4503599...' with size: 4194277 this would exceed the maximum batch size. [ currentBatchEntryCount: 0, currentBatchSize: 0]\r\n\r\nat io.camunda.zeebe.stream.impl.records.RecordBatch.appendRecord ( [io/camunda.zeebe.stream.impl.records/RecordBatch.java:67](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl.records%2FRecordBatch.java&line=67&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.BufferedProcessingResultBuilder.appendRecordReturnEither ( [io/camunda.zeebe.stream.impl/BufferedProcessingResultBuilder.java:62](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FBufferedProcessingResultBuilder.java&line=62&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.api.ProcessingResultBuilder.appendRecord ( [io/camunda.zeebe.stream.api/ProcessingResultBuilder.java:38](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.api%2FProcessingResultBuilder.java&line=38&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.streamprocessor.writers.ResultBuilderBackedEventApplyingStateWriter.appendFollowUpEvent ( [io/camunda.zeebe.engine.processing.streamprocessor.writers/ResultBuilderBackedEventApplyingStateWriter.java:40](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.streamprocessor.writers%2FResultBuilderBackedEventApplyingStateWriter.java&line=40&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.job.JobBatchActivateProcessor.activateJobBatch ( [io/camunda.zeebe.engine.processing.job/JobBatchActivateProcessor.java:125](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.job%2FJobBatchActivateProcessor.java&line=125&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.job.JobBatchActivateProcessor.activateJobs ( [io/camunda.zeebe.engine.processing.job/JobBatchActivateProcessor.java:86](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.job%2FJobBatchActivateProcessor.java&line=86&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.job.JobBatchActivateProcessor.processRecord ( [io/camunda.zeebe.engine.processing.job/JobBatchActivateProcessor.java:63](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.job%2FJobBatchActivateProcessor.java&line=63&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.Engine.process ( [io/camunda.zeebe.engine/Engine.java:127](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine%2FEngine.java&line=127&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.batchProcessing ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:340](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=340&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$2 ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:263](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=263&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run ( [io/camunda.zeebe.db.impl.rocksdb.transaction/ZeebeTransaction.java:84](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.db.impl.rocksdb.transaction%2FZeebeTransaction.java&line=84&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:263](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=263&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:222](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=222&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.readNextRecord ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:198](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=198&project=camunda-cloud-240911) )\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n\r\n- Zeebe Version: 8.2.0-alpha5\r\n\n\n deepthidevaki: @korthout Please check if this is affecting ZPA.\n megglos: The exception changed, surfacing this issue. The cause might just be the high number of jobs to activate.\nTime box it for 30m to double-check if this is an actual bug.\n korthout: Removed from our board, until further notice\n remcowesterhoud: This happened again on the release benchmark of `8.2.x`\r\n\r\nI've increased the max message size to recover the benchmark\n Zelldon: > Removed from our board, until further notice\r\n\r\nThe reasoning would be great here.\n remcowesterhoud: > > Removed from our board, until further notice\r\n> \r\n> The reasoning would be great here.\r\n\r\nI don't remember, but probably because we thought it was related to the stream platform because of the label.\r\n\r\nI'll try to do a deeper analysis this week and see if I can pinpoint the bug.\n remcowesterhoud: I've been trying to reproduce but have been unable to. I can think of 2 possible causes:\r\n\r\n1. The [check if a record of size X can be appended](https://github.com/camunda/zeebe/blob/main/logstreams/src/main/java/io/camunda/zeebe/logstreams/impl/log/Sequencer.java#L56-L63) is not 100% foolproof and in edge cases returns `true` even though it doesn't fit.\r\n2. The [expected event length](https://github.com/camunda/zeebe/blob/main/engine/src/main/java/io/camunda/zeebe/engine/processing/job/JobBatchCollector.java#L87-L96) is not fully accurate. It tries to calculate the event length precisely, but if it's slightly off, we could append it yet still exceed the batch size.\r\n\r\nIMO 2 is the more likely scenario. My proposal would be to add an extra buffer to expected event length. This means we would check if a larger than expected event would fit in the batch. If this isn't the case we won't add it. This buffer can be relatively small. For terminating in batches we used 8 KB, but I think that's excessive for job activations. Instead I'd stick with a few bytes.\r\n\r\nAs this is part of the code is ZPA's responsibility I'll remove this from the ZDP board and add it to ours.\n korthout: ZPA triage:\n- small chance, but high impact\n- small effort: let's add a small margin to the check whether another job fits the batch\n- such a margin could help with #12778 \n- let's prioritize this as `upcoming`",
    "title": "ExceededBatchRecordSizeException: Can't append entry"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11594",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nBasically, it is the same as #11591 but related to timer events (and not messages). In a nutshell, Zeebe checks regularly for due timer events to eventually trigger them and continue with the process flow. The corresponding checker shares the actor of the Stream Processor which will block the Stream Processor while the checker runs. Also, the checker might submit a batch of commands to trigger timer events which will be executed by the Stream Processor in a row without anything else in between.\r\n\r\n**Expected behavior**\r\n* The Stream Processor and the checker do not share an actor so the Stream Processor continues processing while the checker collects timers to trigger.\r\n* (This might be partially already the case, needs to be checked.) The checker only submits a batch with a limited number of commands. For example, when the checker runs it will collect the first 10 due timer events and submit them as a batch to the log stream. And then continues with collecting the next 10 due timer events, and so on until there are due timer events. That way, triggering the timer events would interleave with any incoming commands from users/clients. \r\n* Instead of writing 10 commands, it could write just one command containing the 10 due timer events to trigger. (Might not be easily possible in terms of rolling upgrades, an old version of Zeebe would not be able to process such a command.)\r\n\r\n**Hints**\r\n* A simple prototype to avoid sharing the actor can be found here (in case of expired messages): https://github.com/camunda/zeebe/pull/11550\r\n* Things to consider: When the checker and the Stream Processor do not share an actor, they may run concurrently. That means, the checker reads from RocksDB, and the Stream Processor (mostly) writes to RocksDB. While RocksDB itself is thread-safe, the Zeebe layer may not (like `TransactionContext`, ...).\r\n* Also, while reading the state by the checker, the state might change.\r\n* The same pattern should be applied to the job's timeline and backoff checker.\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.1\r\n\r\n---\r\n\r\n* related to [SUPPORT-15902](https://jira.camunda.com/browse/SUPPORT-15902)\r\n* related to #8991 \n\n Zelldon: Related to https://github.com/camunda/zeebe/issues/8991\n megglos: related to https://github.com/camunda/zeebe/issues/11591\n megglos: Sync with @abbasadel :\r\n- @abbasadel checks in with Nico once https://github.com/camunda/zeebe/issues/11762 is completed on whether the ZPA team can continue with this right after\n korthout: Discussed this issue in the ZPA triage:\n - not urgent for the 8.2 release (other issues have priority)\n - it is an important issue and should be fixed \n - marking it is as `later` as we will first focus on the 8.2 release\n megglos: as this issue strongly relates to the mission of the ZDP team, we would take this one over to drive it forward\n megglos: @abbasadel Ole might need support at least for alignment and dicussion from a ZPA engineer",
    "title": "Triggering due timer events causes periodic latency spikes"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11578",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\nMulti-Instance doesn't get started if it contains a Message Event-based Subprocess that uses the inputElement as correlationKey. Instead an Incident gets created.\r\n\r\n\r\n\r\n\r\n**To Reproduce**\r\n**Update:** For better understanding I created a GitHub Repo with a minimalistic example: https://github.com/j-lindner/multi-instance-with-message-subprocess-bug\r\n\r\n**Expected behavior**\r\nMulti-Instance should get started, get processed and should listen to possible Messages on the event-based subprocess.\r\n\r\n**Log/Stacktrace**\r\n\r\nIncident Info in Operate:\r\n```\r\nfailed to evaluate expression 'myObject.myId': no variable found for name 'myObject'\r\n```\r\n\r\n**Environment:**\r\n- OS: WebModeler or alternatively zeebe-process-test-extension-testcontainer\r\n- Zeebe Version: 8.1.7\r\n- Configuration: -\r\n\r\n**Support:** \r\nhttps://jira.camunda.com/browse/SUPPORT-17059\n\n remcowesterhoud: Thanks for reporting and reproducing @j-lindner! This is very helpful 🚀 \r\n\r\n I will have chat within the team to see if this is expected behaviour or not. From there we will prioritise it accordingly.\n korthout: Thanks for reporting this @j-lindner 👍 It was easy to reproduce in one of Zeebe's engine test cases as well. I think I have a fix, but let's await the review.\n korthout: @j-lindner Bug is fixed and will be patched in upcoming releases of `8.0.15`, `8.1.13`, `8.2.6`, and `8.3.0`. You should notice that this issue will be labeled accordingly during those releases.",
    "title": "Multi-Instance with messageevent-based subprocess that uses inputElement as correlationKey fails"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11414",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n```\r\n java.util.NoSuchElementException: No value present\r\n\tat java.util.Optional.orElseThrow(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.updateInMemoryState(DbProcessState.java:180) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n```\r\n\r\n**To Reproduce**\r\nUnclear\r\n\r\n**Expected behavior**\r\n\r\nInvalid BPMN resources should be handled gracefully and not result in an unhandled `NoSuchElementException`.\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\n java.util.NoSuchElementException: No value present\r\n\tat java.util.Optional.orElseThrow(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.updateInMemoryState(DbProcessState.java:180) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.lookupProcessByIdAndPersistedVersion(DbProcessState.java:314) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.getLatestProcessVersionByProcessId(DbProcessState.java:217) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.transformProcessResource(BpmnResourceTransformer.java:138) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.lambda$transformResource$0(BpmnResourceTransformer.java:77) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.util.Either$Right.map(Either.java:355) ~[zeebe-util-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.lambda$transformResource$1(BpmnResourceTransformer.java:75) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.util.Either$Right.flatMap(Either.java:366) ~[zeebe-util-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.transformResource(BpmnResourceTransformer.java:65) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.DeploymentTransformer.transformResource(DeploymentTransformer.java:120) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.DeploymentTransformer.transform(DeploymentTransformer.java:97) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.DeploymentCreateProcessor.processRecord(DeploymentCreateProcessor.java:96) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.Engine.process(Engine.java:127) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$3(ProcessingStateMachine.java:264) ~[zeebe-stream-platform-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run(ZeebeTransaction.java:84) ~[zeebe-db-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand(ProcessingStateMachine.java:260) ~[zeebe-stream-platform-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord(ProcessingStateMachine.java:209) ~[zeebe-stream-platform-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.readNextRecord(ProcessingStateMachine.java:185) ~[zeebe-stream-platform-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) ~[zeebe-scheduler-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) ~[zeebe-scheduler-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) ~[zeebe-scheduler-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) ~[zeebe-scheduler-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) ~[zeebe-scheduler-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) ~[zeebe-scheduler-8.2.0-alpha3.jar:8.2.0-alpha3] \r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- SaaS\r\n- Zeebe Version: 8.2.0-alpha3\r\n\r\n[Error group](https://console.cloud.google.com/errors/detail/CM_e_rP419icrgE;service=zeebe;time=P7D?project=camunda-cloud-240911)\n\n oleschoenburg: Might be related to https://github.com/camunda/zeebe/issues/11392\n korthout: We need to understand the impact before we can prioritize this. Let's investigate whether this leads to a blacklisted instance.\n remcowesterhoud: I had a look into this with @koevskinikola. We suspect that somehow during the deployment, the deployment record process metadata does not contain a process that we do have in the bpmn file. As a result we have a process in the bpmn file that we have not stored as a `PersistedProcess`, resulting in this exception.\r\n\r\nWe are unsure how this could occur and how we can reproduce it. The exception has occurred 3 times in trial clusters thus far.\n remcowesterhoud: We discussed within the team and we will have another look into this issue together during out next mob-programming hour 2 weeks from now.\n korthout: Today, the team looked again at this issue. Having no way to reproduce it nor a way to find out how it might have happened makes this hard to debug.\r\n\r\nI've had another look at several parts:\r\n- ❌ could this have been caused by #11392 by checking multiple scenarios (e.g. deploying valid processes with id's that were already used in deployments that encountered that bug)\r\n- ❓ what is this code actually doing?\r\n\r\nThis question led me down a path where I noticed something:\r\n- when we deploy a BPMN file, we transform it\r\n- then, for the duplication check, we lookup the latest deployed version \r\n- this either uses the cached version, or it will read it from the state and then cache it\r\n- if it wasn't cached already but does exist in the state, it will then proceed to transform the persisted process XML\r\n- there is no real reason why we transform it here. The latest version is only used to determine version duplicates (and in that case, return the same key, version, etc as a response)\r\n- but this is the transformation where this specific bug happened.\r\n\r\nSo we could perhaps swat two flies at once (this is a Dutch saying):\r\n- remove the code path that was part of this bug\r\n- improve the performance when a new process version is deployed, when the latest isn't cached anymore (not sure if that ever happens, though).\r\n\r\nI'm curious as to what others think about this. Is that worth it? WDYT?\r\n\r\n//cc @remcowesterhoud @koevskinikola @berkaycanbc \n koevskinikola: Hey @korthout, I would vote for the following:\r\n\r\n1. Close this issue since we don't have enough data to reproduce and qualify it.\r\n2. Create an issue for optimizing the performance of the deployment of new process versions (with a low priority).\r\n    * I'm not sure if we should work on this soon, or until we understand the bug.\r\n\r\nMy reasoning is that currently we still don't understand why the bug is happening. The code you're suggesting for removal is just the place where the issue becomes visible (through a `NoSuchElementException`). By removing that code, we might miss any new occurrences of the bug, so it will become more difficult to detect and reproduce.\r\n\r\nUPDATE: Maybe we can wrap the existing exception [here](https://github.com/camunda/zeebe/blob/main/engine/src/main/java/io/camunda/zeebe/engine/state/deployment/DbProcessState.java#L180) into a more understandable message like: \"This might indicate a bug with our deployment process, please raise an issue with our GitHub tracker\"?\n korthout: Thanks @koevskinikola \r\n\r\n1. Agreed 👍 \r\n2. I don't think the performance improvement is a good enough reason to open an issue. The improvement could be tiny. I agree with your reasoning that \"it will become more difficult to detect and reproduce\" 👍 \r\n\r\n>UPDATE: Maybe we can wrap the existing exception [here](https://github.com/camunda/zeebe/blob/main/engine/src/main/java/io/camunda/zeebe/engine/state/deployment/DbProcessState.java#L180) into a more understandable message like: \"This might indicate a bug with our deployment process, please raise an issue with our GitHub tracker\"?\r\n\r\n👍 I like this idea, but perhaps we can change the message. Likely, we read the error message in our own environment. So there is no need to ask users to report the bug if we already know it exists. Let's add some details about the current scenario instead. \r\n\r\nEDIT: I've moved this back into `READY`, so we can make an effort to root cause this\n berkaycanbc: Implemented a PR to create a detailed error message. We should re-open it when we encounter the log message:\n\n> Expected to find executable process in persisted process with key '%s', but after transformation no such executable process could be found\n\ncc: @korthout ",
    "title": "Unhandled `NoSuchElementException` when looking for executable process while deploying BPMN resource"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11355",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nWe got reports of crash looping Zeebe brokers on prod, it looks like the process which is running does some nesting or looping over certain activities. TODO: I will add the process model later.\r\n\r\nThe user tried to cancel the corresponding process instance but [this failed because](https://console.cloud.google.com/logs/query;query=%0AlogName:%22stdout%22%0Aresource.type%3D%22k8s_container%22%0Aresource.labels.location%3D%22us-central1%22%0Aresource.labels.namespace_name%3D%228dca781e-03c0-4a15-9b88-1832c5d60b19-zeebe%22%0Aresource.labels.cluster_name%3D%22prod-worker-3%22%0Aresource.labels.container_name%3D%22zeebe%22%0Aresource.labels.pod_name%3D%22zeebe-0%22%0Aresource.labels.project_id%3D%22camunda-cloud-240911%22;timeRange=2023-01-02T09:49:55.253Z%2F2023-01-02T10:49:55.253Z;cursorTimestamp=2023-01-02T10:19:25.169339248Z?project=camunda-cloud-240911) there were too many activities to terminate. \r\n\r\n```\r\nExpected to write one or more follow-up records for record 'LoggedEvent [type=0, version=0, streamId=2, position=299792, key=4503599627371681, timestamp=1672654759877, sourceEventPosition=297539] RecordMetadata{recordType=COMMAND, intentValue=255, intent=TERMINATE_ELEMENT, requestStreamId=-2147483648, requestId=-1, protocolVersion=3, valueType=PROCESS_INSTANCE, rejectionType=NULL_VAL, rejectionReason=, brokerVersion=8.2.0}' without errors, but exception was thrown.\r\n```\r\n\r\nError group: https://console.cloud.google.com/errors/detail/COWzpqvwz4Cg0wE;service=zeebe;time=P7D?project=camunda-cloud-240911\r\n<!-- A clear and concise description of what the bug is. -->\r\n> **Note:** Even though we replaced the dispatcher this error will still happen since we have this max message size limit.\r\n\r\nI put the severity to high since I see no workaround. BTW due to the loop and which causes the pod crash looping the cluster was in this case unusable.\r\n\r\n**To Reproduce**\r\nHave a process instance with a lot of activities active, and terminate the corresponding process instance.\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\n**Expected behavior**\r\nTermination of instances takes into account the batch size, and terminates activities batch-wise, similar issue as to activitate multi instances.\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.lang.IllegalArgumentException: Expected to claim segment of size 4481608, but can't claim more than 4194304 bytes.\r\n\tat io.camunda.zeebe.dispatcher.Dispatcher.offer(Dispatcher.java:207) ~[zeebe-dispatcher-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.dispatcher.Dispatcher.claimFragmentBatch(Dispatcher.java:164) ~[zeebe-dispatcher-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.logstreams.impl.log.LogStreamBatchWriterImpl.claimBatchForEvents(LogStreamBatchWriterImpl.java:235) ~[zeebe-logstreams-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.logstreams.impl.log.LogStreamBatchWriterImpl.tryWrite(LogStreamBatchWriterImpl.java:212) ~[zeebe-logstreams-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$writeRecords$9(ProcessingStateMachine.java:354) ~[zeebe-stream-platform-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.retry.ActorRetryMechanism.run(ActorRetryMechanism.java:28) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.retry.AbortableRetryStrategy.run(AbortableRetryStrategy.java:45) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\"\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- OS: <!-- [e.g. Linux] -->\r\n- Zeebe Version: 8.2.0-alpha2 <!-- [e.g. 0.20.0] -->\r\n- Configuration: Production G3-S<!-- [e.g. exporters etc.] -->\r\n\r\nrelates to https://jira.camunda.com/browse/SUPPORT-16499\r\n\n\n Zelldon: Another but related error occured on PROD:\r\n\r\n```\r\nio.camunda.zeebe.stream.api.records.ExceededBatchRecordSizeException: Can't append entry: 'RecordBatchEntry[key=2251799813801783, sourceIndex=-1, recordMetadata=RecordMetadata{recordType=COMMAND, intentValue=10, intent=TERMINATE_ELEMENT, requestStreamId=-2147483648, requestId=-1, protocolVersion=3, valueType=PROCESS_INSTANCE, rejectionType=NULL_VAL, rejectionReason=, brokerVersion=8.2.0}, unifiedRecordValue={\"bpmnProcessId\":\"Process_372fbfc7-9a4a-4f0b-aee5-bd96ed3e3e5d\",\"version\":1,\"processDefinitionKey\":2251799813685320,\"processInstanceKey\":2251799813685333,\"elementId\":\"Activity_0vhm20h\",\"flowScopeKey\":2251799813685333,\"bpmnElementType\":\"USER_TASK\",\"bpmnEventType\":\"UNSPECIFIED\",\"parentProcessInstanceKey\":-1,\"parentElementInstanceKey\":-1}]' with size: 335 this would exceed the maximum batch size. [ currentBatchEntryCount: 11814, currentBatchSize: 3957709]\r\n\r\nat io.camunda.zeebe.stream.impl.records.RecordBatch.appendRecord ( [io/camunda.zeebe.stream.impl.records/RecordBatch.java:66](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl.records%2FRecordBatch.java&line=66&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.BufferedProcessingResultBuilder.appendRecordReturnEither ( [io/camunda.zeebe.stream.impl/BufferedProcessingResultBuilder.java:62](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FBufferedProcessingResultBuilder.java&line=62&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.api.ProcessingResultBuilder.appendRecord ( [io/camunda.zeebe.stream.api/ProcessingResultBuilder.java:38](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.api%2FProcessingResultBuilder.java&line=38&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.streamprocessor.writers.ResultBuilderBackedTypedCommandWriter.appendRecord ( [io/camunda.zeebe.engine.processing.streamprocessor.writers/ResultBuilderBackedTypedCommandWriter.java:37](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.streamprocessor.writers%2FResultBuilderBackedTypedCommandWriter.java&line=37&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.streamprocessor.writers.ResultBuilderBackedTypedCommandWriter.appendFollowUpCommand ( [io/camunda.zeebe.engine.processing.streamprocessor.writers/ResultBuilderBackedTypedCommandWriter.java:32](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.streamprocessor.writers%2FResultBuilderBackedTypedCommandWriter.java&line=32&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnStateTransitionBehavior.lambda$terminateChildInstances$3 ( [io/camunda.zeebe.engine.processing.bpmn.behavior/BpmnStateTransitionBehavior.java:332](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn.behavior%2FBpmnStateTransitionBehavior.java&line=332&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnStateTransitionBehavior.terminateChildInstances ( [io/camunda.zeebe.engine.processing.bpmn.behavior/BpmnStateTransitionBehavior.java:330](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn.behavior%2FBpmnStateTransitionBehavior.java&line=330&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.container.ProcessProcessor.onTerminate ( [io/camunda.zeebe.engine.processing.bpmn.container/ProcessProcessor.java:85](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn.container%2FProcessProcessor.java&line=85&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.container.ProcessProcessor.onTerminate ( [io/camunda.zeebe.engine.processing.bpmn.container/ProcessProcessor.java:27](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn.container%2FProcessProcessor.java&line=27&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.processEvent ( [io/camunda.zeebe.engine.processing.bpmn/BpmnStreamProcessor.java:122](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn%2FBpmnStreamProcessor.java&line=122&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.lambda$processRecord$0 ( [io/camunda.zeebe.engine.processing.bpmn/BpmnStreamProcessor.java:95](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn%2FBpmnStreamProcessor.java&line=95&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.util.Either$Right.ifRightOrLeft ( [io/camunda.zeebe.util/Either.java:381](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.util%2FEither.java&line=381&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.processRecord ( [io/camunda.zeebe.engine.processing.bpmn/BpmnStreamProcessor.java:92](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn%2FBpmnStreamProcessor.java&line=92&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.Engine.process ( [io/camunda.zeebe.engine/Engine.java:128](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine%2FEngine.java&line=128&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$3 ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:264](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=264&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run ( [io/camunda.zeebe.db.impl.rocksdb.transaction/ZeebeTransaction.java:84](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.db.impl.rocksdb.transaction%2FZeebeTransaction.java&line=84&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:260](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=260&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:209](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=209&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.readNextRecord ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:185](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=185&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.scheduler.ActorJob.invoke ( [io/camunda.zeebe.scheduler/ActorJob.java:92](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorJob.java&line=92&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.scheduler.ActorJob.execute ( [io/camunda.zeebe.scheduler/ActorJob.java:45](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorJob.java&line=45&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.scheduler.ActorTask.execute ( [io/camunda.zeebe.scheduler/ActorTask.java:119](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorTask.java&line=119&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask ( [io/camunda.zeebe.scheduler/ActorThread.java:106](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorThread.java&line=106&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.scheduler.ActorThread.doWork ( [io/camunda.zeebe.scheduler/ActorThread.java:87](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorThread.java&line=87&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.scheduler.ActorThread.run ( [io/camunda.zeebe.scheduler/ActorThread.java:198](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorThread.java&line=198&project=camunda-cloud-240911) )\r\n```\r\n\r\nError group, https://console.cloud.google.com/errors/detail/CJujpJmq_NqemgE;service=zeebe;time=P7D?project=camunda-cloud-240911\n saig0: :information_source: Currently, the `cancel` command is excluded from blacklisting (see [here](https://github.com/camunda/zeebe/blob/main/protocol/src/main/java/io/camunda/zeebe/protocol/record/intent/ProcessInstanceIntent.java#L22)). As a result, the process instance continues with processing.\n Zelldon: :warning: Happened again this week, and caused another incident\r\n\r\nHappened on 8.1.3 https://console.cloud.google.com/errors/detail/CKvjvtrYm_SiuwE;service=zeebe;time=P7D?project=camunda-cloud-240911 \n Zelldon: I would request to re-evaluate the priority of this by @camunda/zeebe-process-automation \r\n\r\nIncidents shouldn't happen twice. This seems to be an issue that people seem to run into easily, and there is no good way to resolve it.\n korthout: Triage summary:\r\n- Create an EPIC to tackle this problem correctly: support cancelling instances with many tokens (@aleksander-dytko )\r\n- Provide a quick and dirty solution to avoid this producing further incidents.\r\n\r\nLet's continue working on this issue by providing this quick and dirty solution\n aleksander-dytko: @korthout could you please check if I have summarized all the details in https://github.com/camunda/product-hub/issues/1067 ? \r\nThanks!\n korthout: @aleksander-dytko Thanks for creating the EPIC. I think you cover all the details.\n npepinpe: This happened again, except this time the number of child element instances is so great it causes the nodes to first slow down to a crawl due to very high GC times, then be killed due to OOM.\r\n\r\nIncident link: https://camunda.slack.com/archives/C051HA4V63D\r\nData link (incl. heap dump, process BPMN, and the complete node state): https://drive.google.com/drive/folders/1VkseQsD8Czi33dQi_kE_vV-YnfOTuJgu?usp=share_link\r\n\r\nIn case of investigation with this data, the key of the command is `4503599643148887` and its position is `93582578`. It is a `ProcessInstance.TERMINATE_ELEMENT` command.\r\n\r\nAffected version is 8.1.9, though I imagine most versions are affected.\r\n\r\nFrom the heap dump:\r\n\r\n![image](https://user-images.githubusercontent.com/43373/229531128-3fdd7686-3840-4c26-a42b-002a51142bfe.png)\r\n\r\n> The thread io.camunda.zeebe.scheduler.ActorThread @ 0xab7760e8 Broker-2-zb-actors-2 keeps local variables with total size 1.90 GB (98.54%) bytes.\r\nThe memory is accumulated in one instance of java.lang.Object[], loaded by <system class loader>, which occupies 1.90 GB (98.52%) bytes.\r\nThe stacktrace of this Thread is available. See stacktrace. See stacktrace with involved local variables.\r\n>\r\n> Keywords\r\n> - java.lang.Object[]\r\n> - io.camunda.zeebe.engine.state.instance.DbElementInstanceState.lambda$getChildren$2(Ljava/util/List;Lio/camunda/zeebe/db/impl/DbCompositeKey;Lio/camunda/zeebe/db/impl/DbNil;)V\r\nDbElementInstanceState.java:258\r\n> - io.camunda.zeebe.engine.state.instance.DbElementInstanceState.getChildren(J)Ljava/util/List;\r\n> - DbElementInstanceState.java:254\r\n\r\nMemory metrics:\r\n\r\n![image](https://user-images.githubusercontent.com/43373/229530936-c7b62201-eae5-4091-8051-09ab681e5bae.png)\r\n\r\n\r\nIn our case, the cluster was also unusable, and likely the only way to recover it is to give it [ludicrous](https://www.youtube.com/watch?v=oApAdwuqtn8) amounts of memory.\n npepinpe: Relevant support issue: https://jira.camunda.com/browse/SUPPORT-16499\r\n\r\nAnd clusters which run into this are likely to be affected by https://github.com/camunda/zeebe/issues/12239 as well (relevant support issue: https://jira.camunda.com/browse/SUPPORT-16394).\r\n\r\nPlease update the support team once these issues are fixed with a patch ETA :pray: \n remcowesterhoud: I've renamed this issue as the descriptions are not related to deep-nesting. They are related to a process instance which contains many active elements instances.\r\n\r\nFor the deep-nesting we have another issue: \r\n- https://github.com/camunda/zeebe/issues/8955\r\n\r\nI've created an epic to do a proper task breakdown https://github.com/camunda/zeebe/issues/12485",
    "title": "Not possible to cancel process instance with many active element instances"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14563",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "Like any other feature, we must ensure that it behaves correctly.\r\n\r\nWe must add E2E tests to show that DMN decisions can be evaluated in a multi-tenant cluster, where the decision can be owned by specific tenants. Other tenants should not be allowed to access this data.\r\n\r\nWe must ensure the following features:\r\n- evaluate decision through API\r\n- evaluate decision through business rule task\n",
    "title": "Add E2E multi-tenancy tests for DMN decisions"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14390",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "<!--\r\nIn case you have questions about our software we encourage everyone to participate in our community via the\r\n- Camunda Platform community forum https://forum.camunda.io/ or \r\n- Slack https://camunda-cloud.slack.com/ (For invite: https://camunda-slack-invite.herokuapp.com/)\r\n\r\nThere you can exchange ideas with other Zeebe and Camunda Platform 8 users, as well as the product developers, and use the search to find answer to similar questions.\r\n\r\nThis issue template is used by the Zeebe engineers to create general tasks.\r\n-->\r\n\r\n**Description**\r\n\r\nThe error messages refactored in #14334 bring little value to the user, and impact a lot of existing code. As a result, the changes should be reverted.\r\n\n",
    "title": "Revert tenant-aware create process instace error messages"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14252",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nWith the introduction of multi-tenancy in the upcoming 8.3 release, the runtime state stored needs to be migrated because:\r\n\r\n1. For most of the column families, the key layout changes, i.e., the respective `tenantId` becomes part of the key.\r\n2. By including the `tenantId` in the key, the data of the corresponding column families needs to be reindexed to new column families.\r\n\r\nCurrently, the migration only happens when a Zeebe broker becomes the leader for a given partition after upgrading to a newer Zeebe version:\r\n\r\nhttps://github.com/camunda/zeebe/blob/7598696d5af5eebbf233f64deecb6cbeff0f9355/stream-platform/src/main/java/io/camunda/zeebe/stream/impl/StreamProcessor.java#L325-L338\r\n\r\n(Please note: The `DbMigrationController` - to run the actual migration - is called by ` l.onRecovered(streamProcessorContext)`.)\r\n\r\nNow with the upcoming changes introduced by multi-tenancy, a Zeebe broker upgraded from 8.2 to 8.3 won't be able to replay anything because it won't find the necessary data anymore in RocksDB. This is because the key layout changed and the newly introduced column families (which are empty at that point). Meaning, the data is not migrated before the replay, so the latest snapshot corresponds to the 8.2 RocksDB schema. But when applying the events from the logstream, it assumes that the runtime state is already in the 8.3 RocksDB schema.\r\n\r\nIn a nutshell:\r\n* After upgrading to 8.3, the followers won't be able to replay any command as long as the runtime state is not migrated to 8.3.\r\n* Even after an upgrade a Zeebe broker - running in version 8.3 - becomes leader, it won't be able to replay. Hence, the leader won't process any new incoming command.\r\n\r\n---\r\n\r\nrequired by #13315 \r\nrelated to #7248\n\n romansmirnov: ## Solution Sketches\r\n\r\nTogether with @Zelldon, we brainstormed a few options on how to solve the issue.\r\n\r\n### 1. Run a migration app as a dedicated `InitContainer`\r\n\r\nBasically, when upgrading to 8.3, a migration app runs as an [`initContainer`](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/) which migrates the latest snapshots. Once finished, the actual Zeebe broker starts in version 8.3.\r\n\r\nSome notes on this approach:\r\n* When the migration app runs, it migrates the latest snapshot as necessary (defined by the migration plan).\r\n* Once finished, the migration app replaces the latest snapshot with an 8.3 snapshot.\r\n* When the actual Zeebe broker starts in version 8.3, it can replay the events (as a follower or as a leader).\r\n  * It requires that the snapshot is always upgraded previously. There could be a check to verify that the snapshot is in the expected version already, if not, it will prevent starting that partition. The log statement should point the user to run the migration app.\r\n* In a rolling upgrade scenario, processing can continue and is not blocked by a migration (assuming the quorum is still given).\r\n* Snapshots:\r\n  * A follower running on version 8.3 does not accept install requests from a leader running on version 8.2.\r\n  * A follower running on version 8.2 still continues accepting an install request from a leader running on version 8.3.\r\n    * After installing the 8.3 snapshot, the follower will fail to replay any events, i.e., it won't be able to access the required data in RocksDb.\r\n  * Both scenarios are acceptable because these are intermediate states and will eventually resolved once (all) Zeebe brokers are running on version 8.3.\r\n* `initContainer`\r\n  * Helm Charts: Requires some changes there, so when a user upgrades to 8.3 the migration app runs accordingly.\r\n  * Controller (SaaS): Requires some change there as well.\r\n* In a self-managed case (without Kubernetes), the user needs to run the migration app manually (needs to be documented)\r\n* Relates to #7248\r\n\r\n### 2. According to the given RocksDB schema use the corresponding `Db**State` classes\r\n\r\nBasically, it's a kind of code duplication to have 8.2-`Db**State`-compatible classes and 8.3-`Db**State`-compatible classes in the code base. Depending on the RocksDB schema version, the respective `Db**State` classes will be used.\r\n\r\nSome notes:\r\n* Basically, everything remains the same as of now, i.e., with 8.3, events can be replayed, and only when becoming a leader the data get migrated to 8.3.\r\n* Only a Zeebe broker running on version 8.2 won't be able to replay anything when receiving an 8.3 snapshot. This is acceptable as this broker gets upgraded to 8.3 eventually.\n Zelldon: ## 👬 Pairing\n\nToday, we paired again on this topic and started to implement a separate application, which should be used as init container. Later we realized that this might not work as expected.\n\nYou can find the progress and what we did here https://github.com/camunda/zeebe/tree/rc-migration-multi-tenancy\n\n### Init Containers \n\nWe had two potential solutions in mind. One which goes a bit more into low level and iterates over all key-value pairs and the other which reuses the the existing migration infrastructure.\n\nOn both we have the same problem that we can't easily replace a snapshot in a safe way. When using our normal snapshotting logic we must respect that snapshots have to be unique. We would need add a feature to force replace an existing snapshot. That means deleting an old one and writing the new one. Since this is right now not easily possible and might cause some other issues we went a different route.\n\n### Partition transition step\n\nAs alternative we thought about adding it into the partition logic. To be specific adding it as additional PartitionStep which is executed after the Db is opened/recovered but before the StreamProcessor is opened. This allows to easily reuse several infrastructure and benefit of already opened db etc. \n\nIf a role change from Follower to Leader happens we keep the DB which means we can also reuse the migrated state. Only if we step down we might need to re-migrated the state but it might also be that a snapshot has been replicated in this case. We need to decided whether this is acceptable. If the pod restarts we need to migrate the state again but the same would be for the init container if it restarts in between. \n\nBenefit over the current state is here that this can happen on both roles leader and follower, and decoupled from raft. So even if the migration might take long raft can already start and join. Further improvememts can be done such that migration might be split into several actors.\n\nMigration in general is aware of which steps are executed due to unique identifiers of the tasks, this means if we make migration interruptable we might can restart from a certain point.\n\nIn general, for large state this might take a while but also with the other approaches. Here it makes sense to investigate into parallelizing the migration tasks.\n Zelldon: ## 👬🏼 Pairing 2️⃣ \r\n\r\nToday, we investigate whether our current approach would work and whether the migration is executed. \r\n\r\nTL;DR; Yes migration has been successfully, BUT we still struggling whether our current approach is better than the init container. Both have pros and cons.\r\n\r\n### Details\r\n\r\nWith our latest changes, we were able to run the CI successfully and also locally the RollingUpdateTests went threw. \r\n\r\nWe extracted the logs to verify whether the migration was executed:\r\n\r\n\r\n```\r\n2023-09-12 12:48:04.401 [Broker-1] [zb-actors-0] [ZeebePartition-1] INFO \r\n      io.camunda.zeebe.broker.system - Transition to FOLLOWER on term 1 - transitioning ZeebeDb\r\n2023-09-12 12:48:04.469 [Broker-1] [zb-actors-0] [GatewayTopologyManager] DEBUG\r\n      io.camunda.zeebe.gateway - Received metadata change from Broker 1, partitions {}, terms {} and health {1=UNHEALTHY}.\r\n2023-09-12 12:48:04.470 [Broker-1] [zb-actors-0] [GatewayTopologyManager] DEBUG\r\n      io.camunda.zeebe.gateway - Received metadata change from Broker 1, partitions {}, terms {} and health {1=UNHEALTHY}.\r\n2023-09-12 12:48:04.957 [Broker-1] [zb-actors-1] [SnapshotStore-1] DEBUG\r\n      io.camunda.zeebe.logstreams.snapshot - Opened database from '/usr/local/zeebe/data/raft-partition/partitions/1/runtime'.\r\n2023-09-12 12:48:04.960 [Broker-1] [zb-actors-1] [ZeebePartition-1] INFO \r\n      io.camunda.zeebe.broker.system - Transition to FOLLOWER on term 1 - transitioning Migration\r\n2023-09-12 12:48:05.608 [Broker-1] [zb-actors-1] [ZeebePartition-1] INFO \r\n      org.camunda.feel.FeelEngine - Engine created. [value-mapper: CompositeValueMapper(List(io.camunda.zeebe.feel.impl.MessagePackValueMapper@564d37c6)), function-provider: io.camunda.zeebe.feel.impl.FeelFunctionProvider@12dbb1c0, clock: io.camunda.zeebe.el.impl.ZeebeFeelEngineClock@56eaca83, configuration: Configuration(false)]\r\n2023-09-12 12:48:06.025 [Broker-1] [zb-actors-1] [ZeebePartition-1] INFO \r\n      org.camunda.dmn.DmnEngine - DMN-Engine created. [value-mapper: CompositeValueMapper(List(io.camunda.zeebe.feel.impl.MessagePackValueMapper@768ef71e)), function-provider: org.camunda.feel.context.FunctionProvider$EmptyFunctionProvider$@2ca26c7d, audit-loggers: List(), configuration: Configuration(false,false,false)]\r\n2023-09-12 12:48:06.027 [Broker-1] [zb-actors-1] [ZeebePartition-1] INFO \r\n      org.camunda.feel.FeelEngine - Engine created. [value-mapper: CompositeValueMapper(List(org.camunda.dmn.NoUnpackValueMapper@467ceed5)), function-provider: org.camunda.feel.context.FunctionProvider$EmptyFunctionProvider$@2ca26c7d, clock: SystemClock, configuration: Configuration(false)]\r\n2023-09-12 12:48:06.218 [Broker-1] [zb-actors-1] [ZeebePartition-1] INFO \r\n      io.camunda.zeebe.engine.state.migration - Starting processing of migration tasks (use LogLevel.DEBUG for more details) ... \r\n2023-09-12 12:48:06.222 [Broker-1] [zb-actors-1] [ZeebePartition-1] DEBUG\r\n      io.camunda.zeebe.engine.state.migration - Found 9 migration tasks: ProcessMessageSubscriptionSentTimeMigration, MessageSubscriptionSentTimeMigration, TemporaryVariableMigration, DecisionMigration, DecisionRequirementsMigration, ProcessInstanceByProcessDefinitionMigration, ProcessDefinitionVersionMigration, JobTimeoutCleanupMigration, JobBackoffCleanupMigration\r\n2023-09-12 12:48:06.246 [Broker-1] [zb-actors-1] [ZeebePartition-1] DEBUG\r\n      io.camunda.zeebe.engine.state.migration - Skipping ProcessMessageSubscriptionSentTimeMigration migration (1/9).  It was determined it does not need to run right now.\r\n2023-09-12 12:48:06.249 [Broker-1] [zb-actors-1] [ZeebePartition-1] DEBUG\r\n      io.camunda.zeebe.engine.state.migration - Skipping MessageSubscriptionSentTimeMigration migration (2/9).  It was determined it does not need to run right now.\r\n2023-09-12 12:48:06.250 [Broker-1] [zb-actors-1] [ZeebePartition-1] DEBUG\r\n      io.camunda.zeebe.engine.state.migration - Skipping TemporaryVariableMigration migration (3/9).  It was determined it does not need to run right now.\r\n2023-09-12 12:48:06.253 [Broker-1] [zb-actors-1] [ZeebePartition-1] DEBUG\r\n      io.camunda.zeebe.engine.state.migration - Skipping DecisionMigration migration (4/9).  It was determined it does not need to run right now.\r\n2023-09-12 12:48:06.254 [Broker-1] [zb-actors-1] [ZeebePartition-1] DEBUG\r\n      io.camunda.zeebe.engine.state.migration - Skipping DecisionRequirementsMigration migration (5/9).  It was determined it does not need to run right now.\r\n2023-09-12 12:48:06.255 [Broker-1] [zb-actors-1] [ZeebePartition-1] DEBUG\r\n      io.camunda.zeebe.engine.state.migration - Starting ProcessInstanceByProcessDefinitionMigration migration (6/9)\r\n2023-09-12 12:48:06.257 [Broker-1] [zb-actors-1] [ZeebePartition-1] DEBUG\r\n      io.camunda.zeebe.engine.state.migration - ProcessInstanceByProcessDefinitionMigration migration completed in 2 ms.\r\n2023-09-12 12:48:06.258 [Broker-1] [zb-actors-1] [ZeebePartition-1] DEBUG\r\n      io.camunda.zeebe.engine.state.migration - Finished ProcessInstanceByProcessDefinitionMigration migration (6/9)\r\n2023-09-12 12:48:06.262 [Broker-1] [zb-actors-1] [ZeebePartition-1] DEBUG\r\n      io.camunda.zeebe.engine.state.migration - Starting ProcessDefinitionVersionMigration migration (7/9)\r\n2023-09-12 12:48:06.264 [Broker-1] [zb-actors-1] [ZeebePartition-1] DEBUG\r\n      io.camunda.zeebe.engine.state.migration - ProcessDefinitionVersionMigration migration completed in 0 ms.\r\n2023-09-12 12:48:06.264 [Broker-1] [zb-actors-1] [ZeebePartition-1] DEBUG\r\n      io.camunda.zeebe.engine.state.migration - Finished ProcessDefinitionVersionMigration migration (7/9)\r\n2023-09-12 12:48:06.266 [Broker-1] [zb-actors-1] [ZeebePartition-1] DEBUG\r\n      io.camunda.zeebe.engine.state.migration - Starting JobTimeoutCleanupMigration migration (8/9)\r\n2023-09-12 12:48:06.268 [Broker-1] [zb-actors-1] [ZeebePartition-1] DEBUG\r\n      io.camunda.zeebe.engine.state.migration - JobTimeoutCleanupMigration migration completed in 1 ms.\r\n2023-09-12 12:48:06.269 [Broker-1] [zb-actors-1] [ZeebePartition-1] DEBUG\r\n      io.camunda.zeebe.engine.state.migration - Finished JobTimeoutCleanupMigration migration (8/9)\r\n2023-09-12 12:48:06.269 [Broker-1] [zb-actors-1] [ZeebePartition-1] DEBUG\r\n      io.camunda.zeebe.engine.state.migration - Starting JobBackoffCleanupMigration migration (9/9)\r\n2023-09-12 12:48:06.270 [Broker-1] [zb-actors-1] [ZeebePartition-1] DEBUG\r\n      io.camunda.zeebe.engine.state.migration - JobBackoffCleanupMigration migration completed in 0 ms.\r\n2023-09-12 12:48:06.278 [Broker-1] [zb-actors-1] [ZeebePartition-1] DEBUG\r\n      io.camunda.zeebe.engine.state.migration - Finished JobBackoffCleanupMigration migration (9/9)\r\n2023-09-12 12:48:06.279 [Broker-1] [zb-actors-1] [ZeebePartition-1] INFO \r\n      io.camunda.zeebe.engine.state.migration - Completed processing of migration tasks (use LogLevel.DEBUG for more details) ... \r\n2023-09-12 12:48:06.280 [Broker-1] [zb-actors-1] [ZeebePartition-1] DEBUG\r\n      io.camunda.zeebe.engine.state.migration - Executed 4 migration tasks: ProcessInstanceByProcessDefinitionMigration, ProcessDefinitionVersionMigration, JobTimeoutCleanupMigration, JobBackoffCleanupMigration\r\n2023-09-12 12:48:06.281 [Broker-1] [zb-actors-1] [ZeebePartition-1] INFO \r\n      io.camunda.zeebe.broker.system - Transition to FOLLOWER on term 1 - transitioning QueryService\r\n2023-09-12 12:48:06.286 [Broker-1] [zb-actors-1] [ZeebePartition-1] INFO \r\n      io.camunda.zeebe.broker.system - Transition to FOLLOWER on term 1 - transitioning BackupStore\r\n2023-09-12 12:48:06.288 [Broker-1] [zb-actors-1] [ZeebePartition-1] INFO \r\n      io.camunda.zeebe.broker.system - Transition to FOLLOWER on term 1 - transitioning BackupManager\r\n2023-09-12 12:48:06.299 [Broker-1] [zb-actors-1] [ZeebePartition-1] INFO \r\n      io.camunda.zeebe.broker.system - Transition to FOLLOWER on term 1 - transitioning InterPartitionCommandService\r\n2023-09-12 12:48:06.300 [Broker-1] [zb-actors-1] [ZeebePartition-1] INFO \r\n      io.camunda.zeebe.broker.system - Transition to FOLLOWER on term 1 - transitioning StreamProcessor\r\n```\r\n\r\n### Rolling update\r\n \r\nFurthermore, we started a benchmark with 8.2.12 and accumulated some state (~100-200 MB). We migrated to our newest version without issues. We were able to see that even if the follower tries to become candidate the migration is only executed just once (state is not recreated)\r\n\r\n\r\n```\r\nDEBUG 2023-09-12T13:06:51.247417222Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Partition role transitioning from CANDIDATE to FOLLOWER in term 4\r\nDEBUG 2023-09-12T13:06:51.247728551Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Paused replay for partition 2\r\nINFO 2023-09-12T13:06:51.247761456Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Prepare transition from FOLLOWER on term 4 to FOLLOWER\r\nINFO 2023-09-12T13:06:51.248317603Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Prepare transition from FOLLOWER on term 4 to FOLLOWER - preparing Admin API\r\nINFO 2023-09-12T13:06:51.248912244Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Prepare transition from FOLLOWER on term 4 to FOLLOWER - preparing BackupApiRequestHandler\r\nINFO 2023-09-12T13:06:51.249188123Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Prepare transition from FOLLOWER on term 4 to FOLLOWER - preparing ExporterDirector\r\nDEBUG 2023-09-12T13:06:51.249513696Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Closed exporter director 'Exporter-2'.\r\nINFO 2023-09-12T13:06:51.251732741Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Prepare transition from FOLLOWER on term 4 to FOLLOWER - preparing SnapshotDirector\r\nDEBUG 2023-09-12T13:06:51.252178926Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Discard job io.camunda.zeebe.broker.system.partitions.impl.AsyncSnapshotDirector$$Lambda$1955/0x00007f0a709c7310 QUEUED from fastLane of Actor SnapshotDirector-2.\r\nINFO 2023-09-12T13:06:51.252599231Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Prepare transition from FOLLOWER on term 4 to FOLLOWER - preparing StreamProcessor\r\nDEBUG 2023-09-12T13:06:51.253011108Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Closed stream processor controller StreamProcessor-2.\r\nINFO 2023-09-12T13:06:51.253647071Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Prepare transition from FOLLOWER on term 4 to FOLLOWER - preparing InterPartitionCommandService\r\nINFO 2023-09-12T13:06:51.253984624Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Prepare transition from FOLLOWER on term 4 to FOLLOWER - preparing BackupManager\r\nINFO 2023-09-12T13:06:51.254314456Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Prepare transition from FOLLOWER on term 4 to FOLLOWER - preparing BackupStore\r\nINFO 2023-09-12T13:06:51.254605721Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Prepare transition from FOLLOWER on term 4 to FOLLOWER - preparing QueryService\r\nINFO 2023-09-12T13:06:51.254905759Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Prepare transition from FOLLOWER on term 4 to FOLLOWER - preparing Migration\r\nINFO 2023-09-12T13:06:51.255159637Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Prepare transition from FOLLOWER on term 4 to FOLLOWER - preparing ZeebeDb\r\nINFO 2023-09-12T13:06:51.255443546Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Prepare transition from FOLLOWER on term 4 to FOLLOWER - preparing LogStream\r\nINFO 2023-09-12T13:06:51.255778914Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Close appender for log stream logstream-raft-partition-partition-2\r\nINFO 2023-09-12T13:06:51.256060849Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] On closing logstream logstream-raft-partition-partition-2 close 1 readers\r\nINFO 2023-09-12T13:06:51.256370914Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Prepare transition from FOLLOWER on term 4 to FOLLOWER - preparing LogStorage\r\nINFO 2023-09-12T13:06:51.256690345Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Preparing transition from FOLLOWER on term 4 completed\r\nINFO 2023-09-12T13:06:51.256969580Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Transition to FOLLOWER on term 4 starting\r\nINFO 2023-09-12T13:06:51.257242079Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Transition to FOLLOWER on term 4 - transitioning LogStorage\r\nINFO 2023-09-12T13:06:51.257571625Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Transition to FOLLOWER on term 4 - transitioning LogStream\r\nINFO 2023-09-12T13:06:51.259099903Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Transition to FOLLOWER on term 4 - transitioning ZeebeDb\r\nINFO 2023-09-12T13:06:51.259420461Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Transition to FOLLOWER on term 4 - transitioning Migration\r\nINFO 2023-09-12T13:06:51.260004017Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Engine created. [value-mapper: CompositeValueMapper(List(io.camunda.zeebe.feel.impl.MessagePackValueMapper@5636f4b1)), function-provider: io.camunda.zeebe.feel.impl.FeelFunctionProvider@75967a21, clock: io.camunda.zeebe.el.impl.ZeebeFeelEngineClock@7ebeb0c7, configuration: Configuration(false)]\r\nINFO 2023-09-12T13:06:51.261972632Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] DMN-Engine created. [value-mapper: CompositeValueMapper(List(io.camunda.zeebe.feel.impl.MessagePackValueMapper@1f5702c8)), function-provider: org.camunda.feel.context.FunctionProvider$EmptyFunctionProvider$@79717f9a, audit-loggers: List(), configuration: Configuration(false,false,false)]\r\nINFO 2023-09-12T13:06:51.262358849Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Engine created. [value-mapper: CompositeValueMapper(List(org.camunda.dmn.NoUnpackValueMapper@27358b52)), function-provider: org.camunda.feel.context.FunctionProvider$EmptyFunctionProvider$@79717f9a, clock: SystemClock, configuration: Configuration(false)]\r\nINFO 2023-09-12T13:06:51.263245201Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Starting processing of migration tasks (use LogLevel.DEBUG for more details) ...\r\nDEBUG 2023-09-12T13:06:51.263566123Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Found 9 migration tasks: ProcessMessageSubscriptionSentTimeMigration, MessageSubscriptionSentTimeMigration, TemporaryVariableMigration, DecisionMigration, DecisionRequirementsMigration, ProcessInstanceByProcessDefinitionMigration, ProcessDefinitionVersionMigration, JobTimeoutCleanupMigration, JobBackoffCleanupMigration\r\nDEBUG 2023-09-12T13:06:51.263924221Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Migration was executed before ProcessMessageSubscriptionSentTimeMigration migration (1/9). It does not need to run again.\r\nDEBUG 2023-09-12T13:06:51.264226823Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Migration was executed before MessageSubscriptionSentTimeMigration migration (2/9). It does not need to run again.\r\nDEBUG 2023-09-12T13:06:51.264494405Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Migration was executed before TemporaryVariableMigration migration (3/9). It does not need to run again.\r\nDEBUG 2023-09-12T13:06:51.264790749Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Migration was executed before DecisionMigration migration (4/9). It does not need to run again.\r\nDEBUG 2023-09-12T13:06:51.265110907Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Migration was executed before DecisionRequirementsMigration migration (5/9). It does not need to run again.\r\nDEBUG 2023-09-12T13:06:51.265375939Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Migration was executed before ProcessInstanceByProcessDefinitionMigration migration (6/9). It does not need to run again.\r\nDEBUG 2023-09-12T13:06:51.265807952Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Migration was executed before ProcessDefinitionVersionMigration migration (7/9). It does not need to run again.\r\nDEBUG 2023-09-12T13:06:51.266096299Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Migration was executed before JobTimeoutCleanupMigration migration (8/9). It does not need to run again.\r\nDEBUG 2023-09-12T13:06:51.266355838Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Migration was executed before JobBackoffCleanupMigration migration (9/9). It does not need to run again.\r\nINFO 2023-09-12T13:06:51.266572157Z [jsonPayload.context.partitionId: 2] [resource.labels.containerName: zeebe] Completed processing of migration tasks (use LogLevel.DEBUG for more details) ...\r\n```\r\n\r\nFurther logs you can find [here](https://console.cloud.google.com/logs/query;query=resource.type%3D%22k8s_container%22%0Aresource.labels.project_id%3D%22zeebe-io%22%0Aresource.labels.location%3D%22europe-west1-b%22%0Aresource.labels.cluster_name%3D%22zeebe-cluster%22%0Aresource.labels.namespace_name%3D%22rc-migration-rolling-update%22%0Alabels.k8s-pod%2Fapp%3D%22camunda-platform%22%0Alabels.k8s-pod%2Fapp_kubernetes_io%2Fcomponent%3D%22zeebe-broker%22%0Alabels.k8s-pod%2Fapp_kubernetes_io%2Finstance%3D%22rc-migration-rolling-update%22%0Alabels.k8s-pod%2Fapp_kubernetes_io%2Fmanaged-by%3D%22Helm%22%0Alabels.k8s-pod%2Fapp_kubernetes_io%2Fname%3D%22zeebe%22%0Alabels.k8s-pod%2Fapp_kubernetes_io%2Fpart-of%3D%22camunda-platform%22%0Alabels.%22k8s-pod%2Fstatefulset_kubernetes_io%2Fpod-name%22%3D%22rc-migration-rolling-update-zeebe-2%22;pinnedLogId=2023-09-12T13:06:25.458470405Z%2F73jglskafng2uz9s;summaryFields=jsonPayload%252Fcontext%252FpartitionId:false:32:beginning;cursorTimestamp=2023-09-12T13:06:51.270053088Z;duration=PT1H?project=zeebe-io)\r\n\r\n\r\nWe realized that we still have the issue of becoming ready only after a role transition is complete.\r\n\r\nYou can see that the [BrokerHealtCheckService](https://github.com/camunda/zeebe/blob/main/broker/src/main/java/io/camunda/zeebe/broker/system/monitoring/BrokerHealthCheckService.java#L150) marks the broker as ready as soon we either are a follower or leader on all partitions. But this is not the leader or follower of the raft, it is the [PartitionTransition](https://github.com/camunda/zeebe/blob/main/broker/src/main/java/io/camunda/zeebe/broker/system/partitions/PartitionStartupAndTransitionContextImpl.java#L174-L185) \r\n\r\n\r\nThis means if the migration takes too long and we have hit the readiness probe timeout our pod might be restarted and we have to migrate again.\r\n\r\n## Challenges\r\n\r\n### Init containers\r\n\r\nWith the init container approach, we have the issue, of how to write/overwrite the snapshot. We could think of that writing a separate snapshot with a suffix might work, and we read that on restore (preferred over the other).\r\n\r\nBut, the biggest issue is actually that when we start we might join the cluster and can receive a snapshot with an old version, which we can't use and are not able to migrate anymore since the init container is already done.\r\n\r\n### Migration in a transition step\r\n\r\nWith the transition step, we block the complete transition. We might be able to improve the performance, but still, the migration for multi-tenancy might touch the complete state which is enormous. This becomes even more a problem when we migrate a large state. \r\n\r\nTaking long in the migration means that we defer the start of the partition and become ready, which might cause it to be restarted if we hit the readiness timeout. \r\n\r\nWe thought about whether could mark us ready already before, but here the problem is that we might accept commands that we can process in time. This leads to timeouts in clients, which might lead to retries and blow up the problem.\r\n\r\n## Open questions\r\n\r\nDo we accept that the migration might take long? That we might hit the readiness probe? This is also currently the case. If a user has a larger state he should be advised to remove the readniness probe or increase the timeout. Is this acceptable? \r\n\r\nDo we want to combine both approaches? Migration normally via init-container, but also add it as transition step to handle the case where we receive a snapshot? But this will lead again to our problem of hitting the readiness probe. Might be not a solution. \r\n\r\n\r\nCan we do it without migration? Do we really need it? We will not have no tenant at the begin, so why we do the migration? We would need to add separate logic to decided for non-existing/default tenant and existing one. This was assumed to be too complex, but is it really?\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n Zelldon: I asked the [following](https://camunda.slack.com/archives/C037RS2JHB8/p1694532168848989) in the team channel:\r\n\r\n> Today paired with Roman again and we run into some issues again regarding readiness probe. > Just to check it doesn't make sense to mark the broker earlier ready or? Because commands might be written which we can't process in time then timeout happen for clients and retries etc right?\r\n>\r\n> But this is only the case for leaders would it somehow make sense to mark earlier ready if we know we are \"just\" a follower. :thinking_face:\r\n\r\n\r\nInput from @deepthidevaki regarding the ready status:\r\n\r\n> Currently, the broker is marked as ready when all partitions are ready. A partition is ready after\r\n> \r\n> - RaftPartition is ready\r\n> - ZeebePartition is installed\r\n> \r\n> RaftPartition is ready when it's log has caught up with the leader. This is important\r\n> \r\n> - To prevent unavailability during RollingUpdate. We make sure that the restarted follower can be part of the quorum before restarting the next one.\r\n> - To handle dataloss scenarios. When the partition is ready, we know that it has caught up with the leader. So it is safe to restart other nodes.\r\n> \r\n> As far as I know, readiness probe is not used for controlling traffic in our case because the gateways directly talk to the broker pods, not through the service. So even if it is ready or not, gateway will be able to send the request to the brokers.\r\n> \r\n> > We thought about whether could mark us ready already before, but here the problem is that we might accept commands that we can process in time. This leads to timeouts in clients, which might lead to retries and blow up the problem.\r\n> \r\n> This might be ok. Because the TopologyManager is notified only after leader is fully installed. So the gateway will not send the requests to this leader. If the leader installation only completes after migration, then all listerners are notified only after that. So the CommandAPI also won't accept requests for this partition. Please verify again.",
    "title": "When upgrading to 8.3, ensure that the runtime state gets migrated accordingly"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14204",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "### Description\r\n\r\nZeebe is unable to bump the SBE protocol version due to the following problem. During a rolling upgrade:\r\n1. Broker 1 is upgraded and comes back online.\r\n2. Broker 2 is being upgraded\r\n    1. While broker 2 is upgraded, a leader election is triggered\r\n    2. Broker 2's engine starts replaying events\r\n    3. Broker 1 sends event records with a higher protocol version\r\n    4. Broker 2 throws an error that it can't handle these events and crashes\r\n\r\nThe problem is that we're misusing the SBE protocol version:\r\n* SBE uses the protocol version to be able to handle (serialize/deserialize) new fields from the later protocol version.\r\n   * By not bumping the protocol version when adding new fields, we're making it difficult for SBE to work properly\r\n* The Zeebe stream platform (mis)uses the protocol version to check if it will be able to (semantically) understand what was deserialized.\r\n   * This was done to guard against a change in the content of the SBE fields, not against adding new fields (which SBE will easily handle, by ignoring them on older versions).\r\n   * This was also done to avoid changes causing bugs on older brokers\r\n\r\nCurrently, Zeebe has a metadata property `recordVersion` which we currently use to differentiate events when they should be applied differently. I.e. an event is something that happened in the past. It describes a state change and should always produce that state change when replayed. Even when its a bug. When we want to fix a bug in an event applier, we need to change the command processor to append events of a new `recordVersion` and add a new event applier version that fixes the bug.\r\n\r\n### Solution\r\n\r\nThe addition of the `recordVersion` mechanism described above makes the stream-platform protocol version check redundant (and as described above as well, error-prone). As a result, it can be removed.\r\n\r\n### AC\r\n\r\nThe protocol version check is removed.\r\n\r\n### Additional context\r\n* Problem discussion ([internal Slack thread](https://camunda.slack.com/archives/C05Q0EXE9V4/p1693500298465369))\r\n* Related to #9949\n",
    "title": "Remove protocol version check in the broker"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14045",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "## Description\r\n\r\nRight now when we take snapshots we do that into a pending directory. If the snapshot is valid, meaning we reach a certain commit index OR we received all snapshot chunks, [we move (or copy depending on OS and filesystems) the snapshot to the valid snapshot directory](https://github.com/camunda/zeebe/blob/main/snapshot/src/main/java/io/camunda/zeebe/snapshots/impl/FileBasedSnapshotStore.java#L510).\r\n\r\nAfterward, [we check the snapshot checksum](https://github.com/camunda/zeebe/blob/main/snapshot/src/main/java/io/camunda/zeebe/snapshots/impl/FileBasedSnapshotStore.java#L517) (for that we have to read the whole snapshot again). This is mostly due to the case that we might fall back to copy and some error appears during copying. Important to note is that we already validated the checksum on receiving each chunk and the complete snapshot.\r\n\r\nWhat if we don't copy/move the snapshot from the pending, instead we directly write to the correct location? For that, we need a different marker when snapshots are valid. This could be a separate file, which marks the snapshot either as invalid (and removing it would mark it as valid) OR the existence of that file could mark it as valid. An example could be as soon as the checksum file exists the snapshot can be seen as valid.\r\n\r\n### Impact\r\n\r\nThis would mean we have to add some more complexity in reading and detecting valid snapshots, but in contrast, we wouldn't need to copy/move snapshots (interesting for large states) and potentially don't need to recheck the checksums again.\r\n\r\n### Context\r\n\r\nWhen working on https://github.com/camunda/zeebe/issues/13977 and https://github.com/camunda/product-hub/issues/1480 we (@npepinpe and I) discussed several issues and potential improvements regarding snapshot replication and taking snapshots, etc.. One thing that came up was that we create checksums of the whole snapshot, of each chunk, and when persisting we recreate such checksums (re-read the snapshots, etc.)\r\n\n\n npepinpe: We'll do it as a two step process:\n\n1. Stop computing the checksum after the move/copy, and just write the file out from in memory information. Instead, just verify all files are present. **This will let us verify how much time we save very quickly.**\n2. Since we're already handling the case where we crash after moving the snapshot but _before writing the checksum file_, it shouldn't be too much work to get rid of the pending folder, and simply use the checksum file as a marker. This mostly simplifies our data layout and (hopefully) code.\n npepinpe: @Zelldon - do you have time to work on it this week?\n Zelldon: @npepinpe I can try, but can't promise since I also have two other issues assigned, game day, benchmark stuff etc. But I give my best :) \n npepinpe: No worries. Let's sync later today to see what you can do this week.\n Zelldon: I have run two new benchmarks to see what effects bring the most recent changes.\r\n\r\nOne with the metrics added, but without any change, called [ck-persist-snapshot-no-change](https://grafana.dev.zeebe.io/d/zeebe-dashboard/zeebe?orgId=1&var-DS_PROMETHEUS=Prometheus&var-cluster=All&var-namespace=ck-persist-snapshot-no-change&var-pod=All&var-partition=All&from=1694069131830&to=1694087502965). #base\r\n\r\nThe other benchmark contains the most recent changes, meaning re-using the simple file verification checksum collection, and directly persisting it without recalculating it. \r\nBenchmark called:\r\n[ck-not-recalculate-sfv](https://grafana.dev.zeebe.io/d/zeebe-dashboard/zeebe?orgId=1&var-DS_PROMETHEUS=Prometheus&var-cluster=All&var-namespace=ck-not-recalculate-sfv&var-pod=All&var-partition=All&from=1694069131830&to=1694087502965)\r\n\r\n\r\nComparing both we can see the differences quite clearly. For the base, as soon we reach 1 gig of state we are already over 10s (the limit of the buckets) of just persisting a snapshot, including moving and creating checksum, etc. Comparing with the benchmark that includes the changes we can see that this is significant. The P99 is at most around 600 ms, instead of 10 seconds.\r\n\r\n\r\n![persist](https://github.com/camunda/zeebe/assets/2758593/f3e2a855-1849-4ad9-b6b8-139c3877aa5e)\r\n\n Zelldon: I started to replace the movement but realized that this was not straight-forward to do.\r\n\r\nThere was one thing that struck against it. It is the receiving of snapshots and where we expect multiple snapshots (and where we make directories unique) https://github.com/camunda/zeebe/blob/main/snapshot/src/main/java/io/camunda/zeebe/snapshots/impl/FileBasedSnapshotStore.java#L366-L372. It might happen that snapshot replication is restarted due some previous error/fault, then we want to write into a new directory.\r\n\r\nDiscussed this with @npepinpe To us it is not yet clear, why we not instead just remove the older snapshot and reuse the id or overwrite the snapshot. Since this would be part of improving the snapshot replication anyway we will defer this. We plan to make improvements like you can restart replicating snapshots at a certain position etc., which might help here as well.\r\n\r\nAfter deciding that, I hoped to keep the move for the received snapshots and write the transient snapshots directly to the correct snapshot directory already. Unfortunately, it entails several further works, like detecting what is a valid snapshot (only with checksum), deleting pending snapshots (which would be now without checksum), etc.\r\n\r\nI proposed and discussed with @npepinpe to continue with the snapshot replication improvements first, and then continue here to remove the movement. \r\n\r\nWe already benefit from the improvements shown above https://github.com/camunda/zeebe/issues/14045#issuecomment-1709656377, not recalculating the checksums, which is likely most of the benefit we gain out of this right now anyway.\r\n\r\nStill, it is right now weakening a bit the safety/verification, since we don't recalculate the checksum after the move. @npepinpe and I agreed that this is acceptable for now, with the plan to make rather soon progress with improving the replication and removing the move afterward. \r\n\r\n\n Zelldon: Ok I couldn't hold back. If we would decided to just delete pending received snapshots (which would the easiest approach for now) we can go with the following PR https://github.com/camunda/zeebe/pull/14230 and can remove the move.",
    "title": "No longer move/copy snapshots"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14033",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nThe stream transport protocol uses the schema ID 0, which is also the schema ID used by the `protocol` module. This means applying headers would possibly cause inconsistencies if you receive something from the other protocol.\r\n\r\nSeverity is quite low because it's unlikely to happen, but better do it before we release this whole thing.\n\n npepinpe: @megglos - this is technically a breaking change between alpha4 and alpha5 (if we merge it before alpha5), which will cause some errors during rolling update, but as most people are likely not using job push yet, it should be OK-ish.",
    "title": "Change protocol ID of the stream transport protocol"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14032",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nFollowing up on the [introduction of the yield on the due date checker ](https://github.com/camunda/zeebe/pull/9249) and it's recent frequent use to recover from a partition getting stuck on too many due timers, we want to enable it by default for Zeebe 8.3 going forward.\r\n\r\n\r\n```[tasklist]\r\n### Tasks\r\n- [x] enable yielding on the due date checker by default for the upcoming Zeebe 8.3 release\r\n```\r\n\r\n\r\n\n",
    "title": "Enable yielding on due date checker by default"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13959",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nAs a user/developer, I can specify which variables are propagated from parent to child called process instances. They can specify whether all variables or specific ones need to be propagated.\r\n\r\nTherefore, a property called `propagateAllParentVariables` is introduced to enable/disable copying all variables to its child process instance:\r\n\r\n1. When `propagateAllParentVariables` is set to `true`, all variables are propagated to the child process instance.\r\n2. When `propagateAllParentVariables` is set to `false`, only the variables defined via _input mappings_ are propagated to the child process instance.\r\n\r\nThe current default behavior - propagating all variables - does not change, meaning, the property's `propagateAllParentVariables` default value is `true`.\r\n\r\n---\r\n\r\nrelated to https://github.com/camunda/product-hub/issues/141\n",
    "title": "When using a call activity, only the variables defined via input mappings are propagated"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13958",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "Integrate `ClusterTopologyManger` with broker. `ClusterTopologyManagerStep` should start the manager. The step is completed only after the topology is initialized by `ClusterTopologyManager`. The rest of the services should use the partition distribution based on the cluster topology initialized by `ClusterTopologyManager`.\r\n\r\nAlso, add a feature flag for this. When the flag is disabled, it should use the old ways of generating partition distribution from the BrokerCfg.\n",
    "title": "Broker uses configuration from ClusterTopologyManager"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13956",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": " If a coordinator restarts with a data loss, then its local persisted topology will be  empty. If the coordinator generates the  topology from static configuration, there is a possibility of inconsistency if the static configuration does not match  the actual cluster topology.\r\n\r\nTo prevent this, coordinator should first guqery known members before generating the topology. If at least one member return an uninitalized topology, coordinator assumes that this is the bootstrap of the cluster and generates a new topology.\r\n\n\n deepthidevaki: This is done in #13960 ",
    "title": "Config manager in coordinator queries known members before initializing configuration"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13947",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "<!--\r\nIn case you have questions about our software we encourage everyone to participate in our community via the\r\n- Camunda Platform community forum https://forum.camunda.io/ or \r\n- Slack https://camunda-cloud.slack.com/ (For invite: https://camunda-slack-invite.herokuapp.com/)\r\n\r\nThere you can exchange ideas with other Zeebe and Camunda Platform 8 users, as well as the product developers, and use the search to find answer to similar questions.\r\n\r\nThis issue template is used by the Zeebe engineers to create general tasks.\r\n-->\r\n\r\n**Description**\r\n\r\nBuild on main failed https://github.com/camunda/zeebe/actions/runs/5892358015/job/15981438425#step:27:121\r\n\r\nwith:\r\n\r\n```\r\n  /usr/local/bin/docker pull tonistiigi/binfmt:latest\r\n  Error response from daemon: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit\r\nError: The process '/usr/local/bin/docker' failed with exit code 1\r\n```\r\n\r\nThis was already observed before by @deepthidevaki on merging a PR https://camunda.slack.com/archives/C05DH1F5TAR/p1692282136978109\r\n\r\n\r\nWe might get throttled due to infra gha runners, [which might come all from the same IP](https://stackoverflow.com/a/76889883/2165134). \r\n\r\nPotential workaround https://www.project-piper.io/infrastructure/docker-rate-limit/\r\n\r\n\n\n koevskinikola: ZPA triage:\r\n- The Infra team might have resolved this already ([Slack msg src](https://camunda.slack.com/archives/C03AEFWGJ9K/p1692270413052689)). Let's see if the issue occurs again.\r\n- @korthout confirms with Infra if the issue occurred before or after the changes were applied. If it happened before, the issue should be closed.\n megglos: ZDP-Triage:\n- backlog for now as ZPA looks into syncing with infra on this\n korthout: Requested information from infra on [slack](https://camunda.slack.com/archives/C5AHF1D8T/p1693207891880329)\n korthout: Infra [confirmed](https://camunda.slack.com/archives/C5AHF1D8T/p1693213935917919?thread_ts=1693207891.880329&cid=C5AHF1D8T) that their recent changes were made **before** the reported rate-limiting occurrence. \r\n\r\nInfra suggests we login to DockerHub before pulling an image to avoid the standard rate limits. I've [requested](https://camunda.slack.com/archives/C5AHF1D8T/p1693215861778599?thread_ts=1693207891.880329&cid=C5AHF1D8T) an account for this purpose.\r\n\r\nEDIT: we can use the account mentioned [in slack here](https://camunda.slack.com/archives/C5AHF1D8T/p1693216220872309?thread_ts=1693207891.880329&cid=C5AHF1D8T).\n korthout: Moving it back to inbox for triage now that the solution is clear\n korthout: ZPA Triage:\n\nAs this hasn't occurred in a few weeks, it doesn't appear to painful, except when it occurs (up to 6 hours wait time once the limit is reached) (impact medium).\n\nStill, we're planning this as `later` for now. If we run into it again, it's a good opportunity to fix it immediately\n megglos: > it doesn't appear to painful, except when it occurs\r\n\r\n😁 \n korthout: > > it doesn't appear to painful, except when it occurs\r\n> \r\n> 😁\r\n\r\nPerhaps that wasn't clear enough. We meant that it's not so painful because it doesn't occur often. But when it does occur, it's kinda painful. When we see it happen more often, we should increase the priority",
    "title": "Build failed due to docker hub rate limiting"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13923",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "## Description\r\n\r\nWhen the [weekly e2e tests are started](https://github.com/camunda/zeebe/blob/main/.github/workflows/weekly-e2e.yml#L60) we always pass the `Zeebe SNAPSHOT` as a generation. This generation is used as a template for the generation testbench creates.\r\nIn this generation all components are set to a SNAPSHOT version (except for Optimize). For our test we only override the Zeebe version. This means we could end up with a generation such as:\r\n\r\n- `Zeebe:8.0.20-SNAPSHOT`\r\n- `Tasklist:SNAPSHOT`\r\n- `Operate:SNAPSHOT`\r\n\r\nIdeally we would align the versioning to actually test on an 8.0 platform in this case. This would involve using the `Camunda Platform 8.0.19` generation as a template for the generation testbench creates.\r\n\r\n------------\r\n\r\nAll components of the platform should be backwards compatible. This means using an older Zeebe version with a newer Tasklist/Operate should, in theory, not have any impact on the tests.\r\n\n\n remcowesterhoud: ZPA individual triage:\r\n- Nice to have, but I don't see urgency as this shouldn't impact our tests. It's worked like this for years and that's unlikely to change.\r\n- Moving to the backlog as something we can do later\n megglos: ZDP-Triage:\n- it caused noise in the QA tests - e.g. a bug of a recent snapshot of a web component\n- might be an easy fix if there are patch snapshots available of the web components we could just use the same version then - we could check that\n- check on the availability of the web components patch snapshots @megglos \n megglos: patch snapshots are not available from e.g. Operate, we would need to use the previous patch then, thus use the latest generation as template as suggested by Remco.",
    "title": "Align component versions in our e2e tests"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13914",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "## Description\r\n\r\nTo ensure the stability of the new job push/streaming feature, we should integrate it into our long running benchmarks. This means updating the `Worker` application with the following features:\r\n\r\n- [x] Configuration to enable streaming (i.e. call `JobWorkerBuilder#enableStreaming`)\r\n- [x] Configuration for the stream timeout (default to 8 hours)\r\n\r\nLet's enable streaming by default in future benchmarks as well to root out as many errors as possible, and because this is our desired end state. If we see it's too buggy, we can roll it back.\r\n\r\n> **Note**\r\n> When I prototyped this, I hit a bunch of performance issues with the `Worker` application itself. I don't know if this will happen again, but be wary of it.\r\n\r\nOnce https://github.com/camunda/zeebe/pull/13818 is merged, we should also:\r\n\r\n- [x] Migrate `Worker` to use Micrometer + Prometheus (possibly migrate the `Starter` if it makes sense, since they share some code)\r\n- [x] Enable job worker metrics\r\n- [x] Update the dashboard to visualize the job worker metrics\n\n rodrigo-lourenco-lopes: PR https://github.com/camunda/zeebe/pull/13976 was only half of the issue, reopening this",
    "title": "Integrate job worker streaming and metrics into benchmarks"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13880",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "<!--\r\nIn case you have questions about our software we encourage everyone to participate in our community via the\r\n- Camunda Platform community forum https://forum.camunda.io/ or \r\n- Slack https://camunda-cloud.slack.com/ (For invite: https://camunda-slack-invite.herokuapp.com/)\r\n\r\nThere you can exchange ideas with other Zeebe and Camunda Platform 8 users, as well as the product developers, and use the search to find answer to similar questions.\r\n\r\nThis issue template is used by the Zeebe engineers to create general tasks.\r\n-->\r\n\r\n**Description**\r\n\r\nRelated to https://github.com/camunda/zeebe/issues/13879 we should try to avoid and reject such process models which use too high durations. This [recently caused an error ](https://console.cloud.google.com/errors/detail/CI_6pI2Uhor3aQ;service=zeebe;time=P7D?project=camunda-cloud-240911)on prod, when a too-high duration was used in a model.\r\n\r\n![errorprocessduration-process](https://github.com/camunda/zeebe/assets/2758593/7b572983-4202-432d-904c-b689227869c3)\r\n\r\nThis duration can not only given statically but also dynamically via variables which means we need to react on runtime.\r\n\r\nRight now if the model is executed the batch is rolled back and the last user command is rejected. The problem here is that the UX might be not that optimal, it might be worth to create an incident on the problematic element and clearly define what is wrong. This would help the user to fix there model or variable.\n\n remcowesterhoud: Individual triage:\r\n- Same thing as #13879, I'd recommend picking these up together.\r\n- We should check the result of the duration expressions and create an incident it overflows.\r\n- Worst-case this causes a banned instance.\r\n\r\nMarking as `upcoming` as banned instances should be avoided.\r\nSizing as `small` as it's easy to reproduce\r\nMarking impact as `medium`. We don't see this happening often, but when it happens it causes banning which is always bad.",
    "title": "Create incident when timer duration is too large"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13879",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "<!--\r\nIn case you have questions about our software we encourage everyone to participate in our community via the\r\n- Camunda Platform community forum https://forum.camunda.io/ or \r\n- Slack https://camunda-cloud.slack.com/ (For invite: https://camunda-slack-invite.herokuapp.com/)\r\n\r\nThere you can exchange ideas with other Zeebe and Camunda Platform 8 users, as well as the product developers, and use the search to find answer to similar questions.\r\n\r\nThis issue template is used by the Zeebe engineers to create general tasks.\r\n-->\r\n\r\n**Description**\r\n\r\nRelated to https://github.com/camunda/zeebe/issues/2108\r\n\r\nWe decided once to skip this but it turns out that actually, users deploy (who could know this 🤔) such models. This recently caused an error on prod https://console.cloud.google.com/errors/detail/CI_6pI2Uhor3aQ;service=zeebe;time=P7D?project=camunda-cloud-240911\r\n\r\n\r\n```\r\njava.lang.RuntimeException: java.lang.ArithmeticException: long overflow\r\n\tat io.camunda.zeebe.stream.impl.BufferedResult.executePostCommitTasks(BufferedResult.java:57) ~[zeebe-stream-platform-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.executePostCommitTasks(ProcessingStateMachine.java:591) ~[zeebe-stream-platform-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$executeSideEffects$18(ProcessingStateMachine.java:566) ~[zeebe-stream-platform-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.scheduler.retry.ActorRetryMechanism.run(ActorRetryMechanism.java:28) ~[zeebe-scheduler-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.scheduler.retry.AbortableRetryStrategy.run(AbortableRetryStrategy.java:45) ~[zeebe-scheduler-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) [zeebe-scheduler-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:109) [zeebe-scheduler-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:205) [zeebe-scheduler-8.2.11.jar:8.2.11]\r\nCaused by: java.lang.ArithmeticException: long overflow\r\n\tat java.lang.Math.multiplyExact(Unknown Source) ~[?:?]\r\n\tat java.time.Duration.toNanos(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.scheduler.ActorControl.scheduleTimer(ActorControl.java:140) ~[zeebe-scheduler-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.scheduler.ActorControl.runDelayed(ActorControl.java:115) ~[zeebe-scheduler-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingScheduleServiceImpl.lambda$runDelayed$0(ProcessingScheduleServiceImpl.java:50) ~[zeebe-stream-platform-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingScheduleServiceImpl.useActorControl(ProcessingScheduleServiceImpl.java:77) ~[zeebe-stream-platform-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingScheduleServiceImpl.runDelayed(ProcessingScheduleServiceImpl.java:50) ~[zeebe-stream-platform-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingScheduleServiceImpl.runDelayed(ProcessingScheduleServiceImpl.java:55) ~[zeebe-stream-platform-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.stream.impl.ExtendedProcessingScheduleServiceImpl.runDelayed(ExtendedProcessingScheduleServiceImpl.java:56) ~[zeebe-stream-platform-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.engine.processing.scheduled.DueDateChecker.schedule(DueDateChecker.java:54) ~[zeebe-workflow-engine-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.engine.processing.timer.DueDateTimerChecker.scheduleTimer(DueDateTimerChecker.java:41) ~[zeebe-workflow-engine-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.engine.processing.common.CatchEventBehavior.lambda$subscribeToTimerEvent$9(CatchEventBehavior.java:293) ~[zeebe-workflow-engine-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.stream.impl.BufferedResult.executePostCommitTasks(BufferedResult.java:55) ~[zeebe-stream-platform-8.2.11.jar:8.2.11]\r\n\t... 10 more\r\n```\r\n\r\nThe duration contained `PT9999999990S` \r\n\r\n![errorprocessduration-process](https://github.com/camunda/zeebe/assets/2758593/6120d063-2747-4cd1-a230-7b5ee39cde50)\r\n\r\nLikely this was a test process model, but still we should simply such models if the duration is static.\r\n\n\n remcowesterhoud: Individual triage:\r\n- Easily reproducible.\r\n- The chance of this happening seems quite small, especially when it's a static duration like in the example. Of course having an expression makes it less obvious.\r\n- We could catch static expressions during deployment and reject the deployment.\r\n- We should check the result of the duration expressions and create an incident it overflows.\r\n- Worst-case this causes a banned instance.\r\n- We should discuss with the Modeler team about listing rules surrounding this.\r\n\r\nMarking as `upcoming` as banned instances should be avoided.\r\nSizing as `small` as it's easy to reproduce\r\nMarking impact as `medium`. We don't see this happening often, but when it happens it causes banning which is always bad.",
    "title": "Reject static duration which is too large"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13791",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nAs done for dependabot https://github.com/camunda/zeebe/issues/10295 and backport PRs https://github.com/camunda/zeebe/issues/13666, let's automerge renovate PRs when CI is green.\r\n\r\ne.g. https://github.com/camunda/zeebe/pull/13781\n",
    "title": "Automerge green renovate PRs"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13787",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nI noticed this during the release, on 8.0/8.1 building starter & worker fails as e.g. the maven wrapper is not present on 8.1\r\n```\r\n/home/runner/work/_temp/79d164a7-e6b4-4e02-9e4e-89a3d8a459e8.sh: line 1: ./mvnw: No such file or directory\r\nError: Process completed with exit code 127.\r\n```\r\nhttps://github.com/camunda/zeebe/actions/runs/5738456998/job/15552373143\r\n\r\nOn 8.0 also build zeebe fails as there is no DIST=`build` setup in the Dockerfile https://github.com/camunda/zeebe/actions/runs/5202088373/job/14079846461\r\n\r\n\r\n```[tasklist]\n### Tasks\n- [x] backport maven wrapper to 8.0/1 - to avoid workflow merge conflicts from main to stable\n- [x] Add a `benchmark.yaml` workflow to each stable branch - to maintain a stable setup\n- [x] trigger the workflows via [workflow dispatch](https://docs.github.com/en/free-pro-team@latest/rest/actions/workflows?apiVersion=2022-11-28#create-a-workflow-dispatch-event) from the https://github.com/zeebe-io/zeebe-engineering-processes referencing the release_branch\n- [x] delete the `dispatch-benchmark.yaml` workflow from main\n```\r\n\r\n\r\n\n\n megglos: ZDP-Triage:\n- will look into it asap to resolve\n remcowesterhoud: Individual triage:\r\n- Seems to be actively worked on by ZDP at this time. It's not sensible to work on this simultaneously so I'll move it to the ZPA backlog. This also allows us to focus multi-tenancy and migration.",
    "title": "Release: `Repo dispatch Benchmark` fails on 8.0/8.1"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13775",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nWe had an incident where the liveness probes failed after 45s due to large state. This is a cluster with 3 partitions, each with a RocksDB state of over 7GB. For each partition, it took about 10 seconds purely to open the DB, which includes copying the snapshot and opening the DB. Furthermore, it seems like each partition was recovered sequentially and not in parallel, from the logs, but I haven't verified that.\r\n\r\nThe goal here would be to try and either optimize the recovery time, or have it not be w.r.t the size of the state (ideally the second one).\r\n\r\nOne issue which might be the biggest culprit in this case is this one: https://github.com/camunda/zeebe/issues/5682\r\n\n\n npepinpe: Useful link as well: https://github.com/facebook/rocksdb/wiki/Speed-Up-DB-Open\n megglos: ZDP-Triage:\n- sequential partition recover prolongs startup\n- such situations may require increasing the timeout for the liveness probe\n- we need to double-check whether we disabled the WAL of rocksdb for all versions or just 8.2\n- increased number of sst files due to partitoning is default in 8.3 => this may make it worse\n- it raised the question if we aim to support such a large snapshot state\n- on such snapshot sizes snapshot replication is also problematic\n\n=> relates to the epic to support large state - maintaining consistent performance on larger states\n megglos: realtes to another iteration on https://github.com/camunda/product-hub/issues/989\n megglos: @megglos follow up with PM in regards to a potential new epic on improving the ability of zeebe to cope with a large state\n remcowesterhoud: Large state also proofs problematic for the Startup Probe.\r\n\r\n```\r\nWarning  Unhealthy  4m28s (x288 over 71m)  kubelet  Startup probe failed: HTTP probe failed with statuscode: 503\r\n```\r\n\r\nThe snapshot size of the cluster is very large:\r\n<img width=\"743\" alt=\"image\" src=\"https://github.com/camunda/zeebe/assets/5787702/c95535eb-da42-48b5-afa4-55ac6d18e74a\">\r\n\r\nThe last log before the pod got killed:\r\n```\r\nRaftServer{raft-partition-partition-2}{role=FOLLOWER} - Started receiving new snapshot FileBasedReceivedSnapshot{directory=/usr/local/zeebe/data/raft-partition/partitions/2/pending/307189154-1021-698451241-698450032-1, snapshotStore=SnapshotStore-2, metadata=FileBasedSnapshotId{index=307189154, term=1021, processedPosition=698451241, exporterPosition=698450032}} from 1\r\n```\r\n\r\nMy hypothesis is that receiving the snapshot takes longer than the startup probe has before it kills the pod, resulting in a startup loop.\n Zelldon: I think in the incident above the snapshot replication was more the issue, and restarting of replications (due to pod restarts and due to new snapshots etc.). So we have multiple root causes which can have as a symptom that the broker is unable to start when the state is large.\r\n\r\n\n npepinpe: Yeah, Remco and I talked about it, but still felt it was relevant to post here for now to highlight multiple issues with startup time and large state (even if the original issue also talks about liveness)\n megglos: ZDP-Planning:\r\n- initial issue was that partition startup was also sequential - this may be mitigated by parallel startup already (not intended to be backported), might be bound by IO limits then\r\n- another case snapshot replication took a long time\r\n- should be handled as part of the large state\r\n- probe timeouts were increased to give more headroom (from 45s to 1m30s)\r\n- would be worth assessing the risk for other customers and current prospects\r\n\r\n=> just fixing recovery time might not be enough => we need to asses this in context of the epic https://github.com/camunda/product-hub/issues/1480\r\n\n npepinpe: The issue we merged is related, but does not complete this.",
    "title": "Improve ZeebePartition recovery time with large state"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13763",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "## Description\r\n\r\nOn startup we run a bunch of migrations to get the state in a good state for any new features. Often this means populating newly added ColumnFamilies.\r\n\r\nThe first step of a migration is checking if it needs to run. When adding a new ColumnFamily this is simple. We can check if the new ColumnFamily is empty, and if the ColumnFamily that the data is migrated from contains data.\r\n\r\nIn some cases there is no real way to determine if the migration needs to run by just looking at the data. One such example is when adding a field to an object in an existing ColumnFamily. This situation occurred once now, but I can see this happening more often in the future.\r\n\r\nWe should keep track of migrations in the state. This way we always know if a migration already ran or not, and we don't have to get smart with other checks.\r\n\r\n## Desired Solution\r\n- We introduce a new ColumnFamily `STATE_MIGRATIONS`\r\n- We add some identifier to migrations (e.g. some kind of ENUM)\r\n- The `STATE_MIRATIONS` gets the identifier as a key and an enum indicating the state of the migration as value\r\n    - The value enum could be `RUNNING`, `FINISHED`.\r\n        - I chose an enum so could have incremental migrations. Maybe we'll encounter scenarios where a migrations will be done in steps.\r\n    - If the key can't be found in the ColumnFamily it means the migration has not started yet.\r\n- Upon startup we check in this ColumnFamily if the migration has ran. If not we add it with value `RUNNING`. We run the migration and once it's finished we update the value to `FINISHED`.\n",
    "title": "Add ColumnFamily to keep track of executed migrations"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13736",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nSince we merged https://github.com/camunda/zeebe/pull/13706 renovate logs indicate it cannot handle the ubuntu tags using a date as versioning, see the logs here\r\nhttps://developer.mend.io/github/camunda/zeebe/-/job/3cdacac3-5fae-4252-83d7-23371ae59cf6\r\n\r\n```\r\nDependency ubuntu has unsupported/unversioned value jammy-20230624 (versioning=ubuntu)\r\n{\r\n  \"baseBranch\": \"main\"\r\n}\r\n```\r\n\r\nThis issue indicates there seems to be a way to configure renovate to support these though\r\nsee https://docs.renovatebot.com/modules/versioning/#ubuntu-versioning\r\nand a potential workaround here https://github.com/renovatebot/config-help/issues/633\r\n\n\n megglos: ZDP-Triage:\n- even though we do a package update as part of the docker build we should make sure we use the latest base\n remcowesterhoud: Individual triage:\r\n\r\n- We should do this\r\n- As I can see ZDP has it in their Ready column I assume they're planning to work on it. This is why I'll put it in the backlog for ZPA, we don't need both teams working on it simultaneously. This also allows us to focus on the multi-tenancy and migration topics.\n megglos: We ditched the date versioning and went for the plain names release tag + digest pinning, see https://github.com/camunda/zeebe/pull/14071#discussion_r1311176361",
    "title": "Configure Renovate to work with ubuntu tag versioning"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13666",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "<!--\r\nIn case you have questions about our software we encourage everyone to participate in our community via the\r\n- Camunda Platform community forum https://forum.camunda.io/ or \r\n- Slack https://camunda-cloud.slack.com/ (For invite: https://camunda-slack-invite.herokuapp.com/)\r\n\r\nThere you can exchange ideas with other Zeebe and Camunda Platform 8 users, as well as the product developers, and use the search to find answer to similar questions.\r\n\r\nThis issue template is used by the Zeebe engineers to create general tasks.\r\n-->\r\n\r\n**Description**\r\n\r\nBackport PRs that are created by the backport-action could be merged automatically if they succeed the CI. At this stage the code has already been reviewed and if the CI fails there is no reason to not merge it.\r\n\r\nWe already have a similar step to merge dependabot PRs, which we can reuse by simply adding the backport-action user to it. It works by approving the PR using the `GITHUB_TOKEN` of the workflow and adding a `bors merge` comment to merge the PR.\n",
    "title": "Automatically merge backport PRs"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13645",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nIt seems like Snyk is always monitoring the 8.3.0-SNAPSHOT image for all stable releases. The action is incorrectly checking out main instead of whatever the release workflow wants it to check out.\r\n\r\nThat's pretty bad, so should be fixed ASAP\r\n\n",
    "title": "Snyk monitoring wrong Docker images for stable releases"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13640",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n:warning: :warning: :warning:\r\n\r\n👋 Hi Guys, I'm sorry to inform you that we have missed some scenarios while supporting the `converging inclusive gateway`: [Support BPMN Converging Inclusive Gateway](#10031), and encountered a technical problem (for more details, see here: https://github.com/camunda/zeebe/issues/13070#issuecomment-1649387794), and the `8.3`  version will be released soon, I need to shift the focus to the `signal catch event`, ⁣ :warning: :warning: so we don't have enough time to continue to fix this issue, had to temporarily disable `converging inclusive gateway`, after processing the signal catch event task, I will come back to deal with this problem. Sorry again. :bow: :bow:\r\n\r\ncc @korthout, @koevskinikola, @nikku, @barmac.\r\n\r\n```[tasklist]\r\n### Tasks\r\n- [x] Disable Converging Inclusive Gateway for Zeebe\r\n- [x] Disable Converging Inclusive Gateway for Modeler (https://github.com/camunda/camunda-modeler/issues/3747)\r\n```\r\n\n",
    "title": "Disable Converging Inclusive Gateway"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13601",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nIf a PR is stale we want to label these with a `stale` label. This allows us to keep track of PRs that have been inactive for a while and we can decide to close them at some point.\r\n\r\n**Describe the solution you'd like**\r\n- Mark a PR as stale after 1 month of inactivity\r\n- Document in the Contributing guide that a PR is marked as stale after 1 month of inactivity and we could delete if it stays inactive.\r\n\r\nThere is a[ GitHub Action](https://github.com/actions/stale) that can do this for us.\r\n\r\n**Describe alternatives you've considered**\r\nN/A\r\n\n",
    "title": "Mark PR as stale after 1 month"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13520",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "To unblock the teams building multi-tenancy support to the other C8 components, we want to expand our protocol schema, which will be available for later usage, specifically by adding the affected tenant(s) to the record value interfaces.\r\n\r\n## Details\r\nThe new additions to the schema must be marked as experimental.\r\n\r\nAs this is only to unblock other teams, the value will not yet be filled with actual tenant ids. Instead, they will contain an empty string.\r\n\r\n## Schema changes\r\nThe record values involved with the following RPCs need a way to specify a tenant:\r\n\r\n- DeployResource RPC\r\n- CreateProcessInstance RPC\r\n- CreateProcessInstanceWithResult RPC\r\n- EvaluateDecision RPC\r\n- BroadcastSignal RPC\r\n- PublishMessage RPC\r\n\r\nThe record values involved with the following RPCs need a way to specify one or more tenants:\r\n\r\n- ActivateJobs RPC\r\n- StreamJobs RPC (we can consider this one out of scope for this issue)\r\n\r\n## Out of scope\r\nNote that these changes don't yet reflect the tenants a client may have access to. This will be covered in another issue.\r\n\r\n## Additional context\r\n- [Internal details](https://docs.google.com/document/d/1qWx82v7yqlJaL4NRXNca2Uxk4NZnmgh0sd5LQFeobHc/edit?pli=1#heading=h.6nar7bl09aov)\r\n\r\n## Unblocks\r\n- https://github.com/camunda/operate/issues/4778\n",
    "title": "Add experimental multi-tenancy record schema to protocol"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13299",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nSpring displays a warning at startup if both `spring-jcl` and `commons-logging` are found in the same class path:\r\n\r\n```\r\nStandard Commons Logging discovery in action with spring-jcl: please remove commons-logging.jar from classpath in order to avoid potential conflicts\r\n```\r\n\r\n[As per their own Spring Data project (which also uses the Elasticsearch client)](https://github.com/spring-projects/spring-data-elasticsearch/pull/1993), it seems fine to simply exclude `commons-logging`.\r\n\n",
    "title": "Remove Spring warning about spring-jcl and commons-logging"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13281",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nWe added many actors with the new job push feature, but forgot to name them properly. Having properly named actors (with the node ID and behavior) helps a lot when debugging to understand who is doing what.\r\n\n\n npepinpe: Turns out we had to backport the release changes, as the workflow definition used is from the branch under test apparently.",
    "title": "Define actor names for new remote stream actors"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13240",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "Follow up #https://github.com/camunda/zeebe/pull/12839#issuecomment-1564228116 \r\n\r\nIn  PR #12839, when we added new fields to the segment descriptor the descriptor length changed. This can cause issues, when we overwrite an existing descriptor. We have to ensure that we don't accidently overwrite the first entry. This is handled in the PR. But, when ever we make changes to the descriptor this needs to be handled correctly. It is prone to errors, so it is better we use fixed size for the descriptor. We may not use the full allocated bytes, but it allows us to add new fields without the need to change the length again.\r\n\r\nBesides, we also now have to keep track of length of each version. https://github.com/camunda/zeebe/pull/12839#discussion_r1246323206 With fixed lenght, we might be able to handle this better.\r\n\r\nNote: Better do it before 8.3, otherwise it will be more effort to make it backward compatible.\r\n\n\n deepthidevaki: @npepinpe We discussed different things previously. \r\n1. As described in the issue, use a fixed length for descriptor. There will be unused bytes so that we can freely add or remove fields in future with out changing the descriptor length. This means we will have three versions - V1 (old one), V2 (version until 8.2 where the length = sbe encoded length), V3 with fixed length.\r\n2. We also discussed about removing the need to keep track of the version length because we can determine the length while decoding sbe. In this case we will have only two version - V1 (old one) and V2. In V2 depending on sbe version we may get different lengths. We can also add new fields in future which changes the length without changing the version because sbe version is updated.\r\n\r\nI'm now thinking about which way to proceed. Any opinions?\r\nI'm leaning towards option 2. The main problem with variable length is that a new version should not overwrite an existing descriptor with old version. But, this is already handled in the code. So I don't see any other blockers. With this, I don't see any advantage of using fixed length. What do you think? \n npepinpe: The second one as well :+1:\n deepthidevaki: Closing this issue as we decided not to use fixed length descriptor. Instead PR #13618 improves the handling of version in segment descriptor.",
    "title": "Use a fixed length segment descriptor"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13214",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nRelated to https://github.com/camunda/product-hub/issues/1356 all C8 components must adhere to the [best practices](https://confluence.camunda.com/display/HAN/Dockerfile+Base+Images) by implementing them or being an (approved+documented) exception.\r\n\r\nAccording to the current [Adoption Status](https://confluence.camunda.com/display/HAN/Dockerfile+Base+Images#DockerfileBaseImages-AdoptionStatus) for Zeebe this means:\r\n\r\n- introduce [strict pinning](https://confluence.camunda.com/display/HAN/Dockerfile+Base+Images#DockerfileBaseImages-StrictVersionPinningforBaseImages) by changing https://github.com/camunda/zeebe/blob/main/Dockerfile\r\n- check whether Alpine can be adopted or provide written reasoning why it is not feasible\n\n npepinpe: We've already started work via https://github.com/camunda/zeebe/issues/12959\r\n\r\nAs a first step we'll move to the Alpine based Temurin image. Later we can investigate building our own Java base image on top of Alpine, if only to keep us more up to date with base image security fixes, as each additional layer on top of a base image adds delays for security patches to get added.\r\n\r\nI did want to challenge a few things there, if you have some time though (e.g. pro-actively apply security patches ourselves to our own Java base image at the cost of reproducibility).\n cmur2: > As a first step we'll move to the Alpine based Temurin image.\r\n\r\nNice :+1: I think if it is Alpine 3.18 based there shouldn't be too much problems in C8 SaaS but you'll probably test it carefully, anyways.\r\n\r\n> I did want to challenge a few things there, if you have some time though (e.g. pro-actively apply security patches ourselves to our own Java base image at the cost of reproducibility).\r\n\r\nSure, I'm available :) I think manually applying security patches can have more upsides than downsides in your usecase, so definitely open to that :+1:\n npepinpe: So the easy way of applying patches is simply to update installed packages to their latest version. Downside is:\r\n\r\n1. It's blindly updating everything, not just security patches. It seems Alpine doesn't support making a distinction between security updates and other kinds.\r\n2. Builds won't be reproducible anymore (unless we vendor the APKINDEX tar ball =/)\r\n\r\nFor 1, well, there isn't all that much installed in the first place, so maybe it's acceptable. For 2, I can't remember the last time we wanted to reproduce a build, except within very short time frames, so unclear how much value we get from that currently.\n megglos: moving to ready as @npepinpe is already on it\n npepinpe: Another question (though this may be more a product decision) is, do we consider moving from an Ubuntu base to Alpine a breaking change?\r\n\r\nFor example, doing this _will_ break the Helm chart, as it currently overwrites the startup command with a `bash` script - Alpine, by default, does not include `bash`, but `sh`.\r\n\r\nWe can update the Helm chart, but this may break existing user deployments, or anybody who was building on top of our own image.\n hown3d: @npepinpe adding bash in the alpine image by apk would fix that issue at first to not introduce breaking changes. \nI would expect the new alpine based image to work without changes in Kubernetes configuration.\n npepinpe: Hey @hown3d , thanks for the perspective. Bash was just an example though, there might be other changes which would break existing deployments, since ubuntu based images contain about 3 times the number of utilities, and who knows what everyone is relying on.\n\nSince one of the goals here is to reduce effort related to CVE maintenance (including informing users how certain flagged vulnerabilities are irrelevant for us), reinstalling everything is counterproductive (of course we could say only bash is important and everything else is acceptable :shrug:)\n\nAt some point I think we'll have to bite the bullet. The problem is I don't have any real data on how many builds would break by switching to an Alpine based image.\n\nMaybe it's enough to, say, have an image which works out of the box with the existing Helm chart, and we can remove bash in the future. I don't think bash is a big source of CVEs, so should be fine.\n hown3d: If it's about CVE maintenance, maybe check out the images from [chainguard](https://github.com/chainguard-images/images). They provide secure and minimal images, that contain as close to 0 CVEs. The images are based on the [Wolfi OS project](https://github.com/wolfi-dev/os).\n npepinpe: I evaluated the Chainguard images [here](https://github.com/camunda/zeebe/issues/12959). I agree they likely have less vulnerabilities, but it's also a bit of a chicken and egg, where most vulnerability scanners simply have no information about the base image and it's unclear how well it gets scanned for CVEs. Plus, extending them is somewhat painful (much like Google's Distroless) due to the steep learning curve behind apko and melange.\r\n\r\nAt any rate, the issue with breaking changes remains the same, whether it's chainguard or alpine based :shrug: \n hown3d: Indeed, it is accurate to say that extending it can pose some challenges. \r\nMy suggestion would be to utilize the [wolfi-base](https://edu.chainguard.dev/chainguard/chainguard-images/reference/wolfi-base/overview/) image, which includes **apk** and **busybox** enabling the installation of packages. Using the wolfi-base image would allow to build and extend the image by Dockerfile.\r\nAs wolfi employs the APKINDEX, security scanners should also extract data from it. \r\n\r\nA downside would be that it would be neccessary to install jre during build.\r\n\r\nIt is worth noting that there are still lingering breaking changes by those changes, as you rightly mentioned.\r\n\r\n\n npepinpe: > A downside would be that it would be neccessary to install jre during build.\r\n\r\nAt any rate, I was thinking we might do this in the long run anyway, if only to keep our base image faster up to date with security patches (since the official JRE images are often behind by up to a month). The main blocker there is enabling Renovate to detect JRE updates based on our Dockerfile and raise PRs/issues when we need to update it. This should be possible via regex matches and the [Java Version data source](https://docs.renovatebot.com/modules/datasource/java-version/)\n hown3d: I've scribbled around a bit and created a patch to use wolfi as the base image:\r\n```diff\r\ndiff --git a/Dockerfile b/Dockerfile\r\nindex c20495235a..83ab04ea78 100644\r\n--- a/Dockerfile\r\n+++ b/Dockerfile\r\n@@ -5,9 +5,9 @@\r\n ARG JVM=\"eclipse-temurin\"\r\n ARG JAVA_VERSION=\"17\"\r\n # We duplicate the JVM and JAVA_VERSION vars here as renovate will otherwise fail to properly parse\r\n-ARG BASE_IMAGE=\"eclipse-temurin:17-jre-focal\"\r\n-ARG BASE_DIGEST_AMD64=\"sha256:901eeb64e3d1e74d261e82e4158386407b95628eaf723058fb96d4efb9141b88\"\r\n-ARG BASE_DIGEST_ARM64=\"sha256:eb3488634b9b33c601be1bbee8abf59e98bf4f2493abd982baea0f4831f25b31\"\r\n+ARG BASE_IMAGE=\"cgr.dev/chainguard/wolfi-base\"\r\n+ARG BASE_DIGEST_AMD64=\"sha256:dd1801ff8675b4aa2aac0a62384be211ed678d29df4a835c039ffdb71f12de05\"\r\n+ARG BASE_DIGEST_ARM64=\"sha256:a57f3751b6ead1dab3c48ca06092ac4e7a30156f3b84b070e88e48890d77f01c\"\r\n\r\n # set to \"build\" to build zeebe from scratch instead of using a distball\r\n ARG DIST=\"distball\"\r\n@@ -93,10 +93,16 @@ LABEL io.openshift.non-scalable=\"false\"\r\n LABEL io.openshift.min-memory=\"512Mi\"\r\n LABEL io.openshift.min-cpu=\"1\"\r\n\r\n+RUN apk add ca-certificates \\\r\n+    bash \\\r\n+    openjdk-17-jre~17 \\\r\n+    libstdc++ # for rocksdb\r\n+\r\n ENV ZB_HOME=/usr/local/zeebe \\\r\n     ZEEBE_BROKER_GATEWAY_NETWORK_HOST=0.0.0.0 \\\r\n     ZEEBE_STANDALONE_GATEWAY=false \\\r\n-    ZEEBE_RESTORE=false\r\n+    ZEEBE_RESTORE=false \\\r\n+    JAVA_HOME=/usr/lib/jvm/java-17-openjdk\r\n ENV PATH \"${ZB_HOME}/bin:${PATH}\"\r\n # Disable RocksDB runtime check for musl, which launches `ldd` as a shell process\r\n # We know there's no need to check for musl on this image\r\n@@ -108,8 +114,8 @@ VOLUME /tmp\r\n VOLUME ${ZB_HOME}/data\r\n VOLUME ${ZB_HOME}/logs\r\n\r\n-RUN groupadd -g 1000 zeebe && \\\r\n-    adduser -u 1000 zeebe --system --ingroup zeebe && \\\r\n+RUN addgroup -g 1000 zeebe && \\\r\n+    adduser -u 1000 zeebe -S -G zeebe && \\\r\n     chmod g=u /etc/passwd && \\\r\n     # These directories are to be mounted by users, eagerly creating them and setting ownership\r\n     # helps to avoid potential permission issues due to default volume ownership.\r\n```\r\n\n npepinpe: Happy to get feedback on the PR. I created it as a PR, but I actually meant to make it a draft PR to get some early feedback :sweat_smile: ",
    "title": "Align with Docker base image best practices"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13161",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "With #12764 we can now append different versions of records. By default, we append version 1 of a record. However, that's not easy to maintain. As we add more places where records are appended we have to remember all these and update them when we want to create a new version.\r\n\r\nInstead, it makes more sense to append the latest version of a record by default, and allow changing this in specific cases. Adding a new version of a record is then achieved automatically in all places.\n",
    "title": "Append latest version of records by default"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13057",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "The Command Distribution records are not yet logged in compact form by the compact record logger.\n",
    "title": "Implement compact record logger for Command Distribution"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13048",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nThe job streamer API on the gateway side is asynchronous to avoid putting the endpoint's logic into the client streamer's actor context. \r\n\r\nHowever, there is no executor available in the `EndpointManager`. By default, service calls run in the shared gRPC executor (a cached static pool defined in `GrpcUtil`). But there's no easy way to get back to that executor from another thread context.\r\n\r\nThe issue here is to make this executor available at least to the call handling the job stream command.\r\n\r\nPossible solutions:\r\n\r\n- Configure both a gRPC `executor` and `serverCallExecutor`. So now the Netty event loop group will handle the network calls, the executor will handle the gRPC portion, and the server call executor will handle server call pipeline, include the endpoint manager calls. As these are manually configured, we can pass a reference of the `serverCallExecutor` to our endpoint manager as the executor.\r\n- Like the job activation handlers, make the stream job handler an actor.\n\n npepinpe: One option is to define a gRPC executor, and provide the same one for the endpoint manager. Concurrency is then shared amongst handling requests and pushing jobs, in a way.\r\n\r\nAnother option is to handle each request in a one-actor-per-request model, via the gRPC `callExecutor`. This allows providing one executor per call. We could submit an actor then, or keep a running actor pool and forward handling the request to specific actors. This would keep us in the actor scheduler world, while still providing an actor for each request. This may require deployments to increase the number of management threads, which is typically quite low on default gateway deployments.\r\n\r\n",
    "title": "Provide access to an executor in the endpoint manager"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12959",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nMany container scanning tools regularly flag several medium and low CVEs in our official Docker image. While we investigate them, it's a time consuming process, and it's poorly communicated to the outside world. Additionally, many of these come from dependencies that we make no use of, and end up being either just noise (e.g. memory overflow in freetype), or unnecessary attack vectors (e.g. having `nc` or `sshd` installed)\r\n\r\n> **Note**\r\n> Consider this a first step towards a vulnerability free container image.\r\n\r\nOne recurring suggestion to switch base image. This would hopefully mean we can only focus on the vulnerabilities that would be introduced directly by us, over which we have full control.\r\n\r\nThis issue is to investigate this and other possible solutions to modify our official Docker image to reduce not only the number of CVEs currently listed, but reduce the overall amount of effort needed to keep it low.\r\n\n\n npepinpe: # Changing base images\r\n\r\nLet's start with the biggest bang for our buck, changing base image. I've spent some time to analyze the following images:\r\n\r\n- Distroless - [gcr.io/distroless/java17-debian11](https://gcr.io/distroless/java17-debian11)\r\n- Temurin Ubuntu - [eclipse-temurin-17-jre-focal](https://hub.docker.com/layers/library/eclipse-temurin/17-jre-focal/images/sha256-22f133769ce2b956d150ab749cd4630b3e7fbac2b37049911aa0973a1283047c?context=explore)\r\n- Temurin Alpine - [eclipse-temurin:17-jre-alpine](https://hub.docker.com/layers/library/eclipse-temurin/17-jre-alpine/images/sha256-d69f8cf3526fd75992366d2e96348682dfbc04c5d321c08d084e1fc26980d81d)\r\n- Chainguard - [cgr.dev/chainguard/jre:latest](https://github.com/chainguard-images/images/tree/main/images/jre)\r\n- Liberica JRE - [bellsoft/liberica-runtime-container](https://hub.docker.com/r/bellsoft/liberica-runtime-container)\r\n- Zulu Distroless - [azul/zulu-openjdk:17-distroless](https://hub.docker.com/r/azul/zulu-openjdk)\r\n\r\nFor each image, I evaluated the following:\r\n\r\n- Number of existing CVEs\r\n- Release cadence\r\n- Architectures supported\r\n- Governance (i.e. who maintains/supports it)\r\n- Longevity (i.e. how long it's been around)\r\n- DevEx (e.g. tooling support, learning curve, extensibility, etc.)\r\n\r\n## Not considered\r\n\r\nI didn't look deeply into the following images:\r\n\r\n- Amazon Corretto: the official image is Alpine based, and Corretto being an OpenJDK variant, quick scans showed very similar results as the Temurin/Alpine images.\r\n- Azul Zulu Alpine: similarly, these images seem very similar to the Temurin ones, where the vulnerability comes from the base image.\r\n\r\n## Distroless\r\n\r\nImage: [gcr.io/distroless/java17-debian11](https://gcr.io/distroless/java17-debian11)\r\n\r\n### TL;DR\r\n\r\n| CVEs | Release | Arch | Governance | Longevity | DevEx | Size |\r\n|------|---------|------|------------|-----------|-------|------|\r\n| :warning: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :warning: | 231 MB |\r\n\r\n### Evaluation\r\n\r\n[Distroless](https://github.com/GoogleContainerTools/distroless) is actually a set of images provided by Google, which are not, in fact, distro-less. They're based on debian, but a very, very stripped down version of debian.\r\n\r\nEach image is offered in various variants: `nonroot`, `debug`, and `debug-nonroot`. The `nonroot` variants are self explanatory, using a pre-made, non-root user, and the debug variants simply add [busybox](https://busybox.net/) to the image. They're all built for several architectures, but in our case, the important part is they support both `amd64`, `arm64`.\r\n\r\nImages typically built on top of each other. So the Java image has the following hierarchy:\r\n\r\n- gcr.io/distroless/static\r\n- gcr.io/distroless/base\r\n- gcr.io/distroless/java17-debian11\r\n\r\nThe static image contains a stripped down version of debian (version configurable), `ca-certificates`, a `/etc/passwd` entry for a root user, a `/tmp` directory, and time zones (`tzdata`). The base then adds `glibc`, `libssl`, and `openssl`. Then nonroot and debug variants add additional entries and packages, as mentioned above.\r\n\r\n> **Note**\r\n> To fully understand how the distro is stripped down, you have to read the [Bazel](https://bazel.build/) build files. It's fairly understand as these are not complex ones, but it's still a little of a learning curve.\r\n\r\nThe Java image uses debian11 as the base distro, and adds a JDK or a JRE on top. Currently, only images for the LTS versions of the JDK/JRE are produced (11 and 17). These are installed from the official Debian packages.\r\n\r\nIn addition to the JDK/JRE package, it adds several other packages required for a full deployment: `libcrypt`, `libfreetype6`, `libpcre3`, etc. The Bazel file contains an exhaustive list.\r\n\r\n> **Note**\r\n> While I say images are built on top of each other, they're built using Bazel. Meaning in the end, the Java image has no base image at all.\r\n\r\nScanning it with Snyk reveals 33 issues: 1 high, and 2 medium. Scanning with Trivy actually failed at times, and when not, returned 2 high, 5 medium, and 27 low vulnerabilities. Release cadence seems to be fairly regular, with new images pushed whenever new patches are available on for the software included (as obtained via APT). The images have been around for about 6 years at this point, so they're fairly well honed.\r\n\r\nThe biggest downside here is the developer experience. Bazel is used to build the images, which results in an image with no Dockerfile. This means it's less straightforward to understand exactly how it was built, as you need to parse Bazel files. Thankfully these are small enough, so it's not too big of a learning curve, but still. It also means while their Bazel build is made to be extensible, it's a bit more difficult to extend the images via Docker. Finally, their recommended production images (the non-debug variants) are meant to be as stripped as possible. While this reduces possible attack vectors and sources of noisy CVEs, it means developers need to get used to having shell-less, utility-less (e.g. no `ls`, no `df`, etc.) tools, and must use things such as ephemeral containers on Kubernetes (if available), or manually import their tooling (e.g. copy over busybox's statically linked uclibc variant).\r\n\r\n## Temurin Ubuntu\r\n\r\nImage: [eclipse-temurin:17-jre-focal](https://hub.docker.com/_/eclipse-temurin)\r\n\r\n> **Note**\r\n> This is currently the base image for [camunda/zeebe](https://hub.docker.com/camunda/zeebe)\r\n\r\n### TL;DR\r\n\r\n| CVEs | Release | Arch | Governance | Longevity | DevEx | Size |\r\n|------|---------|------|------------|-----------|-------|------|\r\n| :warning: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | 267 MB |\r\n\r\n### Evaluation\r\n\r\nThis image is built by the [Eclipse foundation](https://projects.eclipse.org/projects/adoptium.temurin), and provided via the [Adoptium project](https://adoptium.net/). This is a working group backed by Microsoft, IBM, RedHat, Google, Azul, and others. It's essentially a pre-built OpenJDK distributed by them. As such, there is no change between this and any other OpenJDK implementation to be expected.\r\n\r\n> **Note**\r\n> Eclipse Temurin was formerly the [AdoptOpenJDK project](https://adoptium.net/blog/2021/08/adoptium-celebrates-first-release/).\r\n\r\nThe image is built directly on top of the latest [ubuntu-focal](https://hub.docker.com/_/ubuntu) - see the [official Dockerfile](https://github.com/adoptium/containers/blob/920efae8fe37e2b8f2b288b5f7f9e67134ecad1d/17/jre/ubuntu/focal/Dockerfile.releases.full) here. It contains everything from that image, plus the `tzdata`, `curl`, `wget`, `ca-certificates`, `fontconfig`, `locales`, and `binutils` packages.\r\n\r\n> **Note**\r\n> I don't really see why they add `curl`, and `wget`, other than for convenience and usage of `wget` in their Dockerfile, since Docker supports a remote `ADD` command.\r\n\r\nThe JRE installed is fetched directly as a tar ball from their own servers, and unpacked.\r\n\r\nThe image is released roughly on a monthly basis from experience, keeping up with security patches from the base image. That said, as of now, a Snyk scan of the image results in 31 CVEs, of which 9 are medium. **One major caveat to this**: the latest official Ubuntu base image (Mantic or 23) is vulnerability free, and the last Debian base image (buster or 11, which is what the Temurin image is based on) only has low severity CVEs. One alternative would then be us using the Ubuntu base image and unpacking the official Java tar ball ourselves.\r\n\r\nRunning a Snyk scan returns 31 vulnerabilities, of which 9 are medium, and 22 are low.\r\n\r\nRunning a Trivy scan returns 25 medium, and 57 low vulnerabilities.\r\n\r\n## Temurin Alpine\r\n\r\nImage: [eclipse-temurin:17-jre-alpine](https://hub.docker.com/_/eclipse-temurin)\r\n\r\n### TL;DR\r\n\r\n| CVEs | Release | Arch | Governance | Longevity | DevEx | Size |\r\n|------|---------|------|------------|-----------|-------|------|\r\n| :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :warning: | :heavy_check_mark: | :heavy_check_mark: | 162 MB |\r\n\r\n### Evaluation\r\n\r\nThis image is the [Linux Alpine](https://www.alpinelinux.org/) variant of the Temurin family of images. The main difference with the previous one is its reliance on the [musl libc](https://musl.libc.org/) implementation instead of the more common [glibc](https://www.gnu.org/software/libc/). This sometimes has an impact on native libraries which can make usage of the standard C library. For example, in Zeebe, we previously could not use any Alpine based images because of our dependency on RocksDB, which at the time, did not distribute `musl` variants.\r\n\r\nRunning a Snyk scan of the image returns a single low severity vulnerability, which is impressive. Running a Trivy scan returned a single medium vulnerability. However, keep in mind that Alpine is entirely community driven, and it's difficult to evaluate if the fact it has less CVEs in general is because it *is* more secure, or simply that it's less scrutinized/used. \r\n\r\n> **Warning**\r\n> I haven't been able to find any conclusive evidence that Alpine as a distribution is more secure than Debian. If anyone is aware of anything, I'd be very interested to learn more about it.\r\n\r\nOne major drawback of switching to Alpine is our use of native libraries. When using native libraries, the differences between `glibc` and `musl` may be subtle, and it can be difficult to pinpoint errors. If the libraries are not explicitly testing against `musl`, this could be a major downside. Speaking of the differences, [here is a nice comparison table of the main differences](https://www.etalabs.net/compare_libcs.html).\r\n\r\nThe image is built directly on top of the latest [alpine:3.18](https://hub.docker.com/_/alpine) - see the [official Dockerfile](https://github.com/adoptium/containers/blob/main/17/jre/alpine/Dockerfile.releases.full) here. It contains everything from that image, plus the `fontconfig`, `libretls`, `musl-locales`, `musl-locales-lang`, `ttf-dejavu`, `tzdata`, and `zlib` packages.\r\n\r\n# Chainguard/Wolfi\r\n\r\nImage: [cgr.dev/chainguard/jre:latest](https://github.com/chainguard-images/images/tree/main/images/jre)\r\n\r\n## TL;DR\r\n\r\n| CVEs | Release | Arch | Governance | Longevity | DevEx | Size |\r\n|------|---------|------|------------|-----------|-------|------|\r\n| :question: | :heavy_check_mark: | :heavy_check_mark: | :warning: | :warning: | :warning: | 273 MB |\r\n\r\n## Evaluation\r\n\r\n[Chainguard](https://www.chainguard.dev) is a fairly new startup (founded in 2021) which focuses on software supply chain solutions. They provide their own variant of a [distroless image, which they dub \"Undistro\"](https://www.chainguard.dev/unchained/introducing-wolfi-the-first-linux-un-distro), called [Wolfi](https://github.com/wolfi-dev/os).\r\n\r\nOn the plus side, Chainguard images are rebuilt daily from source, meaning they keep up with CVEs fairly well. While they use the APK format, they aren't an Alpine variant, but really a different Linux distro (or as they would say, undistro, as they don't package a kernel along with their OS, relying on the host's kernel). As they use two in-house tools to build their images - [apko](https://github.com/chainguard-dev/apko) and [melange](https://github.com/chainguard-dev/melange) - their images result in a single fat layer. This makes extending the JRE image, for example, a bit complicated, unless you're willing to pick up knowledge of both tools. Extending the base Wolfi image would certainly be easier.\r\n\r\nSimilar to the distroless images, the Chainguard images tend towards minimal, and typically won't include shells, or sshd, ftpd, etc. Additionally, they don't include a package manager, which removes one additional attack vector (e.g. an attacker gaining access to your image, downloading curl, then downloading their own malicious application). However, the downside is it's harder to debug such images. To circumvent this, much like the distroless images, [Chainguard provides `dev` variants which include many utilities](https://edu.chainguard.dev/chainguard/chainguard-images/debugging-distroless-images/).\r\n\r\nRunning a Snyk scan on the image results in no vulnerabilities found, but Snyk does emit a warning:  \"Note that we currently do not have vulnerability information for Wolfi 20230201, which we detected in your image.\"\r\n\r\nRunning a Trivy scan returns no vulnerabilities, with Wolfi being correctly detected without any warnings this time.\r\n\r\n# Liberica\r\n\r\nImage: [bellsoft/liberica-runtime-container](https://hub.docker.com/r/bellsoft/liberica-runtime-container)\r\n\r\n## TL;DR\r\n\r\n| CVEs | Release | Arch | Governance | Longevity | DevEx | Size |\r\n|------|---------|------|------------|-----------|-------|------|\r\n| :question: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :warning: | :heavy_check_mark: | 124 MB |\r\n\r\n[Liberica JDK](https://bell-sw.com/libericajdk/) is an OpenJDK distribution built by [BellSoft](https://bell-sw.com/), who are notable OpenJDK, having pushed hard for the Alpine port, for example. \r\n\r\n> **Note**\r\n> I haven't had much experience with Liberica JDK. It's supposedly a cloud optimized flavor of OpenJDK, with a focus on a smaller size via module compression. It should otherwise be essentially an OpenJDK clone, functionality wise. \r\n\r\nThe Liberica Runtime Container (which is a JRE image) is made of two layers. The base, BellSoft's Alpine variant called [Alpaquita Linux](https://bell-sw.com/alpaquita-linux/):\r\n\r\n> Alpaquita is distinguished by multiple performance and security optimizations and LTS releases optimal for enterprise use. Alpaquita comes with three additional malloc for various workloads and best Java support. In addition, we enhanced stock musl libc. Our musl perf is similar or superior to glibc performance. We also provide two Alpaquita variants based on musl perf or glibc.\r\n\r\nFull disclosure, I have no idea whether or not these claims are true, and this would definitely require some testing. \r\n\r\nRunning a Snyk scan reveals no vulnerabilities, however it prints a similar warning as with Wolfi, that it doesn't recognize the Linux distribution. A Trivy scan also returned no vulnerabilities.\r\n\r\nIn terms of DevEx, the Liberica image comes with BusyBox, the `apk` package manager, and all that entails.\r\n\r\n# Zulu Distroless\r\n\r\nImage: [azul/zulu-openjdk:17-distroless](https://hub.docker.com/r/azul/zulu-openjdk)\r\n\r\n## TL;DR\r\n\r\n| CVEs | Release | Arch | Governance | Longevity | DevEx | Size |\r\n|------|---------|------|------------|-----------|-------|------|\r\n| :heavy_check_mark: | :heavy_check_mark: | :x: | :heavy_check_mark: | :warning: | :warning: | 192 MB |\r\n\r\n## Evaluation\r\n\r\nAzul is a fairly well known player in the Java sphere, having provided their Zulu JDK for quite a long time now. Their JDK differs substantially from the OpenJDK (e.g. they have their own GC offering), however I included it here because they recently started offering a true distroless variant, containing literally only the binaries necessary to run Java programs. Here are the contents of the image:\r\n\r\n```\r\n─────── ┃ ● Current Layer Contents ┣━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\r\nLayer   Permission     UID:GID       Size  Filetree\r\n   0    drwxr-xr-x         0:0     3.5 MB  ├── lib\r\n   1    drwxr-xr-x         0:0     3.5 MB  │   └── x86_64-linux-gnu\r\n   2    -rw-r--r--         0:0     2.2 MB  │       ├── libc.so.6\r\n   3    -rw-r--r--         0:0      14 kB  │       ├── libdl.so.2\r\n        -rw-r--r--         0:0     941 kB  │       ├── libm.so.6\r\n        -rw-r--r--         0:0      14 kB  │       ├── libnss_dns.so.2\r\n        -rw-r--r--         0:0      14 kB  │       ├── libnss_files.so.2\r\n        -rw-r--r--         0:0      21 kB  │       ├── libpthread.so.0\r\n        -rw-r--r--         0:0      69 kB  │       ├── libresolv.so.2\r\n        -rw-r--r--         0:0      15 kB  │       ├── librt.so.1\r\n        -rw-r--r--         0:0     109 kB  │       ├── libz.so.1\r\n        -rw-r--r--         0:0     109 kB  │       └── libz.so.1.2.11\r\n        drwxr-xr-x         0:0     241 kB  ├── lib64\r\n        -rwxr-xr-x         0:0     241 kB  │   └── ld-linux-x86-64.so.2\r\n        drwxr-xr-x         0:0        0 B  ├── tmp\r\n        drwxr-xr-x         0:0     190 MB  └── usr\r\n        drwxr-xr-x         0:0     190 MB      └── lib\r\n        drwxr-xr-x         0:0     190 MB          └── jvm\r\n        drwxr-xr-x         0:0     190 MB              └─⊕ zulu17\r\n```\r\n\r\nUnsurprisingly, running scans with both Snyk and Trivy here returned no vulnerabilities. This image contains no package manager, no extraneous utilities, it is the most barebones JVM image I can think of. This means a steep learning curve when it comes to debugging, but the same techniques that apply to all distroless images apply here as well.\r\n\r\nIt seems this is distributed only for AMD64 as well.\r\n\r\n# Base Distro\r\n\r\nOne final alternative is selecting our own preferred base distro as an image - whether Ubuntu, Alpine, Alpaquita, or what have you - and deploying a pre-built OpenJDK (e.g. Eclipse Temurin) on it. As evidenced by the Dockerfiles of the Eclipse Temurin project, this is not particularly difficult. The one caveat is hooking our dependency updates to flag OpenJDK updates in order to rebuild our image.\n npepinpe: # Shell-less/utility-less images\r\n\r\nMany small images still container a basic set of utilities, typically something like BusyBox. The distroless variants, however, don't, and this is often advertised as an advantage from a security point of view.\r\n\r\nSome of these utilities rely on things like `openssl`, `libcrypto`, `glibc`, etc., which may have open CVEs (`openssl` being a usual culprit). So removing them and having only your application may indeed reduce the CVE incidence rate.\r\n\r\nAdditionally, some utilities may be vector of attacks in themselves. As a simple example, take [nc](https://en.wikipedia.org/wiki/Netcat), which is packaged with BusyBox. It allows you to open TCP/UDP tunnels, scan ports, send data, and many other things. Now take assume an attacker gains access to a running container with `nc`. They can possibly download some malicious payload via `nc`, and execute it. If there was no shell, no utility, etc., they would be hard pressed to do much.\r\n\r\nOf course, with our Java based image, the point is kind of lost; attackers have access to a complete JVM, which gives them an incredible sandbox to do whatever they want if they have access to the container. But for SCRATCH images which only contain a single binary, then there is some benefit to dropping all the extra stuff.\n npepinpe: ## Zeebe Examples\r\n\r\nHere's a set of sample Zeebe Docker images based on the above base images.\r\n\r\n### Zeebe Alpine\r\n\r\nI wrote two Dockerfiles which results in a Zeebe image based on Alpine. One is essentially based on Alpine and installs OpenJDK via `apk`. The other is the same, but installs the Eclipse Temurin pre-built binary (so much like the `eclipse-temurin` image).\r\n\r\nThere reason I did this and not use the `eclipse-temurin` Alpine image directly is because it's not based on the latest Alpine, and is based on one version which has several DNS issues.\r\n\r\nPre-built image: gcr.io/zeebe-io/zeebe:alpine\r\n<details><summary>Zeebe Alpine OpenJDK</summary>\r\n\r\n```Dockerfile\r\n# syntax=docker/dockerfile:1.4\r\n# This Dockerfile requires BuildKit to be enabled, by setting the environment variable\r\n# DOCKER_BUILDKIT=1\r\n# see https://docs.docker.com/build/buildkit/#getting-started\r\nARG BASE_IMAGE=\"alpine:3.18.0\"\r\nARG BASE_DIGEST_AMD64=\"sha256:c0669ef34cdc14332c0f1ab0c2c01acb91d96014b172f1a76f3a39e63d1f0bda\"\r\nARG BASE_DIGEST_ARM64=\"sha256:30e6d35703c578ee703230b9dc87ada2ba958c1928615ac8a674fcbbcbb0f281\"\r\n\r\n# set to \"build\" to build zeebe from scratch instead of using a distball\r\nARG DIST=\"distball\"\r\n\r\n### Extract zeebe from distball ###\r\nFROM ${BASE_IMAGE} as distball\r\nWORKDIR /zeebe\r\nARG DISTBALL=\"dist/target/camunda-zeebe-*.tar.gz\"\r\nCOPY --link ${DISTBALL} zeebe.tar.gz\r\nRUN mkdir camunda-zeebe && tar xfvz zeebe.tar.gz --strip 1 -C camunda-zeebe\r\n\r\n### Image containing the zeebe distribution ###\r\n# hadolint ignore=DL3006\r\nFROM ${DIST} as dist\r\n\r\n### AMD64 base image ###\r\n# BASE_DIGEST_AMD64 is defined at the top of the Dockerfile\r\n# hadolint ignore=DL3006\r\nFROM ${BASE_IMAGE}@${BASE_DIGEST_AMD64} as base-amd64\r\nARG BASE_DIGEST_AMD64\r\nARG BASE_DIGEST=\"${BASE_DIGEST_AMD64}\"\r\n\r\n### ARM64 base image ##\r\n# BASE_DIGEST_ARM64 is defined at the top of the Dockerfile\r\n# hadolint ignore=DL3006\r\nFROM ${BASE_IMAGE}@${BASE_DIGEST_ARM64} as base-arm64\r\nARG BASE_DIGEST_ARM64\r\nARG BASE_DIGEST=\"${BASE_DIGEST_ARM64}\"\r\n\r\n### Application Image ###\r\n# TARGETARCH is provided by buildkit\r\n# https://docs.docker.com/engine/reference/builder/#automatic-platform-args-in-the-global-scope\r\n# hadolint ignore=DL3006\r\nFROM base-${TARGETARCH} as app\r\n# leave unset to use the default value at the top of the file\r\nARG BASE_IMAGE\r\nARG BASE_DIGEST\r\nARG VERSION=\"\"\r\nARG DATE=\"\"\r\nARG REVISION=\"\"\r\n\r\n# OCI labels: https://github.com/opencontainers/image-spec/blob/main/annotations.md\r\nLABEL org.opencontainers.image.base.digest=\"${BASE_DIGEST}\"\r\nLABEL org.opencontainers.image.base.name=\"docker.io/library/${BASE_IMAGE}\"\r\nLABEL org.opencontainers.image.created=\"${DATE}\"\r\nLABEL org.opencontainers.image.authors=\"zeebe@camunda.com\"\r\nLABEL org.opencontainers.image.url=\"https://zeebe.io\"\r\nLABEL org.opencontainers.image.documentation=\"https://docs.camunda.io/docs/self-managed/zeebe-deployment/\"\r\nLABEL org.opencontainers.image.source=\"https://github.com/camunda/zeebe\"\r\nLABEL org.opencontainers.image.version=\"${VERSION}\"\r\n# According to https://github.com/opencontainers/image-spec/blob/main/annotations.md#pre-defined-annotation-keys\r\n# and given we set the base.name and base.digest, we reference the manifest of the base image here\r\nLABEL org.opencontainers.image.ref.name=\"${BASE_IMAGE}\"\r\nLABEL org.opencontainers.image.revision=\"${REVISION}\"\r\nLABEL org.opencontainers.image.vendor=\"Camunda Services GmbH\"\r\nLABEL org.opencontainers.image.licenses=\"(Apache-2.0 AND LicenseRef-Zeebe-Community-1.1)\"\r\nLABEL org.opencontainers.image.title=\"Zeebe\"\r\nLABEL org.opencontainers.image.description=\"Workflow engine for microservice orchestration\"\r\n\r\n# OpenShift labels: https://docs.openshift.com/container-platform/4.10/openshift_images/create-images.html#defining-image-metadata\r\nLABEL io.openshift.tags=\"bpmn,orchestration,workflow\"\r\nLABEL io.k8s.description=\"Workflow engine for microservice orchestration\"\r\nLABEL io.openshift.non-scalable=\"false\"\r\nLABEL io.openshift.min-memory=\"512Mi\"\r\nLABEL io.openshift.min-cpu=\"1\"\r\n\r\nENV ZB_HOME=/usr/local/zeebe \\\r\n    ZEEBE_BROKER_GATEWAY_NETWORK_HOST=0.0.0.0 \\\r\n    ZEEBE_STANDALONE_GATEWAY=false \\\r\n    ZEEBE_RESTORE=false\r\nENV PATH \"${ZB_HOME}/bin:${PATH}\"\r\n# Disable RocksDB runtime check for musl, which launches `ldd` as a shell process\r\nENV ROCKSDB_MUSL_LIBC=true\r\n\r\nWORKDIR ${ZB_HOME}\r\nEXPOSE 26500 26501 26502\r\nVOLUME /tmp\r\nVOLUME ${ZB_HOME}/data\r\nVOLUME ${ZB_HOME}/logs\r\n\r\nWORKDIR /zeebe\r\nRUN addgroup -g 1000 zeebe && \\\r\n    adduser -u 1000 zeebe --system --ingroup zeebe && \\\r\n    chmod g=u /etc/passwd && \\\r\n    # These directories are to be mounted by users, eagerly creating them and setting ownership\r\n    # helps to avoid potential permission issues due to default volume ownership.\r\n    mkdir ${ZB_HOME}/data && \\\r\n    mkdir ${ZB_HOME}/logs && \\\r\n    chown -R 1000:0 ${ZB_HOME} && \\\r\n    chmod -R 0775 ${ZB_HOME}\r\n\r\nRUN --mount=type=cache,target=/var/cache/apk,id=apk-musl,sharing=locked \\\r\n    ln -s /var/cache/apk /etc/apk/cache && \\\r\n    apk add tini libstdc++ libgcc musl-dev openjdk17-jre-headless\r\n\r\nCOPY --link --chown=1000:0 docker/utils/startup.sh /usr/local/bin/startup.sh\r\nCOPY --from=dist --chown=1000:0 /zeebe/camunda-zeebe ${ZB_HOME}\r\n\r\nENTRYPOINT [\"tini\", \"--\", \"/usr/local/bin/startup.sh\"]\r\n```\r\n\r\n</details>\r\n\r\nPre-built image: gcr.io/zeebe-io/zeebe:temurin-alpine-libressl\r\n\r\n<details><summary>Zeebe Alpine Eclipse Temurin</summary>\r\n\r\n```Dockerfile\r\n# syntax=docker/dockerfile:1.5-labs\r\n# This Dockerfile requires BuildKit to be enabled, by setting the environment variable\r\n# DOCKER_BUILDKIT=1\r\n# see https://docs.docker.com/build/buildkit/#getting-started\r\nARG BASE_IMAGE=\"alpine:3.18.0\"\r\nARG BASE_DIGEST_AMD64=\"sha256:c0669ef34cdc14332c0f1ab0c2c01acb91d96014b172f1a76f3a39e63d1f0bda\"\r\nARG BASE_DIGEST_ARM64=\"sha256:30e6d35703c578ee703230b9dc87ada2ba958c1928615ac8a674fcbbcbb0f281\"\r\n\r\n# set to \"build\" to build zeebe from scratch instead of using a distball\r\nARG DIST=\"distball\"\r\n\r\n### Extract zeebe from distball ###\r\nFROM ${BASE_IMAGE} as distball\r\nWORKDIR /zeebe\r\nARG DISTBALL=\"dist/target/camunda-zeebe-*.tar.gz\"\r\nCOPY --link ${DISTBALL} zeebe.tar.gz\r\nRUN mkdir camunda-zeebe && tar xfvz zeebe.tar.gz --strip 1 -C camunda-zeebe\r\n\r\n### Image containing the zeebe distribution ###\r\n# hadolint ignore=DL3006\r\nFROM ${DIST} as dist\r\n\r\n### AMD64 base image ###\r\n# BASE_DIGEST_AMD64 is defined at the top of the Dockerfile\r\n# hadolint ignore=DL3006\r\nFROM ${BASE_IMAGE}@${BASE_DIGEST_AMD64} as base-amd64\r\nARG BASE_DIGEST_AMD64\r\nARG BASE_DIGEST=\"${BASE_DIGEST_AMD64}\"\r\n\r\n### ARM64 base image ##\r\n# BASE_DIGEST_ARM64 is defined at the top of the Dockerfile\r\n# hadolint ignore=DL3006\r\nFROM ${BASE_IMAGE}@${BASE_DIGEST_ARM64} as base-arm64\r\nARG BASE_DIGEST_ARM64\r\nARG BASE_DIGEST=\"${BASE_DIGEST_ARM64}\"\r\n\r\n### Java Alpine base image\r\n# TARGETARCH is provided by buildkit\r\nFROM base-${TARGETARCH} as java\r\n\r\nARG JAVA_URL=\"https://github.com/adoptium/temurin17-binaries/releases/download/jdk-17.0.7%2B7/OpenJDK17U-jre_x64_alpine-linux_hotspot_17.0.7_7.tar.gz\"\r\nARG SUM=\"sha256:711f837bacf8222dee9e8cd7f39941a4a0acf869243f03e6038ca3ba189f66ca\"\r\n\r\n# Default to UTF-8 file.encoding\r\nENV LANG='en_US.UTF-8' LANGUAGE='en_US:en' LC_ALL='en_US.UTF-8'\r\n\r\n# Setup JAVA_HOME and binaries in the path\r\nENV JAVA_HOME /opt/java/openjdk\r\nENV JAVA_VERSION jdk-17.0.7+7\r\nENV PATH $JAVA_HOME/bin:$PATH\r\n\r\n# Install dependencies, but skip installing fonts and so on as we don't need any front-end\r\n# Additionally, replace OpenSSL with LibreSSL to avoid the vulnerabilities plaguing OpenSSL\r\nRUN --mount=type=cache,target=/var/cache/apk,id=apk-musl,sharing=locked \\\r\n    ln -s /var/cache/apk /etc/apk/cache && \\\r\n    apk del openssl openssl-dev && apk add libressl libressl-dev && \\\r\n    apk add musl-locales musl-locales-lang tzdata zlib\r\n\r\n# Download and install the Java distribution\r\nADD --checksum=${SUM} ${JAVA_URL} /tmp/openjdk.tar.gz\r\nRUN set -eux; \\\r\n\t  mkdir -p \"$JAVA_HOME\"; \\\r\n\t  tar --extract \\\r\n\t      --file /tmp/openjdk.tar.gz \\\r\n\t      --directory \"$JAVA_HOME\" \\\r\n\t      --strip-components 1 \\\r\n\t      --no-same-owner && \\\r\n    rm -f /tmp/openjdk.tar.gz ${JAVA_HOME}/lib/src.zip;\r\n\r\n### Application Image ###\r\n# https://docs.docker.com/engine/reference/builder/#automatic-platform-args-in-the-global-scope\r\n# hadolint ignore=DL3006\r\nFROM java as app\r\n\r\n# leave unset to use the default value at the top of the file\r\nARG BASE_IMAGE\r\nARG BASE_DIGEST\r\nARG VERSION=\"\"\r\nARG DATE=\"\"\r\nARG REVISION=\"\"\r\n\r\n# OCI labels: https://github.com/opencontainers/image-spec/blob/main/annotations.md\r\nLABEL org.opencontainers.image.base.digest=\"${BASE_DIGEST}\"\r\nLABEL org.opencontainers.image.base.name=\"docker.io/library/${BASE_IMAGE}\"\r\nLABEL org.opencontainers.image.created=\"${DATE}\"\r\nLABEL org.opencontainers.image.authors=\"zeebe@camunda.com\"\r\nLABEL org.opencontainers.image.url=\"https://zeebe.io\"\r\nLABEL org.opencontainers.image.documentation=\"https://docs.camunda.io/docs/self-managed/zeebe-deployment/\"\r\nLABEL org.opencontainers.image.source=\"https://github.com/camunda/zeebe\"\r\nLABEL org.opencontainers.image.version=\"${VERSION}\"\r\n# According to https://github.com/opencontainers/image-spec/blob/main/annotations.md#pre-defined-annotation-keys\r\n# and given we set the base.name and base.digest, we reference the manifest of the base image here\r\nLABEL org.opencontainers.image.ref.name=\"${BASE_IMAGE}\"\r\nLABEL org.opencontainers.image.revision=\"${REVISION}\"\r\nLABEL org.opencontainers.image.vendor=\"Camunda Services GmbH\"\r\nLABEL org.opencontainers.image.licenses=\"(Apache-2.0 AND LicenseRef-Zeebe-Community-1.1)\"\r\nLABEL org.opencontainers.image.title=\"Zeebe\"\r\nLABEL org.opencontainers.image.description=\"Workflow engine for microservice orchestration\"\r\n\r\n# OpenShift labels: https://docs.openshift.com/container-platform/4.10/openshift_images/create-images.html#defining-image-metadata\r\nLABEL io.openshift.tags=\"bpmn,orchestration,workflow\"\r\nLABEL io.k8s.description=\"Workflow engine for microservice orchestration\"\r\nLABEL io.openshift.non-scalable=\"false\"\r\nLABEL io.openshift.min-memory=\"512Mi\"\r\nLABEL io.openshift.min-cpu=\"1\"\r\n\r\nENV ZB_HOME=/usr/local/zeebe \\\r\n    ZEEBE_BROKER_GATEWAY_NETWORK_HOST=0.0.0.0 \\\r\n    ZEEBE_STANDALONE_GATEWAY=false \\\r\n    ZEEBE_RESTORE=false\r\nENV PATH \"${ZB_HOME}/bin:${PATH}\"\r\n# Disable RocksDB runtime check for musl, which launches `ldd` as a shell process\r\nENV ROCKSDB_MUSL_LIBC=true\r\n\r\nWORKDIR ${ZB_HOME}\r\nEXPOSE 26500 26501 26502\r\nVOLUME /tmp\r\nVOLUME ${ZB_HOME}/data\r\nVOLUME ${ZB_HOME}/logs\r\n\r\nWORKDIR /zeebe\r\nRUN addgroup -g 1000 zeebe && \\\r\n    adduser -u 1000 zeebe --system --ingroup zeebe && \\\r\n    chmod g=u /etc/passwd && \\\r\n    # These directories are to be mounted by users, eagerly creating them and setting ownership\r\n    # helps to avoid potential permission issues due to default volume ownership.\r\n    mkdir ${ZB_HOME}/data && \\\r\n    mkdir ${ZB_HOME}/logs && \\\r\n    chown -R 1000:0 ${ZB_HOME} && \\\r\n    chmod -R 0775 ${ZB_HOME}\r\n\r\n# Install tini and RocksDB dependencies\r\nRUN --mount=type=cache,target=/var/cache/apk,id=apk-musl,sharing=locked \\\r\n    apk add tini libstdc++ libgcc musl-dev\r\n\r\nCOPY --link --chown=1000:0 docker/utils/startup.sh /usr/local/bin/startup.sh\r\nCOPY --from=dist --chown=1000:0 /zeebe/camunda-zeebe ${ZB_HOME}\r\n\r\nENTRYPOINT [\"tini\", \"--\", \"/usr/local/bin/startup.sh\"]\r\n```\r\n\r\n</details>\r\n\r\n> **Note**\r\n> With the latter one, one of the build stage is just building an own Alpine/Temurin base image, which also replaces OpenSSL with LibreSSL (the OpenBSD variant), hopefully further reducing CVEs\r\n> The idea here is that we can better keep up with the latest Alpine, but would need Renovate to figure out that there are Java updates :+1: \r\n\r\n### Zeebe Liberica\r\n\r\nI also built two Liberica variants, one based on musl and one on glibc (remember the Liberica image is based on Alpaquita, which is an Alpine variant, supporting both glibc and musl).\r\n\r\nPre-built image: gcr.io/zeebe-io/zeebe:liberica-glibc\r\n\r\n<details><summary>Zeebe Liberica GLIBC</summary>\r\n\r\n```Dockerfile\r\n# syntax=docker/dockerfile:1.4\r\n# This Dockerfile requires BuildKit to be enabled, by setting the environment variable\r\n# DOCKER_BUILDKIT=1\r\n# see https://docs.docker.com/build/buildkit/#getting-started\r\nARG BASE_IMAGE=\"bellsoft/liberica-runtime-container:jre-17.0.7_7-slim-glibc\"\r\nARG BASE_DIGEST_AMD64=\"sha256:e31bf84e791405ceb7336647896724b38649557e7ee485568cd151f388bbaf6b\"\r\n\r\n# set to \"build\" to build zeebe from scratch instead of using a distball\r\nARG DIST=\"distball\"\r\n\r\n### Init image containing tini and the startup script ###\r\nFROM bellsoft/alpaquita-linux-base:stream-glibc as init\r\nWORKDIR /zeebe\r\nRUN --mount=type=cache,target=/var/cache/apk,sharing=locked \\\r\n    ln -s /var/cache/apk /etc/apk/cache && \\\r\n    apk add tini libstdc++ libgcc && cp /sbin/tini .\r\nCOPY --link --chown=1000:0 docker/utils/startup.sh .\r\n\r\n### Extract zeebe from distball ###\r\nFROM bellsoft/alpaquita-linux-base:stream-glibc as distball\r\nWORKDIR /zeebe\r\nARG DISTBALL=\"dist/target/camunda-zeebe-*.tar.gz\"\r\nCOPY --link ${DISTBALL} zeebe.tar.gz\r\nRUN mkdir camunda-zeebe && tar xfvz zeebe.tar.gz --strip 1 -C camunda-zeebe\r\n\r\n### Image containing the zeebe distribution ###\r\n# hadolint ignore=DL3006\r\nFROM ${DIST} as dist\r\n\r\n### AMD64 base image ###\r\n# BASE_DIGEST_AMD64 is defined at the top of the Dockerfile\r\n# hadolint ignore=DL3006\r\nFROM ${BASE_IMAGE}@${BASE_DIGEST_AMD64} as base-amd64\r\nARG BASE_DIGEST_AMD64\r\nARG BASE_DIGEST=\"${BASE_DIGEST_AMD64}\"\r\n\r\n### Application Image ###\r\n# TARGETARCH is provided by buildkit\r\n# https://docs.docker.com/engine/reference/builder/#automatic-platform-args-in-the-global-scope\r\n# hadolint ignore=DL3006\r\nFROM base-${TARGETARCH} as app\r\n# leave unset to use the default value at the top of the file\r\nARG BASE_IMAGE\r\nARG BASE_DIGEST\r\nARG VERSION=\"\"\r\nARG DATE=\"\"\r\nARG REVISION=\"\"\r\n\r\n# OCI labels: https://github.com/opencontainers/image-spec/blob/main/annotations.md\r\nLABEL org.opencontainers.image.base.digest=\"${BASE_DIGEST}\"\r\nLABEL org.opencontainers.image.base.name=\"docker.io/library/${BASE_IMAGE}\"\r\nLABEL org.opencontainers.image.created=\"${DATE}\"\r\nLABEL org.opencontainers.image.authors=\"zeebe@camunda.com\"\r\nLABEL org.opencontainers.image.url=\"https://zeebe.io\"\r\nLABEL org.opencontainers.image.documentation=\"https://docs.camunda.io/docs/self-managed/zeebe-deployment/\"\r\nLABEL org.opencontainers.image.source=\"https://github.com/camunda/zeebe\"\r\nLABEL org.opencontainers.image.version=\"${VERSION}\"\r\n# According to https://github.com/opencontainers/image-spec/blob/main/annotations.md#pre-defined-annotation-keys\r\n# and given we set the base.name and base.digest, we reference the manifest of the base image here\r\nLABEL org.opencontainers.image.ref.name=\"${BASE_IMAGE}\"\r\nLABEL org.opencontainers.image.revision=\"${REVISION}\"\r\nLABEL org.opencontainers.image.vendor=\"Camunda Services GmbH\"\r\nLABEL org.opencontainers.image.licenses=\"(Apache-2.0 AND LicenseRef-Zeebe-Community-1.1)\"\r\nLABEL org.opencontainers.image.title=\"Zeebe\"\r\nLABEL org.opencontainers.image.description=\"Workflow engine for microservice orchestration\"\r\n\r\n# OpenShift labels: https://docs.openshift.com/container-platform/4.10/openshift_images/create-images.html#defining-image-metadata\r\nLABEL io.openshift.tags=\"bpmn,orchestration,workflow\"\r\nLABEL io.k8s.description=\"Workflow engine for microservice orchestration\"\r\nLABEL io.openshift.non-scalable=\"false\"\r\nLABEL io.openshift.min-memory=\"512Mi\"\r\nLABEL io.openshift.min-cpu=\"1\"\r\n\r\nENV ZB_HOME=/usr/local/zeebe \\\r\n    ZEEBE_BROKER_GATEWAY_NETWORK_HOST=0.0.0.0 \\\r\n    ZEEBE_STANDALONE_GATEWAY=false \\\r\n    ZEEBE_RESTORE=false\r\nENV PATH \"${ZB_HOME}/bin:${PATH}\"\r\n# Disable RocksDB runtime check for musl, which launches `ldd` as a shell process\r\nENV ROCKSDB_MUSL_LIBC=false\r\n\r\nWORKDIR ${ZB_HOME}\r\nEXPOSE 26500 26501 26502\r\nVOLUME /tmp\r\nVOLUME ${ZB_HOME}/data\r\nVOLUME ${ZB_HOME}/logs\r\n\r\nRUN addgroup -g 1000 zeebe && \\\r\n    adduser -u 1000 zeebe --system --ingroup zeebe && \\\r\n    chmod g=u /etc/passwd && \\\r\n    # These directories are to be mounted by users, eagerly creating them and setting ownership\r\n    # helps to avoid potential permission issues due to default volume ownership.\r\n    mkdir ${ZB_HOME}/data && \\\r\n    mkdir ${ZB_HOME}/logs && \\\r\n    chown -R 1000:0 ${ZB_HOME} && \\\r\n    chmod -R 0775 ${ZB_HOME}\r\n\r\nCOPY --from=init /lib/libstdc++.so.6.0.30 /lib/libstdc++.so.6.0.30\r\nCOPY --from=init /lib/libstdc++.so.6 /lib/libstdc++.so.6\r\nCOPY --from=init /lib/libgcc_s.so.1 /lib/libgcc_s.so.1\r\nCOPY --from=init --chown=1000:0 /zeebe/tini ${ZB_HOME}/bin/\r\nCOPY --from=init --chown=1000:0 /zeebe/startup.sh /usr/local/bin/startup.sh\r\nCOPY --from=dist --chown=1000:0 /zeebe/camunda-zeebe ${ZB_HOME}\r\n\r\nENTRYPOINT [\"tini\", \"--\", \"/usr/local/bin/startup.sh\"]\r\n```\r\n\r\n</details>\r\n\r\nPre-built image: gcr.io/zeebe-io/zeebe:liberica-musl\r\n\r\n<details><summary>Zeebe Liberica MUSL</summary>\r\n\r\n```Dockerfile\r\n# syntax=docker/dockerfile:1.4\r\n# This Dockerfile requires BuildKit to be enabled, by setting the environment variable\r\n# DOCKER_BUILDKIT=1\r\n# see https://docs.docker.com/build/buildkit/#getting-started\r\nARG BASE_IMAGE=\"bellsoft/liberica-runtime-container:jre-17.0.7_7-slim\"\r\nARG BASE_DIGEST_AMD64=\"sha256:e31bf84e791405ceb7336647896724b38649557e7ee485568cd151f388bbaf6b\"\r\n\r\n# set to \"build\" to build zeebe from scratch instead of using a distball\r\nARG DIST=\"distball\"\r\n\r\n### Init image containing tini and the startup script ###\r\nFROM bellsoft/alpaquita-linux-base:stream as init\r\nWORKDIR /zeebe\r\nRUN --mount=type=cache,target=/var/cache/apk,id=apk-musl,sharing=locked \\\r\n    ln -s /var/cache/apk /etc/apk/cache && \\\r\n    apk add tini libstdc++ libgcc musl-dev\r\nCOPY --link --chown=1000:0 docker/utils/startup.sh .\r\n\r\n### Extract zeebe from distball ###\r\nFROM bellsoft/alpaquita-linux-base:stream as distball\r\nWORKDIR /zeebe\r\nARG DISTBALL=\"dist/target/camunda-zeebe-*.tar.gz\"\r\nCOPY --link ${DISTBALL} zeebe.tar.gz\r\nRUN mkdir camunda-zeebe && tar xfvz zeebe.tar.gz --strip 1 -C camunda-zeebe\r\n\r\n### Image containing the zeebe distribution ###\r\n# hadolint ignore=DL3006\r\nFROM ${DIST} as dist\r\n\r\n### AMD64 base image ###\r\n# BASE_DIGEST_AMD64 is defined at the top of the Dockerfile\r\n# hadolint ignore=DL3006\r\nFROM ${BASE_IMAGE}@${BASE_DIGEST_AMD64} as base-amd64\r\nARG BASE_DIGEST_AMD64\r\nARG BASE_DIGEST=\"${BASE_DIGEST_AMD64}\"\r\n\r\n### Application Image ###\r\n# TARGETARCH is provided by buildkit\r\n# https://docs.docker.com/engine/reference/builder/#automatic-platform-args-in-the-global-scope\r\n# hadolint ignore=DL3006\r\nFROM base-${TARGETARCH} as app\r\n# leave unset to use the default value at the top of the file\r\nARG BASE_IMAGE\r\nARG BASE_DIGEST\r\nARG VERSION=\"\"\r\nARG DATE=\"\"\r\nARG REVISION=\"\"\r\n\r\n# OCI labels: https://github.com/opencontainers/image-spec/blob/main/annotations.md\r\nLABEL org.opencontainers.image.base.digest=\"${BASE_DIGEST}\"\r\nLABEL org.opencontainers.image.base.name=\"docker.io/library/${BASE_IMAGE}\"\r\nLABEL org.opencontainers.image.created=\"${DATE}\"\r\nLABEL org.opencontainers.image.authors=\"zeebe@camunda.com\"\r\nLABEL org.opencontainers.image.url=\"https://zeebe.io\"\r\nLABEL org.opencontainers.image.documentation=\"https://docs.camunda.io/docs/self-managed/zeebe-deployment/\"\r\nLABEL org.opencontainers.image.source=\"https://github.com/camunda/zeebe\"\r\nLABEL org.opencontainers.image.version=\"${VERSION}\"\r\n# According to https://github.com/opencontainers/image-spec/blob/main/annotations.md#pre-defined-annotation-keys\r\n# and given we set the base.name and base.digest, we reference the manifest of the base image here\r\nLABEL org.opencontainers.image.ref.name=\"${BASE_IMAGE}\"\r\nLABEL org.opencontainers.image.revision=\"${REVISION}\"\r\nLABEL org.opencontainers.image.vendor=\"Camunda Services GmbH\"\r\nLABEL org.opencontainers.image.licenses=\"(Apache-2.0 AND LicenseRef-Zeebe-Community-1.1)\"\r\nLABEL org.opencontainers.image.title=\"Zeebe\"\r\nLABEL org.opencontainers.image.description=\"Workflow engine for microservice orchestration\"\r\n\r\n# OpenShift labels: https://docs.openshift.com/container-platform/4.10/openshift_images/create-images.html#defining-image-metadata\r\nLABEL io.openshift.tags=\"bpmn,orchestration,workflow\"\r\nLABEL io.k8s.description=\"Workflow engine for microservice orchestration\"\r\nLABEL io.openshift.non-scalable=\"false\"\r\nLABEL io.openshift.min-memory=\"512Mi\"\r\nLABEL io.openshift.min-cpu=\"1\"\r\n\r\nENV ZB_HOME=/usr/local/zeebe \\\r\n    ZEEBE_BROKER_GATEWAY_NETWORK_HOST=0.0.0.0 \\\r\n    ZEEBE_STANDALONE_GATEWAY=false \\\r\n    ZEEBE_RESTORE=false\r\nENV PATH \"${ZB_HOME}/bin:${PATH}\"\r\n# Disable RocksDB runtime check for musl, which launches `ldd` as a shell process\r\nENV ROCKSDB_MUSL_LIBC=true\r\n\r\nWORKDIR ${ZB_HOME}\r\nEXPOSE 26500 26501 26502\r\nVOLUME /tmp\r\nVOLUME ${ZB_HOME}/data\r\nVOLUME ${ZB_HOME}/logs\r\n\r\nRUN addgroup -g 1000 zeebe && \\\r\n    adduser -u 1000 zeebe --system --ingroup zeebe && \\\r\n    chmod g=u /etc/passwd && \\\r\n    # These directories are to be mounted by users, eagerly creating them and setting ownership\r\n    # helps to avoid potential permission issues due to default volume ownership.\r\n    mkdir ${ZB_HOME}/data && \\\r\n    mkdir ${ZB_HOME}/logs && \\\r\n    chown -R 1000:0 ${ZB_HOME} && \\\r\n    chmod -R 0775 ${ZB_HOME}\r\n\r\nCOPY --from=init /lib/libc.musl-x86_64.so.1 /lib/libc.musl-x86_64.so.1\r\nCOPY --from=init /lib/libstdc++.so.6.0.30 /lib/libstdc++.so.6.0.30\r\nCOPY --from=init /lib/libstdc++.so.6 /lib/libstdc++.so.6\r\nCOPY --from=init /lib/libgcc_s.so.1 /lib/libgcc_s.so.1\r\nCOPY --from=init --chown=1000:0 /sbin/tini ${ZB_HOME}/bin/\r\nCOPY --from=init --chown=1000:0 /zeebe/startup.sh /usr/local/bin/startup.sh\r\nCOPY --from=dist --chown=1000:0 /zeebe/camunda-zeebe ${ZB_HOME}\r\n\r\nENTRYPOINT [\"tini\", \"--\", \"/usr/local/bin/startup.sh\"]\r\n```\r\n\r\n</details>\n megglos: ZDP-Triage:\n- would be good to get out of the way sooner\n- ",
    "title": "Reduce container CVE maintenance effort"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12955",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "<!--\r\nIn case you have questions about our software we encourage everyone to participate in our community via the\r\n- Camunda Platform community forum https://forum.camunda.io/ or \r\n- Slack https://camunda-cloud.slack.com/ (For invite: https://camunda-slack-invite.herokuapp.com/)\r\n\r\nThere you can exchange ideas with other Zeebe and Camunda Platform 8 users, as well as the product developers, and use the search to find answer to similar questions.\r\n\r\nThis issue template is used by the Zeebe engineers to create general tasks.\r\n-->\r\n\r\n**Description**\r\n\r\n**In progress** \r\n\r\n|                      | [SST Partitioning](https://github.com/camunda/zeebe/pull/12483) | [Column family separation ](https://github.com/camunda/zeebe/pull/12675)|\r\n|------------------|------------------------|----------------------------------------|\r\n| Description | Enabling the experimental feature flag, to separate the SST creation based on a column family prefix. |Separate some of our high-frequently used virtual column families into physical ones. |\r\n| Impact | The SST files in RocksDB are split based on virtual column family prefixes, which might cause to create more SST files in the end. In general, the configuration is for compaction, which might compact SST files earlier ( or more often). We have seen that even with a large state in RocksDB we can iterate/seek/access data in almost similar performance as with small data sets. | Separating column families, would allow to separate hot data from colder ones to not influence too much different data sets. Allows to increase performance in seeking and accessing certain data. Since we use more physically separated column families this would mean more memory and space for RocksDB is needed. It would allow configuring column families differently, which might help in performance but as consequence also increases complexity. |\r\n| Upgradability | Upgrading should have no major impact. It was tested more in-depth in two chaos days. [[1](https://zeebe-io.github.io/zeebe-chaos/2023/05/15/SST-Partitioning-toggle), [2](https://zeebe-io.github.io/zeebe-chaos/2023/05/19/Continuing-SST-Partitioning-toggle)] and it was confirmed [here](https://groups.google.com/g/rocksdb/c/Ys-yZIznZwU). | Splitting the column families, is no easy-pick and involves some engineering effort. We need to run certain migration steps on startup in order to migrate data from virtual column families in to new physical column families. |\r\n| Risk |  The configuration is marked as experimental in RocksDB. [In a related Google group, it was described as stable and is used by other products](https://groups.google.com/g/rocksdb/c/Ys-yZIznZwU).  | There could be new bugs introduced with separating of column families and migrating data from one to multiple column families. There might be a loss of data (depending on bugs). Migration might be only one way possible, otherwise would increase complexity.  |\r\n| Implementation complexity | **Small.** We provided just a configuration in order to enable this feature in RocksDB. | **Mid**. The feature itself is not complicated ([see PR](https://github.com/camunda/zeebe/pull/12675/files)), but based on the benchmark it is also not 100% clear whether this works without issues. The migration procedure might be a bit more problematic, especially when we want to support going back to one CF. Furthermore, testing and how to configure certain column families, default values etc. can be challenging. | \r\n| **Benchmarks** |  | |\r\n| [JMH](https://github.com/camunda/zeebe/issues/12241) | 656.639 ± 91.394  ops/s  | 681.872  ± 77.924  ops/s |\r\n| Normal | ![general-sst-normal-bench](https://github.com/camunda/zeebe/assets/2758593/41b3127e-3493-498d-a61d-3795effca5bf) Worked without major issues. | ![general-cf-normal-bench](https://github.com/camunda/zeebe/assets/2758593/3764293c-c8d6-46da-ad92-4213e0da8ecd) No major issues, but the changes seem to have some issues with disk usage. |\r\n| Max throughput | ![general-sst-max-bench](https://github.com/camunda/zeebe/assets/2758593/e8f7295f-b427-48fc-8269-cb94a14aa3de) The maximum throughput was similar to a base benchmark. | ![general-cf-max-bench](https://github.com/camunda/zeebe/assets/2758593/7694ffd0-ada8-444f-9280-f158718fbb79) The solution suffered at the begin on degraded performance, but later watched a bit up. |\r\n| Large state |![general-sst-large-bench](https://github.com/camunda/zeebe/assets/2758593/67312278-8a02-4cbb-be5a-d73bf3c68f80) Running for around four days, reaching 80 gig of Snapshot and filling 500 gig of disk. Disk size seems to be the only limit. | ![general-cf-large-bench](https://github.com/camunda/zeebe/assets/2758593/c414371e-d8ff-4a0c-b872-5cd5dabd4168) Survives longer than our base (dies normally after one hour), but is still not enough. |\r\n| Latency  |  There was no specific latency benchmark, but on normal benchmark looks similar to base.|  There was no specific latency benchmark, but on normal benchmark looks similar to base. |\r\n\r\n\r\n\r\nrelates #12033 \n\n Zelldon: Based on the results above I will conclude this comparison and mark SST partitioning as the preferred solution for now.\r\n\r\nIt gives a lot of gains, like performance improvement in a larger state (which was the goal) with smaller risks and effort to implement compared to splitting up the column families.\r\n\r\nWe can always revise the decision later, but for now, it looks like the better and simpler solution to move forward.",
    "title": "Compare SST partitioning with real Column Family split"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12859",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "Zeebe 8.3 is [supposed to](https://confluence.camunda.com/display/HAN/Camunda+8+Supported+Environments) support Elasticsearch 8.7+\r\n\r\nThe curator is no longer compatible with ES8, so we've added the ability to configure an ILM policy:\r\n- https://github.com/camunda/zeebe/pull/12147\r\n- https://github.com/camunda/zeebe/issues/12132\r\n\r\nWhat is still left over is to update the Elasticsearch client to the latest 8.7 version.\r\n\r\nThis is not updated automatically, due to our [dependabot configuration](https://github.com/camunda/zeebe/blob/main/.github/dependabot.yml#L20C1-L23), so we'll need to update it manually:\r\n- https://github.com/camunda/zeebe/pull/9308\r\n- https://github.com/camunda/zeebe/pull/11563\r\n\r\nAlso see, the last dependabot PR for this: \r\n- https://github.com/camunda/zeebe/issues/9287\n\n korthout: ZPA Triage:\n- should be easy: \n  - update the dependency\n  - make sure the CI passes (`mvn test` or `mvn verify` locally)\n  - manually test to see that we can export into ES 8.7 with it\n- should be completed before 8.3 release",
    "title": "Update Elasticsearch client to 8.7  "
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12836",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\n\nSince the merge of https://github.com/camunda/zeebe/pull/12001 zeebe depends on identity for all 8.2.0 release and later.\nAs an identity release may happen right before the zeebe release we need to add a step to the release process to make sure the identity version is the same as the zeebe version or if not update it.\n\nRight now patch level version of zeebe and identity are aligned, so if zeebe 8.2.5 is released the identity version should be 8.2.5 as well. This could be automated.\n\n\n```[tasklist]\n### Tasks\n- [ ] https://github.com/camunda/zeebe/issues/12920\n- [ ] https://github.com/zeebe-io/zeebe-engineering-processes/issues/312\n```\n\n\n\n megglos: Release process now covers checking for an existing release and or awaiting the needed release based on a Github webhook. \r\n![image](https://github.com/camunda/zeebe/assets/209518/69f40751-3da0-4b51-8ef9-44e6b143a8b3)\r\n\r\nto be tested the next days, together with @oleschoenburg \n abbasadel: ZPA Planning: \n- @megglos is there anything still needed form ZPA to close this?\n remcowesterhoud: @megglos I see all tasks here are closed. Can this issue be closed as well?\n megglos: Two releases went fine with this change, closing.",
    "title": "Release: Integrate update of identity version into the release process"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12818",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nIn case on a snapshot inconsistency the broker fails to startup and logs the IllegalArgumentException that contains the information on the index mismatch of the log and the snapshot, it would be helpful to also include the partition id.\r\n\r\n```\r\n[Broker-7-Startup] [Broker-7-zb-actors-2] WARN \r\n        io.camunda.zeebe.broker.system - Aborting startup process due to exception during step Partition Manager\r\n  java.util.concurrent.CompletionException: java.lang.IllegalStateException: Expected to find a snapshot at index >= log's first index 161591, but found snapshot 25. A previous snapshot is most likely corrupted.\r\n```\r\n\r\n<details>\r\n  <summary>Logs</summary>\r\n\r\n  ```\r\n  2023-05-21 14:44:55.018 [Broker-7-Startup] [Broker-7-zb-actors-2] WARN \r\n        io.camunda.zeebe.broker.system - Aborting startup process due to exception during step Partition Manager\r\n  java.util.concurrent.CompletionException: java.lang.IllegalStateException: Expected to find a snapshot at index >= log's first index 161591, but found snapshot 25. A previous snapshot is most likely corrupted.\r\n\t  at java.util.concurrent.CompletableFuture.encodeThrowable(Unknown Source) ~[?:?]\r\n\t  at java.util.concurrent.CompletableFuture.uniApplyNow(Unknown Source) ~[?:?]\r\n\t  at java.util.concurrent.CompletableFuture.uniApplyStage(Unknown Source) ~[?:?]\r\n\t  at java.util.concurrent.CompletableFuture.thenApply(Unknown Source) ~[?:?]\r\n\t  at io.atomix.raft.partition.RaftPartition.open(RaftPartition.java:90) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t  at io.atomix.raft.partition.RaftPartitionGroup.lambda$join$2(RaftPartitionGroup.java:174) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t  at java.util.stream.ReferencePipeline$3$1.accept(Unknown Source) ~[?:?]\r\n\t  at java.util.HashMap$KeySpliterator.forEachRemaining(Unknown Source) ~[?:?]\r\n\t  at java.util.stream.AbstractPipeline.copyInto(Unknown Source) ~[?:?]\r\n\t  at java.util.stream.AbstractPipeline.wrapAndCopyInto(Unknown Source) ~[?:?]\r\n\t  at java.util.stream.AbstractPipeline.evaluate(Unknown Source) ~[?:?]\r\n\t  at java.util.stream.AbstractPipeline.evaluateToArrayNode(Unknown Source) ~[?:?]\r\n\t  at java.util.stream.ReferencePipeline.toArray(Unknown Source) ~[?:?]\r\n\t  at io.atomix.raft.partition.RaftPartitionGroup.join(RaftPartitionGroup.java:175) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t  at io.atomix.primitive.partition.impl.DefaultPartitionService.start(DefaultPartitionService.java:63) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t  at io.camunda.zeebe.broker.partitioning.PartitionManagerImpl.start(PartitionManagerImpl.java:125) ~[zeebe-broker-8.1.9.jar:8.1.9]\r\n\t  at java.util.concurrent.CompletableFuture$AsyncSupply.run(Unknown Source) ~[?:?]\r\n\t  at java.util.concurrent.CompletableFuture$AsyncSupply.exec(Unknown Source) ~[?:?]\r\n\t  at java.util.concurrent.ForkJoinTask.doExec(Unknown Source) ~[?:?]\r\n\t  at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source) ~[?:?]\r\n\t  at java.util.concurrent.ForkJoinPool.scan(Unknown Source) ~[?:?]\r\n\t  at java.util.concurrent.ForkJoinPool.runWorker(Unknown Source) ~[?:?]\r\n\t  at java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source) ~[?:?]\r\n  Caused by: java.lang.IllegalStateException: Expected to find a snapshot at index >= log's first index 161591, but found snapshot 25. A previous snapshot is most likely corrupted.\r\n\t  at io.atomix.raft.utils.StateUtil.verifySnapshotLogConsistent(StateUtil.java:32) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t  at io.atomix.raft.impl.RaftContext.<init>(RaftContext.java:204) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t  at io.atomix.raft.impl.DefaultRaftServer$Builder.build(DefaultRaftServer.java:263) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t  at io.atomix.raft.impl.DefaultRaftServer$Builder.build(DefaultRaftServer.java:237) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t  at io.atomix.raft.partition.impl.RaftPartitionServer.buildServer(RaftPartitionServer.java:185) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t  at io.atomix.raft.partition.impl.RaftPartitionServer.initServer(RaftPartitionServer.java:151) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t  at io.atomix.raft.partition.impl.RaftPartitionServer.start(RaftPartitionServer.java:110) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t  ... 19 more\r\n  2023-05-21 14:44:55.025 [] [main] ERROR\r\n        io.camunda.zeebe.broker.system - Failed to start broker 7!\r\n  java.util.concurrent.ExecutionException: Startup failed in the following steps: [Partition Manager]. See suppressed exceptions for details.\r\n\t  at io.camunda.zeebe.scheduler.future.CompletableActorFuture.get(CompletableActorFuture.java:142) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t  at io.camunda.zeebe.scheduler.future.CompletableActorFuture.get(CompletableActorFuture.java:109) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t  at io.camunda.zeebe.scheduler.FutureUtil.join(FutureUtil.java:21) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t  at io.camunda.zeebe.scheduler.future.CompletableActorFuture.join(CompletableActorFuture.java:198) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t  at io.camunda.zeebe.broker.Broker.internalStart(Broker.java:101) ~[zeebe-broker-8.1.9.jar:8.1.9]\r\n\t  at io.camunda.zeebe.util.LogUtil.doWithMDC(LogUtil.java:23) ~[zeebe-util-8.1.9.jar:8.1.9]\r\n\t  at io.camunda.zeebe.broker.Broker.start(Broker.java:83) ~[zeebe-broker-8.1.9.jar:8.1.9]\r\n\t  at io.camunda.zeebe.broker.StandaloneBroker.run(StandaloneBroker.java:92) ~[camunda-zeebe-8.1.9.jar:8.1.9]\r\n\t  at org.springframework.boot.SpringApplication.callRunner(SpringApplication.java:771) ~[spring-boot-2.7.4.jar:2.7.4]\r\n\t  at org.springframework.boot.SpringApplication.callRunners(SpringApplication.java:755) ~[spring-boot-2.7.4.jar:2.7.4]\r\n\t  at org.springframework.boot.SpringApplication.run(SpringApplication.java:315) ~[spring-boot-2.7.4.jar:2.7.4]\r\n\t  at io.camunda.zeebe.broker.StandaloneBroker.main(StandaloneBroker.java:82) ~[camunda-zeebe-8.1.9.jar:8.1.9]\r\n  Caused by: io.camunda.zeebe.scheduler.startup.StartupProcessException: Startup failed in the following steps: [Partition Manager]. See suppressed exceptions for details.\r\n\t  at io.camunda.zeebe.scheduler.startup.StartupProcess.aggregateExceptionsSynchronized(StartupProcess.java:282) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t  at io.camunda.zeebe.scheduler.startup.StartupProcess.completeStartupFutureExceptionallySynchronized(StartupProcess.java:183) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t  at io.camunda.zeebe.scheduler.startup.StartupProcess.lambda$proceedWithStartupSynchronized$3(StartupProcess.java:167) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t  at io.camunda.zeebe.scheduler.future.FutureContinuationRunnable.run(FutureContinuationRunnable.java:33) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t  at io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t  at io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t  at io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t  at io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t  at io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t  at io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t  Suppressed: io.camunda.zeebe.scheduler.startup.StartupProcessStepException: Bootstrap step Partition Manager failed\r\n\t\t  at io.camunda.zeebe.scheduler.startup.StartupProcess.completeStartupFutureExceptionallySynchronized(StartupProcess.java:185) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t\t  at io.camunda.zeebe.scheduler.startup.StartupProcess.lambda$proceedWithStartupSynchronized$3(StartupProcess.java:167) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t\t  at io.camunda.zeebe.scheduler.future.FutureContinuationRunnable.run(FutureContinuationRunnable.java:33) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t\t  at io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t\t  at io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t\t  at io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t\t  at io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t\t  at io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t\t  at io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t  Caused by: java.util.concurrent.CompletionException: java.lang.IllegalStateException: Expected to find a snapshot at index >= log's first index 161591, but found snapshot 25. A previous snapshot is most likely corrupted.\r\n\t\t  at java.util.concurrent.CompletableFuture.encodeThrowable(Unknown Source) ~[?:?]\r\n\t\t  at java.util.concurrent.CompletableFuture.uniApplyNow(Unknown Source) ~[?:?]\r\n\t\t  at java.util.concurrent.CompletableFuture.uniApplyStage(Unknown Source) ~[?:?]\r\n\t\t  at java.util.concurrent.CompletableFuture.thenApply(Unknown Source) ~[?:?]\r\n\t\t  at io.atomix.raft.partition.RaftPartition.open(RaftPartition.java:90) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t\t  at io.atomix.raft.partition.RaftPartitionGroup.lambda$join$2(RaftPartitionGroup.java:174) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t\t  at java.util.stream.ReferencePipeline$3$1.accept(Unknown Source) ~[?:?]\r\n\t\t  at java.util.HashMap$KeySpliterator.forEachRemaining(Unknown Source) ~[?:?]\r\n\t\t  at java.util.stream.AbstractPipeline.copyInto(Unknown Source) ~[?:?]\r\n\t\t  at java.util.stream.AbstractPipeline.wrapAndCopyInto(Unknown Source) ~[?:?]\r\n\t\t  at java.util.stream.AbstractPipeline.evaluate(Unknown Source) ~[?:?]\r\n\t\t  at java.util.stream.AbstractPipeline.evaluateToArrayNode(Unknown Source) ~[?:?]\r\n\t\t  at java.util.stream.ReferencePipeline.toArray(Unknown Source) ~[?:?]\r\n\t\t  at io.atomix.raft.partition.RaftPartitionGroup.join(RaftPartitionGroup.java:175) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t\t  at io.atomix.primitive.partition.impl.DefaultPartitionService.start(DefaultPartitionService.java:63) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t\t  at io.camunda.zeebe.broker.partitioning.PartitionManagerImpl.start(PartitionManagerImpl.java:125) ~[zeebe-broker-8.1.9.jar:8.1.9]\r\n\t\t  at java.util.concurrent.CompletableFuture$AsyncSupply.run(Unknown Source) ~[?:?]\r\n\t\t  at java.util.concurrent.CompletableFuture$AsyncSupply.exec(Unknown Source) ~[?:?]\r\n\t\t  at java.util.concurrent.ForkJoinTask.doExec(Unknown Source) ~[?:?]\r\n\t\t  at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source) ~[?:?]\r\n\t\t  at java.util.concurrent.ForkJoinPool.scan(Unknown Source) ~[?:?]\r\n\t\t  at java.util.concurrent.ForkJoinPool.runWorker(Unknown Source) ~[?:?]\r\n\t\t  at java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source) ~[?:?]\r\n\t  Caused by: java.lang.IllegalStateException: Expected to find a snapshot at index >= log's first index 161591, but found snapshot 25. A previous snapshot is most likely corrupted.\r\n\t\t  at io.atomix.raft.utils.StateUtil.verifySnapshotLogConsistent(StateUtil.java:32) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t\t  at io.atomix.raft.impl.RaftContext.<init>(RaftContext.java:204) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t\t  at io.atomix.raft.impl.DefaultRaftServer$Builder.build(DefaultRaftServer.java:263) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t\t  at io.atomix.raft.impl.DefaultRaftServer$Builder.build(DefaultRaftServer.java:237) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t\t  at io.atomix.raft.partition.impl.RaftPartitionServer.buildServer(RaftPartitionServer.java:185) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t\t  at io.atomix.raft.partition.impl.RaftPartitionServer.initServer(RaftPartitionServer.java:151) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t\t  at io.atomix.raft.partition.impl.RaftPartitionServer.start(RaftPartitionServer.java:110) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t\t  at io.atomix.raft.partition.RaftPartition.open(RaftPartition.java:90) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t\t  at io.atomix.raft.partition.RaftPartitionGroup.lambda$join$2(RaftPartitionGroup.java:174) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t\t  at java.util.stream.ReferencePipeline$3$1.accept(Unknown Source) ~[?:?]\r\n\t\t  at java.util.HashMap$KeySpliterator.forEachRemaining(Unknown Source) ~[?:?]\r\n\t\t  at java.util.stream.AbstractPipeline.copyInto(Unknown Source) ~[?:?]\r\n\t\t  at java.util.stream.AbstractPipeline.wrapAndCopyInto(Unknown Source) ~[?:?]\r\n\t\t  at java.util.stream.AbstractPipeline.evaluate(Unknown Source) ~[?:?]\r\n\t\t  at java.util.stream.AbstractPipeline.evaluateToArrayNode(Unknown Source) ~[?:?]\r\n\t\t  at java.util.stream.ReferencePipeline.toArray(Unknown Source) ~[?:?]\r\n\t\t  at io.atomix.raft.partition.RaftPartitionGroup.join(RaftPartitionGroup.java:175) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t\t  at io.atomix.primitive.partition.impl.DefaultPartitionService.start(DefaultPartitionService.java:63) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t\t  at io.camunda.zeebe.broker.partitioning.PartitionManagerImpl.start(PartitionManagerImpl.java:125) ~[zeebe-broker-8.1.9.jar:8.1.9]\r\n\t\t  at java.util.concurrent.CompletableFuture$AsyncSupply.run(Unknown Source) ~[?:?]\r\n\t\t  at java.util.concurrent.CompletableFuture$AsyncSupply.exec(Unknown Source) ~[?:?]\r\n\t\t  at java.util.concurrent.ForkJoinTask.doExec(Unknown Source) ~[?:?]\r\n\t\t  at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source) ~[?:?]\r\n\t\t  at java.util.concurrent.ForkJoinPool.scan(Unknown Source) ~[?:?]\r\n\t\t  at java.util.concurrent.ForkJoinPool.runWorker(Unknown Source) ~[?:?]\r\n\t\t  at java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source) ~[?:?]\r\n  ```\r\n</details>\r\n\n",
    "title": "StateUtil.verifySnapshotLogConsistent message does not indicate which partition is affected"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12798",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "There is an in-memory state tracked in `MutablePendingMessageSubscriptionState`. Usage of this is scheduled in tasks through multiple places. This shared mutable state makes it impossible to run these tasks async from the stream processor.\r\n\r\nWe need to make sure that `MutablePendingMessageSubscriptionState` does not share mutable state. \r\n\r\nThis is a blocker for:\r\n- #12302 \n",
    "title": "Don't share mutable state through `MutablePendingMessageSubscriptionState`"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12764",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "<!--\r\nIn case you have questions about our software we encourage everyone to participate in our community via the\r\n- Camunda Platform community forum https://forum.camunda.io/ or \r\n- Slack https://camunda-cloud.slack.com/ (For invite: https://camunda-slack-invite.herokuapp.com/)\r\n\r\nThere you can exchange ideas with other Zeebe and Camunda Platform 8 users, as well as the product developers, and use the search to find answer to similar questions.\r\n\r\nThis issue template is used by the Zeebe engineers to create general tasks.\r\n-->\r\n\r\n**Description**\r\n\r\nWe need to introduce a way to version our Event Appliers. \r\n\r\nThis is required for:\r\n- #11456 \r\n\r\nCurrently, the `Deployment.CREATED` event applier inserts a deployment record into the state. This is used to be able to re-distribute the deployment later in case distribution failed. This deployment record is deleted (cleaned up) by the `Deployment.FULLY_DISTRIBUTED` event applier.\r\n\r\nWith generalized record distribution, we will still write the `Deployment.CREATED` events, but there is no need anymore for this deployment record because it is stored for distribution in a generalized way (see #11456). In addition, we no longer write the `Deployment.FULLY_DISTRIBUTED` event. So, we cannot clean up the state anymore if we would continue to insert a deployment record on `Deployment.CREATED`.\r\n\r\nAfter updating to a version with generalized record distribution, unprocessed deployment distribution commands may exist on the log, and deployment distribution events must be replayed in the same way as they were when they were written. So, in order to stay backward compatible, both the existing Deployment Distribution mechanism and the Generalized Record Distribution need to co-exist. \r\n\r\nTo allow the co-existence of these mechanisms, we need to have a way to determine how the event should be applied. This can be done by versioning our event appliers.\r\n\r\n**Considered alternatives**\r\n\r\n1. **Continue to write the `Deployment.FULLY_DISTRIBUTED` event for the Generalized Record Distribution of deployments**. This would allow us to keep the `Deployment.CREATED` event applier as is, continuing to insert the deployment record into the state. This alternative has some downsides:\r\n- we are inserting the deployment record twice into the state (once redundantly), which is a performance regression (it was only once before).\r\n- the `Deployment.FULLY_DISTRIBUTED` event leaks the distribution responsibility into deployments, which is what we were trying to remove with generalized record distribution.\r\n- there is no possibility to remove the `Deployment.FULLY_DISTRIBUTED` event in future releases, exactly for the problem above. We'd need to change the `Deployment.CREATED` event applier, which is not possible without versioning.\r\n\r\n2. **Replace `Deployment.CREATED` with a `Deployment.CREATED_V2` intent**. Intents are the standard way to change the behavior of processing/applying records. This alternative has some downsides:\r\n- it's not pretty to have a version number in the intent. We could avoid this by carefully thinking about alternative intent names, like `DEPLOYED` but it would never fit the `CREATE` command.\r\n- users might depend on consuming the `Deployment.CREATED` event in an exporter to extract the deployed resources.\r\n\r\n3. **Stop writing the `Deployment.CREATED` event altogether**. The event applier can stay as is, to replay previously written events. This alternative has some downsides:\r\n- each command must be followed up by a follow-up record to indicate that it was processed. A `Deployment.CREATE` command without resources is rejected. Those with acceptable resources will always produce `Process.CREATED` or `Decision.CREATED` events. However, it is not so nice if the `Deployment.CREATE` command does not have `Deployment.CREATED` follow-up event.\r\n- users might depend on consuming the `Deployment.CREATED` event in an exporter to extract the deployed resources.\n",
    "title": "Version event appliers"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12695",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "<!--\r\nIn case you have questions about our software we encourage everyone to participate in our community via the\r\n- Camunda Platform community forum https://forum.camunda.io/ or \r\n- Slack https://camunda-cloud.slack.com/ (For invite: https://camunda-slack-invite.herokuapp.com/)\r\n\r\nThere you can exchange ideas with other Zeebe and Camunda Platform 8 users, as well as the product developers, and use the search to find answer to similar questions.\r\n\r\nThis issue template is used by the Zeebe engineers to create general tasks.\r\n-->\r\n\r\n**Description**\r\n\r\nWhile implementing #12085 (PR https://github.com/camunda/zeebe/pull/12694) duplicated code occurred in job command precondition implementations. Further, this duplication already exists with previous implementations.\r\n\r\n**Expected Outcome**\r\nThe expected outcome of refactoring is to eliminate duplications in job command preconditions and make it easier to add new preconditions in the future.\r\n\n",
    "title": "Refactor Job Command Preconditions"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12667",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nCurrently, on startup, we scan all segments to detect corruption and to re-build journal index. This result in a high startup duration.\r\n\r\nFor example, in the following experiment, a broker took almost 2 minutes to startup when there were around 60 segments per partition.\r\n![image](https://user-images.githubusercontent.com/1997478/236187198-da42768f-d7f6-4ba7-9c2b-a003a8e95c5c.png)\r\n\r\nA partition can end up with large number of segments usually if the exporting is slow or not making progress. Other unexpected bugs in processing or snapshot can also lead to the situation. Although it is important to fix the root cause, we should also try to reduce the impact of it on startup duration. We have observed that high startup duration make things worse, because during that time the broker is not ready, it may not receive events until it has scanned all segments, and as a result needs longer time to catch up with the leader. If more than one broker is restarted, the cluster is unavailable for a long time. This has resulted in a few incidents in production. \r\n\r\nTo improve this situation, we should remove the scan of all segments during startup. This has the following impact.\r\n1. We cannot detect corruption on startup. But this might be ok, because we will detect it later when the corrupted entry is read. If it is never read, then it would be ok to ignore it. An entry is never read again if it was already replicated to all brokers, exported, and processed, which means it will be compacted soon.\r\n2. We cannot rebuilt the journal index, which means the seeks might be slow. We should verify how much impact it has, and see if we can improve it. Some ideas - a. persist the index b. dynamically build it when we think it is needed. c. build it async in background. \n\n deepthidevaki: I did a [spike](https://github.com/camunda/zeebe/tree/dd-spike-remove-segment-scan) to remove scanning of all entries on startup. To have some integrity checks, in the spike I stored the the position and index of the last entry of a segment in it's segment descriptor. This helps to verify if the segments are ordered correctly, and if there are any missing segments. \r\n\r\nThe startup duration reduced to 30s or lower.\r\n![image](https://user-images.githubusercontent.com/1997478/236194716-ba2c99fb-30c4-4a6c-a489-a79f5a805da4.png)\r\n\n npepinpe: Let's time box that and evaluate the impact of building the index as we go instead of at startup.\n deepthidevaki: I wrote a [jmh benchmark](https://github.com/camunda/zeebe/commit/0fe01e524c989e168e3e755deae0583f97f39206) to evaluate the impact on seek if there is no index. To simulate a journal with no index, indexDensity is set to a high value. The benchmark evaluates the time to seek to the last entry of the segment.\r\n\r\n```\r\n\r\nBenchmark                                       (indexDensity)  (segmentSizeInMB)  Mode  Cnt          Score          Error  Units\r\nSegmentedJournalReaderJmhTest.seekToLast             100                128  avgt    5        502.369 ±       15.029  ns/op\r\nSegmentedJournalReaderJmhTest.seekToLast             100                512  avgt    5       1913.527 ±      161.392  ns/op\r\nSegmentedJournalReaderJmhTest.seekToLast      1000000000                128  avgt    5  122563053.457 ±  9138218.052  ns/op\r\nSegmentedJournalReaderJmhTest.seekToLast      1000000000                512  avgt    5  472341864.454 ± 40436424.711  ns/op\r\n\r\n```",
    "title": "Reduce startup time when there are many segments"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12655",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\nAfter a release we must send a list of fixed issues related to support tickets. For this we look at the `support` label on issues. It's easy to forget to add this label.\n\n**Describe the solution you'd like**\nIntroduce a GitHub action that checks new issues and comments in issues if the text contains `SUPPORT-XXXX`. If it find any the action should add the `support` label.\n\n**Describe alternatives you've considered**\nN/A\n\n**Additional context**\nN/A\n\n\n remcowesterhoud: @abbasadel fyi 🙂 \n megglos: ZDP-Triage:\n- would be great to be done for all C8 teams actually\n- @megglos will take this over as part of the support/engineering collaboration\n",
    "title": "Automatically add `support` label to support related issues"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12601",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "<!--\r\nIn case you have questions about our software we encourage everyone to participate in our community via the\r\n- Camunda Platform community forum https://forum.camunda.io/ or \r\n- Slack https://camunda-cloud.slack.com/ (For invite: https://camunda-slack-invite.herokuapp.com/)\r\n\r\nThere you can exchange ideas with other Zeebe and Camunda Platform 8 users, as well as the product developers, and use the search to find answer to similar questions.\r\n\r\nThis issue template is used by the Zeebe engineers to create general tasks.\r\n-->\r\n\r\n**Description**\r\n\r\nCame up in https://github.com/camunda-cloud/alerts/pull/91, raised by @multani \r\n\r\nRight now our UIDs are not human readibly see https://github.com/camunda/zeebe/blob/11c54b23ab2d567c6933cf7925327481562d0b32/monitor/grafana/zeebe.json#L14934 which means it is hard to reference them and understand what the request dashboard is about.\r\n\r\nBenefit references like this https://github.com/camunda-cloud/alerts/blob/e2245885e6010d08de2924d0da4750be9d33bb32/generated/camunda-cloud-240911/development-worker-1/monitoring.coreos.com_v1_prometheusrule_zeebe-crashlooping.yaml#L17 would be more clear\r\n\r\n\r\nChanging the UID would improve this, I think this would be a simple and straight forward change. Similar to https://github.com/camunda-cloud/grafana-dashboards/issues/21\r\n\n\n npepinpe: Let's do it sooner than later before more things reference the old UIDs and it becomes more of a pain to change them.\n\nPlease remember to update the references (e.g. Cloud Bot, alerts - check with SREs or do a GitHub search of the ID in the camunda and camunda-cloud orgs).\n koevskinikola: ZPA triage:\r\n@Zelldon is something expected from ZPA on this issue?\n Zelldon: @koevskinikola benchmarks and grafana dashboard is seen as shared responsibility so yes you could happily do this as well :)",
    "title": "Make dashboard UIDs human-readable"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12577",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nFor main, stable/8.1, probably also stable/8.2 renovate seems to have issues updating the digest.\r\n\r\n```\r\nDEBUG: Digest is not updated(packageFile=\"Dockerfile\", branch=\"renovate/stable/8.1-eclipse-temurin-17.x\")\r\n{\r\n  \"baseBranch\": \"stable/8.1\",\r\n  \"manager\": \"dockerfile\",\r\n  \"expectedValue\": \"sha256:22f133769ce2b956d150ab749cd4630b3e7fbac2b37049911aa0973a1283047c\",\r\n  \"foundValue\": \"sha256:b10df4660e02cf944260b13182e4815fc3e577ba510de7f4abccc797e93d9106\"\r\n}\r\nWARN: Error updating branch: update failure(branch=\"renovate/stable/8.1-eclipse-temurin-17.x\")\r\n{\r\n  \"baseBranch\": \"stable/8.1\"\r\n}\r\n```\r\n\r\nSee further logs at https://app.renovatebot.com/dashboard#github/camunda/zeebe/\r\n\n\n megglos: turns out the recent introduction of JVM and JAVA_VERSION broke the updates\r\n```\r\n          {\r\n            \"autoReplaceStringTemplate\": \"{{depName}}{{#if newValue}}:{{newValue}}{{/if}}{{#if newDigest}}@{{newDigest}}{{/if}}\",\r\n            \"datasource\": \"docker\",\r\n            \"depType\": \"stage\",\r\n            \"replaceString\": \"${JVM}:${JAVA_VERSION}-jre-focal@sha256:54f64f1cf8e9b984a92d06d3ad5c10fbbb9e9869144f1f45decdf530d64a4163\",\r\n            \"skipReason\": \"invalid-name\",\r\n            \"updates\": []\r\n          },\r\n```\n koevskinikola: ZPA triage:\n- @megglos it looks like ZDP is already working on this issue. Is there any reason why the issue is on the ZPA board as well?\n- ZPA will remove it from the team board. If there is anything we need to do, please re-add it to the ZPA board.\n megglos: changes are applied, I'm monitoring it till the next updates are applied to all branches",
    "title": "Renovate fails updating docker digest"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12469",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "Happened twice now:\r\n- https://github.com/camunda/zeebe/actions/runs/4731056097/jobs/8395540393\r\n- https://github.com/camunda/zeebe/actions/runs/4730193543/jobs/8393592546\r\n\r\nFor smoke tests we set additional JVM options:\r\nhttps://github.com/camunda/zeebe/blob/c51d6ab7a0f4d2e6250602f90f8b76e4c59baa2a/.github/workflows/ci.yml#L172\r\n\r\nWe should re-evaluate those. I suspect that we can simply remove them.\n",
    "title": "Smoke tests fail due to full code cache"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12427",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "<!--\r\nIn case you have questions about our software we encourage everyone to participate in our community via the\r\n- Camunda Platform community forum https://forum.camunda.io/ or \r\n- Slack https://camunda-cloud.slack.com/ (For invite: https://camunda-slack-invite.herokuapp.com/)\r\n\r\nThere you can exchange ideas with other Zeebe and Camunda Platform 8 users, as well as the product developers, and use the search to find answer to similar questions.\r\n\r\nThis issue template is used by the Zeebe engineers to create general tasks.\r\n-->\r\n\r\n**Description**\r\n\r\nI recently came across this library: https://github.com/acm19/aws-request-signing-apache-interceptor\r\nThis provides a request interceptor to take care AWS request signing. Currently we are maintaining our own interceptor. It would be great to switch it for this maintained library so we can forget about it ourselves.\r\n\n\n korthout: ZPA triage:\n- we like the idea, this was suggested by someone from AWS\n- pretty low-hanging fruit (just switch out the used interceptor and remove our own code)\n- marking as `later` not needed immediately but could be done when we work on opensearch again",
    "title": "Switch custom AwsSignHttpRequestInterceptor in Opensearch exporter to library"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12418",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "For `main` we run the following : \r\n- Weekly benchmarks (rolling 4)\r\n- E2E tests (weekly)\r\n- QA/Chaos tests (daily) \r\n\r\nFor stable branches we are only running daily chaos tests.\r\n\r\nSince we don't do any additional QA during patch release, we could run atleast a weekly benchmark one per stable branch. In addition we can also run the e2e tests weekly on the stable branch.\n\n npepinpe: Great idea, I would be fine with running the E2E weekly on stable and nothing for the patch releases :+1: This puts a little bit more work on the medic potentially, but I guess that's just putting pressure on us to make sure things are stable ;)\n korthout: ZPA triage: doesn't sound urgent, could be done by a medic during a quiet week.\n megglos: ZDP triage: would be great to get out of the way asap, should be a low hanging fruit.\n\ne2e: more urgent => to simplify the patch release process\nqa: already done\nweekly: to be done, but less urgent",
    "title": "Improve QA of stable branches"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12390",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nIn order for the brokers to push jobs all the way back to clients, whenever a client starts a new job stream RPC, a `ClientStreamConsumer` adapter must be registered with the given `ClientStreamer`. Since you now have to convert the `JobActivationProperties`, it's likely you will need to move this and its implementation into the `protocol-impl` module. \r\n\r\nAs part of the endpoint implementation, it should:\r\n\r\n- [ ] Convert the `ActivateJobsRequest` into serializable `JobActivationProperties`\r\n- [ ] Register the consumer along with its job type as stream type, the job properties as metadata, and the consumer adapter for this observer\r\n- [ ] Ensure that when the observer is cancelled or closed (using `ServerStreamObserver#setOnCloseHandler` and `ServerStreamObserver#setOnCancelledHandler`), that the consumer is removed using the UUID obtained on registration\r\n\r\nSince the `ClientStreamer` is an actor, it's likely you will also need to handle registration and pushes in an actor context, similarly to how we deal with the long polling job activation. Don't hesitate to ask if you have questions. I would advise against simply calling `ActorFuture#join` on registration to get the UUID, as you'd be blocking the gRPC thread.\r\n\r\nAdditionally, you may want to check on whether it's necessary to register the consumer only when the observer is ready. When a client is slow, it may not be ready to receive messages for a while, so it's pointless to register it until it's ready. That said, evaluate if it's worth the trade off in terms of complexity/UX.\r\n\n\n npepinpe: FYI @berkaycanbc, I re-read the gRPC docs, and we shouldn't use the `setOnReadyHandler` for stream registration:\r\n\r\n> Set a Runnable that will be executed every time the stream isReady() state changes from false to true. While it is not guaranteed that the same thread will always be used to execute the Runnable, it is guaranteed that executions are serialized with calls to the 'inbound' StreamObserver.\r\n> May only be called during the initial call to the application, before the service returns its StreamObserver.\r\n> Because there is a processing delay to deliver this notification, it is possible for concurrent writes to cause isReady() == false within this callback. Handle \"spurious\" notifications by checking isReady()'s current value instead of assuming it is now true. If isReady() == false the normal expectations apply, so there would be another onReadyHandler callback.\r\n\r\nSo essentially the stream may become ready multiple over its lifetime. One obvious case is what we expected, during the initial set up. Another one is when gRPC's backpressure kicks in - then the stream is not ready anymore, and becomes ready again only when the client wants to accept new messages.\r\n\r\nWe could use it to detect when the client is ready to accept new messages, however, but we will integrate this in our flow control solution in the next phase.\r\n\r\nSee [this](https://github.com/grpc/grpc-java/tree/master/examples/src/main/java/io/grpc/examples/manualflowcontrol) for more on gRPC's built in flow control.",
    "title": "Register ClientStreamConsumer adapters for every gRPC stream observer"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12389",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nThe ZDP provided `ClientStreamer` allows registering `ClientStreamConsumer` instances. These consumers will receive serialized `ActivatedJob` instances. These serialized payloads are owned by the ZPA team, and ideally should reuse the same type, so will likely have to be put in something like the `protocol-impl` module, along with the interface that currently lives in the `workflow-engine` module.\r\n\r\n- [ ] Deserialize & validate the payload\r\n- [ ] Convert it to the appropriate gRPC type\r\n- [ ] Forward it to the gRPC stream observer\r\n\r\nIf an error occurs at any point, the consumer should throw an appropriate exception. The caller of the `push` method will take care of retrying or yielding the job back on error. **Note that if the error occurs when calling the `StreamObserver#onNext` method, the consumer implementation must call `StreamObserver#onError` as per the gRPC API**.\r\n\r\n\r\n\n\n deepthidevaki: Nicolas and me had some questions, and here is what we found out. It might be relevant when you implement the consumer.\r\n\r\n- If we sent 10 jobs back to back on a Grpc stream, but the 5th job failed to sent, what happens to the rest of it?\r\n  What I learned is that Grpc stream guarantees ordered delivery. That would mean, if the 5th job failed jobs 6-10 is not sent.\r\n [ref1](https://grpc.io/docs/what-is-grpc/core-concepts/) [ref2](https://groups.google.com/g/grpc-io/c/f37hmkJrWHY)\r\n  > Server streaming RPCs where the client sends a request to the server and gets a stream to read a sequence of messages back. The client reads from the returned stream until there are no more messages. gRPC guarantees message ordering within an individual RPC call.\r\n\r\n\r\n\r\n- Can we know when the 5th job is failed?\r\n   So far it is not guaranteed. Implementation of `onNext`can be non-blocking. So it might fail later and we will know about it when we call `onNext` next time, which might be when we sent the 7th job. So there are chances that we assume the job is sent successfully, but it was not. I couldn't find an api where we can wait on each message sent via `onNext`. ",
    "title": "Implement ClientStreamConsumer adapter to forward jobs to gRPC clients"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12388",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nBefore the ZPA team gets started on their side in the gateway, they should already have access to a provisioned `ClientStreamer` which they can use to add/remove streams in order to forward activated jobs.\r\n\r\nIt's perfectly fine if the streamer is initially unused, as we'll likely get to this before them, and the goal here is simply to make sure they're unblocked.\r\n\n",
    "title": "Provide ClientStreamer instance to the GatewayGrpcService/EndpointManager"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12387",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nIn order for the job push' `ClientStreamService` to detect brokers being added/removed, we need a way to get this information via the topology. We could use a plain membership listener, but that one is unaware of whether a node is a broker or a gateway, whereas the `BrokerTopologyManager` can already provide this information.\r\n\r\nWe should add the following capabilities:\r\n\r\n- [ ] Add a new listener which gets notified when a broker is added or removed\r\n- [ ] When the listener is initially added, it is also initialized the current list of known brokers, to avoid race conditions\r\n- [ ] The listener can be removed via its identity\r\n\n",
    "title": "Allow listening for updates in BrokerTopologyListener"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12386",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "## Description\r\nThe current error handling strategy in the job push pipeline is to yield the job back to partition if there are any observed errors until it reaches the client.\r\n\r\nThis can have a significant performance impact, since it means writing a new command - this command has to be written, replicated, committed, then processed, and the events must be committed, and then the job is activated and sent out.\r\n\r\nTo speed things up, if there is another logically equivalent stream (whether on the gateway or broker), we should try pushing to that one instead.\r\n\r\nFor the first phase, we can do this naively, ignoring any flow control or optimum work distribution.\n\n npepinpe: @deepthidevaki - this makes me think, maybe the gateway should never yield the job back directly, and instead the broker will always be the one doing this. \r\n\r\nSince a retry strategy could be:\r\n\r\nBroker pushes job to gateway 1/stream A -> gateway 1 fails to forward job, tries with stream B -> gateway 1 fails to forward job, sends it back to the broker -> broker pushes to gateway 2/stream A' -> success\n deepthidevaki: > this makes me think, maybe the gateway should never yield the job back directly, and instead the broker will always be the one doing this.\r\n> \r\n> Since a retry strategy could be:\r\n> \r\n> Broker pushes job to gateway 1/stream A -> gateway 1 fails to forward job, tries with stream B -> gateway 1 fails to forward job, sends it back to the broker -> broker pushes to gateway 2/stream A' -> success\r\n\r\nMakes sense. We could start with this approach.\r\n\r\n",
    "title": "Try pushing activated job to next logical stream on failure before yielding"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12384",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nIn order to get more visibility, we should add the following metrics in the gateway components:\r\n\r\n- [ ] The number of registered client streams in the `ClientStreamManager`\r\n- [ ] The rate of error/failures in the job receiver\r\n\r\nDuring kick off, decide if we want to add more dimensions here (e.g. failure code, registered stream state, etc.), and how this will look like.\n",
    "title": "Additional job push metrics"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12083",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "## Introduction\r\n\r\nBroadcasting to notify listeners when new jobs become available is done in the `DbJobState`. This doesn't work well with the new push-based job activation mechanism.\r\n\r\n## AC\r\nBroadcasting to notify listeners when new jobs become available is done as a side-effect (see [SideEffectProducer](https://github.com/camunda/zeebe/blob/6dac668956e2a7a79df9df31bf0f8238e87eb7c3/stream-platform/src/main/java/io/camunda/zeebe/stream/api/SideEffectProducer.java)) in the following processors:\r\n\r\n- [ ] [JobWorkerTaskProcessor](https://github.com/camunda/zeebe/blob/319813a684325c97556fb59013eb8b88ea27b3f2/engine/src/main/java/io/camunda/zeebe/engine/processing/bpmn/task/JobWorkerTaskProcessor.java#L54)\r\n- [ ] [JobTimeOutProcessor](https://github.com/camunda/zeebe/blob/ad1d5c92a3d4d6009da3a9d968238b83b9dd5c5c/engine/src/main/java/io/camunda/zeebe/engine/processing/job/JobTimeOutProcessor.java#L20)\r\n- [ ] [JobFailProcessor](https://github.com/camunda/zeebe/blob/813c86f780f54496fcd089810704c1a4e4958141/engine/src/main/java/io/camunda/zeebe/engine/processing/job/JobFailProcessor.java)\r\n- [ ] [JobRecurProcessor](https://github.com/camunda/zeebe/blob/813c86f780f54496fcd089810704c1a4e4958141/engine/src/main/java/io/camunda/zeebe/engine/processing/job/JobRecurProcessor.java)\r\n- [ ] [ResolveIncidentProcessor](https://github.com/camunda/zeebe/blob/18dd3c5e8df9bd2164e4d0fc73f5429c9d38b05c/engine/src/main/java/io/camunda/zeebe/engine/processing/incident/ResolveIncidentProcessor.java)\r\n\r\n## Blocked by\r\n* #12082\n",
    "title": "Broadcasting for available jobs is done as a processor side-effect"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12082",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "## Introduction\r\nThe `JobStreamer` API is available to the engine processors.\r\n\r\n## AC\r\nIt needs to be passed to the following job processors:\r\n- [ ] [JobWorkerTaskProcessor](https://github.com/camunda/zeebe/blob/319813a684325c97556fb59013eb8b88ea27b3f2/engine/src/main/java/io/camunda/zeebe/engine/processing/bpmn/task/JobWorkerTaskProcessor.java#L54) / [BpmnJobBehavior](https://github.com/camunda/zeebe/blob/4d46a4947e6d3ac72cb4e0af324f4c978b591989/engine/src/main/java/io/camunda/zeebe/engine/processing/bpmn/behavior/BpmnJobBehavior.java#L81-L92)\r\n- [ ] [JobTimeOutProcessor](https://github.com/camunda/zeebe/blob/ad1d5c92a3d4d6009da3a9d968238b83b9dd5c5c/engine/src/main/java/io/camunda/zeebe/engine/processing/job/JobTimeOutProcessor.java#L20)\r\n- [ ] [JobFailProcessor](https://github.com/camunda/zeebe/blob/813c86f780f54496fcd089810704c1a4e4958141/engine/src/main/java/io/camunda/zeebe/engine/processing/job/JobFailProcessor.java)\r\n- [ ] [JobRecurProcessor](https://github.com/camunda/zeebe/blob/813c86f780f54496fcd089810704c1a4e4958141/engine/src/main/java/io/camunda/zeebe/engine/processing/job/JobRecurProcessor.java)\r\n- [ ] [ResolveIncidentProcessor](https://github.com/camunda/zeebe/blob/18dd3c5e8df9bd2164e4d0fc73f5429c9d38b05c/engine/src/main/java/io/camunda/zeebe/engine/processing/incident/ResolveIncidentProcessor.java)\r\n\r\nA \"behavior\" class can be wrapped around the `JobStreamer` API to ease working with it through multiple processors.\r\n\r\n## Additional context\r\nThe refactoring should be partially done by #12067\n",
    "title": "The JobStreamer API is available to the engine processors"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12041",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "<!--\r\nIn case you have questions about our software we encourage everyone to participate in our community via the\r\n- Camunda Platform community forum https://forum.camunda.io/ or \r\n- Slack https://camunda-cloud.slack.com/ (For invite: https://camunda-slack-invite.herokuapp.com/)\r\n\r\nThere you can exchange ideas with other Zeebe and Camunda Platform 8 users, as well as the product developers, and use the search to find answer to similar questions.\r\n\r\nThis issue template is used by the Zeebe engineers to create general tasks.\r\n-->\r\n\r\n**Description**\r\n\r\n_Related to https://github.com/camunda/zeebe/issues/12035_\r\n_Related to https://github.com/camunda/zeebe/issues/12033_\r\n_Came up in https://github.com/camunda/zeebe/issues/11813_\r\n\r\n\r\n> _Blacklist checks_\r\n> \r\n> With (almost) every command, Zeebe will check if the given process instance is blacklisted, even if the process instance is just created. Also, this check happens all the time during batch processing:\r\n> \r\n> https://github.com/camunda/zeebe/blob/ae6dbfc5439c889ad8b8fd24d7431b46110c7900/engine/src/main/java/io/camunda/zeebe/engine/Engine.java#L130-L134\r\n> \r\n> Just some a thoughts on how this could be improved:\r\n> * Cache the information that there is no blacklisted process instance so that Zeebe avoids the lookup in RocksDB.\r\n> * Avoid doing the check with every (follow-up) command during batch processing.\r\n> * When creating a new process instance, the check is unnecessary.\r\n> * TBD: If there are blacklisted process instances, how to make the lookup efficient.\r\n> \r\n\n\n megglos: https://github.com/camunda/zeebe/pull/12306\r\ncovers:\r\n\r\n- [x] Cache the information that there is no blacklisted process instance so that Zeebe avoids the lookup in RocksDB.\r\n- [x] When creating a new process instance, the check is unnecessary.\n Zelldon: This is done via https://github.com/camunda/zeebe/pull/12306",
    "title": "Unnecessary Blacklist checks"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12028",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "<!--\r\nIn case you have questions about our software we encourage everyone to participate in our community via the\r\n- Camunda Platform community forum https://forum.camunda.io/ or \r\n- Slack https://camunda-cloud.slack.com/ (For invite: https://camunda-slack-invite.herokuapp.com/)\r\n\r\nThere you can exchange ideas with other Zeebe and Camunda Platform 8 users, as well as the product developers, and use the search to find answer to similar questions.\r\n\r\nThis issue template is used by the Zeebe engineers to create general tasks.\r\n-->\r\n\r\n### Issue Description\r\n\r\nThe current compilation time with maven `mvn clean install -DskipTests -T1C` is for me locally ~1.5 min, but this highly depends on the machine. The execution of all the tests is way too long and I never do it locally (which is bad). If I do changes I normally just run the test in the affected module.\r\n\r\nThe CI pipeline takes ~33 min, to build, test and deploy snapshots. See this [33m 28s](https://github.com/camunda/zeebe/actions/runs/4416915694/usage).\r\n\r\nIf we do a calculation of how many engineers we are working on this project ~9 and say everyone is pushing at least once to a branch we run 33 * 9 min = 297 min per day. If we say we have around 200 working days in a year, we have 59400 min CI time, only for our work. \r\n\r\nThis doesn't include daily builds, dependabot (which are a lot), stable branches, etc.\r\n\r\nIf we are reducing the test and compile time, this will have a major effect on costs (since we using self-managed runners) but also gives us a boost in effectivity, since we getting faster feedback.\r\n\r\n\r\n### Potential Idea:\r\n\r\nI think there are several ideas to tackle this issue, improving the tests, reducing integration tests and increasing unit test coverage, running partial builds, etc.\r\n\r\nBut there is also another thing that might be interesting for us to investigate. We have several modules we haven't touched for years. There are also modules that we touch only from time to time, maybe every quarter once.\r\n\r\nI would propose we release one version of each of these modules and pin them in our dependency. This would allow us to skip the compiling and running tests of these modules on every build. For example, it is not necessary to compile and run tests every time for `Zeebe Msgpack Core`, `Zeebe Msgpack Value`, or others.\r\n\r\nI think it is important that if we change one of these modules we directly release a new version of it. IMHO that is ok since these modules are and should only be used internally. As long we keep our public release intact and release `dist` and `clients` with the correct release number this should be fine. This would decrease the time of the general CI pipeline by a lot.\r\n\r\nBTW When we started with Zeebe (or TNGP) we had each module its own REPO, but we realized that since our APIs were so fragile we had to change a lot on multi modules so it didn't make much sense to have them in several repos and release them every time (or use snapshot). But I think this has changed since a lot of modules are more mature now.\r\n\r\n### First iteration scope\r\nOptimize overhead of test jobs => target max time 10m\r\n\r\n```[tasklist]\n### Tasks\n- [x] https://github.com/camunda/zeebe/pull/12406 - will optimize the order of tests module tests always offset qa integration tests\n- [x] https://github.com/camunda/zeebe/pull/12497 - will optimize duration on test success (most impact on unit test job)\n- [ ] https://github.com/camunda/zeebe/issues/12417 - single long running test classes limit the yield of forks\n- [x] https://github.com/camunda/zeebe/pull/12424 - a considerable amount of time is spend downloading deps, we use cahces in some but not all jobs with this PR caching will be used by default\n```\r\n\r\n\n\n korthout: >I would propose we release one version of each of these modules and pin them in our dependency. This would allow us to skip the compiling and running tests of these modules on every build. For example, it is not necessary to compile and run tests every time for Zeebe Msgpack Core, Zeebe Msgpack Value, or others\r\n\r\nThis makes sense to me. Especially as you say: \r\n>But I think this has changed since a lot of modules are more mature now.\r\n\r\nBefore choosing one solution, I want to propose an alternative:\r\n- only run ci on modules with changes (or changes in dependencies) (can be easily checked using git, but there are also maven extensions for this)\n Zelldon: Yep this is what I meant with `running partial builds` but the last time I checked was the maven support not that optimal (at least the plugin was no longer maintained etc.) \r\n\r\n\n korthout: ZPA triage summary:\n- Let's discuss with infra to see the actual CI costs, so we can help quantify these improvements\n- @abbasadel Please discuss with @megglos to coordinate efforts\n megglos: ZDP planning:\n- we will look into a reasonable scope for the next iteration\n megglos: Given with recent improvements we brought the CI stage down to about 10m I would conclude the first iteration scope complete. See e.g. https://github.com/camunda/zeebe/actions/runs/5068424932?pr=12850\r\n\r\nI would still consider https://github.com/camunda/zeebe/issues/12417 worth doing anytime soon to ensure the forking of IT jobs is efficient.\r\n\r\nLet's check in on how to proceed.\n megglos: ZDP-Triage:\n- right now flaky tests seem to be a bigger annoyance\n- would be worth revisiting while\n- moving back to backlog pausing for for now",
    "title": "Build and test pipeline takes too long"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11914",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "We need to be able to redistribute pending command distributions. This will work in a similar way as the [DeploymentRedistributor](https://github.com/camunda/zeebe/blob/main/engine/src/main/java/io/camunda/zeebe/engine/processing/deployment/distribute/DeploymentRedistributor.java).\r\n\r\n**Always**\r\n- Create `CommandRedistributor`\r\n- Remove `@Ignore` in `MultiPartitionDeploymentLifecycleTest.shouldRejectCompleteDeploymentDistributionWhenAlreadyCompleted`\r\n\r\n**Option 1**\r\n- Deprecate `DeploymentRedistributor`, we need to keep it to stay backwards compatible\r\n- See if we can disable the `DeploymentRedistributor` from running if there are no pending deployments\r\n\r\n**Option 2**\r\n- Migrate pending deployment distributions to generalised distribution\n\n remcowesterhoud: Option 1 is the least amount of work. However, it leaves a bunch of unused code in the codebase. With option 2 we don't have this issue. The data should be relatively easy to migrate.\n\nWith option 2 we can remove a few things, but not everything:\n- `DbDeploymentState` is no longer required. After migration there won't be data in these column families. The column families itself will remain as they will be used for the migration.\n- `EventAppliers` and `Processors` must remain. There might still be an old command on the log that isn't processed yet. We have to make sure we stay backwards compatible.\n- `Redistributor` can be removed. This looks into the migrated column families, which should be empty after migration. Instead any redistributing will happen by the new `CommandRedistributor`.\n- **Open questions**\n    - The old processors/appliers are still populating the migrated column families. What do we do here? Can we modify these so they use the generalised way of distributing?",
    "title": "Create a CommandRedistributor"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11884",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nI think that this needs to be done for basically two reasons:\r\n1. Consistency. We will start to use a single class to gain the info and in case we need to modify the caller part - we just modify `JobClient`\r\n2. Metrics. Source: https://camunda-platform.slack.com/archives/C6WGNHV2A/p1677234921004609. So, this continues the previous point - we need to have a single place to introduce metrics - I bet that the `JobClient` and `ZeebeClient` would be better ones.\r\n\r\n**Describe the solution you'd like**\r\nWe implement the method `public ActivateJobsCommandStep1 newActivateJobsCommand()` in the `JobClient` and use it in the `JobPoller`.\r\n\r\n**Describe alternatives you've considered**\r\nWe could just use the same method in the `ZeebeClient` but it feels not right to me 😅. Let all job calls will be in the `JobClient`.\r\n\r\n**Additional context**\r\nI could take this issue if you are okay with it. Also, I would like to take worker's metrics as well.\n\n aivinog1: @npepinpe Do you mind if I take it and https://github.com/camunda/zeebe/issues/4700?\n npepinpe: No, go for it. Ideally we should expose the same metrics as with the Go client.\n\nIt might be interesting to use a well established metrics facade here, e.g. OpenTelemetry, but I'll let someone from the automation team decide, as that would add a new dependency to our public API.\n korthout: I think this is a great idea @aivinog1 👍 Thanks for raising it.\r\n\r\nI see no reason to use the gateway stub directly. Note that the JobRunnerFactory already [uses the `jobClient` to fail the job](https://github.com/camunda/zeebe/blob/main/clients/java/src/main/java/io/camunda/zeebe/client/impl/worker/JobRunnableFactory.java#L56) when an uncaught exception is thrown in the handler. So your suggestion would be in line with other implementation details.\r\n\r\n👍 Feel free to go ahead with this refactoring.\r\n\r\nI can see that this would be useful to collect metrics on the job client in a single place, but I wonder what metrics would be collected here. Would this only measure requests made with the client, or can we also use this for the metric mentioned in #4700 specifically:\r\n>monitor the amount of jobs currently enqueued\r\n\r\n@aivinog1 Let me know if you already have ideas to tackle that part.\r\n\r\n@npepinpe You raise a good point about using an established metrics facade. I think I would also prefer it, but I need to read up on this topic first. @aivinog1, if you already have a  design in mind, please share it with me.\n Zelldon: @korthout regarding metrics we have this https://github.com/camunda/zeebe/issues/11570 I guess this cover this pretty much. I hope we tackle this soon (that I have time for it soon)\n korthout: @aivinog1 I've assigned you. Please go ahead if you want to work on this 👍 \n\nI've also marked this issue as `later later` as it is not among our team's priorities.",
    "title": "Migrate from `GatewayStub` to `JobClient` in the `JobPoller`"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11799",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "<!--\r\nIn case you have questions about our software we encourage everyone to participate in our community via the\r\n- Camunda Platform community forum https://forum.camunda.io/ or \r\n- Slack https://camunda-cloud.slack.com/ (For invite: https://camunda-slack-invite.herokuapp.com/)\r\n\r\nThere you can exchange ideas with other Zeebe and Camunda Platform 8 users, as well as the product developers, and use the search to find answer to similar questions.\r\n\r\nThis issue template is used by the Zeebe engineers to create general tasks.\r\n-->\r\n\r\n**Description**\r\n\r\nIt looks like we using the Zeebe client in the gateway, which is somehow weird. It add an unnecessary dependency to the client, and makes unnecessary requests. \r\n\r\nThis was recently discovered here https://github.com/camunda/zeebe/pull/11599#discussion_r1109846523 \r\n\r\nFurthermore, the impact was discovered also in one of the chaos days https://zeebe-io.github.io/zeebe-chaos/2023/02/23/Recursive-call-activity/\r\n\r\n\r\n![general](https://user-images.githubusercontent.com/2758593/220933420-06a44b51-a549-4b71-8611-6aa9d1b2d691.png)\r\n\r\n> One interesting fact is that the topology request rate is also up to 0.400 per second, so potentially every 2.5 seconds we send a topology request to the gateway. But there is no application deployed that does this. [I have recently found out again](https://github.com/camunda/zeebe/pull/11599#discussion_r1109846523), that we have the Zeebe client usage in the gateway to request the topology. Might be worth investigating whether this is an issue.\r\n\r\n\r\nRight now I'm not sure how problematic it is, whether it is just toil or whether this might be an actual bigger issue.\r\n\r\nrelates to https://jira.camunda.com/browse/SUPPORT-16945\r\n\n\n korthout: We're unsure what this health check's expected behavior should be. We can discuss this with SRE, as they are the users of this API in SaaS. Marking this as `later` for now.\n\n//cc @aleksander-dytko \n npepinpe: Support related now - see https://jira.camunda.com/browse/SUPPORT-16945?focusedCommentId=285709&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel\r\n\r\nHaving to configure authentication, security, etc., just overly complicates things. Then factor in things like hostname verification checks and so on - let's just get rid of it and hook in the topology manager directly.\n npepinpe: imo this would then fall to the ZDP team to do\n megglos: this actually overlaps with https://github.com/camunda/zeebe/issues/13431 , let's not work on it until that one is clarified",
    "title": "Investigate usage of client in gateway"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11713",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nWhen a client sends a job stream call (as defined in #11708), the gateway should keep track of these in an in-memory register.\r\n\r\nThis will be a long living stream observer. It's important that the gateway properly keep track of it, including when it is closed (gracefully or not, including cancellations), and should only be available to receive jobs when it is ready. \r\n\r\nSee [the prototype for an example](https://github.com/camunda/zeebe/blob/56873bbda3456df153629f915703384667a0a1f3/gateway/src/main/java/io/camunda/zeebe/gateway/GatewayGrpcService.java#L71-L95).\r\n\r\nClients which are logically equivalent based on their activation properties should be grouped together using some unique ID. This unique ID will be used to communicate with the broker when it comes to adding/removing workers, and pushing jobs.\r\n\r\nWhen a new logical client is added, then gateway should send a `AddWorker` request to all brokers. If the broker returns that the request is invalid, then the error should be propagated to the client. Any other errors should be retried.\r\n\r\n> **Note** We may want to limit this of course, but in theory if a broker never replies successfully but the request is correct, likely the broker is dead and will be removed from the topology soon.\r\n\r\nIf there are no brokers, the client is not cancelled or closed.\r\n\r\nWhen a new broker is added to the topology, an `AddWorker` request should be sent for all workers.\r\n\r\n> **Note** We can look into batching this as an optimization in a further iteration.\r\n\r\nWhen a broker is removed from the topology, nothing is to be done.\r\n\r\nWhen the gateway is shutting down, it should send a `RemoveWorkers` with its own member ID to all brokers. Requests are not retried here, of course, it's just a best-of effort to be polite.\n\n deepthidevaki: A first version for stream-management support for gateway is available in PR #12116. For easy tracking, I'm listing pending tasks here:\r\n\r\n- [ ] Aggregate multiple client streams with same metadata and streamType to a single stream. This should be done in transport, and transparently to the consumers of the API `ClientStreamer#add(streamType, metadata, consumer)`. https://github.com/camunda/zeebe/issues/12253 \r\n- [x] ~Re-visit how to handle node restarts. Currently, it fully relies on MembershipService to notify when a node is added or removed. However, there are some edge-cases which we have to handle. For example, if the broker restarts too fast, gateway may not detect the broker is dead, and as a result will not notify when the broker is restarted.~ \r\n   - This is not a problem. [see comment](https://github.com/camunda/zeebe/issues/11713#issuecomment-1496108623)\r\n- [ ] Optimal handling of retries. We might have to stop retrying sending add/remove request to a server at some point. Currently, it is retried indefinitely. It might also be good to retry with a backoff.\r\n- [ ] Integrate transport stream-management with gateway so that implementation of grpc api https://github.com/camunda/zeebe/issues/11708 can use them.\n npepinpe: Can we add these points to the open questions in the epic issue? We should evaluate the cost of not doing these now, and the benefits of adding them in the second phase. If we're not sure about the benefits, then identify scenarios which will help us evaluate these.\r\n\r\nExcept the last one of course, that we just have to do :smile: \n deepthidevaki: > Can we add these points to the open questions in the epic issue? \r\n\r\nI expect that most of these will be done in the scope of this issue. Let's first discuss what is necessary and what can be postponed, and then move them to open questions. I will not close this issue until then. So we won't loose track of it.\n deepthidevaki: Another edge case to consider: If a client stream is removed, while there is an ongoing \"AddRequest\" retry, it is possible that the stream is added to the broker after it has been removed. \r\n\r\n- [ ] Allow cancelling ongoing retries\r\n\n deepthidevaki: > * Re-visit how to handle node restarts. Currently, it fully relies on MembershipService to notify when a node is added or removed. However, there are some edge-cases which we have to handle. For example, if the broker restarts too fast, gateway may not detect the broker is dead, and as a result will not notify when the broker is restarted.\r\n\r\nThis is not a problem. When a node is restarted, it's incarnation number will be chaged. Swim detects this and post and MEMBER_REMOVED, and MEMBER_ADDED event. So we will always get notified of node restarts, and we will re-send AddStream requests. \r\n\r\nhttps://github.com/camunda/zeebe/blob/c055d58fc86afa675e27f7c8b7e6ce6110a29592/atomix/cluster/src/main/java/io/atomix/cluster/protocol/SwimMembershipProtocol.java#L251   \r\n``` java\r\n    // If the term has been increased, update the member and record a gossip event.\r\n    else if (member.incarnationNumber() > swimMember.getIncarnationNumber()) {\r\n      // If the member's version has changed, remove the old member and add the new member.\r\n      if (!Objects.equals(member.version(), swimMember.version())) {\r\n        members.remove(member.id());\r\n        post(new GroupMembershipEvent(GroupMembershipEvent.Type.MEMBER_REMOVED, swimMember.copy()));\r\n        ....\r\n        LOGGER.debug(\"{} - Evicted member for new version {}\", localMember.id(), swimMember);\r\n        post(new GroupMembershipEvent(GroupMembershipEvent.Type.MEMBER_ADDED, swimMember.copy()));\r\n        recordUpdate(swimMember.copy());\r\n      }\r\n\r\n```",
    "title": "Add gRPC stream API gateway implementation"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11712",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nIn order to push jobs from the gateway to the client, the gateway has to be able to receive them from the broker.\r\n\r\nTo do this, it should set up a receiving endpoint (i.e. subscribe to an Atomix topic). This endpoint will only receive one type of request, which will contain the job key, the job record, and the stream worker unique ID for which the job was activated.\r\n\r\nIf the gateway has no clients for this unique ID, it should make the job available again to the broker by sending a fail command (but keeping the same number of retries), much like we do in the long polling handler.\r\n\r\n> **Note** We may want to also send a `RemoveWorker` request to be safe, in case there was some bug.\r\n\r\nIf there are some clients for the unique ID (as registered in #11713), then we should pick one of them and push the job to it.\r\n\r\nIf an error occurs when pushing (and the error indicates the job was not sent to the client), we can pick the next one, and push it to that one. If we cannot decide if the job was sent or not, we should assume it was.\r\n\r\n> **Note** Please challenge this! Let's see what makes for the best UX here.\r\n\n\n deepthidevaki: This issue can be split into two:\r\n\r\n- [x] Adding support for receiving and handling payloads received via a stream. This will be implemented in transport. Stream management in transport receives payload from a broker and handles it over to a registered client stream.\r\n- [ ] Sending the activated jobs to a client. This will be implemented in gateway, which will process the payload handled over from the transport. The gateway know if the client is available or not, and should implement proper error handling such as closing the stream, marking the job re-activatable etc.\n npepinpe: I've extracted the remaining point to a separate issue which the ZPA team will take over. So we can close this for now.",
    "title": "Introduce job receiver in the gateway"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11710",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nOnce #11707, #11708, #11709, and #11713, we should integrate them together and test the complete job stream lifecycle.\r\n\r\n- [x] A client can register a new job stream worker all the way to the broker\r\n- [x] Two stream workers with equivalent properties, on the same gateway, become one on the broker\r\n- [x] Two streams with different properties, on the same gateway, are two different workers on the broker\r\n- [x] When a client call is closed (gracefully or not), and there are no other logically equivalent clients on the gateway, the gateway will remove the stream worker from the broker\r\n- [x] When a client call is closed (gracefully or not), and there is at least one other logically equivalent client on the gateway, the gateway will not remove the stream worker from the broker\r\n- [x] When a gateway shuts down gracefully, it should remove all associated stream workers from the broker.\r\n\r\nI opted against testing with multiple gateways at the QA level. One thing is that our test infrastructure doesn't support multiple gateways (yet). Another is that we already test with multiple members at the transport level (but using integration test), and from a client point of view, it doesn't really matter.\r\n\n",
    "title": "Integrate end-to-end job stream lifecycle management"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11661",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "- Switching should be backward compatible\r\n- We don't delete the existing processors for `Deployment` commands\r\n- We don't delete the existing appliers for `Deployment` events\r\n- Switching happens by changing the processor's logic\r\n- ~~We also need to switch the logic in the `DeploymentRedistributor`~~\r\n- `DeploymentCreateProcessor` should be idempotent\r\n\r\nDepends on\r\n- #11660 \n\n remcowesterhoud: Moving this back to the backlog or now as it got deprioritised",
    "title": "Switch `Deployment:Create` processor to new record distribution behavior"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11590",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "Examples:\r\n- https://github.com/camunda/zeebe/actions/runs/4134636764/jobs/7146014786\r\n- https://github.com/camunda/zeebe/actions/runs/4130548513/jobs/7137350957\r\n\r\nCould be related to MTU or GCP network interference. See https://github.com/zeebe-io/infra/blob/main/gcp/zeebe-io/zeebe-ci-1/runner-arm64-4.yml for the configuration we use for these runners\n\n megglos: Even after runner downgrade (https://github.com/zeebe-io/infra/pull/32 ) we experienced the network issues \r\nhttps://github.com/camunda/zeebe/actions/runs/4173174037/jobs/7228803794\r\n\r\n```\r\n k describe pod actions-runner-arm64-4-fjmsf-rl828                                                                                              \r\n    Image:          summerwind/actions-runner-dind@sha256:c2fff4d42cd3cd456038dbb3372846a588dca5d26da18ac8718cb2195b767d80\r\n```\r\n\r\nAlso checked the network interfaces mtu, actually looked fine:\r\n```\r\nrunner@actions-runner-arm64-4-fjmsf-rl828:/$ ifconfig\r\ndocker0: flags=4099<UP,BROADCAST,MULTICAST>  mtu 1460\r\n...\r\n\r\neth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1460\r\n...\r\n```",
    "title": "ARM64 CI Jobs frequently fail due to connection resets"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/9970",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nIn `BrokerAdminServiceImpl` we distinguish between leaders and followers. For leaders:\r\nhttps://github.com/camunda/zeebe/blob/6720e2e25e9432901c478b62491608d8048343f3/broker/src/main/java/io/camunda/zeebe/broker/system/management/BrokerAdminServiceImpl.java#L202-L208\r\nFor followers:\r\nhttps://github.com/camunda/zeebe/blob/6720e2e25e9432901c478b62491608d8048343f3/broker/src/main/java/io/camunda/zeebe/broker/system/management/BrokerAdminServiceImpl.java#L164-L165\r\n\r\nI believe this is not necessary since followers replay and all information that we return for leaders is also available on followers.\r\n\r\nThis would make writing some integration tests easier, for example for https://github.com/camunda/zeebe/pull/9954\n",
    "title": "Followers should return the full partition status for the `/partitions` actuator"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/9609",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nThe `zeebe-expression-language` depends on the `scheduler` module because it accepts an `ActorClock` as it's source of time. This is a bit inelegant and we should make changes so that  `zeebe-expression-language` doesn't have a direct dependency on `scheduler`, for example by injecting `ZeebeFeelEngineClock` instead of `ActorClock`.\r\n\r\nSee related discussion here: https://github.com/camunda/zeebe/pull/9597#discussion_r907021048\r\n\n\n Zelldon: I see this on our plate, since we are in charge of the scheduler, we should provide an interface that fulfills all needs and that we can use in expression-language.",
    "title": "`expression-language` module shouldn't depend on `scheduler`"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/9245",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nWe should integrate Snyk into our CI to check for dependency vulnerabilities and licensing issues.\r\n\r\nFor licensing issues, refer to https://confluence.camunda.com/pages/viewpage.action?pageId=76485290 to create an appropriate policy.\r\n\r\nFor vulnerabilities, we want a check which fails when introducing vulnerabilities with `Medium` or higher vulnerabilities. To allow for exceptions when the vulnerability does not affect us, you can use a `.snyk` file which you can create/edit with the Snyk CLI.\r\n\r\nWe can use the [Snyk GitHub actions](https://github.com/snyk/actions) to help us out here, as they will be more flexible than the basic GitHub integration.\n\n npepinpe: Hey @megglos, I see the checks are already on PR - is it in testing or is this done? :thinking: \n megglos: Hey @npepinpe ,\r\n\r\nthanks for poking here. As a first iteration I just enabled the PR support from the Web UI to raise awareness.\r\n\r\nWhat is missing is:\r\n- [ ] assess the current license policy in place, haven't digged into it yet, the snyk setup relies within the responsibility of the infosec team now, might be a matter of communication only\r\n- [ ] customize fail conditions, right now the options are based on what is configurable in the [web ui of snyk](https://app.snyk.io/org/team-zeebe/manage/integrations/github) it will fail on any vulnerabilities\r\n- [ ] define how we handle vulnerabilities\n npepinpe: Is it only running PR checks or is it running daily checks? Security vulnerabilities can be found after the fact, so we need both. Last I remember, setting up the daily check via the web UI was very manual and cumbersome :(\n megglos: ZDP-Triage:\n- needs to be updated, @npepinpe will take care of this\n- @npepinpe is already working on this as part of the Snyk work he does currently\n npepinpe: Repurposing this old issue :smiley: \r\n\r\n### Goals\r\n\r\nOne core value we have in terms of DevEx is that we want to automate our workflows as much as possible. If it's not automated, chances are we will either forget, or make an error.\r\n\r\nSo we want to:\r\n\r\n1. Automate vulnerability checks during CI to avoid including known vulnerabilities as early as possible\r\n2. Automate license checking during CI to avoid including commercially problematic licenses such as GPL\r\n3. Continuously monitor our distributed artifacts to be notified of new vulnerabilities \r\n\r\n> **Note**\r\n> In the future, having automated fixes for these would be best, but for now we will rely on our normal dependency update features.\r\n\r\nLet's flesh this out a bit. We want to monitor the following:\r\n\r\n- The HEAD of every supported version branch (aka stable) as the development version for that version\r\n- The HEAD of main as the development version for the next version\r\n- For each supported version, the actual release as the production variant of the version; this includes the Docker image, the Java distribution, the clients, and all modules specified in the Maven BOM.\r\n\r\nWe distinguish between the \"production\" and \"development\" mainly because both can be out of sync, and distinguishing between each is useful as we can then be notified of vulnerabilities  on the production variant, that were automatically fixed via dependabot on the development variant, and require a release.\r\n\r\n### Automation\r\n\r\nThere are three main ways to import projects to Snyk. The Web UI (via the GitHub integration), the CLI, and the REST API. The Web UI is not really automate-able, as it is, well, a UI. So we're left with two options: CLI vs REST API.\r\n\r\n> **NOTE**\r\n> The REST API is an enterprise plan only feature, but we are on one, so as long as we remain on it, all good.\r\n\r\nThe [REST API](https://snyk.docs.apiary.io/#introduction/rest-api) allows us to import projects via one of the integrations (e.g. GitHub). The main advantage there is its tight integration with automatic PR fixes, and automated PR checks. The downside with it is there is no client library for it. We have to manually write our own calls with our own authentication. However, perhaps this is a good use case for our REST connector, and we simply integrate this as part of the Zeebe release process. The REST API allows us to import and delete projects, so we could potentially automate everything through it, including deleting projects once a minor version drops off.\r\n\r\nThe [CLI](https://docs.snyk.io/snyk-cli) is much more straightforward, but is also more limited in what it can do. The CLI lets you test specific projects to automatically obtain SARIF reports about existing vulnerabilities, and also lets you `monitor` your projects. Monitoring will upload a snapshot of your dependency graph (per project) to Snyk, and essentially is the equivalent of the Web UI project import. Snyk will then notify you (via any of the channels) about new vulnerabilities in your dependencies. The advantage of the CLI is it's very easy to integrate into our CI and release process. The downside is that it does not integrate with automated PR checks, fixes, and so on. You can also not delete projects with the CLI, only import them (via the `monitor`) command. Finally, the CLI supports the `.snyk` file, a way to commit your Snyk ignore, exclusion, and trust policies directly with your code. This offers a portable way of propagating ignore rules, which is quite nice, as the web UI does not allow this.\r\n\r\nHere is a sample workflow which would test using the CLI:\r\n\r\n<details><summary>.github/workflows/snyk.yml</summary>\r\n\r\n```yaml\r\nname: Snyk License & Vulnerability Scan\r\n\r\non:\r\n  workflow_dispatch:\r\n    inputs:\r\n      version:\r\n        description: The project version; defaults to the version as defined by the root POM\r\n        required: false\r\n        type: string\r\n      target:\r\n        description: Allows overriding the project target reference directly; defaults to the current branch\r\n        required: false\r\n        type: string\r\n      monitor:\r\n        description: Upload Snyk snapshot instead of test\r\n        required: false\r\n        type: boolean\r\n        default: false\r\n  workflow_call:\r\n    inputs:\r\n      version:\r\n        description: The project version; defaults to the version as defined by the root POM\r\n        required: false\r\n        type: string\r\n      target:\r\n        description: Allows overriding the project target reference directly; defaults to the current branch\r\n        required: false\r\n        type: string\r\n      monitor:\r\n        description: Upload Snyk snapshot instead of test\r\n        required: false\r\n        type: boolean\r\n        default: false\r\n\r\ndefaults:\r\n  run:\r\n    # use bash shell by default to ensure pipefail behavior is the default\r\n    # see https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#exit-codes-and-error-action-preference\r\n    shell: bash\r\n\r\njobs:\r\n  scan:\r\n    name: Snyk Scan\r\n    # Run on self-hosted to make building Zeebe much faster\r\n    runs-on: [ self-hosted, linux, amd64, \"16\" ]\r\n    permissions:\r\n      security-events: write # required to upload SARIF files\r\n    steps:\r\n      - name: Install Snyk CLI\r\n        uses: snyk/actions/setup@master\r\n      - uses: actions/checkout@v3\r\n      - uses: ./.github/actions/setup-zeebe\r\n        with:\r\n          maven-cache-key-modifier: snyk\r\n          secret_vault_secretId: ${{ secrets.VAULT_SECRET_ID }}\r\n          secret_vault_address: ${{ secrets.VAULT_ADDR }}\r\n          secret_vault_roleId: ${{ secrets.VAULT_ROLE_ID }}\r\n      # We need to build the Docker image (and thus the distribution) to scan it\r\n      - uses: ./.github/actions/build-zeebe\r\n        id: build-zeebe\r\n      - uses: ./.github/actions/build-docker\r\n        id: build-docker\r\n        with:\r\n          distball: ${{ steps.build-zeebe.outputs.distball }}\r\n      # Prepares the bash environment for the step which will actually run Snyk, to avoid mixing too\r\n      # much the GitHub Action contexts/syntax and bash itself.\r\n      - name: Build Snyk Environment\r\n        id: info\r\n        run: |\r\n          set -x\r\n          export TARGET=$([[ ! -z '${{ inputs.target }}' ]] && echo '${{ inputs.target }}' || echo \"${GITHUB_HEAD_REF:-${GITHUB_REF#refs/heads/}}\")\r\n          export VERSION=$([[ ! -z '${{ inputs.version }}' ]] && echo '${{ inputs.version }}' || ./mvnw -q -Dexec.executable=echo -Dexec.args='${project.version}' --non-recursive exec:exec 2>/dev/null)\r\n          export VERSION_TAG=$([[ \"${VERSION}\" == *-SNAPSHOT ]] && echo 'development' || echo \"${VERSION}\")\r\n          export LIFECYCLE=$([[ \"${VERSION}\" == *-SNAPSHOT ]] && echo 'development' || echo 'production')\r\n          echo \"SNYK_ARGS=\" \\\r\n            \"--show-vulnerable-paths=all\" \\\r\n            \"--severity-threshold=high\" \\\r\n            \"--org=team-zeebe\" \\\r\n            \"--project-lifecycle=${LIFECYCLE}\" \\\r\n            \"--project-tags=version=${VERSION_TAG}\" \\\r\n            \"--target-reference=${TARGET}\" >> $GITHUB_ENV\r\n          echo \"SNYK_COMMAND=${{ (inputs.monitor && 'monitor') || 'test' }}\" >> $GITHUB_ENV\r\n          echo \"SARIF=${{ (inputs.monitor && '') || 'true' }}\" >> $GITHUB_ENV\r\n          echo \"DOCKER_IMAGE=${{ steps.build-docker.outputs.image }}\" >> $GITHUB_ENV\r\n      - name: Run Snyk\r\n        env:\r\n          SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}\r\n        # The script is set up to scan all distributable artifacts: the projects listed in the bom,\r\n        # the Go client, and the official Docker image.\r\n        # If we add things to the BOM, for example, we should also add them here to the\r\n        # JAVA_PROJECTS variable.\r\n        run: |\r\n          # To avoid exiting on the first failure, we instead flip this to 1 as soon as one of the\r\n          # command fails, and return this at the end; anything non 0 will cause the step to fail\r\n          exitCode=0\r\n          JAVA_PROJECTS=(bom bpmn-model clients/java dist exporter-api gateway-protocol-impl protocol)\r\n          # Remember that when called from a sub-shell, the environment/globals are different\r\n          function output() {\r\n            local sarif=\"$1\"\r\n            local name=\"$2\"\r\n            [ \"${sarif}\" == 'true' ] && echo \"--sarif-file-output=sarif-results/${name}.sarif\"\r\n          }\r\n          # Print out command if debug logging is enabled\r\n          set -x\r\n          snyk \"${SNYK_COMMAND}\" --file=clients/go/go.mod --project-name=clients/go ${SNYK_ARGS} \"$(output \"${SARIF}\" 'go')\" || exitCode=1\r\n          for project in \"${JAVA_PROJECTS[@]}\"; do\r\n          snyk \"${SNYK_COMMAND}\" --file=${project}/pom.xml --project-name=${project} ${SNYK_ARGS} \"$(output \"${SARIF}\" \"${project/\\//-}\")\" || exitCode=1\r\n          done\r\n          snyk container \"${SNYK_COMMAND}\" \"${DOCKER_IMAGE}\" --file=Dockerfile --project-name=camunda/zeebe --exclude-app-vulns ${SNYK_ARGS} \"$(output \"${SARIF}\" 'docker')\" || exitCode=1\r\n          exit \"${exitCode}\"\r\n      # This makes the result of our test available in GitHub Code Scanning (look for the Snyk tools)\r\n      # You can filter by your PR ID, by branch, etc.\r\n      # This step is only executed if we're testing, as otherwise no SARIF files are emitted\r\n      - name: Upload Snyk results to GitHub Code Scanning\r\n        if: ${{ ! inputs.monitor }}\r\n        uses: github/codeql-action/upload-sarif@v2\r\n        with:\r\n          sarif_file: sarif-results/\r\n      - name: Code Scanning summary\r\n        if: ${{ ! inputs.monitor }}\r\n        run: |\r\n          export PR_NUMBER=$(echo $GITHUB_REF | awk 'BEGIN { FS = \"/\" } ; { print $3 }')\r\n          cat >> $GITHUB_STEP_SUMMARY <<EOF\r\n            ## Result Links\r\n            - [Code scanning (PR)](https://github.com/camunda/zeebe/security/code-scanning?query=pr:${PR_NUMBER}+tool:\"Snyk+Container\",\"Snyk+Open+Source\"+is:open)\r\n            - [Code scanning (branch)](https://github.com/camunda/zeebe/security/code-scanning?query=branch:${{ github.ref_name }}+tool:\"Snyk+Container\",\"Snyk+Open+Source\"+is:open)\r\n            - [Snyk projects](https://app.snyk.io/org/team-zeebe/projects)\r\n          EOF\r\n```\r\n\r\n</details>\r\n\r\n\r\n### REST API examples\r\n\r\nImporting a project via the REST API follows this flow:\r\n\r\n- Submit an import job for the projects you want, under a specific integration (e.g. GitHub, Docker Hub) (one REST call)\r\n- Poll the import job status until failed or successful (one REST call on each poll)\r\n- Add the required version tag to each project (one REST call per project)\r\n- Apply the required attributes to each project (one REST call per project)\r\nAs for the REST API\r\nAnd here is a sample REST command (with the credentials omitted obviously) to import a project:\r\n\r\n<details><summary>Import Minor Version</summary>\r\n\r\n```\r\ncurl -XPOST https://api.snyk.io/v1/org/team-zeebe/integrations/<GITHUB_INTEGRATION_ID>/import  \\\r\n  -H 'Content-Type: application/json; charset=utf-8' -H \"Authorization: token $SNYK_TOKEN\" \\\r\n  --data-binary @- <<EOF\r\n  {\r\n    \"target\": {\r\n      \"owner\": \"camunda\",\r\n      \"name\": \"zeebe\",\r\n      \"branch\": \"stable/8.0\"\r\n    },\r\n    \"exclusionGlobs\": \"target,vendor\",\r\n    \"files\": [\r\n      { \"path\": \"protocol/pom.xml\" },\r\n      { \"path\": \"bom/pom.xml\" },\r\n      { \"path\": \"clients/java/pom.xml\" },\r\n      { \"path\": \"clients/go/go.mod\" },\r\n      { \"path\": \"exporter-api/pom.xml\" },\r\n      { \"path\": \"dist/pom.xml\" },\r\n      { \"path\": \"bpmn-model/pom.xml\" }\r\n    ]\r\n  }\r\nEOF\r\ncurl -XPOST https://api.snyk.io/v1/org/team-zeebe/integrations/<DOCKER_INTEGRATION_ID>/import \\\r\n  -H 'Content-Type: application/json; charset=utf-8' -H \"Authorization: token $SNYK_TOKEN\" \\\r\n  -d '{ \"target\": { \"name\": \"camunda/zeebe:8.3.0\" }] }'\r\n```\r\n\r\n</details>\r\n\r\nAnd here's a sample script which would perform the complete import for code projects (i.e. it's missing the Docker import, which would have to be another job):\r\n\r\n<details><summary>snyk-import.sh</summary>\r\n\r\n```shell\r\n#!/bin/bash -eux\r\n\r\n# Secrets to inject\r\nSNYK_TOKEN=\"\"\r\nGITHUB_INTEGRATION_ID=\"\"\r\n\r\nfunction importCodeProjects() {\r\n  curl -s -D - -XPOST \"https://api.snyk.io/v1/org/team-zeebe/integrations/$GITHUB_INTEGRATION_ID/import\"  \\\r\n  -H 'Content-Type: application/json; charset=utf-8' -H \"Authorization: token $SNYK_TOKEN\" \\\r\n  --data-binary @- <<EOF | grep -i '^Location:' | cut -d: -f2- | tr -d \"[:space:]\"\r\n  {\r\n    \"target\": {\r\n      \"owner\": \"camunda\",\r\n      \"name\": \"zeebe\",\r\n      \"branch\": \"stable/8.0\"\r\n    },\r\n    \"exclusionGlobs\": \"target,vendor\",\r\n    \"files\": [\r\n      { \"path\": \"protocol/pom.xml\" },\r\n      { \"path\": \"bom/pom.xml\" },\r\n      { \"path\": \"clients/java/pom.xml\" },\r\n      { \"path\": \"clients/go/go.mod\" },\r\n      { \"path\": \"exporter-api/pom.xml\" },\r\n      { \"path\": \"dist/pom.xml\" },\r\n      { \"path\": \"bpmn-model/pom.xml\" }\r\n    ]\r\n  }\r\nEOF\r\n}\r\n\r\nfunction pollImport() {\r\n  local jobUrl=\"$1\"\r\n  local status=\"pending\"\r\n\r\n  while [ \"${status}\" != \"complete\"  ]; do\r\n    status=$(curl -s ${jobUrl} -H 'Accept: application/json' -H \"Authorization: token $SNYK_TOKEN\" | jq -r '.status')\r\n    if [ \"${status}\" == \"aborted\" ] || [ \"${status}\" == \"failed\" ]; then\r\n      echo \"Import job failed; see ${jobUrl} for more\"\r\n      return 1\r\n    fi\r\n    [ \"${status}\" != \"complete\" ] && sleep 5 # avoid getting throttled\r\n  done\r\n\r\n  return 0\r\n}\r\n\r\nfunction addProjectTag() {\r\n  local projectUrl=\"$1\"\r\n  local version=\"${2:-development}\"\r\n  \r\n  curl -s -XPOST \"${projectUrl}/tags\" \\\r\n    -H 'Accept: application/json' -H 'Content-Type: application/json; charset=utf-8' -H \"Authorization: token $SNYK_TOKEN\" \\\r\n    -d \"{\\\"key\\\": \\\"version\\\", \\\"value\\\": \\\"${version}\\\"}\"\r\n}\r\n\r\nfunction applyProjectAttributes() {\r\n  local projectUrl=\"$1\"\r\n  local lifecycle=\"${2:-development}\"\r\n  \r\n  curl -s -XPOST \"${projectUrl}/attributes\" \\\r\n    -H 'Accept: application/json' -H 'Content-Type: application/json; charset=utf-8' -H \"Authorization: token $SNYK_TOKEN\" \\\r\n    -d \"{\\\"lifecycle\\\": [\\\"${lifecycle}\\\"]}\"\r\n}\r\n\r\nCODE_JOB_URL=$(importCodeProjects \"\")\r\npollImport \"${CODE_JOB_URL}\"\r\n\r\nPROJECT_URLS=$(curl -s ${CODE_JOB_URL} -H 'Accept: application/json' -H \"Authorization: token $SNYK_TOKEN\" | jq -r '.logs[].projects[].projectUrl')\r\nfor project in $PROJECT_URLS; do\r\n  addProjectTag \"${project}\"\r\n  applyProjectAttributes \"${project}\"\r\ndone\r\n```\r\n\r\n</details>\r\n\r\n> **Warning**\r\n> Right now, it seems only Group administrators can add tags/attributes. I'm an organization administrator, but I keep getting a 403. Go figure.\r\n\r\nDeleting projects would be similar and would require us to fetch the list of projects for a specific grouping (e.g. stable/8.0) and delete them.\r\n\r\n### Summary\r\n\r\nAll in all, neither CLI or REST API is an ideal approach. And I still need to flesh out which approach is best for what we want to achieve.\n npepinpe: ## Workflow for CLI integration\r\n\r\nWith the CLI integration, as mentioned, automated PR checks would be more effort to implement. We would have to manually implement a check that compares the base branch results to the target branch result, and extract only the information about new vulnerabilities. Otherwise, we would be failing a PR based on vulnerabilities that are not related to the PR. This could cause tons of PRs to fail simultaneously for the same cause.\r\n\r\nAs such, we would drop checking every PR. Instead, the workflow would be the following:\r\n\r\n- For every stable branch, on every SNAPSHOT deploy, we would `monitor` this version. This would be the `development` variant of that particular supported version. For `main`, this would be our development head.\r\n- On every release (except alphas?), we would also `monitor` the release version, under the appropriate group. This would be our `production` variant for that version.\r\n- The Slack integration would send us notifications about new CVEs every day for each of these variants, so we would at most figure out a PR introduced a vulnerability after 24h (or 72h for a whole weekend).\r\n\r\nThe automation would be done entirely via GitHub actions by hooking into our release workflow as a post-release task.\r\n\r\n> **Warning**\r\n> Deleting a project (or multiple projects) is not do-able via CLI, so we would anyway have to rely on the REST API for that.\r\n\r\nThe advantages of the CLI approach is that it's very easy to implement (one liners), and very easy to understand/maintain. Additionally, we can make use of the `.snyk` file to check in our Snyk configuration, which includes ignore rules, trust policies, etc. The downsides are that we can't use the automated PR checks/fixes available in the GitHub integration, and we can't delete projects (so we still need either a manual task to do so, or using the REST API).\r\n\r\n## Workflow with the REST API\r\n\r\nFor the REST API, integration would be done via Zeebe, using our release process. A new BPMN process would be created to model the process of importing Snyk projects, and deleting the previous, now unsupported version.\r\n\r\n> **Note**\r\n> Ideally we can mostly use the REST connector, but I believe for the import job it might not be possible, as the result is not returned via the response by via the `Location` header.\r\n\r\nHere is what the import process could look like:\r\n\r\n![image](https://github.com/camunda/zeebe/assets/43373/a725a0d2-17d1-45fc-be27-75b0c1d2b2fc)\r\n\r\n> **Warning**\r\n> Currently there are some permission issues with tagging, but I will try to sort this out with our admins.\r\n\r\nWe would hook into the main release process and add the above as a post-release call activity. Additionally, we would also add a manual task which would trigger the deletion (via another call activity) to delete the previous, now unsupported minor version.\r\n\r\n> **Note**\r\n> Whether we pick CLI or REST, we likely need to do the deletion via REST anyway.\r\n\r\nSince this would import projects using the GitHub and Docker Hub integrations, there is no need to run something on every SNAPSHOT push. Instead, importing the projects once is enough, and Snyk will refresh them regularly. Additionally, the automated PR checks will work with the given projects.\r\n\r\n> **Warning**\r\n> Project names are mangled often in the PR check, seems it cannot handle which base branch to use. But it will scan the appropriate project, so I think it's OK.\r\n\r\nAdvantages with this approach is that it's easy to understand via the BPMN, and all tasks are confined to the release process. We also get access to automated PR checks and fixes. Downside here is fiddling with a poorly documented API, there is no available client, and it's harder to test locally obviously as you need to use the deployed Zeebe process and workers. We also can't make use of the `.snyk` file, meaning that ignores and the likes are manual and not transparent :(",
    "title": "Use Snyk to check for license issues or dependency vulnerabilities"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/8846",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nAs we use SpotBugs ourselves, and the FindBugs project is dormant, we should replace any and all usage of its annotations with SpotBugs. This means replacing:\r\n\r\n```xml\r\n<dependency>\r\n  <groupId>com.google.code.findbugs</groupId>\r\n  <artifactId>jsr305</artifactId>\r\n</dependency>\r\n```\r\n\r\nWith:\r\n\r\n```xml\r\n<dependency>\r\n  <groupId>com.github.spotbugs</groupId>\r\n  <artifactId>spotbugs-annotations</artifactId>\r\n</dependency>\r\n\r\n<dependency>\r\n  <groupId>net.jcip</groupId>\r\n  <artifactId>jcip-annotations</artifactId>\r\n</dependency>\r\n```\r\n\r\nThe source here is the SpotBugs documentation itself. And as we do rely on SpotBugs, I would encourage us exploring making better use of their annotations to leverage the tool.\r\n\r\nNOTE: this might also mean replacing usages of any other `javax.annotation` or JSR305 implementations with SpotBugs's own, if possible. I don't think we use any other, but I could be wrong.\n\n menski: Potential topic for the weeks around the next release for someone to pick-up, no urgency right now, therefore I removed it from the board.",
    "title": "Replace usages of JSR305 from FindBugs with SpotBugs annotations"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/8571",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nThe DMN engine parses a DMN resource into an in-memory data object that is used to evaluate the decision. The parsing of DMN decisions is (relatively) costly. Instead of parsing it every time, we can cache the parsed DMN (e.g. using an LRU cache).\r\n\r\n* store the parsed DMN decisions in a cache\r\n* the cache should have a fixed capacity (i.e. using an LRU cache - similar to the [cache from agrona](https://github.com/real-logic/agrona/blob/master/agrona/src/main/java/org/agrona/collections/IntLruCache.java))\r\n* the cache may be used transparently in a behavior class and is filled by the DMN state\r\n\r\n\r\nRelates to https://jira.camunda.com/browse/SUPPORT-17656\r\n\n\n megglos: A PoC to assess the impact of this improvement would be a great first scope.\n megglos: ZDP-Planning:\r\n- it surfaced for 3-4+ customers so far\r\n- PoC focus would be: build a prototype to be evaluated in a performance test\n megglos: As synced with @remcowesterhoud here https://github.com/camunda/zeebe/pull/13713#issuecomment-1663557887\r\n**we need to include this into the next iteration aiming for the September patch or earlier for two customers**",
    "title": "Avoid parsing DMN decisions for every evaluation"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/6034",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\n\r\nGo 1.16 will have native support for embedding files so, after updating, we can use that and remove go-bindata as a dependency.\r\nhttps://tip.golang.org/doc/go1.16\n\n Zelldon: I think this should be a quick thing to do, we already moved to go 1.17\n abbasadel: We already moved to 1.19 in #12633. Does this issue still make sense?\n npepinpe: Yes :) The idea is to migrate off of using the go-bindata utility and use the built-in compiler capabilities via the `go:embed` annotation. Definitely worth doing to reduce our dependencies on external projects which may eventually prevent us from, say, migrating to another Go version.\n npepinpe: Actually, I might take a crack a it, because I think we can solve this without either go-bindata or go:embed, and just via build flags.\r\n\r\nEDIT: nevermind, I remember now why we couldn't do this - then all users would need to pass the right build flag whenever they build their own application :smile: ",
    "title": "Remove go-bindata after Go1. 16"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/6003",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nTo help tune the config parameters of Swim protocol, it would be useful to add some metrics. \r\nExamples:\r\n- Probe latency - RTT for a probe request\r\n- Gossip latency - how long until a metadata change on a node is propagated to another node.\r\n\r\nRelated to https://github.com/zeebe-io/zeebe/issues/4827#issuecomment-740624583 https://github.com/zeebe-io/zeebe/issues/4827#issuecomment-740768778\r\n\r\n\n\n Zelldon: I feel with adding messaging service metrics we this covered as well https://github.com/camunda/zeebe/pull/11353 we have labels for the message types, which allows us to see also probe and sync req-response latencies etc.\r\n\r\nwdyt @npepinpe \n Zelldon: I could add a separate section to the dashboard for swim using these metrics\n rodrigo-lourenco-lopes: Since we are broadcasting the gossip of these updates with no answers, how could we go about measuring this gossip latency?\r\nHow about implementing a response for these broadcasts along with the metric and putting everything behind a feature flag?\r\nOr ideally, there is a more straightforward way to measure this.\r\n\r\nhttps://github.com/camunda/zeebe/blob/5280440c43122cf2da5b4b83dbff76a644273e7c/atomix/cluster/src/main/java/io/atomix/cluster/protocol/SwimMembershipProtocol.java#LL753C1-L758C4\r\n\r\nwdyt? @npepinpe @Zelldon \n npepinpe: Honestly, it sounds to me like distributed tracing would be the tool we want to have here. Start an operation somewhere, measure when it completes somewhere else, possibly with hops.\r\n\r\nAs we don't have that yet, I would postpone this. However, there may be other metrics we'd like just from the local node?\n rodrigo-lourenco-lopes: The other thing we could measure perhaps is sync() we have a response for this one.\n oleschoenburg: We could also try to just export the current state as metrics. Then we could _derive_ additional properties such as \"how long does it take to propagate changes throughout the entire cluster\" by calculating it on the metrics. Not ideal but I feel like just exporting the current state of SWIM as metrics is useful already.\r\n\n rodrigo-lourenco-lopes: @oleschoenburg If I understood correctly we would export the local state of all members on each node? ",
    "title": "Add metrics to SwimMembershipProtocol"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/5121",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nWe should reevaluate the banning concept, whether it makes sense in the way it is currently built and how we can improve it.\r\nCurrently we store banned instances forever, which means we can end up in situations where we have a lot of banned instances, which can't be removed from the state.\r\n\n\n Zelldon: Do we have a time plan for this? I think there was already some decisions that we want to replace it with incidents right?",
    "title": "Reevaluate Banning concept"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13058",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Documentation"
    },
    "gitHubIssueText": "As we're switching the Deployment Distribution over on the Generalized Record Distribution (aka Command Distribution), we should document how the logic of distribution works. This description should be general, but also include a section on resource deployments.\n\n remcowesterhoud: I question the value of including a section on resource deployments. Distribution is generic, it will work the same for all commands. I don't see a need to touch upon specific cases.\r\nI could add a full example and use resource deployment for this, but other than that I wouldn't go deeper into it.",
    "title": "Document new Deployment Distribution logic"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12584",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Documentation"
    },
    "gitHubIssueText": "**Description**\r\n\r\nAdd a guide on what to do when a flaky test is encountered. The guide should enable contributors to make progress when flaky tests occur in their contributions.\r\n\r\nThis was an action derived from one of the ZPA team's recent retros.\r\n\r\n\n",
    "title": "Document guidelines on how to handle flaky tests"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/14528",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/14518",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/14516",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/14514",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/14512",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/14493",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/14239",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/14184",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/14074",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/14071",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/13816",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/13376",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/13168",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/13069",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/12633",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/12534",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/12402",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/12263",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/12174",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/12170",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5496",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5493",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5274",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5488",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5467",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5452",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5486",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5483",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5484",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5456",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5231",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5429",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5463",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5457",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5404",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5444",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5143",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5418",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5329",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5316",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5307",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5399",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5330",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5332",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5331",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5324",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5323",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5292",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5277",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5322",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5321",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5303",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5244",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5243",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5533",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5518",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5501",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5297",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5311",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5489",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5492",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5465",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5451",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5473",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5468",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5474",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5461",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5458",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5421",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5250",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5249",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5381",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5446",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5434",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5391",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5382",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5365",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5341",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5228",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5373",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5337",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5538",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5536",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5524",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5502",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5509",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5513",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5503",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5355",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5411",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5440",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5424",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5476",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5481",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5478",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5469",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5475",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5427",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5426",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5437",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5425",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5454",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5433",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5460",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5438",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5449",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5450",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5199",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5453",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5263",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5325",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5406",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5335",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5378",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5393",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5375",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5448",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4915",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5443",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5423",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5420",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5402",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5383",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5374",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5400",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5405",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5338",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5371",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5259",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5268",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5363",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5328",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4983",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5336",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5220",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5304",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5239",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5320",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3594",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2142",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2128",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2139",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2123",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2119",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2078",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2091",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2089",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2072",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2062",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2044",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2050",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2028",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2047",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2036",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2026",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2025",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2024",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2023",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2033",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1980",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1962",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1961",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1963",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1878",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1837",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1830",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1823",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1827",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1818",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1816",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1791",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1789",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1749",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2143",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2137",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2132",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2130",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2099",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2093",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2092",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2081",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1972",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2080",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2077",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2061",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2064",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2058",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2059",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2056",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2040",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2022",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2021",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2020",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2019",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2018",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2017",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2016",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2015",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2014",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2013",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2012",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2002",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1717",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1968",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1953",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1941",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1940",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1933",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1932",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1931",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1921",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1912",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1899",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1907",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1890",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1881",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1889",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1886",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1879",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1876",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1874",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1814",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1825",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1824",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1804",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1780",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1775",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1768",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1758",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1757",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1756",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1737",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2146",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2140",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2127",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2079",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2076",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2075",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2071",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2070",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2027",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2055",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2053",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2052",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2051",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2041",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2042",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2039",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2037",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2038",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2035",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2034",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2006",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2007",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2011",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2010",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2009",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2008",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2005",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2004",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2003",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2001",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2000",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1999",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1998",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1997",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1984",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1986",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1985",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1965",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1964",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1958",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1951",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1954",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1952",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1947",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1949",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1948",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1946",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1945",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1944",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1942",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1943",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1934",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1939",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1937",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1935",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1925",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1924",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1908",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1923",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1919",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1922",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1918",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1916",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1917",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1914",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1913",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1904",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1911",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1909",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1910",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1896",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1906",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1897",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1902",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1877",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1895",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1893",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1894",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1891",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1892",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1849",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1883",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1888",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1882",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1875",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1865",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1863",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1864",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1861",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1862",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1856",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1854",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1852",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1834",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1850",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1839",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1829",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1838",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1833",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1836",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1832",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1826",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1828",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1820",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1822",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1808",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1806",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1807",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1805",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1798",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1795",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1800",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1533",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1796",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1794",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1781",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1782",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1792",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1777",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1778",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1776",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1759",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1772",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1769",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1764",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1765",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1761",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1747",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1755",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1754",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1753",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1752",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1735",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1736",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14019",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Broker",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\n\r\nFor multi-tenancy we need to store a tenant id in the state. We've decided to do this using a new object: `DbTenantAwareKey`.\r\n\r\nThis key will implement the `DbKey` interface. It will always contain a `DbString` which is used to store the tenantId. Besides this it can wrap any other key.\r\n\r\nEg:\r\n```java\r\npublic record DbTenantAwareKey<WrappedKey extends DbKey>(DbString tenantKey, WrappedKey wrappedKey)\r\n    implements DbKey\r\n```\n",
    "title": "Create `DbTenantAwareKey`"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13988",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Broker",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nZeebe auth data (ex. the user’s tenant access list) should be sent from the gateway to the broker in a way that is easily extendable in the future. Extendability is important since Zeebe might need to support user permissions (https://github.com/camunda/product-hub/issues/495) in the future, and it would be better if we already have the code that can contain these future extensions without any significant changes.\r\n\r\nIdeally, the data interchange protocol/mechanism should be pluggable, so that Zeebe has the ability to move to something different in the future.\r\n\r\nFor this iteration, unsigned JWT tokens will be used, for the following reasons:\r\n* Using JWT is future-proof.\r\n   * If we decide to export auth data in the future, we can use signed JWTs (JWS), so users have a high confidence that the auth data is valid.\r\n* JWTs are already used by other C8 components, so we ensure wider compatibility.\r\n* We already have TLS secure communication so we can trust unsigned JWT tokens (for this iteration).\r\n* Lower implementation effor than other options (ex. MsgPack), as JWT libraries already provide APIs to work with different Java types.\r\n\r\n## AC\r\n- [x] A general API for auth data encoding is available.\r\n- [x] A general API for auth data decoding is available.\r\n- [x] Auth data can be encoded in a JWT token string\r\n- [x] Auth data can be decoded from a JWT token string\n",
    "title": "Provide an API for data interchange of Zeebe auth data"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13987",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Broker",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nA `TenantAccessChecker` class should be implemented in the `engine` module. It should be used by `*Processor` classes, or any other classes that process `Command` records created by Client requests. The purpose of the class is to determine if the User making the request has access to the requested Tenant data.\r\n\r\nThe class should provide the following methods:\r\n* `hasAccess(String tenantId, , List<String> authorizedTenants)` - to determine if a user has access to data from a given tenant. The output can be `Either<Exception, String>` providing the tenantId.\r\n   * If the `tenantId` is listed in the `RecordValueWithTenantPermissions`, the `tenantId` is returned. Otherwise, an error is raised.\r\n* `hasFullAccess(List<String> tenantIds, List<String> authorizedTenants)` - to determine if a user has access to all of the `tenantIds` provided in a list. This will be used later for job polling.\r\n\r\n:information_source: We might expand or refactor this class according to different needs as development on the multi-tenancy topic progresses.\r\n\r\n### AC\r\n- [x] A `TenantAccessChecker` class\r\n- [x] Test coverage for the `TenantAccessChecker` class\n",
    "title": "Provide a `TenantAccessChecker` API for multi-tenancy"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13989",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Broker",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nZeebe auth data (ex. the user’s tenant access list) should be sent from the gateway to the broker in a way that is easily extendable in the future. Extendability is important since Zeebe might need to support user permissions (https://github.com/camunda/product-hub/issues/495) in the future, and it would be better if we already have the code that can contain these future extensions without any significant changes.\r\n\r\nSince Zeebe auth data may be used in all types of Zeebe Records, it would be better to place it in the `RecordMetadata` since:\r\n* It is a single change that makes auth data available to all records.\r\n* Conceptually, auth data is more closely related to `Intent`s which are already placed in the `RecordMetadata`.\r\n* It doesn't pollute the record value since auth data is irrelevant to the outcome of commands.\r\n\r\nWhen placed in the `RecordMetadata` the auth data should be encoded so that:\r\n* It can be extracted only when needed\r\n* It can be extended without any further changes to the `RecordMetadata` strucutre\r\n\r\n### AC\r\n- [x] Auth data is contained in `RecordMetadata`\r\n- [x] Auth data is encoded within the `RecordMetadata`\r\n- [x] Auth data contains a flag to indicate the mechanism used to encode/decode it\r\n- [x] Auth data is contained in `ExecuteCommandRequest` (to enable Gateway-to-Broker requests)\r\n\n",
    "title": "Add auth data to Zeebe records"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13988",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Gateway",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nZeebe auth data (ex. the user’s tenant access list) should be sent from the gateway to the broker in a way that is easily extendable in the future. Extendability is important since Zeebe might need to support user permissions (https://github.com/camunda/product-hub/issues/495) in the future, and it would be better if we already have the code that can contain these future extensions without any significant changes.\r\n\r\nIdeally, the data interchange protocol/mechanism should be pluggable, so that Zeebe has the ability to move to something different in the future.\r\n\r\nFor this iteration, unsigned JWT tokens will be used, for the following reasons:\r\n* Using JWT is future-proof.\r\n   * If we decide to export auth data in the future, we can use signed JWTs (JWS), so users have a high confidence that the auth data is valid.\r\n* JWTs are already used by other C8 components, so we ensure wider compatibility.\r\n* We already have TLS secure communication so we can trust unsigned JWT tokens (for this iteration).\r\n* Lower implementation effor than other options (ex. MsgPack), as JWT libraries already provide APIs to work with different Java types.\r\n\r\n## AC\r\n- [x] A general API for auth data encoding is available.\r\n- [x] A general API for auth data decoding is available.\r\n- [x] Auth data can be encoded in a JWT token string\r\n- [x] Auth data can be decoded from a JWT token string\n",
    "title": "Provide an API for data interchange of Zeebe auth data"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13987",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Gateway",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nA `TenantAccessChecker` class should be implemented in the `engine` module. It should be used by `*Processor` classes, or any other classes that process `Command` records created by Client requests. The purpose of the class is to determine if the User making the request has access to the requested Tenant data.\r\n\r\nThe class should provide the following methods:\r\n* `hasAccess(String tenantId, , List<String> authorizedTenants)` - to determine if a user has access to data from a given tenant. The output can be `Either<Exception, String>` providing the tenantId.\r\n   * If the `tenantId` is listed in the `RecordValueWithTenantPermissions`, the `tenantId` is returned. Otherwise, an error is raised.\r\n* `hasFullAccess(List<String> tenantIds, List<String> authorizedTenants)` - to determine if a user has access to all of the `tenantIds` provided in a list. This will be used later for job polling.\r\n\r\n:information_source: We might expand or refactor this class according to different needs as development on the multi-tenancy topic progresses.\r\n\r\n### AC\r\n- [x] A `TenantAccessChecker` class\r\n- [x] Test coverage for the `TenantAccessChecker` class\n",
    "title": "Provide a `TenantAccessChecker` API for multi-tenancy"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13989",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Gateway",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nZeebe auth data (ex. the user’s tenant access list) should be sent from the gateway to the broker in a way that is easily extendable in the future. Extendability is important since Zeebe might need to support user permissions (https://github.com/camunda/product-hub/issues/495) in the future, and it would be better if we already have the code that can contain these future extensions without any significant changes.\r\n\r\nSince Zeebe auth data may be used in all types of Zeebe Records, it would be better to place it in the `RecordMetadata` since:\r\n* It is a single change that makes auth data available to all records.\r\n* Conceptually, auth data is more closely related to `Intent`s which are already placed in the `RecordMetadata`.\r\n* It doesn't pollute the record value since auth data is irrelevant to the outcome of commands.\r\n\r\nWhen placed in the `RecordMetadata` the auth data should be encoded so that:\r\n* It can be extracted only when needed\r\n* It can be extended without any further changes to the `RecordMetadata` strucutre\r\n\r\n### AC\r\n- [x] Auth data is contained in `RecordMetadata`\r\n- [x] Auth data is encoded within the `RecordMetadata`\r\n- [x] Auth data contains a flag to indicate the mechanism used to encode/decode it\r\n- [x] Auth data is contained in `ExecuteCommandRequest` (to enable Gateway-to-Broker requests)\r\n\n",
    "title": "Add auth data to Zeebe records"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13752",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nWhenever I'm using the Java client with variables, I can end up writing `job.getVariablesAsMap().get(\"name\")` a lot. Of course, I can use `getVariablesAsType` with a record; but a shorthand method would be useful.\r\n\r\n**Describe the solution you'd like**\r\n`job.getVariable(\"name\")`.\r\n\r\nUnder the hood, this makes a call to `job.getVariablesAsMap()`, and *caches* the map (to make it as performant as deserialising to a map, then accessing various variables in user code), then returns `.get(\"name\")` from that map.\r\n\r\n**Additional Context**\r\n\r\nYes, there are better ways to do the whole thing - including deserialising to an object/class/record or using Springboot. \r\n\r\nHowever, for first experience with the platform (including C8 Platform training), reducing ceremonial boilerplate will be good. \r\n\n\n korthout: I believe this would be useful (reasonable desire -> `impact/medium`), and the effort is `small`\n korthout: ZPA Triage:\n- the simple solution (this just being a convenience method) seems low effort, size: x-small\n- a more performant solution could be done later\n- @jwulf would you be willing to contribute this change?\n- we're marking it as later for our own priorities\n remcowesterhoud: @abbasadel FYI the team thinks that this is low-hanging fruit. However, it doesn't show up on the board. Could we improve the query in the board to also include issues that are x-small and medium impact?",
    "title": "job.getVariable ergonomic method in Java client"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/4700",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\n\r\n> We want to provide metrics about our zeebe workers to better monitor them. Specifically, how many jobs a worker has scheduled.\r\n\r\nThe goal here is to align the Java client with the Go client and allow users to better monitor their workers. See #4500 \r\n\r\n**Describe the solution you'd like**\r\n\r\nI would like to be able to add a metrics facade (whether an interface or something like Micrometer is to be discussed) to a job worker to monitor the amount of jobs currently enqueued. You can have a look at the Go solution in #4501 \r\n\r\n**Describe alternatives you've considered**\r\n\r\nImplementing your own JobWorker (which is, after all, a QoL feature).\r\n\n\n npepinpe: Proposal would be to add the following metrics:\r\n\r\n- Count of jobs activated\r\n  - The rate can be derived from this count\r\n- Count of jobs handled\r\n  - The rate can be derived from this count\r\n  - The amount of non-handled jobs can also be derived by subtracting both series\r\n\r\nAs the last one may not be so accurate, we could also provide a count of the queued jobs. This would be a sum of remaining jobs and a new counter related to jobs streamed. We can't just use `remainingJobs` since that is only used for polling, and we don't want to use it for streaming as well since polling should remain independent.\r\n\r\nWe will limit it to this, as most of the other things can be effectively measured by users at the moment:\r\n\r\n- Observing the back off supplier can be done by wrapping the default one at the moment and instrumenting the one method for it\r\n- Observing the executor can be done by passing a custom, instrumented executor\r\n",
    "title": "Introduce JobWorker metrics for the Java client"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13321",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "### Description\r\n\r\nThe Java client provides a `DeployResourceCommand` for performing resource deployments to Zeebe. This command should support multi-tenancy by exposing an **optional** `tenantId` property/method.\r\n\r\nThe following error codes may be returned:\r\n* PERMISSION_DENIED (code: 7) \r\n   * when a user attempts to access data of a tenant they are not authorized for, when multi-tenancy is enabled.\r\n* INVALID_ARGUMENT (code: 3)\r\n   * For a provided tenant id, when multi-tenancy is disabled\r\n   * For a missing tenant id, when multi-tenancy is enabled\r\n   * For an invalid tenant id (i.e. doesn't match the pre-defined format), when multi-tenancy is enabled.\r\n\r\n### AC\r\n- [x] A `ClientProperties#DEFAULT_TENANT_ID` with value `zeebe.client.tenantId` is defined.\r\n- [x] The `ZeebeClientBuilderImpl` class is expanded with a `defaultTenantId` property.\r\n       - The `ZeebeClientBuilderImpl#withProperties(...)` method may set the `defaultTenantId` property to a value defined by `zeebe.client.tenantId` in a `.properties` file (see point above).\r\n- [x] The `DeployResourceCommand` provides a new `tenantId(String tenantId)` method.\r\n       - The command will set the `tenantId` to the value of `zeebe.client.tenantId` if provided through a `.properties` file.\r\n       - The default value if the `tenantId` property should be `null`.\r\n\r\n### Blocked by\r\n- #13319\r\n\r\n### Blocks\r\n- [Web Modeler](https://github.com/camunda/web-modeler/issues/5058)\r\n- [Desktop Modeler](https://github.com/camunda/camunda-modeler/issues/3716)\n",
    "title": "Java client supports multi-tenancy for DeployResource RPC"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14044",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nObserved 100% backpressure on one partition. On further investigation, we found that there was back-to-back role transition.\r\n\r\nThe node was leader for partition 2. It transitioned to follower. The transition was cancelled in between because it became leader again.\r\n\r\nBecause of [this fix](https://github.com/camunda/zeebe/pull/13541), command api is not notified that it became follower. So when it transitioned to leader again, it reuse the limiter from previous leader role because it was not removed. https://github.com/camunda/zeebe/blob/615c751216c3fdc99493792d8f1c19644633d275/broker/src/main/java/io/camunda/zeebe/broker/transport/backpressure/PartitionAwareRequestLimiter.java#L147\r\n\r\nThe partition started processing when a new leader was elected after a few hours.\r\n\r\n**To Reproduce**\r\n\r\nNot easy to reproduce. Leader -> Follower -> Leader transition should happen where the follower transition is cancelled.\r\n\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.2.12\r\n\n",
    "title": "Backpressure queue is not reset when back-to-back role transitions"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13936",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nIt seems that an Inclusive Gateway with a single outgoing Sequence Flow ignores the Condition\r\n\r\n<img width=\"974\" alt=\"Screenshot 2023-08-17 at 09 52 07\" src=\"https://github.com/camunda/zeebe/assets/3511026/57eb2d2d-8646-4947-b7c6-3aa05472521a\">\r\n\r\nThe BPMN spec is quite clear about the expected behavior:\r\n\r\n> A default path can optionally be identified, to be taken in the event that none of the conditional `Expressions` evaluate to `true`. If a default path is not specified and the **Process** is executed such that none of the conditional `Expressions` evaluates to `true`, a runtime exception occurs. - [10.6.3 Inclusive Gateway](https://www.omg.org/spec/BPMN/2.0.2/PDF#10.6.3%20Inclusive%20Gateway)\r\n\r\nFor C8, such a runtime exception should be represented by [an incident](https://docs.camunda.io/docs/next/components/concepts/incidents/) at the inclusive gateway.\r\n\r\nOriginally reported on the forums: https://forum.camunda.io/t/strange-behavior-with-only-one-sequence-flow-with-condition/46938\r\n\r\n**To Reproduce**\r\n\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\n- deploy a process with an inclusive gateway that has one outgoing sequence flow with the condition `= false`\r\n- create an instance of it\r\n- notice that the sequence flow is taken and that no incident is raised\r\n\r\n**Expected behavior**\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\nAn incident is raised at the inclusive gateway\n\n korthout: Mid severity, because there is a workaround: use two outgoing sequence flows - set the second one to `= false` and let it flow to a none end event\n lzgabel: 👋 Hi @korthout. Please assign this task to me. I'll take a look.",
    "title": "Condition ignored on Inclusive Gateway with singular outgoing Sequence Flow"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13093",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nRunning `RandomizedRaftTest.consistencyTestWithSnapshot` on 8.0.17 leads to the following exception:\r\n```\r\njava.lang.IllegalStateException: Expected to delete index after 404, but it is lower than the commit index 405. Deleting committed entries can lead to inconsistencies and is prohibited. at io.atomix.raft.storage.log.RaftLog.deleteAfter(RaftLog.java:186) at io.atomix.raft.roles.PassiveRole.replaceExistingEntry(PassiveRole.java:623) at io.atomix.raft.roles.PassiveRole.tryToAppend(PassiveRole.java:562) at io.atomix.raft.roles.PassiveRole.appendEntries(PassiveRole.java:514) at io.atomix.raft.roles.PassiveRole.handleAppend(PassiveRole.java:370) at io.atomix.raft.roles.ActiveRole.onAppend(ActiveRole.java:50) at io.atomix.raft.roles.FollowerRole.onAppend(FollowerRole.java:187) at io.atomix.raft.impl.RaftContext.lambda$registerHandlers$13(RaftContext.java:263) at \r\n...\r\n```\r\n\r\nSee [Test results for unit tests(1).zip](https://github.com/camunda/zeebe/files/11734753/Test.results.for.unit.tests.1.zip) from test run https://github.com/camunda/zeebe/actions/runs/5255172343/jobs/9494733250\r\n\r\n**To Reproduce**\r\n| jqwick | explanation |\r\n|--------|--------|\r\n| tries = 10                    | # of calls to property |\r\n| checks = 10                   | # of not rejected calls |\r\n| generation = RANDOMIZED       | parameters are randomly generated |\r\n| after-failure = PREVIOUS_SEED | use the previous seed |\r\n| when-fixed-seed = ALLOW       | fixing the random seed is allowed |\r\n| edge-cases#mode = NONE        | edge cases are not explicitly generated |\r\n| seed = 2349382887260487435    | random seed to reproduce generated values |\r\n\r\n**Expected behavior**\r\n\r\nTest does not fail.\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.lang.IllegalStateException: Expected to delete index after 404, but it is lower than the commit index 405. Deleting committed entries can lead to inconsistencies and is prohibited. at io.atomix.raft.storage.log.RaftLog.deleteAfter(RaftLog.java:186) at io.atomix.raft.roles.PassiveRole.replaceExistingEntry(PassiveRole.java:623) at io.atomix.raft.roles.PassiveRole.tryToAppend(PassiveRole.java:562) at io.atomix.raft.roles.PassiveRole.appendEntries(PassiveRole.java:514) at io.atomix.raft.roles.PassiveRole.handleAppend(PassiveRole.java:370) at io.atomix.raft.roles.ActiveRole.onAppend(ActiveRole.java:50) at io.atomix.raft.roles.FollowerRole.onAppend(FollowerRole.java:187) at io.atomix.raft.impl.RaftContext.lambda$registerHandlers$13(RaftContext.java:263) at io.atomix.raft.impl.RaftContext.lambda$runOnContext$20(RaftContext.java:274) at io.atomix.raft.DeterministicSingleThreadContext$WrappedRunnable.run(DeterministicSingleThreadContext.java:129) at org.jmock.lib.concurrent.DeterministicScheduler$CallableRunnableAdapter.call(DeterministicScheduler.java:176) at org.jmock.lib.concurrent.DeterministicScheduler$ScheduledTask.run(DeterministicScheduler.java:251) at org.jmock.lib.concurrent.DeterministicScheduler.runNextPendingCommand(DeterministicScheduler.java:66) at io.atomix.raft.ControllableRaftContexts.runNextTask(ControllableRaftContexts.java:246) at io.atomix.raft.RaftOperation.lambda$getDefaultRaftOperations$0(RaftOperation.java:62) at io.atomix.raft.RaftOperation.run(RaftOperation.java:42) at io.atomix.raft.RandomizedRaftTest.consistencyTest(RandomizedRaftTest.java:123) at io.atomix.raft.RandomizedRaftTest.consistencyTestWithSnapshot(RandomizedRaftTest.java:88) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:568) at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725) at org.junit.platform.commons.support.ReflectionSupport.invokeMethod(ReflectionSupport.java:198) at net.jqwik.engine.execution.CheckedPropertyFactory.lambda$createRawFunction$1(CheckedPropertyFactory.java:84) at net.jqwik.engine.execution.CheckedPropertyFactory.lambda$createRawFunction$2(CheckedPropertyFactory.java:91) at net.jqwik.engine.properties.CheckedFunction.execute(CheckedFunction.java:17) at net.jqwik.api.lifecycle.AroundTryHook.lambda$static$0(AroundTryHook.java:57) at net.jqwik.engine.execution.lifecycle.HookSupport.lambda$wrap$2(HookSupport.java:48) at net.jqwik.engine.hooks.lifecycle.TryLifecycleMethodsHook.aroundTry(TryLifecycleMethodsHook.java:57) at net.jqwik.engine.execution.lifecycle.HookSupport.lambda$wrap$3(HookSupport.java:53) at net.jqwik.engine.execution.CheckedPropertyFactory.lambda$createTryExecutor$0(CheckedPropertyFactory.java:60) at net.jqwik.engine.execution.lifecycle.AroundTryLifecycle.execute(AroundTryLifecycle.java:23) at net.jqwik.engine.properties.GenericProperty.testPredicate(GenericProperty.java:166) at net.jqwik.engine.properties.GenericProperty.check(GenericProperty.java:68) at net.jqwik.engine.execution.CheckedProperty.check(CheckedProperty.java:67) at net.jqwik.engine.execution.PropertyMethodExecutor.executeProperty(PropertyMethodExecutor.java:90) at net.jqwik.engine.execution.PropertyMethodExecutor.executeMethod(PropertyMethodExecutor.java:69) at net.jqwik.engine.execution.PropertyMethodExecutor.lambda$execute$0(PropertyMethodExecutor.java:49) at net.jqwik.api.lifecycle.AroundPropertyHook.lambda$static$0(AroundPropertyHook.java:46) at net.jqwik.engine.execution.lifecycle.HookSupport.lambda$wrap$0(HookSupport.java:26) at net.jqwik.engine.hooks.lifecycle.PropertyLifecycleMethodsHook.aroundProperty(PropertyLifecycleMethodsHook.java:57) at net.jqwik.engine.execution.lifecycle.HookSupport.lambda$wrap$1(HookSupport.java:31) at net.jqwik.engine.execution.lifecycle.HookSupport.lambda$wrap$0(HookSupport.java:26) at net.jqwik.engine.hooks.statistics.StatisticsHook.aroundProperty(StatisticsHook.java:37) at net.jqwik.engine.execution.lifecycle.HookSupport.lambda$wrap$1(HookSupport.java:31) at net.jqwik.engine.execution.lifecycle.HookSupport.lambda$wrap$0(HookSupport.java:26) at net.jqwik.engine.hooks.lifecycle.AutoCloseableHook.aroundProperty(AutoCloseableHook.java:13) at net.jqwik.engine.execution.lifecycle.HookSupport.lambda$wrap$1(HookSupport.java:31) at net.jqwik.engine.execution.PropertyMethodExecutor.execute(PropertyMethodExecutor.java:47) at net.jqwik.engine.execution.PropertyTaskCreator.executeTestMethod(PropertyTaskCreator.java:166) at net.jqwik.engine.execution.PropertyTaskCreator.lambda$createTask$1(PropertyTaskCreator.java:51) at net.jqwik.engine.execution.lifecycle.CurrentDomainContext.runWithContext(CurrentDomainContext.java:28) at net.jqwik.engine.execution.PropertyTaskCreator.lambda$createTask$2(PropertyTaskCreator.java:50) at net.jqwik.engine.execution.pipeline.ExecutionTask$1.lambda$execute$0(ExecutionTask.java:31) at net.jqwik.engine.execution.lifecycle.CurrentTestDescriptor.runWithDescriptor(CurrentTestDescriptor.java:17) at net.jqwik.engine.execution.pipeline.ExecutionTask$1.execute(ExecutionTask.java:31) at net.jqwik.engine.execution.pipeline.ExecutionPipeline.runToTermination(ExecutionPipeline.java:82) at net.jqwik.engine.execution.JqwikExecutor.execute(JqwikExecutor.java:46) at net.jqwik.engine.JqwikTestEngine.executeTests(JqwikTestEngine.java:70) at net.jqwik.engine.JqwikTestEngine.execute(JqwikTestEngine.java:53) at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107) at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88) at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54) at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67) at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52) at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114) at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86) at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86) at org.apache.maven.surefire.junitplatform.LazyLauncher.execute(LazyLauncher.java:55) at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:234) at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133) at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:228) at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:175) at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:131) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:456) at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:169) at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:595) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:581) \r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.0.17\r\n\r\n\r\n\n\n megglos: ZDP-Triage:\n- unexpected breaking behavior\n- needs to be investigated\n megglos: ZDP-Planning:\n- this indicates a high severity bug and we need to investigate this asap\n deepthidevaki: @npepinpe You might not be able to reproduce this easily. When I was investigating sometime ago, I was wondering why it is not reproducible with the seed, and it looks like https://github.com/camunda/zeebe/blob/bcfe82dcb3be17b6f4a1ee732d40574d374cc1d8/atomix/cluster/src/main/java/io/atomix/raft/cluster/impl/RaftMemberContext.java#L137 this is the reason. I didn't debug further, but relying on System time will be non-deterministic. So most likely this is the reason. This doesn't help in your investigation, but just a hint in case you failed to reproduce it  :smile: \n npepinpe: We have `System.currentTimeMillis()` in other parts of our system as well :sweat: For example, with heartbeat related code, member context, quorum timeout calculation, etc.\r\n\r\nI guess the next step would be to use some clock for full reproducibility :+1: \n npepinpe: Observations:\r\n\r\n1. Expected to delete all entries after 404, but the commit index is 405.\r\n1. We tried to delete after 404 because the replicated entry did not have the same term as the existing entry\r\n1. The mismatched entries index was 405, hence why we want to delete it and anything after (so we say after 404, exclusive)\r\n1. The existing entry was committed\r\n\r\nSo the issue is either:\r\n\r\n1. The entry should not have been committed\r\n1. The terms should not have been mismatched\n npepinpe: A little above, we see the following line:\r\n\r\n```\r\n12:08:19.021 [] WARN  io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - AppendRequest{term=23, leader=1, prevLogIndex=404, prevLogTerm=19, entries=145, commitIndex=839} to 0 failed: java.util.concurrent.TimeoutException\r\n```\r\n\r\nHad to scroll way up higher to find when the commit index was not 405 on any of these nodes. So 405 was appended on 2 as:\r\n\r\n```\r\n12:08:18.230 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=405, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@5d2e7a6a}\r\n```\r\n\r\nIts term was 19.\r\n\r\nIt was appended on 0 as:\r\n\r\n```\r\n12:08:18.229 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=405, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@2dc97eb8}\r\n```\r\n\r\nAlso term 19. Alright. Let's check then when we try to re-append it, what the term was. Cool, we don't have that in the logs :)\r\n\r\nThis is the request we get which triggers the error:\r\n\r\n```\r\n12:08:19.023 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received AppendRequest{term=23, leader=1, prevLogIndex=404, prevLogTerm=19, entries=145, commitIndex=839}\r\n```\r\n\r\nSo the prevLogTerm seems correct, but unfortunately we don't log the mismatched entries, so I can't check it.\r\n\r\nBy the time the error occurs, the term is 23.\r\n\r\nOne weird thing, it seems the leader sends this request to 0:\r\n\r\n```\r\n12:08:19.021 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Sending AppendRequest{term=23, leader=1, prevLogIndex=873, prevLogTerm=23, entries=0, commitIndex=839} to 0\r\n```\r\n\r\nThere is no log for when the response is received from 0, no time out or anything; and then it suddenly tries to send starting at 405 :thinking: Unclear to me what caused it to do that. Possibly it's backed up and has many messages buffered?\r\n\r\nCorrect. Way above, we see:\r\n\r\n```\r\n12:08:19.021 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Received AppendResponse{status=OK, term=23, succeeded=false, lastLogIndex=578, lastSnapshotIndex=404} from 0\r\n12:08:19.021 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Reset next index for RaftMemberContext{member=0, term=23, configIndex=0, snapshotIndex=404, nextSnapshotIndex=0, nextSnapshotChunk=null, matchIndex=0, heartbeatTime=1686658099013, appending=0, appendSucceeded=false, appendTime=1686658099018, configuring=false, installing=false, failures=0} to 579\r\n12:08:19.021 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Sending AppendRequest{term=23, leader=1, prevLogIndex=404, prevLogTerm=19, entries=145, commitIndex=839} to 0\r\n```\r\n\r\nBut if the last log index on 0 is 578, why aren't we sending starting at 579?? Why are we sending starting at 405?\r\n\r\nAh, then we get \r\n\r\n```\r\n12:08:19.021 [] WARN  io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - AppendRequest{term=23, leader=1, prevLogIndex=404, prevLogTerm=19, entries=145, commitIndex=839} to 0 failed: java.util.concurrent.TimeoutException\r\n```\r\n\r\nFollowed quickly by:\r\n\r\n```\r\n12:08:19.021 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Sending AppendRequest{term=23, leader=1, prevLogIndex=873, prevLogTerm=23, entries=0, commitIndex=839} to 0\r\n```\r\n\r\nUnclear why a timeout would lead to the prevLogIndex to be reset...\r\n\r\nAnyway, that second one is never really processed by 0.\n npepinpe: If we keep going further back in time, we see the last append that was processed by 0 was:\r\n\r\n```\r\n12:08:19.018 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Sending AppendRequest{term=23, leader=1, prevLogIndex=860, prevLogTerm=23, entries=0, commitIndex=405} to 0\r\n12:08:19.020 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received AppendRequest{term=23, leader=1, prevLogIndex=860, prevLogTerm=23, entries=0, commitIndex=405}\r\n12:08:19.020 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Rejected AppendRequest{term=23, leader=1, prevLogIndex=860, prevLogTerm=23, entries=0, commitIndex=405}: Previous index (860) is greater than the local log's last index (578)\r\n12:08:19.021 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending AppendResponse{status=OK, term=23, succeeded=false, lastLogIndex=578, lastSnapshotIndex=404}\r\n```\r\n\r\nThis correlates with the reset we saw on 1 earlier, when it reset to 579 (presumably - apparently not!).\r\n\r\nIn fact, we see many of these. Due to time outs, 1 keeps trying to send the same append request, and 0 keeps rejecting it because the previous index is greater than the local log's last index. So let's try to find the last successful append (579).\r\n\r\n```\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received AppendRequest{term=19, leader=1, prevLogIndex=560, prevLogTerm=19, entries=18, commitIndex=405}\r\n12:08:18.284 [] INFO  io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Found leader 1\r\n12:08:18.284 [] TRACE io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Set leader 1\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=561, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@7a75a78e}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=562, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@5058a8e2}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=563, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@1187855a}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=564, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@1f7952dd}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=565, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@4867cf6b}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=566, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@5d6d613d}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=567, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@706d929f}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=568, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@2557d9f5}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=569, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@533d56a3}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=570, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@656fb170}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=571, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@790a2e28}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=572, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@6209d88d}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=573, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@5b381221}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=574, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@43c4dc07}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=575, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@6b17d139}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=576, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@47f6a166}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=577, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@5c441aa3}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Appended IndexedRaftLogEntryImpl{index=578, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=io.atomix.raft.storage.log.PersistedRaftRecord@3985d1c8}\r\n12:08:18.284 [] TRACE io.atomix.raft.storage.system.MetaStore - Store last flushed index 578\r\n12:08:18.284 [] TRACE io.atomix.raft.storage.system.MetaStore - Skip storing same last flushed index 578\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending AppendResponse{status=OK, term=19, succeeded=true, lastLogIndex=578, lastSnapshotIndex=404}\r\n12:08:18.284 [] TRACE io.atomix.raft.impl.RandomizedElectionTimer - Election timeout scheduled for PT3.733S\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received AppendRequest{term=19, leader=1, prevLogIndex=578, prevLogTerm=19, entries=0, commitIndex=405}\r\n12:08:18.284 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending AppendResponse{status=OK, term=19, succeeded=true, lastLogIndex=578, lastSnapshotIndex=404}\r\n```\r\n\r\nSo we can see all the other entries were also during term 19. So it looks like after term 19, 0 never appended anything.\r\n\r\nThen we see 0 soon after trying to get elected for term 20:\r\n\r\n```\r\n12:08:18.285 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 0\r\n12:08:18.285 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 0\r\n12:08:18.285 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - No heartbeat from 1 since 1ms\r\n12:08:18.285 [] TRACE io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Set leader null\r\n12:08:18.285 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending poll requests to all active members: [DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.453Z}, DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.453Z}]\r\n12:08:18.285 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.453Z} for next term 20\r\n12:08:18.285 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.453Z} for next term 20\r\n12:08:18.285 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick 50ms on 0\r\n12:08:18.285 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 0\r\n12:08:18.285 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Failed to poll a majority of the cluster in PT2.5S\r\n```\r\n\r\nThis fails. So let's try and find all the successful elections between 19 and 23.\r\n\r\nThere seems to be no successful election for term 20:\r\n\r\n```\r\n12:08:18.289 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Rejected AppendRequest{term=19, leader=1, prevLogIndex=578, prevLogTerm=19, entries=0, commitIndex=405}: request term is less than the current term (20)\r\n12:08:18.289 [] TRACE io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Sending AppendResponse{status=OK, term=20, succeeded=false, lastLogIndex=578, lastSnapshotIndex=404}\r\n12:08:18.290 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 0\r\n12:08:18.290 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - No heartbeat from a known leader since 6ms\r\n```\r\n\r\nBy the way, during this time, 1 still thinks it's the leader and tries to send append requests.\r\n\r\n```\r\n12:08:18.289 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Received AppendRequest{term=19, leader=1, prevLogIndex=559, prevLogTerm=19, entries=0, commitIndex=405}\r\n12:08:18.289 [] INFO  io.atomix.raft.impl.RaftContext - RaftServer{2-partition-1} - Found leader 1\r\n12:08:18.289 [] TRACE io.atomix.raft.impl.RaftContext - RaftServer{2-partition-1} - Set leader 1\r\n12:08:18.289 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Sending AppendResponse{status=OK, term=19, succeeded=true, lastLogIndex=559, lastSnapshotIndex=404}\r\n12:08:18.289 [] TRACE io.atomix.raft.impl.RandomizedElectionTimer - Election timeout scheduled for PT2.743S\r\n```\r\n\r\nThen later on:\r\n\r\n```\r\n12:08:18.290 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received PollRequest{term=19, candidate=2, lastLogIndex=559, lastLogTerm=19}\r\n12:08:18.290 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Rejected PollRequest{term=19, candidate=2, lastLogIndex=559, lastLogTerm=19}: candidate's term is less than the current term\r\n12:08:18.290 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending PollResponse{status=OK, term=20, accepted=false}\r\n12:08:18.290 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 1\r\n12:08:18.290 [] WARN  io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - AppendRequest{term=19, leader=1, prevLogIndex=559, prevLogTerm=19, entries=0, commitIndex=405} to 2 failed: java.util.concurrent.TimeoutException\r\n12:08:18.290 [] WARN  io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - AppendRequest{term=19, leader=1, prevLogIndex=578, prevLogTerm=19, entries=0, commitIndex=405} to 0 failed: java.util.concurrent.TimeoutException\r\n12:08:18.290 [] TRACE io.atomix.raft.roles.LeaderRole - RaftServer{1-partition-1}{role=LEADER} - Appended IndexedRaftLogEntryImpl{index=613, term=19, entry=ApplicationEntry{lowestPosition=0, highestPosition=1}, record=PersistedJournalRecord[metadata=RecordMetadata[checksum=3177859851, length=77], record=RecordData[index=613, asqn=0, data=UnsafeBuffer{addressOffset=140583371101078, capacity=49, byteArray=null, byteBuffer=java.nio.DirectByteBuffer[pos=9159 lim=10240 cap=10240]}]]}\r\n12:08:18.290 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Sending AppendRequest{term=19, leader=1, prevLogIndex=559, prevLogTerm=19, entries=0, commitIndex=405} to 2\r\n12:08:18.290 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Sending AppendRequest{term=19, leader=1, prevLogIndex=578, prevLogTerm=19, entries=0, commitIndex=405} to 0\r\n12:08:18.290 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 0\r\n12:08:18.290 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - No heartbeat from a known leader since 6ms\r\n12:08:18.290 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending poll requests to all active members: [DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.453Z}, DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.453Z}]\r\n12:08:18.290 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.453Z} for next term 21\r\n12:08:18.290 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.453Z} for next term 21\r\n```\r\n\r\nThen we get this weird one about term 1??\r\n\r\n```\r\n12:08:18.290 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received AppendRequest{term=19, leader=1, prevLogIndex=578, prevLogTerm=19, entries=0, commitIndex=405}\r\n12:08:18.290 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Rejected AppendRequest{term=19, leader=1, prevLogIndex=578, prevLogTerm=19, entries=0, commitIndex=405}: request term is less than the current term (20)\r\n12:08:18.290 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending AppendResponse{status=OK, term=20, succeeded=false, lastLogIndex=578, lastSnapshotIndex=404}\r\n12:08:18.290 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Expected heartbeat from null in term 20, but received one from 1 in term 1, ignoring it\r\n```\r\n\r\nThen rapidly 0 will keep doing elections, bumping the term from 20 to 21, then 22:\r\n\r\n```\r\n12:08:18.291 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Received PollRequest{term=20, candidate=0, lastLogIndex=578, lastLogTerm=19}\r\n12:08:18.291 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Accepted PollRequest{term=20, candidate=0, lastLogIndex=578, lastLogTerm=19}: candidate's log is up-to-date\r\n12:08:18.291 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Sending PollResponse{status=OK, term=20, accepted=true}\r\n12:08:18.291 [] INFO  io.atomix.raft.RandomizedRaftTest - Receive next message on 2\r\n[...]\r\n12:08:18.292 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received accepted poll from DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.453Z}\r\n12:08:18.292 [] INFO  io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Transitioning to CANDIDATE\r\n12:08:18.292 [] INFO  io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Starting election\r\n12:08:18.292 [] TRACE io.atomix.raft.storage.system.MetaStore - Store term 21\r\n12:08:18.292 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote null\r\n12:08:18.292 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Set term 21\r\n12:08:18.292 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote 0\r\n12:08:18.292 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Voted for 0\r\n12:08:18.292 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Requesting votes for term 21\r\n12:08:18.292 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Requesting vote from DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.453Z} for term 21\r\n12:08:18.292 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Requesting vote from DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.453Z} for term 21\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 1\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Run next task on 2\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Run next task on 2\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Run next task on 0\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Run next task on 2\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick heartbeatTimeout on 2\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 0\r\n12:08:18.292 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Election timed out. Restarting election.\r\n12:08:18.292 [] TRACE io.atomix.raft.storage.system.MetaStore - Store term 22\r\n12:08:18.292 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote null\r\n12:08:18.292 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Set term 22\r\n12:08:18.292 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote 0\r\n12:08:18.292 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Voted for 0\r\n12:08:18.292 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Requesting votes for term 22\r\n12:08:18.292 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Requesting vote from DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.453Z} for term 22\r\n12:08:18.292 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Requesting vote from DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.453Z} for term 22\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Run next task on 2\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Append on leader on 0\r\n12:08:18.292 [] INFO  TEST - Appending on leader 1\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Append on leader on 0\r\n12:08:18.292 [] INFO  TEST - Appending on leader 1\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Take snapshot on 2\r\n12:08:18.292 [] INFO  TEST - Snapshot taken at index 404. Current commit index is 405\r\n12:08:18.292 [] DEBUG io.camunda.zeebe.journal.file.SegmentsManager - No segments can be deleted with index < 404 (first log index: 313)\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Receive next message on 1\r\n12:08:18.292 [] INFO  io.atomix.raft.RandomizedRaftTest - Run next task on 0\r\n12:08:18.292 [] WARN  io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - null\r\n```\r\n\r\nThen we get this cool warning:\r\n\r\n```\r\n12:08:18.292 [] WARN  io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - null\r\n```\r\n\r\n:smile:\r\n\r\n2 will vote for 0 for term 22:\r\n\r\n```\r\n12:08:18.292 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Received VoteRequest{term=22, candidate=0, lastLogIndex=578, lastLogTerm=19}\r\n12:08:18.292 [] TRACE io.atomix.raft.storage.system.MetaStore - Store term 22\r\n12:08:18.292 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote null\r\n12:08:18.292 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{2-partition-1} - Set term 22\r\n12:08:18.292 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Accepted VoteRequest{term=22, candidate=0, lastLogIndex=578, lastLogTerm=19}: candidate's log is up-to-date\r\n12:08:18.292 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote 0\r\n12:08:18.292 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{2-partition-1} - Voted for 0\r\n12:08:18.292 [] TRACE io.atomix.raft.impl.RandomizedElectionTimer - Election timeout scheduled for PT3.103S\r\n12:08:18.292 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Sending VoteResponse{status=OK, term=22, voted=true}\r\n```\r\n\r\nStill, no election succeeded yet. Then 2 bumps term to 23:\r\n\r\n```\r\n12:08:18.293 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 2\r\n12:08:18.293 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - No heartbeat from a known leader since 1ms\r\n12:08:18.293 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Sending poll requests to all active members: [DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.455Z}, DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.455Z}]\r\n12:08:18.293 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.455Z} for next term 23\r\n12:08:18.293 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.455Z} for next term 23\r\n```\r\n\r\nThen after all this time, 1 processes the message from 0 about term increasing to 21 (yes, 1 was still on term 19 trying to append stuff to the followers):\r\n\r\n```\r\n12:08:18.293 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 1\r\n12:08:18.293 [] TRACE io.atomix.raft.storage.system.MetaStore - Store term 21\r\n12:08:18.293 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote null\r\n12:08:18.293 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{1-partition-1} - Set term 21\r\n12:08:18.293 [] INFO  io.atomix.raft.roles.LeaderRole - RaftServer{1-partition-1}{role=LEADER} - Received greater term from 0\r\n12:08:18.293 [] INFO  io.atomix.raft.impl.RaftContext - RaftServer{1-partition-1} - Transitioning to FOLLOWER\r\n12:08:18.296 [] TRACE io.atomix.raft.roles.LeaderRole - RaftServer{1-partition-1}{role=LEADER} - Cancelling append timer\r\n12:08:18.296 [] TRACE io.atomix.raft.impl.RandomizedElectionTimer - Election timeout scheduled for PT4.57S\r\n12:08:18.296 [] TRACE io.atomix.raft.roles.LeaderRole - RaftServer{1-partition-1}{role=LEADER} - Received VoteRequest{term=21, candidate=0, lastLogIndex=578, lastLogTerm=19}\r\n12:08:18.296 [] DEBUG io.atomix.raft.roles.LeaderRole - RaftServer{1-partition-1}{role=LEADER} - Rejected VoteRequest{term=21, candidate=0, lastLogIndex=578, lastLogTerm=19}: candidate's last log entry (578) is at a lower index than the local log (621)\r\n12:08:18.296 [] TRACE io.atomix.raft.roles.LeaderRole - RaftServer{1-partition-1}{role=LEADER} - Sending VoteResponse{status=OK, term=21, voted=false}\r\n[...]\r\n12:08:18.297 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Second round of election timed out. Transitioning to follower.\r\n12:08:18.297 [] INFO  io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Transitioning to FOLLOWER\r\n12:08:18.297 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{0-partition-1}{role=CANDIDATE} - Cancelling election\r\n12:08:18.297 [] TRACE io.atomix.raft.impl.RandomizedElectionTimer - Election timeout scheduled for PT3.95S\r\n12:08:18.297 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 1\r\n12:08:18.297 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - No heartbeat from a known leader since 4ms\r\n12:08:18.297 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Sending poll requests to all active members: [DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.454Z}, DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.454Z}]\r\n12:08:18.297 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.454Z} for next term 22\r\n12:08:18.297 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.454Z} for next term 22\r\n[...]\r\n12:08:18.298 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Failed to poll a majority of the cluster in PT2.5S\r\n12:08:18.298 [] TRACE io.atomix.raft.impl.RandomizedElectionTimer - Election timeout scheduled for PT3.769S\r\n12:08:18.298 [] INFO  io.atomix.raft.RandomizedRaftTest - Drop next message on 2\r\n12:08:18.298 [] INFO  TEST: - Dropped a message to 2\r\n12:08:18.298 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick heartbeatTimeout on 2\r\n12:08:18.298 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick 50ms on 1\r\n12:08:18.298 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 1\r\n12:08:18.298 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Failed to poll a majority of the cluster in PT2.5S\r\n12:08:18.298 [] TRACE io.atomix.raft.impl.RandomizedElectionTimer - Election timeout scheduled for PT4.267S\r\n[...]\r\n12:08:18.298 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Received VoteRequest{term=22, candidate=0, lastLogIndex=578, lastLogTerm=19}\r\n12:08:18.298 [] TRACE io.atomix.raft.storage.system.MetaStore - Store term 22\r\n12:08:18.298 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote null\r\n12:08:18.298 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{1-partition-1} - Set term 22\r\n12:08:18.298 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Rejected VoteRequest{term=22, candidate=0, lastLogIndex=578, lastLogTerm=19}: candidate's last log entry (578) is at a lower index than the local log (621)\r\n12:08:18.298 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Sending VoteResponse{status=OK, term=22, voted=false}\r\n[...]\r\n12:08:18.298 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Expected heartbeat from null in term 22, but received one from 1 in term 1, ignoring it\r\n12:08:18.298 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received PollRequest{term=22, candidate=2, lastLogIndex=559, lastLogTerm=19}\r\n12:08:18.298 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Rejected PollRequest{term=22, candidate=2, lastLogIndex=559, lastLogTerm=19}: candidate's last log entry (559) is at a lower index than the local log (578)\r\n12:08:18.298 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending PollResponse{status=OK, term=22, accepted=false}\r\n[...]\r\n12:08:18.299 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Sending poll requests to all active members: [DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.455Z}, DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.455Z}]\r\n12:08:18.299 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.455Z} for next term 23\r\n12:08:18.299 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.455Z} for next term 23\r\n[...]\r\n12:08:18.299 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received PollRequest{term=22, candidate=2, lastLogIndex=559, lastLogTerm=19}\r\n12:08:18.299 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Rejected PollRequest{term=22, candidate=2, lastLogIndex=559, lastLogTerm=19}: candidate's last log entry (559) is at a lower index than the local log (578)\r\n12:08:18.299 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending PollResponse{status=OK, term=22, accepted=false}\r\n[...]\r\n12:08:18.299 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Received PollRequest{term=22, candidate=2, lastLogIndex=559, lastLogTerm=19}\r\n12:08:18.299 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Rejected PollRequest{term=22, candidate=2, lastLogIndex=559, lastLogTerm=19}: candidate's last log entry (559) is at a lower index than the local log (621)\r\n12:08:18.299 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Sending PollResponse{status=OK, term=22, accepted=false}\r\n[...]\r\n12:08:18.299 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - No heartbeat from a known leader since 6ms\r\n12:08:18.299 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Sending poll requests to all active members: [DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.454Z}, DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.454Z}]\r\n12:08:18.299 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.454Z} for next term 23\r\n12:08:18.299 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{1-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.454Z} for next term 23\r\n```\r\n\r\nSo we see both 0 and 1 keep rejecting 2 as a candidate, and since 1 has the longest log, it should become elected eventually. Last we see it tries an election on term 23.\r\n\r\n```\r\n12:08:18.299 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received PollRequest{term=22, candidate=1, lastLogIndex=621, lastLogTerm=19}\r\n12:08:18.299 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Accepted PollRequest{term=22, candidate=1, lastLogIndex=621, lastLogTerm=19}: candidate's log is up-to-date\r\n12:08:18.299 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending PollResponse{status=OK, term=22, accepted=true}\r\n[...]\r\n12:08:18.299 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{1-partition-1} - Set term 23\r\n12:08:18.299 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote 1\r\n12:08:18.299 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{1-partition-1} - Voted for 1\r\n12:08:18.299 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{1-partition-1}{role=CANDIDATE} - Requesting votes for term 23\r\n12:08:18.299 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{1-partition-1}{role=CANDIDATE} - Requesting vote from DefaultRaftMember{id=2, type=ACTIVE, updated=2023-06-13T12:08:17.454Z} for term 23\r\n12:08:18.299 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{1-partition-1}{role=CANDIDATE} - Requesting vote from DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.454Z} for term 23\r\n[...]\r\n12:08:18.299 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Received VoteRequest{term=23, candidate=1, lastLogIndex=621, lastLogTerm=19}\r\n12:08:18.299 [] TRACE io.atomix.raft.storage.system.MetaStore - Store term 23\r\n12:08:18.299 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote null\r\n12:08:18.299 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Set term 23\r\n12:08:18.300 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Accepted VoteRequest{term=23, candidate=1, lastLogIndex=621, lastLogTerm=19}: candidate's log is up-to-date\r\n12:08:18.300 [] TRACE io.atomix.raft.storage.system.MetaStore - Store vote 1\r\n12:08:18.300 [] DEBUG io.atomix.raft.impl.RaftContext - RaftServer{0-partition-1} - Voted for 1\r\n12:08:18.300 [] TRACE io.atomix.raft.impl.RandomizedElectionTimer - Election timeout scheduled for PT2.939S\r\n12:08:18.300 [] TRACE io.atomix.raft.roles.FollowerRole - RaftServer{0-partition-1}{role=FOLLOWER} - Sending VoteResponse{status=OK, term=23, voted=true}\r\n[...]\r\n12:08:18.300 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - No heartbeat from a known leader since 8ms\r\n12:08:18.300 [] INFO  io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Sending poll requests to all active members: [DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.455Z}, DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.455Z}]\r\n12:08:18.300 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.455Z} for next term 23\r\n12:08:18.300 [] DEBUG io.atomix.raft.roles.FollowerRole - RaftServer{2-partition-1}{role=FOLLOWER} - Polling DefaultRaftMember{id=1, type=ACTIVE, updated=2023-06-13T12:08:17.455Z} for next term 23\r\n12:08:18.300 [] INFO  io.atomix.raft.RandomizedRaftTest - Append on leader on 0\r\n12:08:18.300 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick heartbeatTimeout on 0\r\n12:08:18.300 [] INFO  io.atomix.raft.RandomizedRaftTest - Tick electionTimeout on 1\r\n12:08:18.300 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{1-partition-1}{role=CANDIDATE} - Received successful vote from DefaultRaftMember{id=0, type=ACTIVE, updated=2023-06-13T12:08:17.454Z}\r\n12:08:18.300 [] INFO  io.atomix.raft.impl.RaftContext - RaftServer{1-partition-1} - Transitioning to LEADER\r\n12:08:18.300 [] DEBUG io.atomix.raft.roles.CandidateRole - RaftServer{1-partition-1}{role=CANDIDATE} - Cancelling election\r\n```\r\n\r\nSo now we get to term 23. After that is a long time of just sending append requests trying to keep appending on the nodes, the log of which I already put above.\r\n\n npepinpe: So that the commit index is 405 is fine, and it would make sense that the entry is committed. We saw that both nodes received further appends (2 up to 559, and 0 up to 578), both requests which started at 406. So 405 should be committed, with term 19.\r\n\r\nSo possibly the problem is that during replication, 1 should have started sending entries from 579, but instead it started at 405? Or some other weird thing happened...but it's likely a replication issue.\n npepinpe: One thing to note, the index we delete after does not come from the entry we're trying to replace, but instead from the AppendRequest's prevLogIndex. It then gets incremented as we append...\r\n\r\nIs this correct? We saw that 0 replies that it's prevLogIndex is not 404, but instead 578.\r\n\r\nSee:\r\n\r\n```\r\n12:08:19.021 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Received AppendResponse{status=OK, term=23, succeeded=false, lastLogIndex=578, lastSnapshotIndex=404} from 0\r\n12:08:19.021 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Reset next index for RaftMemberContext{member=0, term=23, configIndex=0, snapshotIndex=404, nextSnapshotIndex=0, nextSnapshotChunk=null, matchIndex=0, heartbeatTime=1686658099013, appending=0, appendSucceeded=false, appendTime=1686658099018, configuring=false, installing=false, failures=0} to 579\r\n12:08:19.021 [] TRACE io.atomix.raft.roles.LeaderAppender - RaftServer{1-partition-1} - Sending AppendRequest{term=23, leader=1, prevLogIndex=404, prevLogTerm=19, entries=145, commitIndex=839} to 0\r\n```\r\n\r\n\n npepinpe: From what I can see, it reset to 578, but then when building the next AppendRequest, it builds it from the previous entry which is...404? Is it because the snapshot is at 404?\r\n\r\nSo on the leader, if the previous entry (previous from what we should send) is not found, then we will send the prevLogEntry is the snapshot. Since 0 reports its last entry as 405, we try to get the prevLogEntry - which is not there, because 405 is the first entry in 1's log. So we send the snapshot - 404 - with term 19. But the first entry in the request is 579.\r\n\r\nSo on the follower (0), our last log entry is 578, greater than the reported `prevLogEntry` (404), so we think we have to replace some existing entries, starting at 405 (even if the first entry in the request is 578).\r\n\r\nI still don't get why the terms were mismatched though - it seems to me they should be the same, 19, so we shouldn't even have caught this bug :scream: \n npepinpe: @deepthidevaki - let's sync tomorrow on this\r\n\r\nI don't remember why we use the prevLogIndex as the start index instead of just using the entry's index themselves.\n deepthidevaki: > Is this correct? We saw that 0 replies that it's prevLogIndex is not 404, but instead 578.\r\n\r\nThis doesn't look like the expected behavior. Were there other AppendRequests in between, which would have truncated the entries in 0 back to 404? \n npepinpe: So it did compact right before:\r\n\r\n```\r\n12:08:19.021 [] INFO  io.atomix.raft.RandomizedRaftTest - Take snapshot on 1\r\n12:08:19.021 [] INFO  TEST - Snapshot taken at index 791. Current commit index is 839\r\n12:08:19.021 [] DEBUG io.camunda.zeebe.journal.file.SegmentsManager - atomix - Deleting log up from 313 up to 625 (removing 4 segments)\r\n```\n npepinpe: Alright, so the bug is that we have two snapshot listeners: one triggers compaction, and one updates the locale reference of the persisted snapshot in the Raft context. While both execute on the Raft thread, they may be executed in any order (since the listeners on the snapshot store are a set, iteration is not ordered).\r\n\r\nSo what happened here is:\r\n\r\n1. A new snapshot was taken up to entry 839\r\n2. The log is compacted up to 839\r\n3. 1 sent an append request for 839 to 0\r\n4. 0 rejected saying its last log index is 578\r\n5. 1 tries to reset 0 to 578, but the entry does not exist\r\n6. 1 sends a new append request using the local snapshot reference for prevLogIndex and prevLogTerm\r\n7. The snapshot reference is updated to 839\r\n\r\nSolution is to ensure that all the changes to the Raft context (updating the reference, compacting, etc.) are done in a single listener so we can control the ordering: update snapshot ref, then compact.\n npepinpe: This likely affect all versions. The severity is high in this case, as the partition will go inactive due to an uncaught exception. Workaround is to restart your broker.",
    "title": "`RandomizedRaftTest.consistencyTestWithSnapshot` fails with unexpected exception"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/7855",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nIt seems we are closing the resources not in the correct order. We still accept or want to send an response but the message service is closed concurrently and the sending fails and causes on error on closing the Broker.\r\n\r\nOccurred 10 times in two error groups within `1.2.0-alpha2`\r\n\r\n\r\nError groups:\r\n\r\n * https://console.cloud.google.com/errors/CJeh5tGzv8X1Rg?service=zeebe&time=P7D&refresh=off&project=camunda-cloud-240911\r\n * https://console.cloud.google.com/errors/CKKJ762u3J3ZUw?service=zeebe&time=P7D&refresh=off&project=camunda-cloud-240911\r\n\r\n\r\n**To Reproduce**\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n - when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\nI assume run the service and close the broker.\r\n\r\n**Expected behavior**\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nNo error, correct sequence of closing resources.\r\n\r\n**Log/Stacktrace**\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.lang.IllegalStateException: Messaging service is not running.\r\nat io.camunda.zeebe.transport.impl.AtomixClientTransportAdapter.tryToSend (AtomixClientTransportAdapter.java:105)\r\nat io.camunda.zeebe.transport.impl.AtomixClientTransportAdapter.lambda$handleResponse$7 (AtomixClientTransportAdapter.java:206)\r\nat io.camunda.zeebe.util.sched.ActorJob.invoke (ActorJob.java:76)\r\nat io.camunda.zeebe.util.sched.ActorJob.execute (ActorJob.java:39)\r\nat io.camunda.zeebe.util.sched.ActorTask.execute (ActorTask.java:122)\r\nat io.camunda.zeebe.util.sched.ActorThread.executeCurrentTask (ActorThread.java:94)\r\nat io.camunda.zeebe.util.sched.ActorThread.doWork (ActorThread.java:78)\r\nat io.camunda.zeebe.util.sched.ActorThread.run (ActorThread.java:191)\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n\r\n**Environment:**\r\n\r\n - Zeebe Version: 1.2.0-alpha2\r\n\r\n\n\n Zelldon: Error message introduce via https://github.com/camunda-cloud/zeebe/pull/7568\n Zelldon: related https://github.com/camunda-cloud/zeebe/issues/5521\n Zelldon: @pihme do you think this is solved due to recent refactorings?\n pihme: @Zelldon No I don't. The order of atomix and embedded gateway has not changed by the refactoring. One thing I wonder is whether this occurred in the Broker or in a Standalone Gateway. Do you know where it occurred?\r\n\r\nIn terms of macro-order of shutdown steps this should not happen.\r\n \r\nIt could happen if either shutdown of the actor does not cancel all subsequent planned tasks. Or it could happen if it is possible to schedule new tasks after an actor has been shutdown.\n pihme: Reoccurred recently:\r\n\r\nLogs:\r\n```\r\nD 2022-01-16T09:29:16.098476Z Closing Broker-0 [6/10]: embedded gateway closed in 15 ms \r\nI 2022-01-16T09:29:16.098659Z Closing Broker-0 [7/10]: cluster services \r\nD 2022-01-16T09:29:16.098802Z Closing Broker-0 [7/10]: cluster services closed in 0 ms \r\nI 2022-01-16T09:29:16.098978Z Closing Broker-0 [8/10]: subscription api \r\nD 2022-01-16T09:29:16.102018Z Closing Broker-0 [8/10]: subscription api closed in 2 ms \r\nI 2022-01-16T09:29:16.102259Z Closing Broker-0 [9/10]: command api transport and handler \r\nI 2022-01-16T09:29:16.232428Z Stopped \r\nD 2022-01-16T09:29:16.233107Z Closing Broker-0 [9/10]: command api transport and handler closed in 131 ms \r\nI 2022-01-16T09:29:16.233481Z Closing Broker-0 [10/10]: Migrated Startup Steps \r\nD 2022-01-16T09:29:16.235563Z Shutdown was called with context: io.camunda.zeebe.broker.bootstrap.BrokerStartupContextImpl@36539fa \r\nI 2022-01-16T09:29:16.235975Z Shutdown Cluster Services (Creation) \r\nI 2022-01-16T09:29:16.238984Z Stopped \r\nI 2022-01-16T09:29:16.240144Z Stopped \r\nI 2022-01-16T09:29:16.241539Z 0 - Member deactivated: Member{id=0, address=zeebe-0.zeebe-broker-service.0bbb3676-e3ad-445b-891e-0e1350800e41-zeebe.svc.cluster.local:26502, properties={brokerInfo=EADJAAAAAwAAAAAAAgAAAAMAAAADAAAAAAABCgAAAGNvbW1hbmRBcGlfAAAAemVlYmUtMC56ZWViZS1icm9rZXItc2VydmljZS4wYmJiMzY3Ni1lM2FkLTQ0NWItODkxZS0wZTEzNTA4MDBlNDEtemVlYmUuc3ZjLmNsdXN0ZXIubG9jYWw6MjY1MDEFAAIBAAAAAQIAAAABDAAABQAAADEuMi45BQACAQAAAAECAAAAAQ==, event-service-topics-subscribed=KIIDAGpvYnNBdmFpbGFibOU=}} \r\nI 2022-01-16T09:29:16.241947Z Stopped \r\nI 2022-01-16T09:29:16.242539Z Local node Member{id=0, address=zeebe-0.zeebe-broker-service.0bbb3676-e3ad-445b-891e-0e1350800e41-zeebe.svc.cluster.local:26502, properties={brokerInfo=EADJAAAAAwAAAAAAAgAAAAMAAAADAAAAAAABCgAAAGNvbW1hbmRBcGlfAAAAemVlYmUtMC56ZWViZS1icm9rZXItc2VydmljZS4wYmJiMzY3Ni1lM2FkLTQ0NWItODkxZS0wZTEzNTA4MDBlNDEtemVlYmUuc3ZjLmNsdXN0ZXIubG9jYWw6MjY1MDEFAAIBAAAAAQIAAAABDAAABQAAADEuMi45BQACAQAAAAECAAAAAQ==, event-service-topics-subscribed=KIIDAGpvYnNBdmFpbGFibOU=}} left the bootstrap servide \r\nI 2022-01-16T09:29:16.243398Z Stopped cluster membership service for member Member{id=0, address=zeebe-0.zeebe-broker-service.0bbb3676-e3ad-445b-891e-0e1350800e41-zeebe.svc.cluster.local:26502, properties={brokerInfo=EADJAAAAAwAAAAAAAgAAAAMAAAADAAAAAAABCgAAAGNvbW1hbmRBcGlfAAAAemVlYmUtMC56ZWViZS1icm9rZXItc2VydmljZS4wYmJiMzY3Ni1lM2FkLTQ0NWItODkxZS0wZTEzNTA4MDBlNDEtemVlYmUuc3ZjLmNsdXN0ZXIubG9jYWw6MjY1MDEFAAIBAAAAAQIAAAABDAAABQAAADEuMi45BQACAQAAAAECAAAAAQ==, event-service-topics-subscribed=KIIDAGpvYnNBdmFpbGFibOU=}} \r\nE 2022-01-16T09:29:18.270930Z Expected to handle gRPC request, but an unexpected error occurred \r\nE 2022-01-16T09:29:18.272130Z Expected to handle gRPC request, but an unexpected error occurred \r\nE 2022-01-16T09:29:18.273074Z Expected to handle gRPC request, but an unexpected error occurred \r\nI 2022-01-16T09:29:18.362961Z Stopped \r\nI 2022-01-16T09:29:18.363819Z Stopped \r\nI 2022-01-16T09:29:18.365121Z Shutdown monitoring services \r\nD 2022-01-16T09:29:18.366707Z Finished shutdown process \r\nD 2022-01-16T09:29:18.367117Z Closing Broker-0 [10/10]: Migrated Startup Steps closed in 2134 ms \r\nI 2022-01-16T09:29:18.367404Z Closing Broker-0 succeeded. Closed 10 steps in 2419 ms. \r\nI 2022-01-16T09:29:18.367629Z Broker shut down. \r\nD 2022-01-16T09:29:18.367920Z Closing actor thread ground 'Broker-0-zb-fs-workers' \r\nD 2022-01-16T09:29:18.368650Z Closing actor thread ground 'Broker-0-zb-actors' \r\nD 2022-01-16T09:29:18.369899Z Closing actor thread ground 'Broker-0-zb-actors': closed successfully \r\nD 2022-01-16T09:29:18.369957Z Closing actor thread ground 'Broker-0-zb-fs-workers': closed successfully \r\n```\n Zelldon: This happened again on our benchmark week 27\r\n\r\nhttps://console.cloud.google.com/errors/detail/CJ-difGB-LXsAg;service=zeebe;version=medic-cw-27-56ad2b36c8-benchmark;time=P7D?project=zeebe-io\n korthout: Happened again on benchmark week 27 ([newly reported error](https://console.cloud.google.com/errors/detail/CK6Khe-c7YzlFw;service=zeebe;version=medic-cw-27-56ad2b36c8-benchmark;time=P7D?project=zeebe-io)) with a different stacktrace. \r\n\r\nThis time because of `AtomixClientTransportAdapter.lambda$sendRequestInternal$2` instead of `AtomixClientTransportAdapter.lambda$handleResponse$7`.\n Zelldon: Last seen: 1 day ago.\n oleschoenburg: Happened again on 8.1.8: https://console.cloud.google.com/errors/detail/CK6Khe-c7YzlFw;service=zeebe;time=P7D?project=camunda-cloud-240911\n Zelldon: Happened in [zeebe:8.0.19-SNAPSHOT-stable-8.0-c326e93b](https://console.cloud.google.com/errors/detail/CKKJ762u3J3ZUw;service=zeebe;version=8.0.19-SNAPSHOT-stable-8.0-c326e93b;time=P7D?project=camunda-saas-int-chaos)\r\n\r\nhttps://console.cloud.google.com/errors/detail/CKKJ762u3J3ZUw;service=zeebe;time=P7D?project=camunda-saas-int-chaos\n deepthidevaki: Observed in 8.2.12\r\n\r\nRoot cause analysis:\r\n\r\nThe error message originates in `AtomixClientTransportAdapter`. This is created in Gateway BrokerClient\r\n\r\nhttps://github.com/camunda/zeebe/blob/18657c586a1974f6bb3ce3f86c1e16359458050d/gateway/src/main/java/io/camunda/zeebe/gateway/impl/broker/BrokerClientImpl.java#L64\r\n\r\nBut never closed by it\r\nhttps://github.com/camunda/zeebe/blob/0945c3088629e5a72b260778c104418afb113b08/gateway/src/main/java/io/camunda/zeebe/gateway/impl/broker/BrokerClientImpl.java#L70-L87\r\n\r\nFix:\r\nMessaging service is only closed after BrokerClient (I guess so because the lifecycle is managed by Spring, so the dependency order should be preserved). So closing `AtomixClientTransportAdapter` when closing `BrokerClientImpl` should ensure that no request are send after it is closed.\n oleschoenburg: Observed again on 8.1.16: https://console.cloud.google.com/errors/detail/CK6Khe-c7YzlFw;service=zeebe;time=P7D?project=camunda-saas-int-chaos\n megglos: ZDP-Triage:\n- mostly noise during shutdown\n- maybe the close is not sync",
    "title": "Messaging service is not running"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/5209",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Description**\r\nCurrently we start zeebe partition services only after atomix is fully started. That means if a node has 3 partitions, we wait until all 3 partitions has successfully started their raft servers. In some benchmarks it was observed that sometimes 2 partitions in a node succesfuly starts and become the raft leaders. But the third partition did not join, or took long (hours) to join (due to #5208 ). During this time, this node is the raft leader for 2 partitions, but no processing is happening because we don't install stream processor and other leader services until the atomix start is completed. As a result, the service is unavailable or only partially available even though there are leaders for all partitions.\r\n\n\n Zelldon: I think this goes in hand with removing the complete atomix wrapper and bootstrap logic, which wanted to do anyway at some point. I think it makes sense to remove it and have independent starters for each partition.\n deepthidevaki: Is there an issue for it already? If so, let's link it here.\n Zelldon: Unfortunately I haven't found it in the backlog. That's why I just commented it :smile: \n npepinpe: Do we see any blockers to starting partitions in parallel without waiting for the others to be started?\n Zelldon: I see no blockers. Maybe @deepthidevaki has some thoughts?\r\n\n deepthidevaki: No real blockers that I know. We would have to refactor atomix interface. There should be a way to detect when an individual partition is started and ready. For a single partition, the steps in bootstrap are the following in order.\r\n1. RaftPartitionServer is created\r\n2. Become follower\r\n3. Leader election happens\r\n4. Join completes\r\n5. Catch up if needed\r\n6. Marked as Ready -> Startup complete.\r\n\r\nWe have to ensure that zeebe services are installed only after step 6. Currently, we do `atomix.start().join()`, this is guaranteed.\r\n  \n Zelldon: What we could to instead is:\r\n\r\n * start atomix transport\r\n * membership\r\n * start topology\r\n * start monitoring \r\n * start disk space\r\n * split up in partition starts with sub steps for installing processors etc.\n npepinpe: Would love to improve the bootstrapping logic next quarter - let's see if we can make it happen.\n deepthidevaki: We observed an e2e test failure as a consequence of this.\r\n\r\nBroker 2 was not becoming ready, because partition 1 was not able to receive heartbeats from leader (due to another bug). But it was receiving events for partitions 2 and 3. Since raft partition 1 is not ready, Zeebe services  (processing and snapshoting) were not started for any partitions. So partitions 2 and 3 starts accumulating events in the log, but no snapshot were taken.\r\n\r\nLater, the root issue was resolved, partition 1 startup was unblocked and broker 2 became ready. Immediately after that, broker 2 became the leader for partition 3. Since it was not replaying and taking snapshots before, it's last snapshot was from 12 hours ago. As a result, partition 3 had to replay events generated in the last 12 hours before it can start processing. The replay took almost 1 hour, and during that time partition 3 was essentially unavailable for processing. All instances that were created before the leader change, was completed only after 1 hour.\n megglos: planning: affects higher partition count setups more heavily, risk of incidents,\nlet's time-box the investigation to 2h\n oleschoenburg: I looked into this, mostly to understand the startup process.\r\nTo me it looks like we have two \"barriers\" that wait for all partitions before continuing the startup process:\r\n1. `RaftPartitionGroup#join` waits for all raft partitions [to become `READY`](https://github.com/camunda/zeebe/blob/f574c79929f01e51f33a3d5902fc651aa15c3d46/atomix/cluster/src/main/java/io/atomix/raft/impl/DefaultRaftServer.java#L191-L197). This is [called by the PartitionManger](https://github.com/camunda/zeebe/blob/6a454eedc03e14d4f7fa56e5560c4febfbe57f89/broker/src/main/java/io/camunda/zeebe/broker/partitioning/PartitionManagerImpl.java#L128-L129) before scheduling `ZeebePartition`s. \r\n2.  `PartitionManagerImpl#start` waits for all `ZeebePartition`s to start. This blocks at least the installation of `AdminApiRequestHandler`\r\n\r\nThe first one is the crux I think, we don't want to wait for the partition _group_ to start, we just want to wait for each partition individually.\r\n\r\nThe second one appears to be trivial to solve:\r\n\r\n```patch\r\ndiff --git a/broker/src/main/java/io/camunda/zeebe/broker/partitioning/PartitionManagerImpl.java b/broker/src/main/java/io/camunda/zeebe/broker/partitioning/PartitionManagerImpl.java\r\n--- a/broker/src/main/java/io/camunda/zeebe/broker/partitioning/PartitionManagerImpl.java\t(revision 4fa6c0dc0009f5faedb987b09a055f9135629495)\r\n+++ b/broker/src/main/java/io/camunda/zeebe/broker/partitioning/PartitionManagerImpl.java\t(revision 6a454eedc03e14d4f7fa56e5560c4febfbe57f89)\r\n@@ -127,7 +127,7 @@\r\n \r\n     return partitionService\r\n         .start()\r\n-        .thenApply(\r\n+        .thenApplyAsync(\r\n             ps -> {\r\n               LOGGER.info(\"Registering Partition Manager\");\r\n \r\n@@ -158,12 +158,10 @@\r\n                       topologyManager,\r\n                       brokerCfg.getExperimental().getFeatures().toFeatureFlags()));\r\n \r\n-              final var futures =\r\n-                  partitions.stream()\r\n-                      .map(partition -> CompletableFuture.runAsync(() -> startPartition(partition)))\r\n-                      .toArray(CompletableFuture[]::new);\r\n+              for (final var partition : partitions) {\r\n+                CompletableFuture.runAsync(() -> startPartition(partition));\r\n+              }\r\n \r\n-              CompletableFuture.allOf(futures).join();\r\n               return null;\r\n             });\r\n   }\r\n```\r\n\r\n",
    "title": "Startup failure of one raft partition server affects the availability of other healthy partition"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14146",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nWhen we delete a DRG it is not removed from the cache. This means it remains available until the broker is restarted. We must make sure we delete the DRG from the cache.\r\n\r\nThe unit test supposed to verify this contains a bug:\r\n\r\n```java\r\n    assertThat(\r\n        decisionState\r\n            .findDecisionsByDecisionRequirementsKey(drg1.getDecisionRequirementsKey())\r\n            .isEmpty());\r\n```\r\n\r\nThe parentheses are off. This means this assertion doesn't actually assert anything. It's the equivalent of `assertThat(false)` at the moment.\r\n\r\n\n",
    "title": "Remove DRG from cache upon deletion"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14028",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n@philippfromme discovered that a link throw event can activate a link catch event inside an (event) subprocess. See the following process.  \r\n\r\n![Screenshot from 2023-08-28 09-41-57](https://github.com/camunda/zeebe/assets/4305769/58753ebb-d565-4a25-86ab-e39e30023139)\r\n\r\nAccording to the BPMN specification and the Camunda documentation, it should not be possible to activate a link catch event that is not in the same scope as the link throw event. Both events must be in the same scope.\r\n\r\nThis issue is kind of related to https://github.com/camunda/zeebe/issues/10854.\r\n\r\n**To Reproduce**\r\n\r\n1. Deploy the following process \r\n[link-event-subprocess.bpmn](https://github.com/camunda/zeebe/files/12452163/link-event-subprocess.bpmn.txt)\r\n2. Create a new instance of the process\r\n3. Verify that the link catch event is activated\r\n\r\n**Expected behavior**\r\n\r\nThe link catch event inside the (event) subprocess is not activated.\r\n\r\nThe deployment of the process is rejected because there is no link catch event in the same scope.\r\n\r\n**Log/Stacktrace**\r\n\r\n<details><summary>Output from Zeebe-Play</summary>\r\n <p>\r\n\r\n![Screenshot from 2023-08-28 10-22-54](https://github.com/camunda/zeebe/assets/4305769/cc3cbd5d-9e0e-4e7b-97b7-7645e9663265)\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- OS: <!-- [e.g. Linux] -->\r\n- Zeebe Version: `8.2.0`\r\n- Configuration: <!-- [e.g. exporters etc.] -->\r\n\n\n lzgabel: Hi @saig0. You can assign this task to me and I'll take a look. :bow:\n saig0: @lzgabel awesome. :rocket: Thank you for your engagement. :bow: ",
    "title": "Should not activate link catch event in subprocess"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13881",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nUpgrading a Zeebe cluster to a new version that includes fixes for https://github.com/camunda/zeebe/issues/12797 and https://github.com/camunda/zeebe/issues/13041, entries in the job deadline column family might be leaked, causing repeated error logs that a corresponding job cannot be found.\r\nThe cause is a change in behavior where we now expect no duplicate deadline entries and don't clean up duplicated or orphaned entries. \r\n\r\n**To Reproduce**\r\n\r\nUse 8.2.8, force the creation of duplicated deadline entries (probably happens with a long processing queue) and then upgrade to 8.2.9 or later.\r\n\r\n**Expected behavior**\r\n\r\nA migration cleans up orphaned entries so that they are removed from the state and do not cause error logs forever.\r\n\r\n**Environment:**\r\nZeebe <=8.2.8 upgrades to > 8.2.8\r\nZeebe <= 8.1.? upgrades to > 8.1.?\r\nZeebe <= 8.0.? upgrades to > 8.0.? \n",
    "title": "Upgrading leaves deadline entries without jobs"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13867",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nA `ConcurrentModificationException` occurs when clearing obsolete job activation (long polling) requests. The exception is thrown when clearing the `activeRequests` LinkedList which isn't thread-safe ([code](https://github.com/camunda/zeebe/blob/8ef7c6ffd293ab11c9d2f2ee2f79f98b47941d1d/gateway/src/main/java/io/camunda/zeebe/gateway/impl/job/InFlightLongPollingActivateJobsRequestsState.java#L77)).\r\n\r\n**To Reproduce**\r\n\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\n**Expected behavior**\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nA `ConcurrentModificationException` doesn't occur. \r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.util.ConcurrentModificationException: null\r\n\r\nat java.util.LinkedList$ListItr.checkForComodification\r\nat java.util.LinkedList$ListItr.remove\r\nat java.util.Collection.removeIf\r\nat io.camunda.zeebe.gateway.impl.job.InFlightLongPollingActivateJobsRequestsState.removeObsoleteRequestsAndUpdateMetrics ( io/camunda.zeebe.gateway.impl.job/InFlightLongPollingActivateJobsRequestsState.java:77 )\r\nat io.camunda.zeebe.gateway.impl.job.InFlightLongPollingActivateJobsRequestsState.getPendingRequests ( io/camunda.zeebe.gateway.impl.job/InFlightLongPollingActivateJobsRequestsState.java:71 )\r\nat io.camunda.zeebe.gateway.impl.job.LongPollingActivateJobsHandler.resetFailedAttemptsAndHandlePendingRequests ( io/camunda.zeebe.gateway.impl.job/LongPollingActivateJobsHandler.java:245 )\r\nat io.camunda.zeebe.gateway.impl.job.LongPollingActivateJobsHandler.lambda$onNotification$6 ( io/camunda.zeebe.gateway.impl.job/LongPollingActivateJobsHandler.java:181 )\r\nat io.camunda.zeebe.scheduler.ActorJob.invoke ( io/camunda.zeebe.scheduler/ActorJob.java:92 )\r\nat io.camunda.zeebe.scheduler.ActorJob.execute ( io/camunda.zeebe.scheduler/ActorJob.java:45 )\r\nat io.camunda.zeebe.scheduler.ActorTask.execute ( io/camunda.zeebe.scheduler/ActorTask.java:119 )\r\nat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask ( io/camunda.zeebe.scheduler/ActorThread.java:109 )\r\nat io.camunda.zeebe.scheduler.ActorThread.doWork ( io/camunda.zeebe.scheduler/ActorThread.java:87 )\r\nat io.camunda.zeebe.scheduler.ActorThread.run ( io/camunda.zeebe.scheduler/ActorThread.java:204 )\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- OS: <!-- [e.g. Linux] --> Linux\r\n- Zeebe Version: <!-- [e.g. 0.20.0] --> 8.3.0-SNAPSHOT\r\n- Configuration: <!-- [e.g. exporters etc.] -->\r\n- Error occured in CW32 benchmark ([src](https://console.cloud.google.com/logs/query;query=error_group%2528%22CKD6icXrpN_oLA%22%2529%0AlogName:%22stdout%22%0Aresource.type%3D%22k8s_container%22%0Aresource.labels.pod_name%3D%22medic-y-2023-cw-32-1f55f22-benchmark-zeebe-gateway-5fbb459ctcwr%22%0Aresource.labels.container_name%3D%22zeebe-gateway%22%0Aresource.labels.location%3D%22europe-west1-b%22%0Aresource.labels.project_id%3D%22zeebe-io%22%0Aresource.labels.cluster_name%3D%22zeebe-cluster%22%0Aresource.labels.namespace_name%3D%22medic-y-2023-cw-32-1f55f22-benchmark%22;cursorTimestamp=2023-08-10T20:21:27.752805343Z;startTime=2023-08-10T19:51:57.752Z;endTime=2023-08-10T20:51:57.752Z?project=zeebe-io)).\r\n\n\n Zelldon: This seems to be something new introduced, maybe due to some changes to the job stuff ? @npepinpe \r\n\r\nI can see a trial cluster which has more than 1k occurrences of this. I think if this happens the client can't activate jobs.\r\n\r\n\r\n![occur2](https://github.com/camunda/zeebe/assets/2758593/ece83e92-2157-4bdc-b77f-90e0d3b5fdd6)\r\n![occur1](https://github.com/camunda/zeebe/assets/2758593/bb9d1679-aa39-4707-bb7c-4a435e5d5c98)\r\n![activate](https://github.com/camunda/zeebe/assets/2758593/a71587c3-4180-4512-92e2-eafbc59bac34)\r\n\r\n\r\n* https://console.cloud.google.com/errors/detail/CLu0t_7q98T1pQE;service=zeebe;time=P7D?project=camunda-cloud-240911\r\n* https://console.cloud.google.com/errors/detail/CM-6h67j1ejk0AE;service=zeebe;time=P7D?project=camunda-cloud-240911\r\n\r\n\r\nI would mark it as critical so we take a look asap (since I think this bug has newly introduced and blocks the client to make progress). If we find out and think it is less of a problem we can also decrease the severity again.\n npepinpe: Yes, my bad, we have a state modification in a callback that gets executed on the gRPC executor and not the actor itself =/\r\n\r\nThis was merged 11th of July though, and AFAIK not back ported, so I'm unsure how a trial cluster would have it. Can they use alpha versions?\n Zelldon: > This was merged 11th of July though, and AFAIK not back ported, so I'm unsure how a trial cluster would have it. Can they use alpha versions?\r\n\r\nYes I think there was a decision to get them used alpha versions.\n npepinpe: Fix: https://github.com/camunda/zeebe/pull/13875\r\n\r\nNot really sure how to write a good regression test for that =/\n Zelldon: Unfortunately, this is part of alpha4 https://console.cloud.google.com/errors/detail/CKaE2PeurPD7Ng;service=zeebe;time=P7D?project=camunda-cloud-240911. Luckily this is already fixed thanks to @npepinpe \r\n\n Zelldon: @npepinpe wondering whether bot are fixed not the ConcurrentModification and the index out of bounce, but I think so there were related right?\n npepinpe: Most likely, since we were modifying non-thread-safe structures concurrently :+1: ",
    "title": "ConcurrentModificationException when clearing obsolete job activation requests"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13814",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nIt seems right now job streaming may unnecessary trigger job polling when a job is handled/completed. This is because we call `JobWorkerImpl#handleJobFinished`, which then decrements `remainingJob` (wrong) and then might trigger polling.\r\n\r\n**To Reproduce**\r\n\r\nJust use job streaming :upside_down_face: \r\n\r\n**Expected behavior**\r\n\r\nWe do not trigger unnecessary polling and do not touch the `remainingJobs` counter.\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.3.0-alpha4\r\n\n",
    "title": "Job streaming may trigger unnecessary polling"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13796",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nAn exception is thrown whenever the `JobStreamRemover` tries to remove a stream in the gateway. This is due to the future completing without an executor, thus completing within the actor context, and then calling `Actor.call`. As we already wanted to ensure that an executor was used, we should do that as well.\r\n\r\n**To Reproduce**\r\n\r\nRegister a stream via the command. Cancel it. An exception is thrown and the stream is not removed from the gateway nor the broker (even if the client has gone away).\r\n\r\n**Expected behavior**\r\n\r\nThe stream is removed and no error is thrown.\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.3.0-alpha4\r\n\n",
    "title": "IllegalStateArgument when removing job stream"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13787",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Description**\r\n\r\nI noticed this during the release, on 8.0/8.1 building starter & worker fails as e.g. the maven wrapper is not present on 8.1\r\n```\r\n/home/runner/work/_temp/79d164a7-e6b4-4e02-9e4e-89a3d8a459e8.sh: line 1: ./mvnw: No such file or directory\r\nError: Process completed with exit code 127.\r\n```\r\nhttps://github.com/camunda/zeebe/actions/runs/5738456998/job/15552373143\r\n\r\nOn 8.0 also build zeebe fails as there is no DIST=`build` setup in the Dockerfile https://github.com/camunda/zeebe/actions/runs/5202088373/job/14079846461\r\n\r\n\r\n```[tasklist]\n### Tasks\n- [x] backport maven wrapper to 8.0/1 - to avoid workflow merge conflicts from main to stable\n- [x] Add a `benchmark.yaml` workflow to each stable branch - to maintain a stable setup\n- [x] trigger the workflows via [workflow dispatch](https://docs.github.com/en/free-pro-team@latest/rest/actions/workflows?apiVersion=2022-11-28#create-a-workflow-dispatch-event) from the https://github.com/zeebe-io/zeebe-engineering-processes referencing the release_branch\n- [x] delete the `dispatch-benchmark.yaml` workflow from main\n```\r\n\r\n\r\n\n\n megglos: ZDP-Triage:\n- will look into it asap to resolve\n remcowesterhoud: Individual triage:\r\n- Seems to be actively worked on by ZDP at this time. It's not sensible to work on this simultaneously so I'll move it to the ZPA backlog. This also allows us to focus multi-tenancy and migration.",
    "title": "Release: `Repo dispatch Benchmark` fails on 8.0/8.1"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13521",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nIf activating a call activity with large variables causes processing to exceed the batch size, the complete process instance is banned and left in limbo.\r\n\r\nThe current workaround is to ensure that we don't run into this case; however, once you hit it, there's nothing you can do but recreate the instance with smaller payloads.\r\n\r\nSupport:\r\n-  https://jira.camunda.com/browse/SUPPORT-17661\r\n- https://jira.camunda.com/browse/SUPPORT-17882\r\n\r\n**To Reproduce**\r\n\r\nCreate a process with a dummy service task which leads to a call activity. Once the task is activated, separately as to not exceed the batch size, create 4 large variables, e.g. of 1MB each (using the set variable command, one at a time). Then complete the task. Activating the call activity will fail and result in a banned instance.\r\n\r\n**Expected behavior**\r\n\r\nAn incident is raised, such that I can modify my variables to save this instance.\r\n\r\n**Log/Stacktrace**\r\n\r\nI've anonymized the stacktrace below. You can find more logs here: https://cloudlogging.app.goo.gl/aarrs1J5TY22X5gXA\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\nExpected to process record 'TypedRecordImpl{metadata=RecordMetadata{recordType=COMMAND, valueType=PROCESS_INSTANCE, intent=ACTIVATE_ELEMENT}, value={\"bpmnElementType\":\"CALL_ACTIVITY\",\"elementId\":\"Activity_1aba60i\",\"bpmnProcessId\":\"process\",\"version\":8,\"processDefinitionKey\":2251800414371946,\"processInstanceKey\":4503599979491273,\"flowScopeKey\":4503599979491273,\"bpmnEventType\":\"UNSPECIFIED\",\"parentProcessInstanceKey\":-1,\"parentElementInstanceKey\":-1}}' without errors, but exception occurred with message 'Can't append entry: 'RecordBatchEntry[recordMetadata=RecordMetadata{recordType=EVENT, valueType=VARIABLE, intent=CREATED}, key=4503599980117234, sourceIndex=-1, unifiedRecordValue={\"name\":\"foo\",\"value\":\"U2VkIHV0IHBlcnNwaWNpYXRpcyB1bmRlIG9tbmlzIGlzdGUgbmF0dXMgZXJyb3Igc2l0IHZvbHVwdGF0ZW0gYWNjdXNhbnRpdW0gZG9sb3JlbXF1ZSBsYXVkYW50aXVtLCB0b3RhbSByZW0gYXBlcmlhbSwgZWFxdWUgaXBzYSBxdWFlIGFiIGlsbG8gaW52ZW50b3JlIHZlcml0YXRpcyBldCBxdWFzaSBhcmNoaXRlY3RvIGJlYXRhZSB2aXRhZSBkaWN0YSBzdW50IGV4cGxpY2Fiby4gTmVtbyBlbmltIGlwc2FtIHZvbHVwdGF0ZW0gcXVpYSB2b2x1cHRhcyBzaXQgYXNwZXJuYXR1ciBhdXQgb2RpdCBhdXQgZnVnaXQsIHNlZCBxdWlhIGNvbnNlcXV1bnR1ciBtYWduaSBkb2xvcmVzIGVvcyBxdWkgcmF0aW9uZSB2b2x1cHRhdGVtIHNlcXVpIG5lc2NpdW50LiBOZXF1ZSBwb3JybyBxdWlzcXVhbSBlc3QsIHF1aSBkb2xvcmVtIGlwc3VtIHF1aWEgZG9sb3Igc2l0IGFtZXQsIGNvbnNlY3RldHVyLCBhZGlwaXNjaSB2ZWxpdCwgc2VkIHF1aWEgbm9uIG51bXF1YW0gZWl1cyBtb2RpIHRlbXBvcmEgaW5jaWR1bnQgdXQgbGFib3JlIGV0IGRvbG9yZSBtYWduYW0gYWxpcXVhbSBxdWFlcmF0IHZvbHVwdGF0ZW0uIFV0IGVuaW0gYWQgbWluaW1hIHZlbmlhbSwgcXVpcyBub3N0cnVtIGV4ZXJjaXRhdGlvbmVtIHVsbGFtIGNvcnBvcmlzIHN1c2NpcGl0IGxhYm9yaW9zYW0sIG5...' with size: 1725121 this would exceed the maximum batch size. [ currentBatchEntryCount: 18, currentBatchSize: 2874115]'.\r\n\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- OS: Ubuntu Focal\r\n- Zeebe Version: 8.2.8\r\n- Configuration: SaaS\r\n\n\n Zelldon: Related https://github.com/camunda/zeebe/issues/13016\n korthout: Thanks for raising this @npepinpe \n\nAs there is no good workaround available and the chance to run into this is reasonable, I'm prioritizing this as `upcoming`.\n\n>**Expected behavior**\n>An incident is raised, such that I can modify my variables to save this instance.\n\nTo resolve this, we'll need to handle errors in the `BpmnStreamProcessor`. However, we're unable to raise an incident for every process instance element type yet. So, we either need to pass the error handling along to the respective `BpmnElementProcessor` (i.e. `CallActivityProcessor`) or have element type-specific code in the BpmnStreamProcessor.\n\nSince this is not entirely trivial, I'm sizing this as `Medium`.\n oleschoenburg: This has come up in another support case: https://jira.camunda.com/browse/SUPPORT-17882\r\nIn this case, the variables weren't even that large - just around 300KiB. IMO this bug is pretty bad, effectively causing data loss. When we ban an instance, we can't recover it. Depending on the use case, this can be really severe. It's also hard to predict, I wouldn't have expected 300KiB variables to cause this.\r\n\r\nI've raised the priority to critical. IMO all cases where we ban instances have to be considered critical.",
    "title": "Process instances are banned when trying to activate a call activity with large variables"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12699",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "In https://github.com/camunda/zeebe/issues/12591, we found that when engine rejects the deployment request it cannot write the rejection record because it was attempting to write the whole deployment which is greater than maxMessageSize. This resulted in a loop in StreamProcessor, where it repeatedly fails to write the rejection record leading to the partition being fully blocked and not making any progress. https://github.com/camunda/zeebe/issues/12591#issuecomment-1535879364 \r\n\r\nTo fix this, now we reject the request in CommandAPI before writing to the logstream https://github.com/camunda/zeebe/pull/12676. But, to be safer we should also ensure that we are not writing rejection records that are too large, to prevent such accidental cases.\r\n\r\nEnsure that rejections records can be written reliably, even if the original command is rejected due to ExceededBatchSize. Ideally, rejection record should only have a reference to the original command, but not the full command. If possible, do not write the entire command for any type of rejections. Or atleast, trim rejection record in case of ExceededBatchSize exception.\r\n\r\n\r\n\n\n berkaycanbc: ZPA Triage:\n\nWe think this is a bug. We are planning to process this in the next mob-programming session.\n korthout: ~~Mob~~Pair programming with @koevskinikola results:\r\n- Run `io.camunda.zeebe.it.client.command.CreateDeploymentTest::shouldRejectDeployIfResourceIsTooLarge`\r\n- Notice that an error is logged, this should not be the case\r\n- Make sure that [rejection is written safely](https://github.com/camunda/zeebe/blob/main/engine/src/main/java/io/camunda/zeebe/engine/Engine.java#L186-L194)\n abbasadel: ZPA planning: \n- Moving back to the backlog since we don't have time for it in the next iteration.",
    "title": "Do not write the entire command for rejection"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13516",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Description**\r\nIT tests for the Job Push is missing. To cover the happy path scenario end to end, we need to implement IT tests. ~~These cases should also cover closing a stream which will actually test the `onClose` hook.~~ Please refer to these two discussions for more details: \r\nhttps://github.com/camunda/zeebe/pull/13351#discussion_r1263414875\r\nhttps://github.com/camunda/zeebe/pull/13351#discussion_r1263834385\r\n\n\n berkaycanbc: @koevskinikola Based on the investigation we did, the only way to trigger `onClose` handler is to call `onComplete` or `onError` from the server side which is not the case for our implementation. Therefore, we will opt-out tests for it. At the same time, we want to keep it in case we decide to call `onComplete` later on. (e.g. during server shutdown)\r\n\r\nCc: @npepinpe ",
    "title": "Add integration tests for Job Push"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13354",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "# Description\r\nIf a process contains a signal start event we must make sure to unsubscribe this signal.\r\n\r\nIf the deleted process was the latest, the previous version will become the new latest version. If this previous version contains a signal start event we should make sure that the signal subscription is created and process instances are started as expected.\r\n\r\nBlocked by: #9769 \n",
    "title": "Delete and recreate signal subscription of previous version"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13343",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "## Description\r\nThis ColumnFamily must contains **all** running process instances. We must create a migration script that initially fills this ColumnFamily.\r\n\r\n- Create the migration script\r\n    - Include tests!\r\n\r\n**Blocked by**\r\n#13340 \n",
    "title": "Migrate running process instance into `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13253",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nCritical issues are defined as a \"stop the world\" issue. This means they require immediate attention. Currently, when an issue is labeled as critical there's no way of knowing, unless the person who added the label notifies the team. If this doesn't happen, there's a good chance we won't notice until the next triage.\r\n\r\n**Describe the solution you'd like**\r\nWe can easily add a workflow that checks if an issue gets labelled as critical. If this is the case we could send a notification to the Zeebe slack channel to notify the engineers that attention is required.\r\n\r\n**Describe alternatives you've considered**\r\nPeople could ping us manually when they do this. But that's a manual step and could be forgotten.\r\n\r\n**Additional context**\r\nN/A\r\n\n\n remcowesterhoud: ZPA Triage:\n- Helps us identify critical bugs more quickly\n- Removes a manual step that could be forgotten otherwise\n- Marking it as `upcoming` as we want to be aware of critical bugs asap\n- We already send slack message from our CI. Should be easy to take inspiration from these other places.\n- Low effort, potentially high impact.\n megglos: ZDP-Triage:\n- could help us being faster to react\n- need to clarify who reacts on it\n- right now we would assume this is done by zeebe engineers who can also escalate directly when identifying a critical issue\n- marking as later for now",
    "title": "Notify Zeebe Team in Slack when an issue is labelled as critical"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12942",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "In our case，the directory of `/usr/local/zeebe/data/raft-partition/partitions/1` has about 60 .log files with each one has 128MB size data,and we have 3 patitions,so all the files‘s size almost reach 66GB.When we do backup api,it always failed by throwing exception like below:\r\n![image](https://github.com/camunda/zeebe/assets/12196018/ea47d5f7-9fe6-40dc-9d62-92ece0445266)\r\n\r\n`\r\n{'backupId': 1685672012, 'status': 'FAILED', 'failureReason': \"Backup on partition 3 failed due to java.util.concurrent.CompletionException: software.amazon.awssdk.core.exception.SdkClientException: Unable to execute HTTP request: Acquire operation took longer than the configured maximum time. This indicates that a request cannot get a connection from the pool within the specified maximum time. This can be due to high request rate.\\nConsider taking any of the following actions to mitigate the issue: increase max connections, increase acquire timeout, or slowing the request rate.\\nIncreasing the max connections can increase client throughput (unless the network interface is already fully utilized), but can eventually start to hit operation system limitations on the number of file descriptors used by the process. If you already are fully utilizing your network interface or cannot further increase your connection count, increasing the acquire timeout gives extra time for requests to acquire a connection before timing out. If the connections doesn't free up, the subsequent requests will still timeout.\\nIf the above mechanisms are not able to fix the issue, try smoothing out your requests so that large traffic bursts cannot overload the client, being more efficient with the number of times you need to call AWS, or by increasing the number of hosts sending requests.}\r\n`\r\nI also see the code source,S3BackupStore.java create NettyNioAsyncHttpClient with static number 45s of ConnectionAcquireTimeout,and other configuration used the default value.There is no any chance to customizing httpClient configurtation data .\r\nI'm not sure if i have missing somthing important document.Could you help me with this problem?  Thanks.\r\n@oleschoenburg \r\n![image](https://github.com/camunda/zeebe/assets/12196018/fd472cf0-be41-4d2f-ae4e-222a4b1725c9)\r\n\r\n\n\n oleschoenburg: Thanks for reporting @codingman1990 :+1: \r\n\r\nPlease correct me if I'm wrong but the situation is basically this:\r\nWhen a partition has a lot of log segments, uploading them in parallel will cause some uploads to wait for longer than 45 seconds on an available connection. This results in an exception and failure to complete the backup.\r\n\r\nWe should definitely improve this, although I'm not sure if just configuring the connection acquisition timeout is the right way to go about it.\r\n\r\nIn my opinion, the issue is rather that there is no limiting of parallel uploads. This is an issue here, with 66 log segments. It will also become an issue for large states, especially if #12483 is merged. So I would suggest to rather limit the number of concurrent uploads. The limit can be configurable of course.\r\n\r\n\r\n@codingman1990 Slightly unrelated but I was a bit surprised when I read that you have >60 log segments. I'd have expected much lower numbers. Did you adjust the snapshot interval or know any other reasons why your brokers keep more segments than usual?\r\n\r\n\n codingman1990: > Thanks for reporting @codingman1990 👍\r\n> \r\n> Please correct me if I'm wrong but the situation is basically this: When a partition has a lot of log segments, uploading them in parallel will cause some uploads to wait for longer than 45 seconds on an available connection. This results in an exception and failure to complete the backup.\r\n> \r\n> We should definitely improve this, although I'm not sure if just configuring the connection acquisition timeout is the right way to go about it.\r\n> \r\n> In my opinion, the issue is rather that there is no limiting of parallel uploads. This is an issue here, with 66 log segments. It will also become an issue for large states, especially if #12483 is merged. So I would suggest to rather limit the number of concurrent uploads. The limit can be configurable of course.\r\n> \r\n> @codingman1990 Slightly unrelated but I was a bit surprised when I read that you have >60 log segments. I'd have expected much lower numbers. Did you adjust the snapshot interval or know any other reasons why your brokers keep more segments than usual?\r\n\r\nIn my situation,there are abount 200 running instances at the same time.I almost had not edit any configuration except something like ES,BackupStore.The full configuration data is like below(Some sensitive data are marsked):\r\n\r\n`\r\n{\r\n    \"network\": {\r\n        \"host\": \"0.0.0.0\",\r\n        \"portOffset\": 0,\r\n        \"maxMessageSize\": \"4MB\",\r\n        \"advertisedHost\": \"***\",\r\n        \"commandApi\": {\r\n            \"host\": \"0.0.0.0\",\r\n            \"port\": 26501,\r\n            \"advertisedHost\": \"***\",\r\n            \"advertisedPort\": 26501,\r\n            \"address\": \"0.0.0.0:26501\",\r\n            \"advertisedAddress\": \"***\"\r\n        },\r\n        \"internalApi\": {\r\n            \"host\": \"0.0.0.0\",\r\n            \"port\": 26502,\r\n            \"advertisedHost\": \"***\",\r\n            \"advertisedPort\": 26502,\r\n            \"address\": \"0.0.0.0:26502\",\r\n            \"advertisedAddress\": \"***\"\r\n        },\r\n        \"security\": {\r\n            \"enabled\": false,\r\n            \"certificateChainPath\": null,\r\n            \"privateKeyPath\": null\r\n        },\r\n        \"maxMessageSizeInBytes\": 4194304\r\n    },\r\n    \"cluster\": {\r\n        \"initialContactPoints\": [\r\n            \"***\",\r\n            \"***\",\r\n            \"***\"\r\n        ],\r\n        \"partitionIds\": [\r\n            1,\r\n            2,\r\n            3\r\n        ],\r\n        \"nodeId\": 1,\r\n        \"partitionsCount\": 3,\r\n        \"replicationFactor\": 3,\r\n        \"clusterSize\": 3,\r\n        \"clusterName\": \"camunda-zeebe\",\r\n        \"heartbeatInterval\": \"PT0.25S\",\r\n        \"electionTimeout\": \"PT2.5S\",\r\n        \"membership\": {\r\n            \"broadcastUpdates\": false,\r\n            \"broadcastDisputes\": true,\r\n            \"notifySuspect\": false,\r\n            \"gossipInterval\": \"PT0.25S\",\r\n            \"gossipFanout\": 2,\r\n            \"probeInterval\": \"PT1S\",\r\n            \"probeTimeout\": \"PT0.1S\",\r\n            \"suspectProbes\": 3,\r\n            \"failureTimeout\": \"PT10S\",\r\n            \"syncInterval\": \"PT10S\"\r\n        },\r\n        \"raft\": {\r\n            \"enablePriorityElection\": true\r\n        },\r\n        \"messageCompression\": \"NONE\"\r\n    },\r\n    \"threads\": {\r\n        \"cpuThreadCount\": 3,\r\n        \"ioThreadCount\": 3\r\n    },\r\n    \"data\": {\r\n        \"directory\": \"/usr/local/zeebe/data\",\r\n        \"logSegmentSize\": \"128MB\",\r\n        \"snapshotPeriod\": \"PT5M\",\r\n        \"logIndexDensity\": 100,\r\n        \"diskUsageMonitoringEnabled\": true,\r\n        \"diskUsageReplicationWatermark\": 0.87,\r\n        \"diskUsageCommandWatermark\": 0.85,\r\n        \"diskUsageMonitoringInterval\": \"PT1S\",\r\n        \"backup\": {\r\n            \"store\": \"S3\",\r\n            \"s3\": {\r\n                \"bucketName\": \"zeebe-proc\",\r\n                \"endpoint\": \"***\",\r\n                \"region\": \"cn-hangzhou\",\r\n                \"accessKey\": \"***\",\r\n                \"secretKey\": \"***\",\r\n                \"apiCallTimeout\": \"PT3M\"\r\n            }\r\n        },\r\n        \"logSegmentSizeInBytes\": 134217728,\r\n        \"freeDiskSpaceCommandWatermark\": 5052946022,\r\n        \"freeDiskSpaceReplicationWatermark\": 4379219886\r\n    },\r\n    \"exporters\": {\r\n        \"elasticsearch\": {\r\n            \"jarPath\": null,\r\n            \"className\": \"io.camunda.zeebe.exporter.ElasticsearchExporter\",\r\n            \"args\": {\r\n                \"index\": {\r\n                    \"prefix\": \"zeebe-record\"\r\n                },\r\n                \"authentication\": {\r\n                    \"password\": \"***\",\r\n                    \"username\": \"***\"\r\n                },\r\n                \"url\": \"***\"\r\n            },\r\n            \"external\": false\r\n        }\r\n    },\r\n    \"gateway\": {\r\n        \"network\": {\r\n            \"host\": \"0.0.0.0\",\r\n            \"port\": 26500,\r\n            \"minKeepAliveInterval\": \"PT30S\"\r\n        },\r\n        \"cluster\": {\r\n            \"initialContactPoints\": [\r\n                \"0.0.0.0:26502\"\r\n            ],\r\n            \"requestTimeout\": \"PT15S\",\r\n            \"clusterName\": \"zeebe-cluster\",\r\n            \"memberId\": \"gateway\",\r\n            \"host\": \"0.0.0.0\",\r\n            \"advertisedHost\": \"0.0.0.0\",\r\n            \"port\": 26502,\r\n            \"advertisedPort\": 26502,\r\n            \"membership\": {\r\n                \"broadcastUpdates\": false,\r\n                \"broadcastDisputes\": true,\r\n                \"notifySuspect\": false,\r\n                \"gossipInterval\": \"PT0.25S\",\r\n                \"gossipFanout\": 2,\r\n                \"probeInterval\": \"PT1S\",\r\n                \"probeTimeout\": \"PT0.1S\",\r\n                \"suspectProbes\": 3,\r\n                \"failureTimeout\": \"PT10S\",\r\n                \"syncInterval\": \"PT10S\"\r\n            },\r\n            \"security\": {\r\n                \"enabled\": false,\r\n                \"certificateChainPath\": null,\r\n                \"privateKeyPath\": null\r\n            },\r\n            \"messageCompression\": \"NONE\"\r\n        },\r\n        \"threads\": {\r\n            \"managementThreads\": 1\r\n        },\r\n        \"security\": {\r\n            \"enabled\": false,\r\n            \"certificateChainPath\": null,\r\n            \"privateKeyPath\": null\r\n        },\r\n        \"longPolling\": {\r\n            \"enabled\": true\r\n        },\r\n        \"interceptors\": [],\r\n        \"initialized\": true,\r\n        \"enable\": false\r\n    },\r\n    \"backpressure\": {\r\n        \"enabled\": true,\r\n        \"algorithm\": \"VEGAS\",\r\n        \"aimd\": {\r\n            \"requestTimeout\": \"PT1S\",\r\n            \"initialLimit\": 100,\r\n            \"minLimit\": 1,\r\n            \"maxLimit\": 1000,\r\n            \"backoffRatio\": 0.9\r\n        },\r\n        \"fixed\": {\r\n            \"limit\": 20\r\n        },\r\n        \"vegas\": {\r\n            \"alpha\": 3,\r\n            \"beta\": 6,\r\n            \"initialLimit\": 20\r\n        },\r\n        \"gradient\": {\r\n            \"minLimit\": 10,\r\n            \"initialLimit\": 20,\r\n            \"rttTolerance\": 2.0\r\n        },\r\n        \"gradient2\": {\r\n            \"minLimit\": 10,\r\n            \"initialLimit\": 20,\r\n            \"rttTolerance\": 2.0,\r\n            \"longWindow\": 600\r\n        }\r\n    },\r\n    \"experimental\": {\r\n        \"maxAppendsPerFollower\": 2,\r\n        \"maxAppendBatchSize\": \"32KB\",\r\n        \"disableExplicitRaftFlush\": false,\r\n        \"rocksdb\": {\r\n            \"columnFamilyOptions\": {},\r\n            \"enableStatistics\": false,\r\n            \"memoryLimit\": \"512MB\",\r\n            \"maxOpenFiles\": -1,\r\n            \"maxWriteBufferNumber\": 6,\r\n            \"minWriteBufferNumberToMerge\": 3,\r\n            \"ioRateBytesPerSecond\": 0,\r\n            \"disableWal\": false\r\n        },\r\n        \"raft\": {\r\n            \"requestTimeout\": \"PT5S\",\r\n            \"maxQuorumResponseTimeout\": \"PT0S\",\r\n            \"minStepDownFailureCount\": 3,\r\n            \"preferSnapshotReplicationThreshold\": 100,\r\n            \"preallocateSegmentFiles\": true\r\n        },\r\n        \"partitioning\": {\r\n            \"scheme\": \"ROUND_ROBIN\",\r\n            \"fixed\": []\r\n        },\r\n        \"queryApi\": {\r\n            \"enabled\": false\r\n        },\r\n        \"consistencyChecks\": {\r\n            \"enablePreconditions\": false,\r\n            \"enableForeignKeyChecks\": false,\r\n            \"settings\": {\r\n                \"enablePreconditions\": false,\r\n                \"enableForeignKeyChecks\": false\r\n            }\r\n        },\r\n        \"engine\": {\r\n            \"messages\": {\r\n                \"ttlCheckerBatchLimit\": 2147483647,\r\n                \"ttlCheckerInterval\": \"PT1M\"\r\n            }\r\n        },\r\n        \"features\": {\r\n            \"enableYieldingDueDateChecker\": false,\r\n            \"enableActorMetrics\": false,\r\n            \"enableBackup\": true,\r\n            \"enableMessageTtlCheckerAsync\": false\r\n        },\r\n        \"maxAppendBatchSizeInBytes\": 32768\r\n    },\r\n    \"executionMetricsExporterEnabled\": false,\r\n    \"processing\": {\r\n        \"maxCommandsInBatch\": 1\r\n    }\r\n}\r\n`\r\n@oleschoenburg Could you please point out the problem.\n megglos: ZDP-Triage:\n- to be discussed at planning as flagged by Ole\n megglos: ZDP-Planning:\r\n- will become more likely with sst partitioning enabled",
    "title": "Support S3 backup httpclient custom configuration."
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12283",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nI want to be able to open processes modeled with `zeebe-bpmn-model` in the Desktop Modeler as a C8 process by default. Currently, I need to set a lot of attributes by hand.\r\n\r\n```java\r\nDefinitions definitions = modelInstance.newInstance(Definitions.class);\r\n            definitions.setTargetNamespace(BPMN20_NS);\r\n            //definitions.setExporter(\"Camunda Modeler\");\r\n            //definitions.setExporterVersion(\"5.8.0\");\r\n            definitions.setAttributeValueNs(\"http://camunda.org/schema/modeler/1.0\",\"modeler:executionPlatform\",\"Camunda Cloud\");\r\n            modelInstance.setDefinitions(definitions);\r\n```\r\n\r\n**Describe the solution you'd like**\r\nProvide reasonable defaults for these attributes.\r\n\r\n**Describe alternatives you've considered**\r\nNone, the workaround above works as well.\r\n\r\n**Additional context**\r\nRequested by @superbeagle \n\n korthout: ZPA triage:\n\n- @remcowesterhoud mentioned that we should indicate somewhere in these attributes that this was generated with the zeebe-bpmn-model\n- @koevskinikola reproduced the issue already and it wasn't very pleasant\n- @koevskinikola the values should be defaults and can be changed by the user via the API (at least like above, or perhaps with an improved API)\n- @remcowesterhoud we should also add the other missing attributes\n- our own priority for this is low, but we think it's a good first issue for new contributors\n\n",
    "title": "Zeebe BPMN Model should provide reasonable defaults for definition attributes"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14137",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Description**\r\n\r\nA method should be added to retrieve the latest form by form id to `DbFormState` class.\n",
    "title": "Implement query to retrieve the latest form by formId from the state"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14135",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Description**\r\n\r\nA `Start Event` can be linked to a deployed form by the new `formId` field. The implementation is expected to include following:\r\n\r\n* Add the new `formId` attribute into the `ZeebeFormDefinition`\r\n* Update `ZeebeElementValidator.validate()` method in order to enable `zeebe` to validate also for group of fields. In the case, a start event with linked form is valid if only one field between `formKey` and `formId` is present\n",
    "title": "Allow binding forms to start events by formId"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14134",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Description**\r\n\r\nA `User Task` can be linked to a deployed form by the new `formId` field. The implementation is expected to include following:\r\n\r\n* Add the new `formId` attribute into the `ZeebeFormDefinition`\r\n* Update `ZeebeElementValidator.validate()` method in order to enable `zeebe` to validate also for group of fields. In the case, a user task with linked form is valid if only one field between `formKey` and `formId` is present\n",
    "title": "Allow binding forms to user tasks by formId"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14133",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Description**\r\n\r\nThis issue is created to implement state behaviour of the Form. It is expected to include following:\r\n\r\n* Create Form DB classes\r\n* Create Form column family definitions\r\n* Create a method to store the Form\r\n* Create a method to retrieve the latest form by form id\r\n* Create a method to retrieve a Form by key\n",
    "title": "Save Form to the state"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13319",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "### Description\r\n\r\nThe Gateway supports receiving and forwarding (to the Broker) DeployResource RPC calls with a `tenantId`.\r\n\r\n### AC\r\n- [x] The following gRPC messages contain a new `tenantId` property:\r\n       - [ ] `DeployResourceRequest`\r\n       - [ ] `ProcessMetadata`\r\n       - [ ] `DecisionMetadata`\r\n       - [ ] `DecisionRequirementsMetadata`\r\n- [x] The `deployResource(...)` Gateway endpoint passes the gRPC `DeployResourceRequest#tenantId` property to the `BrokerDeployResourceRequest`. The following scenarios are possible as well:\r\n      - If multi-tenancy is disabled (see #13237), the `BrokerDeployResourceRequest#tenantId` is set to `<default>`.\r\n      - If multi-tenancy is enabled, and `DeployResourceRequest#tenantId` is `null`, the deployment is rejected.\r\n- [ ] ~The `BrokerDeployResourceRequest#tenantIds` is set to the list of user accessible tenant ids provided through a gRPC context by the Identity SDK.~\r\n       -  The user's authorization list will be set in the `RecordMetadata`. It will be implemented through #13989 and #13237 \r\n\r\n### Blocked by\r\n- #13320\n",
    "title": "Gateway supports multi-tenancy in deployment RPCs"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14045",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "## Description\r\n\r\nRight now when we take snapshots we do that into a pending directory. If the snapshot is valid, meaning we reach a certain commit index OR we received all snapshot chunks, [we move (or copy depending on OS and filesystems) the snapshot to the valid snapshot directory](https://github.com/camunda/zeebe/blob/main/snapshot/src/main/java/io/camunda/zeebe/snapshots/impl/FileBasedSnapshotStore.java#L510).\r\n\r\nAfterward, [we check the snapshot checksum](https://github.com/camunda/zeebe/blob/main/snapshot/src/main/java/io/camunda/zeebe/snapshots/impl/FileBasedSnapshotStore.java#L517) (for that we have to read the whole snapshot again). This is mostly due to the case that we might fall back to copy and some error appears during copying. Important to note is that we already validated the checksum on receiving each chunk and the complete snapshot.\r\n\r\nWhat if we don't copy/move the snapshot from the pending, instead we directly write to the correct location? For that, we need a different marker when snapshots are valid. This could be a separate file, which marks the snapshot either as invalid (and removing it would mark it as valid) OR the existence of that file could mark it as valid. An example could be as soon as the checksum file exists the snapshot can be seen as valid.\r\n\r\n### Impact\r\n\r\nThis would mean we have to add some more complexity in reading and detecting valid snapshots, but in contrast, we wouldn't need to copy/move snapshots (interesting for large states) and potentially don't need to recheck the checksums again.\r\n\r\n### Context\r\n\r\nWhen working on https://github.com/camunda/zeebe/issues/13977 and https://github.com/camunda/product-hub/issues/1480 we (@npepinpe and I) discussed several issues and potential improvements regarding snapshot replication and taking snapshots, etc.. One thing that came up was that we create checksums of the whole snapshot, of each chunk, and when persisting we recreate such checksums (re-read the snapshots, etc.)\r\n\n\n npepinpe: We'll do it as a two step process:\n\n1. Stop computing the checksum after the move/copy, and just write the file out from in memory information. Instead, just verify all files are present. **This will let us verify how much time we save very quickly.**\n2. Since we're already handling the case where we crash after moving the snapshot but _before writing the checksum file_, it shouldn't be too much work to get rid of the pending folder, and simply use the checksum file as a marker. This mostly simplifies our data layout and (hopefully) code.\n npepinpe: @Zelldon - do you have time to work on it this week?\n Zelldon: @npepinpe I can try, but can't promise since I also have two other issues assigned, game day, benchmark stuff etc. But I give my best :) \n npepinpe: No worries. Let's sync later today to see what you can do this week.\n Zelldon: I have run two new benchmarks to see what effects bring the most recent changes.\r\n\r\nOne with the metrics added, but without any change, called [ck-persist-snapshot-no-change](https://grafana.dev.zeebe.io/d/zeebe-dashboard/zeebe?orgId=1&var-DS_PROMETHEUS=Prometheus&var-cluster=All&var-namespace=ck-persist-snapshot-no-change&var-pod=All&var-partition=All&from=1694069131830&to=1694087502965). #base\r\n\r\nThe other benchmark contains the most recent changes, meaning re-using the simple file verification checksum collection, and directly persisting it without recalculating it. \r\nBenchmark called:\r\n[ck-not-recalculate-sfv](https://grafana.dev.zeebe.io/d/zeebe-dashboard/zeebe?orgId=1&var-DS_PROMETHEUS=Prometheus&var-cluster=All&var-namespace=ck-not-recalculate-sfv&var-pod=All&var-partition=All&from=1694069131830&to=1694087502965)\r\n\r\n\r\nComparing both we can see the differences quite clearly. For the base, as soon we reach 1 gig of state we are already over 10s (the limit of the buckets) of just persisting a snapshot, including moving and creating checksum, etc. Comparing with the benchmark that includes the changes we can see that this is significant. The P99 is at most around 600 ms, instead of 10 seconds.\r\n\r\n\r\n![persist](https://github.com/camunda/zeebe/assets/2758593/f3e2a855-1849-4ad9-b6b8-139c3877aa5e)\r\n\n Zelldon: I started to replace the movement but realized that this was not straight-forward to do.\r\n\r\nThere was one thing that struck against it. It is the receiving of snapshots and where we expect multiple snapshots (and where we make directories unique) https://github.com/camunda/zeebe/blob/main/snapshot/src/main/java/io/camunda/zeebe/snapshots/impl/FileBasedSnapshotStore.java#L366-L372. It might happen that snapshot replication is restarted due some previous error/fault, then we want to write into a new directory.\r\n\r\nDiscussed this with @npepinpe To us it is not yet clear, why we not instead just remove the older snapshot and reuse the id or overwrite the snapshot. Since this would be part of improving the snapshot replication anyway we will defer this. We plan to make improvements like you can restart replicating snapshots at a certain position etc., which might help here as well.\r\n\r\nAfter deciding that, I hoped to keep the move for the received snapshots and write the transient snapshots directly to the correct snapshot directory already. Unfortunately, it entails several further works, like detecting what is a valid snapshot (only with checksum), deleting pending snapshots (which would be now without checksum), etc.\r\n\r\nI proposed and discussed with @npepinpe to continue with the snapshot replication improvements first, and then continue here to remove the movement. \r\n\r\nWe already benefit from the improvements shown above https://github.com/camunda/zeebe/issues/14045#issuecomment-1709656377, not recalculating the checksums, which is likely most of the benefit we gain out of this right now anyway.\r\n\r\nStill, it is right now weakening a bit the safety/verification, since we don't recalculate the checksum after the move. @npepinpe and I agreed that this is acceptable for now, with the plan to make rather soon progress with improving the replication and removing the move afterward. \r\n\r\n\n Zelldon: Ok I couldn't hold back. If we would decided to just delete pending received snapshots (which would the easiest approach for now) we can go with the following PR https://github.com/camunda/zeebe/pull/14230 and can remove the move.",
    "title": "No longer move/copy snapshots"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14032",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nFollowing up on the [introduction of the yield on the due date checker ](https://github.com/camunda/zeebe/pull/9249) and it's recent frequent use to recover from a partition getting stuck on too many due timers, we want to enable it by default for Zeebe 8.3 going forward.\r\n\r\n\r\n```[tasklist]\r\n### Tasks\r\n- [x] enable yielding on the due date checker by default for the upcoming Zeebe 8.3 release\r\n```\r\n\r\n\r\n\n",
    "title": "Enable yielding on due date checker by default"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13959",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nAs a user/developer, I can specify which variables are propagated from parent to child called process instances. They can specify whether all variables or specific ones need to be propagated.\r\n\r\nTherefore, a property called `propagateAllParentVariables` is introduced to enable/disable copying all variables to its child process instance:\r\n\r\n1. When `propagateAllParentVariables` is set to `true`, all variables are propagated to the child process instance.\r\n2. When `propagateAllParentVariables` is set to `false`, only the variables defined via _input mappings_ are propagated to the child process instance.\r\n\r\nThe current default behavior - propagating all variables - does not change, meaning, the property's `propagateAllParentVariables` default value is `true`.\r\n\r\n---\r\n\r\nrelated to https://github.com/camunda/product-hub/issues/141\n",
    "title": "When using a call activity, only the variables defined via input mappings are propagated"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13958",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "Integrate `ClusterTopologyManger` with broker. `ClusterTopologyManagerStep` should start the manager. The step is completed only after the topology is initialized by `ClusterTopologyManager`. The rest of the services should use the partition distribution based on the cluster topology initialized by `ClusterTopologyManager`.\r\n\r\nAlso, add a feature flag for this. When the flag is disabled, it should use the old ways of generating partition distribution from the BrokerCfg.\n",
    "title": "Broker uses configuration from ClusterTopologyManager"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13956",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": " If a coordinator restarts with a data loss, then its local persisted topology will be  empty. If the coordinator generates the  topology from static configuration, there is a possibility of inconsistency if the static configuration does not match  the actual cluster topology.\r\n\r\nTo prevent this, coordinator should first guqery known members before generating the topology. If at least one member return an uninitalized topology, coordinator assumes that this is the bootstrap of the cluster and generates a new topology.\r\n\n\n deepthidevaki: This is done in #13960 ",
    "title": "Config manager in coordinator queries known members before initializing configuration"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13923",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "## Description\r\n\r\nWhen the [weekly e2e tests are started](https://github.com/camunda/zeebe/blob/main/.github/workflows/weekly-e2e.yml#L60) we always pass the `Zeebe SNAPSHOT` as a generation. This generation is used as a template for the generation testbench creates.\r\nIn this generation all components are set to a SNAPSHOT version (except for Optimize). For our test we only override the Zeebe version. This means we could end up with a generation such as:\r\n\r\n- `Zeebe:8.0.20-SNAPSHOT`\r\n- `Tasklist:SNAPSHOT`\r\n- `Operate:SNAPSHOT`\r\n\r\nIdeally we would align the versioning to actually test on an 8.0 platform in this case. This would involve using the `Camunda Platform 8.0.19` generation as a template for the generation testbench creates.\r\n\r\n------------\r\n\r\nAll components of the platform should be backwards compatible. This means using an older Zeebe version with a newer Tasklist/Operate should, in theory, not have any impact on the tests.\r\n\n\n remcowesterhoud: ZPA individual triage:\r\n- Nice to have, but I don't see urgency as this shouldn't impact our tests. It's worked like this for years and that's unlikely to change.\r\n- Moving to the backlog as something we can do later\n megglos: ZDP-Triage:\n- it caused noise in the QA tests - e.g. a bug of a recent snapshot of a web component\n- might be an easy fix if there are patch snapshots available of the web components we could just use the same version then - we could check that\n- check on the availability of the web components patch snapshots @megglos \n megglos: patch snapshots are not available from e.g. Operate, we would need to use the previous patch then, thus use the latest generation as template as suggested by Remco.",
    "title": "Align component versions in our e2e tests"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13914",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "## Description\r\n\r\nTo ensure the stability of the new job push/streaming feature, we should integrate it into our long running benchmarks. This means updating the `Worker` application with the following features:\r\n\r\n- [x] Configuration to enable streaming (i.e. call `JobWorkerBuilder#enableStreaming`)\r\n- [x] Configuration for the stream timeout (default to 8 hours)\r\n\r\nLet's enable streaming by default in future benchmarks as well to root out as many errors as possible, and because this is our desired end state. If we see it's too buggy, we can roll it back.\r\n\r\n> **Note**\r\n> When I prototyped this, I hit a bunch of performance issues with the `Worker` application itself. I don't know if this will happen again, but be wary of it.\r\n\r\nOnce https://github.com/camunda/zeebe/pull/13818 is merged, we should also:\r\n\r\n- [x] Migrate `Worker` to use Micrometer + Prometheus (possibly migrate the `Starter` if it makes sense, since they share some code)\r\n- [x] Enable job worker metrics\r\n- [x] Update the dashboard to visualize the job worker metrics\n\n rodrigo-lourenco-lopes: PR https://github.com/camunda/zeebe/pull/13976 was only half of the issue, reopening this",
    "title": "Integrate job worker streaming and metrics into benchmarks"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13880",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "<!--\r\nIn case you have questions about our software we encourage everyone to participate in our community via the\r\n- Camunda Platform community forum https://forum.camunda.io/ or \r\n- Slack https://camunda-cloud.slack.com/ (For invite: https://camunda-slack-invite.herokuapp.com/)\r\n\r\nThere you can exchange ideas with other Zeebe and Camunda Platform 8 users, as well as the product developers, and use the search to find answer to similar questions.\r\n\r\nThis issue template is used by the Zeebe engineers to create general tasks.\r\n-->\r\n\r\n**Description**\r\n\r\nRelated to https://github.com/camunda/zeebe/issues/13879 we should try to avoid and reject such process models which use too high durations. This [recently caused an error ](https://console.cloud.google.com/errors/detail/CI_6pI2Uhor3aQ;service=zeebe;time=P7D?project=camunda-cloud-240911)on prod, when a too-high duration was used in a model.\r\n\r\n![errorprocessduration-process](https://github.com/camunda/zeebe/assets/2758593/7b572983-4202-432d-904c-b689227869c3)\r\n\r\nThis duration can not only given statically but also dynamically via variables which means we need to react on runtime.\r\n\r\nRight now if the model is executed the batch is rolled back and the last user command is rejected. The problem here is that the UX might be not that optimal, it might be worth to create an incident on the problematic element and clearly define what is wrong. This would help the user to fix there model or variable.\n\n remcowesterhoud: Individual triage:\r\n- Same thing as #13879, I'd recommend picking these up together.\r\n- We should check the result of the duration expressions and create an incident it overflows.\r\n- Worst-case this causes a banned instance.\r\n\r\nMarking as `upcoming` as banned instances should be avoided.\r\nSizing as `small` as it's easy to reproduce\r\nMarking impact as `medium`. We don't see this happening often, but when it happens it causes banning which is always bad.",
    "title": "Create incident when timer duration is too large"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13879",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "<!--\r\nIn case you have questions about our software we encourage everyone to participate in our community via the\r\n- Camunda Platform community forum https://forum.camunda.io/ or \r\n- Slack https://camunda-cloud.slack.com/ (For invite: https://camunda-slack-invite.herokuapp.com/)\r\n\r\nThere you can exchange ideas with other Zeebe and Camunda Platform 8 users, as well as the product developers, and use the search to find answer to similar questions.\r\n\r\nThis issue template is used by the Zeebe engineers to create general tasks.\r\n-->\r\n\r\n**Description**\r\n\r\nRelated to https://github.com/camunda/zeebe/issues/2108\r\n\r\nWe decided once to skip this but it turns out that actually, users deploy (who could know this 🤔) such models. This recently caused an error on prod https://console.cloud.google.com/errors/detail/CI_6pI2Uhor3aQ;service=zeebe;time=P7D?project=camunda-cloud-240911\r\n\r\n\r\n```\r\njava.lang.RuntimeException: java.lang.ArithmeticException: long overflow\r\n\tat io.camunda.zeebe.stream.impl.BufferedResult.executePostCommitTasks(BufferedResult.java:57) ~[zeebe-stream-platform-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.executePostCommitTasks(ProcessingStateMachine.java:591) ~[zeebe-stream-platform-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$executeSideEffects$18(ProcessingStateMachine.java:566) ~[zeebe-stream-platform-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.scheduler.retry.ActorRetryMechanism.run(ActorRetryMechanism.java:28) ~[zeebe-scheduler-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.scheduler.retry.AbortableRetryStrategy.run(AbortableRetryStrategy.java:45) ~[zeebe-scheduler-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) [zeebe-scheduler-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:109) [zeebe-scheduler-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:205) [zeebe-scheduler-8.2.11.jar:8.2.11]\r\nCaused by: java.lang.ArithmeticException: long overflow\r\n\tat java.lang.Math.multiplyExact(Unknown Source) ~[?:?]\r\n\tat java.time.Duration.toNanos(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.scheduler.ActorControl.scheduleTimer(ActorControl.java:140) ~[zeebe-scheduler-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.scheduler.ActorControl.runDelayed(ActorControl.java:115) ~[zeebe-scheduler-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingScheduleServiceImpl.lambda$runDelayed$0(ProcessingScheduleServiceImpl.java:50) ~[zeebe-stream-platform-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingScheduleServiceImpl.useActorControl(ProcessingScheduleServiceImpl.java:77) ~[zeebe-stream-platform-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingScheduleServiceImpl.runDelayed(ProcessingScheduleServiceImpl.java:50) ~[zeebe-stream-platform-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingScheduleServiceImpl.runDelayed(ProcessingScheduleServiceImpl.java:55) ~[zeebe-stream-platform-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.stream.impl.ExtendedProcessingScheduleServiceImpl.runDelayed(ExtendedProcessingScheduleServiceImpl.java:56) ~[zeebe-stream-platform-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.engine.processing.scheduled.DueDateChecker.schedule(DueDateChecker.java:54) ~[zeebe-workflow-engine-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.engine.processing.timer.DueDateTimerChecker.scheduleTimer(DueDateTimerChecker.java:41) ~[zeebe-workflow-engine-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.engine.processing.common.CatchEventBehavior.lambda$subscribeToTimerEvent$9(CatchEventBehavior.java:293) ~[zeebe-workflow-engine-8.2.11.jar:8.2.11]\r\n\tat io.camunda.zeebe.stream.impl.BufferedResult.executePostCommitTasks(BufferedResult.java:55) ~[zeebe-stream-platform-8.2.11.jar:8.2.11]\r\n\t... 10 more\r\n```\r\n\r\nThe duration contained `PT9999999990S` \r\n\r\n![errorprocessduration-process](https://github.com/camunda/zeebe/assets/2758593/6120d063-2747-4cd1-a230-7b5ee39cde50)\r\n\r\nLikely this was a test process model, but still we should simply such models if the duration is static.\r\n\n\n remcowesterhoud: Individual triage:\r\n- Easily reproducible.\r\n- The chance of this happening seems quite small, especially when it's a static duration like in the example. Of course having an expression makes it less obvious.\r\n- We could catch static expressions during deployment and reject the deployment.\r\n- We should check the result of the duration expressions and create an incident it overflows.\r\n- Worst-case this causes a banned instance.\r\n- We should discuss with the Modeler team about listing rules surrounding this.\r\n\r\nMarking as `upcoming` as banned instances should be avoided.\r\nSizing as `small` as it's easy to reproduce\r\nMarking impact as `medium`. We don't see this happening often, but when it happens it causes banning which is always bad.",
    "title": "Reject static duration which is too large"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13791",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nAs done for dependabot https://github.com/camunda/zeebe/issues/10295 and backport PRs https://github.com/camunda/zeebe/issues/13666, let's automerge renovate PRs when CI is green.\r\n\r\ne.g. https://github.com/camunda/zeebe/pull/13781\n",
    "title": "Automerge green renovate PRs"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13787",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nI noticed this during the release, on 8.0/8.1 building starter & worker fails as e.g. the maven wrapper is not present on 8.1\r\n```\r\n/home/runner/work/_temp/79d164a7-e6b4-4e02-9e4e-89a3d8a459e8.sh: line 1: ./mvnw: No such file or directory\r\nError: Process completed with exit code 127.\r\n```\r\nhttps://github.com/camunda/zeebe/actions/runs/5738456998/job/15552373143\r\n\r\nOn 8.0 also build zeebe fails as there is no DIST=`build` setup in the Dockerfile https://github.com/camunda/zeebe/actions/runs/5202088373/job/14079846461\r\n\r\n\r\n```[tasklist]\n### Tasks\n- [x] backport maven wrapper to 8.0/1 - to avoid workflow merge conflicts from main to stable\n- [x] Add a `benchmark.yaml` workflow to each stable branch - to maintain a stable setup\n- [x] trigger the workflows via [workflow dispatch](https://docs.github.com/en/free-pro-team@latest/rest/actions/workflows?apiVersion=2022-11-28#create-a-workflow-dispatch-event) from the https://github.com/zeebe-io/zeebe-engineering-processes referencing the release_branch\n- [x] delete the `dispatch-benchmark.yaml` workflow from main\n```\r\n\r\n\r\n\n\n megglos: ZDP-Triage:\n- will look into it asap to resolve\n remcowesterhoud: Individual triage:\r\n- Seems to be actively worked on by ZDP at this time. It's not sensible to work on this simultaneously so I'll move it to the ZPA backlog. This also allows us to focus multi-tenancy and migration.",
    "title": "Release: `Repo dispatch Benchmark` fails on 8.0/8.1"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13775",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nWe had an incident where the liveness probes failed after 45s due to large state. This is a cluster with 3 partitions, each with a RocksDB state of over 7GB. For each partition, it took about 10 seconds purely to open the DB, which includes copying the snapshot and opening the DB. Furthermore, it seems like each partition was recovered sequentially and not in parallel, from the logs, but I haven't verified that.\r\n\r\nThe goal here would be to try and either optimize the recovery time, or have it not be w.r.t the size of the state (ideally the second one).\r\n\r\nOne issue which might be the biggest culprit in this case is this one: https://github.com/camunda/zeebe/issues/5682\r\n\n\n npepinpe: Useful link as well: https://github.com/facebook/rocksdb/wiki/Speed-Up-DB-Open\n megglos: ZDP-Triage:\n- sequential partition recover prolongs startup\n- such situations may require increasing the timeout for the liveness probe\n- we need to double-check whether we disabled the WAL of rocksdb for all versions or just 8.2\n- increased number of sst files due to partitoning is default in 8.3 => this may make it worse\n- it raised the question if we aim to support such a large snapshot state\n- on such snapshot sizes snapshot replication is also problematic\n\n=> relates to the epic to support large state - maintaining consistent performance on larger states\n megglos: realtes to another iteration on https://github.com/camunda/product-hub/issues/989\n megglos: @megglos follow up with PM in regards to a potential new epic on improving the ability of zeebe to cope with a large state\n remcowesterhoud: Large state also proofs problematic for the Startup Probe.\r\n\r\n```\r\nWarning  Unhealthy  4m28s (x288 over 71m)  kubelet  Startup probe failed: HTTP probe failed with statuscode: 503\r\n```\r\n\r\nThe snapshot size of the cluster is very large:\r\n<img width=\"743\" alt=\"image\" src=\"https://github.com/camunda/zeebe/assets/5787702/c95535eb-da42-48b5-afa4-55ac6d18e74a\">\r\n\r\nThe last log before the pod got killed:\r\n```\r\nRaftServer{raft-partition-partition-2}{role=FOLLOWER} - Started receiving new snapshot FileBasedReceivedSnapshot{directory=/usr/local/zeebe/data/raft-partition/partitions/2/pending/307189154-1021-698451241-698450032-1, snapshotStore=SnapshotStore-2, metadata=FileBasedSnapshotId{index=307189154, term=1021, processedPosition=698451241, exporterPosition=698450032}} from 1\r\n```\r\n\r\nMy hypothesis is that receiving the snapshot takes longer than the startup probe has before it kills the pod, resulting in a startup loop.\n Zelldon: I think in the incident above the snapshot replication was more the issue, and restarting of replications (due to pod restarts and due to new snapshots etc.). So we have multiple root causes which can have as a symptom that the broker is unable to start when the state is large.\r\n\r\n\n npepinpe: Yeah, Remco and I talked about it, but still felt it was relevant to post here for now to highlight multiple issues with startup time and large state (even if the original issue also talks about liveness)\n megglos: ZDP-Planning:\r\n- initial issue was that partition startup was also sequential - this may be mitigated by parallel startup already (not intended to be backported), might be bound by IO limits then\r\n- another case snapshot replication took a long time\r\n- should be handled as part of the large state\r\n- probe timeouts were increased to give more headroom (from 45s to 1m30s)\r\n- would be worth assessing the risk for other customers and current prospects\r\n\r\n=> just fixing recovery time might not be enough => we need to asses this in context of the epic https://github.com/camunda/product-hub/issues/1480\r\n\n npepinpe: The issue we merged is related, but does not complete this.",
    "title": "Improve ZeebePartition recovery time with large state"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13763",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "## Description\r\n\r\nOn startup we run a bunch of migrations to get the state in a good state for any new features. Often this means populating newly added ColumnFamilies.\r\n\r\nThe first step of a migration is checking if it needs to run. When adding a new ColumnFamily this is simple. We can check if the new ColumnFamily is empty, and if the ColumnFamily that the data is migrated from contains data.\r\n\r\nIn some cases there is no real way to determine if the migration needs to run by just looking at the data. One such example is when adding a field to an object in an existing ColumnFamily. This situation occurred once now, but I can see this happening more often in the future.\r\n\r\nWe should keep track of migrations in the state. This way we always know if a migration already ran or not, and we don't have to get smart with other checks.\r\n\r\n## Desired Solution\r\n- We introduce a new ColumnFamily `STATE_MIGRATIONS`\r\n- We add some identifier to migrations (e.g. some kind of ENUM)\r\n- The `STATE_MIRATIONS` gets the identifier as a key and an enum indicating the state of the migration as value\r\n    - The value enum could be `RUNNING`, `FINISHED`.\r\n        - I chose an enum so could have incremental migrations. Maybe we'll encounter scenarios where a migrations will be done in steps.\r\n    - If the key can't be found in the ColumnFamily it means the migration has not started yet.\r\n- Upon startup we check in this ColumnFamily if the migration has ran. If not we add it with value `RUNNING`. We run the migration and once it's finished we update the value to `FINISHED`.\n",
    "title": "Add ColumnFamily to keep track of executed migrations"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13736",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nSince we merged https://github.com/camunda/zeebe/pull/13706 renovate logs indicate it cannot handle the ubuntu tags using a date as versioning, see the logs here\r\nhttps://developer.mend.io/github/camunda/zeebe/-/job/3cdacac3-5fae-4252-83d7-23371ae59cf6\r\n\r\n```\r\nDependency ubuntu has unsupported/unversioned value jammy-20230624 (versioning=ubuntu)\r\n{\r\n  \"baseBranch\": \"main\"\r\n}\r\n```\r\n\r\nThis issue indicates there seems to be a way to configure renovate to support these though\r\nsee https://docs.renovatebot.com/modules/versioning/#ubuntu-versioning\r\nand a potential workaround here https://github.com/renovatebot/config-help/issues/633\r\n\n\n megglos: ZDP-Triage:\n- even though we do a package update as part of the docker build we should make sure we use the latest base\n remcowesterhoud: Individual triage:\r\n\r\n- We should do this\r\n- As I can see ZDP has it in their Ready column I assume they're planning to work on it. This is why I'll put it in the backlog for ZPA, we don't need both teams working on it simultaneously. This also allows us to focus on the multi-tenancy and migration topics.\n megglos: We ditched the date versioning and went for the plain names release tag + digest pinning, see https://github.com/camunda/zeebe/pull/14071#discussion_r1311176361",
    "title": "Configure Renovate to work with ubuntu tag versioning"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13520",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "To unblock the teams building multi-tenancy support to the other C8 components, we want to expand our protocol schema, which will be available for later usage, specifically by adding the affected tenant(s) to the record value interfaces.\r\n\r\n## Details\r\nThe new additions to the schema must be marked as experimental.\r\n\r\nAs this is only to unblock other teams, the value will not yet be filled with actual tenant ids. Instead, they will contain an empty string.\r\n\r\n## Schema changes\r\nThe record values involved with the following RPCs need a way to specify a tenant:\r\n\r\n- DeployResource RPC\r\n- CreateProcessInstance RPC\r\n- CreateProcessInstanceWithResult RPC\r\n- EvaluateDecision RPC\r\n- BroadcastSignal RPC\r\n- PublishMessage RPC\r\n\r\nThe record values involved with the following RPCs need a way to specify one or more tenants:\r\n\r\n- ActivateJobs RPC\r\n- StreamJobs RPC (we can consider this one out of scope for this issue)\r\n\r\n## Out of scope\r\nNote that these changes don't yet reflect the tenants a client may have access to. This will be covered in another issue.\r\n\r\n## Additional context\r\n- [Internal details](https://docs.google.com/document/d/1qWx82v7yqlJaL4NRXNca2Uxk4NZnmgh0sd5LQFeobHc/edit?pli=1#heading=h.6nar7bl09aov)\r\n\r\n## Unblocks\r\n- https://github.com/camunda/operate/issues/4778\n",
    "title": "Add experimental multi-tenancy record schema to protocol"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13048",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nThe job streamer API on the gateway side is asynchronous to avoid putting the endpoint's logic into the client streamer's actor context. \r\n\r\nHowever, there is no executor available in the `EndpointManager`. By default, service calls run in the shared gRPC executor (a cached static pool defined in `GrpcUtil`). But there's no easy way to get back to that executor from another thread context.\r\n\r\nThe issue here is to make this executor available at least to the call handling the job stream command.\r\n\r\nPossible solutions:\r\n\r\n- Configure both a gRPC `executor` and `serverCallExecutor`. So now the Netty event loop group will handle the network calls, the executor will handle the gRPC portion, and the server call executor will handle server call pipeline, include the endpoint manager calls. As these are manually configured, we can pass a reference of the `serverCallExecutor` to our endpoint manager as the executor.\r\n- Like the job activation handlers, make the stream job handler an actor.\n\n npepinpe: One option is to define a gRPC executor, and provide the same one for the endpoint manager. Concurrency is then shared amongst handling requests and pushing jobs, in a way.\r\n\r\nAnother option is to handle each request in a one-actor-per-request model, via the gRPC `callExecutor`. This allows providing one executor per call. We could submit an actor then, or keep a running actor pool and forward handling the request to specific actors. This would keep us in the actor scheduler world, while still providing an actor for each request. This may require deployments to increase the number of management threads, which is typically quite low on default gateway deployments.\r\n\r\n",
    "title": "Provide access to an executor in the endpoint manager"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11710",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nOnce #11707, #11708, #11709, and #11713, we should integrate them together and test the complete job stream lifecycle.\r\n\r\n- [x] A client can register a new job stream worker all the way to the broker\r\n- [x] Two stream workers with equivalent properties, on the same gateway, become one on the broker\r\n- [x] Two streams with different properties, on the same gateway, are two different workers on the broker\r\n- [x] When a client call is closed (gracefully or not), and there are no other logically equivalent clients on the gateway, the gateway will remove the stream worker from the broker\r\n- [x] When a client call is closed (gracefully or not), and there is at least one other logically equivalent client on the gateway, the gateway will not remove the stream worker from the broker\r\n- [x] When a gateway shuts down gracefully, it should remove all associated stream workers from the broker.\r\n\r\nI opted against testing with multiple gateways at the QA level. One thing is that our test infrastructure doesn't support multiple gateways (yet). Another is that we already test with multiple members at the transport level (but using integration test), and from a client point of view, it doesn't really matter.\r\n\n",
    "title": "Integrate end-to-end job stream lifecycle management"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/8571",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nThe DMN engine parses a DMN resource into an in-memory data object that is used to evaluate the decision. The parsing of DMN decisions is (relatively) costly. Instead of parsing it every time, we can cache the parsed DMN (e.g. using an LRU cache).\r\n\r\n* store the parsed DMN decisions in a cache\r\n* the cache should have a fixed capacity (i.e. using an LRU cache - similar to the [cache from agrona](https://github.com/real-logic/agrona/blob/master/agrona/src/main/java/org/agrona/collections/IntLruCache.java))\r\n* the cache may be used transparently in a behavior class and is filled by the DMN state\r\n\r\n\r\nRelates to https://jira.camunda.com/browse/SUPPORT-17656\r\n\n\n megglos: A PoC to assess the impact of this improvement would be a great first scope.\n megglos: ZDP-Planning:\r\n- it surfaced for 3-4+ customers so far\r\n- PoC focus would be: build a prototype to be evaluated in a performance test\n megglos: As synced with @remcowesterhoud here https://github.com/camunda/zeebe/pull/13713#issuecomment-1663557887\r\n**we need to include this into the next iteration aiming for the September patch or earlier for two customers**",
    "title": "Avoid parsing DMN decisions for every evaluation"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/5121",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nWe should reevaluate the banning concept, whether it makes sense in the way it is currently built and how we can improve it.\r\nCurrently we store banned instances forever, which means we can end up in situations where we have a lot of banned instances, which can't be removed from the state.\r\n\n\n Zelldon: Do we have a time plan for this? I think there was already some decisions that we want to replace it with incidents right?",
    "title": "Reevaluate Banning concept"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14045",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "## Description\r\n\r\nRight now when we take snapshots we do that into a pending directory. If the snapshot is valid, meaning we reach a certain commit index OR we received all snapshot chunks, [we move (or copy depending on OS and filesystems) the snapshot to the valid snapshot directory](https://github.com/camunda/zeebe/blob/main/snapshot/src/main/java/io/camunda/zeebe/snapshots/impl/FileBasedSnapshotStore.java#L510).\r\n\r\nAfterward, [we check the snapshot checksum](https://github.com/camunda/zeebe/blob/main/snapshot/src/main/java/io/camunda/zeebe/snapshots/impl/FileBasedSnapshotStore.java#L517) (for that we have to read the whole snapshot again). This is mostly due to the case that we might fall back to copy and some error appears during copying. Important to note is that we already validated the checksum on receiving each chunk and the complete snapshot.\r\n\r\nWhat if we don't copy/move the snapshot from the pending, instead we directly write to the correct location? For that, we need a different marker when snapshots are valid. This could be a separate file, which marks the snapshot either as invalid (and removing it would mark it as valid) OR the existence of that file could mark it as valid. An example could be as soon as the checksum file exists the snapshot can be seen as valid.\r\n\r\n### Impact\r\n\r\nThis would mean we have to add some more complexity in reading and detecting valid snapshots, but in contrast, we wouldn't need to copy/move snapshots (interesting for large states) and potentially don't need to recheck the checksums again.\r\n\r\n### Context\r\n\r\nWhen working on https://github.com/camunda/zeebe/issues/13977 and https://github.com/camunda/product-hub/issues/1480 we (@npepinpe and I) discussed several issues and potential improvements regarding snapshot replication and taking snapshots, etc.. One thing that came up was that we create checksums of the whole snapshot, of each chunk, and when persisting we recreate such checksums (re-read the snapshots, etc.)\r\n\n\n npepinpe: We'll do it as a two step process:\n\n1. Stop computing the checksum after the move/copy, and just write the file out from in memory information. Instead, just verify all files are present. **This will let us verify how much time we save very quickly.**\n2. Since we're already handling the case where we crash after moving the snapshot but _before writing the checksum file_, it shouldn't be too much work to get rid of the pending folder, and simply use the checksum file as a marker. This mostly simplifies our data layout and (hopefully) code.\n npepinpe: @Zelldon - do you have time to work on it this week?\n Zelldon: @npepinpe I can try, but can't promise since I also have two other issues assigned, game day, benchmark stuff etc. But I give my best :) \n npepinpe: No worries. Let's sync later today to see what you can do this week.\n Zelldon: I have run two new benchmarks to see what effects bring the most recent changes.\r\n\r\nOne with the metrics added, but without any change, called [ck-persist-snapshot-no-change](https://grafana.dev.zeebe.io/d/zeebe-dashboard/zeebe?orgId=1&var-DS_PROMETHEUS=Prometheus&var-cluster=All&var-namespace=ck-persist-snapshot-no-change&var-pod=All&var-partition=All&from=1694069131830&to=1694087502965). #base\r\n\r\nThe other benchmark contains the most recent changes, meaning re-using the simple file verification checksum collection, and directly persisting it without recalculating it. \r\nBenchmark called:\r\n[ck-not-recalculate-sfv](https://grafana.dev.zeebe.io/d/zeebe-dashboard/zeebe?orgId=1&var-DS_PROMETHEUS=Prometheus&var-cluster=All&var-namespace=ck-not-recalculate-sfv&var-pod=All&var-partition=All&from=1694069131830&to=1694087502965)\r\n\r\n\r\nComparing both we can see the differences quite clearly. For the base, as soon we reach 1 gig of state we are already over 10s (the limit of the buckets) of just persisting a snapshot, including moving and creating checksum, etc. Comparing with the benchmark that includes the changes we can see that this is significant. The P99 is at most around 600 ms, instead of 10 seconds.\r\n\r\n\r\n![persist](https://github.com/camunda/zeebe/assets/2758593/f3e2a855-1849-4ad9-b6b8-139c3877aa5e)\r\n\n Zelldon: I started to replace the movement but realized that this was not straight-forward to do.\r\n\r\nThere was one thing that struck against it. It is the receiving of snapshots and where we expect multiple snapshots (and where we make directories unique) https://github.com/camunda/zeebe/blob/main/snapshot/src/main/java/io/camunda/zeebe/snapshots/impl/FileBasedSnapshotStore.java#L366-L372. It might happen that snapshot replication is restarted due some previous error/fault, then we want to write into a new directory.\r\n\r\nDiscussed this with @npepinpe To us it is not yet clear, why we not instead just remove the older snapshot and reuse the id or overwrite the snapshot. Since this would be part of improving the snapshot replication anyway we will defer this. We plan to make improvements like you can restart replicating snapshots at a certain position etc., which might help here as well.\r\n\r\nAfter deciding that, I hoped to keep the move for the received snapshots and write the transient snapshots directly to the correct snapshot directory already. Unfortunately, it entails several further works, like detecting what is a valid snapshot (only with checksum), deleting pending snapshots (which would be now without checksum), etc.\r\n\r\nI proposed and discussed with @npepinpe to continue with the snapshot replication improvements first, and then continue here to remove the movement. \r\n\r\nWe already benefit from the improvements shown above https://github.com/camunda/zeebe/issues/14045#issuecomment-1709656377, not recalculating the checksums, which is likely most of the benefit we gain out of this right now anyway.\r\n\r\nStill, it is right now weakening a bit the safety/verification, since we don't recalculate the checksum after the move. @npepinpe and I agreed that this is acceptable for now, with the plan to make rather soon progress with improving the replication and removing the move afterward. \r\n\r\n\n Zelldon: Ok I couldn't hold back. If we would decided to just delete pending received snapshots (which would the easiest approach for now) we can go with the following PR https://github.com/camunda/zeebe/pull/14230 and can remove the move.",
    "title": "No longer move/copy snapshots"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14033",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nThe stream transport protocol uses the schema ID 0, which is also the schema ID used by the `protocol` module. This means applying headers would possibly cause inconsistencies if you receive something from the other protocol.\r\n\r\nSeverity is quite low because it's unlikely to happen, but better do it before we release this whole thing.\n\n npepinpe: @megglos - this is technically a breaking change between alpha4 and alpha5 (if we merge it before alpha5), which will cause some errors during rolling update, but as most people are likely not using job push yet, it should be OK-ish.",
    "title": "Change protocol ID of the stream transport protocol"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13736",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nSince we merged https://github.com/camunda/zeebe/pull/13706 renovate logs indicate it cannot handle the ubuntu tags using a date as versioning, see the logs here\r\nhttps://developer.mend.io/github/camunda/zeebe/-/job/3cdacac3-5fae-4252-83d7-23371ae59cf6\r\n\r\n```\r\nDependency ubuntu has unsupported/unversioned value jammy-20230624 (versioning=ubuntu)\r\n{\r\n  \"baseBranch\": \"main\"\r\n}\r\n```\r\n\r\nThis issue indicates there seems to be a way to configure renovate to support these though\r\nsee https://docs.renovatebot.com/modules/versioning/#ubuntu-versioning\r\nand a potential workaround here https://github.com/renovatebot/config-help/issues/633\r\n\n\n megglos: ZDP-Triage:\n- even though we do a package update as part of the docker build we should make sure we use the latest base\n remcowesterhoud: Individual triage:\r\n\r\n- We should do this\r\n- As I can see ZDP has it in their Ready column I assume they're planning to work on it. This is why I'll put it in the backlog for ZPA, we don't need both teams working on it simultaneously. This also allows us to focus on the multi-tenancy and migration topics.\n megglos: We ditched the date versioning and went for the plain names release tag + digest pinning, see https://github.com/camunda/zeebe/pull/14071#discussion_r1311176361",
    "title": "Configure Renovate to work with ubuntu tag versioning"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/13816",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/14184",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/14074",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/14071",
      "component": "release/8.3.0-alpha6",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5312",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3438",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2078",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2091",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2089",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2072",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2062",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2044",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2050",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2028",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2047",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2036",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2026",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2025",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2024",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2023",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2033",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1980",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1962",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1961",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1963",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1878",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1837",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1830",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1823",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1827",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1818",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1816",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1791",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1789",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1749",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2093",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2092",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2081",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1972",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2080",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2077",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2061",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2064",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2058",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2059",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2056",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2040",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2022",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2021",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2020",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2019",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2018",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2017",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2016",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2015",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2014",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2013",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2012",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2002",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1717",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1968",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1953",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1941",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1940",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1933",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1932",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1931",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1921",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1912",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1899",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1907",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1890",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1881",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1889",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1886",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1879",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1876",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1874",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1814",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1825",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1824",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1804",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1780",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1775",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1768",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1758",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1757",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1756",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1737",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2079",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2076",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2075",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2071",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2070",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2027",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2055",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2053",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2052",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2051",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2041",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2042",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2039",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2037",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2038",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2035",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2034",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2006",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2007",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2011",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2010",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2009",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2008",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2005",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2004",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2003",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2001",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/2000",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1999",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1998",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1997",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1984",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1986",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1985",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1965",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1964",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1958",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1951",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1954",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1952",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1947",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1949",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1948",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1946",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1945",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1944",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1942",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1943",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1934",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1939",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1937",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1935",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1925",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1924",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1908",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1923",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1919",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1922",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1918",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1916",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1917",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1914",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1913",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1904",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1911",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1909",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1910",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1896",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1906",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1897",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1902",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1877",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1895",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1893",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1894",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1891",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1892",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1849",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1883",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1888",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1882",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1875",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1865",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1863",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1864",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1861",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1862",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1856",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1854",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1852",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1834",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1850",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1839",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1829",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1838",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1833",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1836",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1832",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1826",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1828",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1820",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1822",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1808",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1806",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1807",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1805",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1798",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1795",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1800",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1533",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1796",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1794",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1781",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1782",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1792",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1777",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1778",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1776",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1759",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1772",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1769",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1764",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1765",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1761",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1747",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1755",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1754",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1753",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1752",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1735",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha6",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1736",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12382). This was done in order to better comply with the [OWASP recommendations on Docker Security](https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-2-set-a-user)",
      "component": "Zeebe",
      "subcomponent": "Docker - user",
      "context": "Breaking Changes ❗ "
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nCurrently the zeebe process is run by the root user in the zeebe docker image:\r\n```\r\nroot@5ce6a5346a36:/usr/local/zeebe# ps aux\r\nUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\r\nroot         1  0.3  0.0   1940   448 ?        Ss   14:26   0:00 tini -- /usr/local/bin/startup.sh\r\nroot         7  197  1.4 4216156 295668 ?      Sl   14:26   0:05 /opt/java/openjdk/bin/java -Xms128m -XX:+ExitOnOutOfMemoryError -Dfile.encoding=UTF-8 -classpath /usr/local/zeebe/config:/usr/local/zeebe/lib/* -Dapp.name=broker -Dapp.pid=7 -Dapp.repo=/usr/local/zeebe\r\nroot        42  1.0  0.0   6880  3364 pts/0    Ss   14:26   0:00 /bin/bash\r\nroot        55  0.0  0.0   8476  2808 pts/0    R+   14:26   0:00 ps aux\r\n```\r\n\r\nTo harden the security of the docker image it should **default** to run it with an unprivileged user instead, [see the OWASP recommendation](https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-2-set-a-user).\r\n\r\n**Describe the solution you'd like**\r\nThe zeebe image **should by default run with an unprivileged user.** [There is already a zeebe user setup with uid 1000](https://github.com/camunda/zeebe/blob/main/Dockerfile#L105-L111) it's just not used.\r\n\r\n**Additional context**\r\nRelates to https://github.com/camunda/product-hub/issues/717\r\nsupport for running with an unprivileged user was added with https://github.com/camunda/zeebe/issues/11784\r\nRelates to https://jira.camunda.com/browse/SUPPORT-15969\n\n mvawork: Please review my pull request.\r\n\r\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code class=\"notranslate\"># ps -aux\r\nUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\r\nzeebe        1  0.1  0.0   2504   580 pts/0    Ss   12:28   0:00 tini -- /usr/local/bin/startup.sh /bin/bash\r\nzeebe       12 35.8  1.7 12603416 455936 pts/0 Sl+  12:28   1:18 /opt/java/openjdk/bin/java -XX:+ExitOnOutOfMemoryError -Dfile.encoding=UTF-8 -classpath /usr/local/zeebe/config:/usr/local/zeeb\r\nroot       100  0.0  0.0   2612   536 pts/1    Ss   12:28   0:00 /bin/sh\r\nroot       109  0.0  0.0   8892  3308 pts/1    R+   12:32   0:00 ps -aux\r\n# \r\n</code></pre><div class=\"zeroclipboard-container position-absolute right-0 top-0\">\r\n    <clipboard-copy aria-label=\"Copy\" class=\"ClipboardButton btn js-clipboard-copy m-2 p-0 tooltipped-no-delay\" data-copy-feedback=\"Copied!\" data-tooltip-direction=\"w\" value=\"root@5ce6a5346a36:/usr/local/zeebe# ps aux\r\nUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\r\nroot         1  0.3  0.0   1940   448 ?        Ss   14:26   0:00 tini -- /usr/local/bin/startup.sh\r\nroot         7  197  1.4 4216156 295668 ?      Sl   14:26   0:05 /opt/java/openjdk/bin/java -Xms128m -XX:+ExitOnOutOfMemoryError -Dfile.encoding=UTF-8 -classpath /usr/local/zeebe/config:/usr/local/zeebe/lib/* -Dapp.name=broker -Dapp.pid=7 -Dapp.repo=/usr/local/zeebe\r\nroot        42  1.0  0.0   6880  3364 pts/0    Ss   14:26   0:00 /bin/bash\r\nroot        55  0.0  0.0   8476  2808 pts/0    R+   14:26   0:00 ps aux\" tabindex=\"0\" role=\"button\" style=\"display: inherit;\">\r\n      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-copy js-clipboard-copy-icon m-2\">\r\n    <path d=\"M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z\"></path><path d=\"M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z\"></path>\r\n</svg>\r\n      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-check js-clipboard-check-icon color-fg-success m-2 d-none\">\r\n    <path d=\"M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z\"></path>\r\n</svg>\r\n    </clipboard-copy>\r\n  </div></div>\r\n\n jessesimpson36: I proposed an [alternative PR ](https://github.com/camunda/zeebe/pull/13418)to accomplish the same goal, without gosu.  I believe there was a concern that if you set `USER zeebe` about whether you could exec into the container as root, and I did verify manually that you can run the process as root via `docker run --user root ...`  as well as `docker exec -it --user root ...`.\r\n\r\nThis change will require a helm chart change, but I'm on the team that works on that, so coordinating that change shouldn't be much of a challenge.",
    "title": "Docker: Run the zeebe process with an unprivileged user by default"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13024) from it. We recommend you to install it locally.",
      "component": "Zeebe",
      "subcomponent": "Docker - zbctl not included in the Docker image anymore",
      "context": "Breaking Changes ❗ "
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nMany of the recently reported [CVEs](https://github.com/camunda/zeebe/issues/12649) were related to the `zbctl` in the docker image, which only used in debugging and troubleshooting of SaaS clusters.\r\n\r\n**Describe the solution you'd like**\r\nRemove `zbctl` from the docker images from 8.3 going forward. This would eliminate any CVEs reported on our docker images that are golang related. \r\n\r\n\n\n megglos: ZDP-Triage:\n- would be nice to get out of the way, might be worth combining it with #12959 \n npepinpe: The simplest approach is to remove it from the distribution entirely. Would that be alright? Otherwise, we can remove it only from the Dockerfile.\n megglos: > The simplest approach is to remove it from the distribution entirely. Would that be alright? Otherwise, we can remove it only from the Dockerfile.\r\n\r\nI guess that would be fine 🤔  we attach it as artifact on every release anyway\n npepinpe: I've opened the PR where it's removed from the Dockerfile. It's also not that big a deal to do it then, and I guess it's a smaller breaking change :shrug:\r\n\r\nI'm pretty 50/50 on this honestly.\n megglos: > I've opened the PR where it's removed from the Dockerfile. It's also not that big a deal to do it then, and I guess it's a smaller breaking change 🤷\r\n> \r\n> I'm pretty 50/50 on this honestly.\r\n\r\nif it's already done on the dockerfile that's also fine then, the smaller scope would give us more flexibility to revise that decision ^^",
    "title": "Remove zbctl from 8.3 images going forward"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13473",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\n\r\nTo simplify migration to job streaming, we will integration the new `StreamJobsCommand` into the job worker API.\r\n\r\nAs job streaming still under development, this will be an opt-in feature initially, and should be disabled by default, such that the job worker behaves just as it used to unless streaming is enabled.\r\n\r\nWith this issue, the job worker builder should:\r\n\r\n- [ ] Expose a opt-in method for streaming\r\n- [ ] Expose an additional timeout API to set the stream timeout (if any). We cannot reuse the existing `requestTimeout`, since that's used for the polling mechanism, and will likely be much smaller than the streaming timeout.\r\n\r\nWhen opted in, the worker will:\r\n\r\n- [ ] Open a long living stream on `open`, using the same parameters as for the `ActivateJobsCommand` (where applicable).\r\n- [ ] Jobs activated via the stream are handled exactly like jobs activated via `ActivateJobsCommand`\r\n- [ ] Jobs activated via the stream do not count towards the `remainingJobs` which control polling; since polling serves to back fill older jobs, we don't want continuous load on a stream to prevent that.\r\n- [ ] Polling should still work and remain completely independent from streaming.\r\n- [ ] If the stream is closed while the worker is still opened, it should be re-opened. We can reuse the back off mechanism on error.\r\n- [ ] Close the stream when closing itself (and the stream is not reopened)\r\n\r\n\n",
    "title": "Stream jobs using job worker"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13460",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThis feature is related to #13040 . Using Java Client I want to complete `.newModifyProcessInstanceCommand()` by using only a single variable\r\n\r\n**Describe the solution you'd like**\r\n`client.newModifyProcessInstanceCommand(job).variable(\"name\", value)`\r\n\r\n**Describe alternatives you've considered**\r\n`client.newModifyProcessInstanceCommand(job).variables(Map.of(\"name\", value))`, but `Map.of()` is available only with Java 9 or above\r\n\n",
    "title": "newModifyProcessInstanceCommand: complete command with single variable"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13458",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThis feature is related to #13040 . Using Java Client I want to complete `.newThrowErrorCommand()` by using only a single variable\r\n\r\n**Describe the solution you'd like**\r\n`client.newThrowErrorCommand(job).variable(\"name\", value)`\r\n\r\n**Describe alternatives you've considered**\r\n`client.newThrowErrorCommand(job).variables(Map.of(\"name\", value))`, but `Map.of()` is available only with Java 9 or above\r\n\n",
    "title": "newThrowErrorCommand: complete command with single variable"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13456",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThis feature is related to #13040  Using Java Client I want to complete `.newFailCommand()` by using only a single variable\r\n\r\n**Describe the solution you'd like**\r\n`client.newFailCommand(job).variable(\"name\", value)`\r\n\r\n**Describe alternatives you've considered**\r\n`client.newFailCommand(job).variables(Map.of(\"name\", value))`, but Map.of() is available only with Java 9 or above\r\n\n",
    "title": "newFailCommand: complete command with single variable"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13451",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThis feature is related to #13040. Using Java Client I want to complete `.newBroadcastingSignalCommand()` by using only a single variable\r\n\r\n**Describe the solution you'd like**\r\n`client.newBroadcastingSignalCommand(job).variable(\"name\", value)`\r\n\r\n**Describe alternatives you've considered**\r\n`client.newBroadcastingSignalCommand(job).variables(Map.of(\"name\", value))`, but Map.of() is available only with Java 9 or above\r\n\n",
    "title": "newBroadcastingSignalCommand: complete command with single variable"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13449",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThis feature is related to #13040. Using Java Client I want to complete `.newEvaluateDecisionCommand()` by using only a single variable\r\n\r\n**Describe the solution you'd like**\r\n`client.newEvaluateDecisionCommand(job).variable(\"name\", value)`\r\n\r\n**Describe alternatives you've considered**\r\n`client.newEvaluateDecisionCommand(job).variables(Map.of(\"name\", value))`, but Map.of() is available only with Java 9 or above\r\n\n",
    "title": "newEvaluateDecisionCommand: complete command with single variable"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13447",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThis feature is related to #13040. Using Java Client I want to complete .newPublishMessageCommand() by using only a single variable\r\n\r\n**Describe the solution you'd like**\r\n`client.newPublishMessageCommand(job).variable(\"name\", value)`\r\n\r\n**Describe alternatives you've considered**\r\n`client.newPublishMessageCommand(job).variables(Map.of(\"name\", value))`, but `Map.of()` is available only with Java 9 or above\r\n\n",
    "title": "newPublishMessageCommand: complete command with single variable"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13443",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThis feature is related to #13040. Using Java Client I want to complete `.newCreateInstanceCommand()` by using only a single variable\r\n\r\n**Describe the solution you'd like**\r\n`client.newCreateInstanceCommand(job).variable(\"name\", value)`\r\n\r\n**Describe alternatives you've considered**\r\n`client.newCreateInstanceCommand(job).variables(Map.of(\"name\", value))`,  but `Map.of()` is available only with Java 9 or above\r\n\n",
    "title": "newCreateInstanceCommand: complete command with single variable"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13428",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\n\r\nI would like to use virtual threads for my job worker executor, and also measure the time a job spends waiting, once submitted, before it's processed. Right now, all I can configure is the number of threads in the job worker's pool. Additionally, that thread pool is always global per client.\r\n\r\n**Describe the solution you'd like**\r\n\r\nI'd like to use a thread-per-task execution model, relying on virtual threads for execution. By providing my own executor I can also instrument it more easily.\r\n\r\n**Describe alternatives you've considered**\r\n\r\n- I can re-implement the job worker myself - cumbersome, I'd rather not do that.\r\n- I can instrument the job handler. This doesn't count time spent in the executor's queue, however.\r\n- I can have the job handler forward it to my own custom executor. This is the best workaround, but it feels unnecessary.\r\n\r\n**Additional context**\r\n\r\nThis is very low priority, but it's definitely a nice to have :)\r\n\n\n koevskinikola: ZPA triage:\n- Setting the `scope/clients-java`. The issue deals with improving performance and UX, so adding the appropriate labels.\n- The Java client still supports Java 8, which conflicts with this feature.\n- @npepinpe we're closing this issue since we don't see a possibility in implementing it in our Java client. If you have any ideas on how to implement this, please provide them. If not in the official Java client, maybe they can be implemented in community projects.\n npepinpe: I'm not sure how allowing custom executors for job workers conflicts with Java 8, but I guess me saying Virtual Threads made it sound scary ;)\r\n\r\nIf I just open a PR for it, will you consider it? It's what, 20 lines of code?\n npepinpe: OK so this was slightly more than 20 lines of code, but still not too big. Mostly tests and comments :upside_down_face: ",
    "title": "Allow custom job worker executors"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12302",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Enhancements"
    },
    "gitHubIssueText": "The stream platform supports scheduling tasks that run once after a delay or repeatedly with a fixed rate.\nWith #11591, we added support for _async_ scheduled tasks that do not run on the processing actor but then only used this for `MessageTimeToLiveChecker` and `MessageObserver` to fix #11591 while other components still rely on tasks running on the processing actor.\n\nTo prevent all scheduled tasks from blocking processing, (see #11594 for another example), we'd like to migrate all scheduled tasks to run async, outside of the processing actor.\n\nThis requires some changes to make this safe. @korthout [already found two blockers](https://github.com/camunda/zeebe/issues/11761#issuecomment-1438750339),  one in  `JobTimeoutTrigger` and one via `MutablePendingMessageSubscriptionState`.\n\n```[tasklist]\n### Breakdown\n- [ ] https://github.com/camunda/zeebe/issues/12797\n- [ ] https://github.com/camunda/zeebe/issues/13041\n- [ ] https://github.com/camunda/zeebe/issues/12798\n- [ ] https://github.com/camunda/zeebe/issues/12308\n- [ ] https://github.com/camunda/zeebe/issues/13164\n- [ ] https://github.com/camunda/zeebe/issues/13584\n- [ ] https://github.com/camunda/zeebe/issues/13548\n```\n\n\n korthout: @oleschoenburg We've introduced a feature flag for running the Message TTL Checker async to the stream processor which is still disabled by default. I've opened an issue to enable it by default:\r\n- #12307 \r\n\r\nI think it would be a good idea to take a similar approach to this topic and backport these in the same way to 8.1 (and 8.2). This way the code stays relatively similar between the versions and we keep it easy to backport other changes. As long as behavior stays equivalent on updating to a new patch this should be fine IMO.\r\n\r\nFeature flags would also allow you to work on this now, without needing my changes immediately. It's just not safe to use yet.\n oleschoenburg: Good point regarding feature flags, thanks @korthout :+1: \r\n\r\nI'd propose the following:\r\n\r\nRemove `zeebe.broker.experimental.features.enableMessageTtlCheckerAsync` and replace it with one feature flag that controls this for all scheduled tasks at once, i.e. `zeebe.broker.experimental.features.asyncScheduledTasks`. That would render #12307 obsolete too.\r\n\r\nYou are right that I could implement everything already but then only enable the feature by default once we resolved the blockers. \r\n\r\nSince #11594 needs to be fixed in 8.1 (it's flagged as a perf bug), I'd like to backport the generic solution with `zeebe.broker.experimental.features.asyncScheduledTasks` to 8.1 and 8.2.\n oleschoenburg: This week I talked with @korthout, specifically about  https://github.com/camunda/zeebe/issues/12797 and https://github.com/camunda/zeebe/issues/12798.\r\n\r\nThere are more or less obvious fixes to both issues but we noticed that a common theme in these two issues and other components is the desire to write commands, either locally or on a remote partition, at most every couple of seconds.\r\n\r\nFor example, checking for pending message subscriptions will try and send commands to a remote partition until the remote partition has responded with an acknowledgement and it has been processed locally. Because local or remote log could be very long, the checker needs to be patient and only re-sends the command after a considerable timeout (e.g. 10 seconds).\r\nSimilarily, when jobs time out, a periodic checker finds them and writes a timeout command to the local log. This only needs to happen once, as long as the command is written it will be processed eventually. If the written command is not committed and a leader change occurs, the new leader will write a new timeout command.\r\nDeployment distribution is another case, similar to message subscriptions, where one partition wants to send a command to a remote partition and only retry this after a long backoff.\r\n\r\nWe were hoping that a new command writer that supports writing commands to local and remote partitions could neatly support all of the above use cases and simplify a lot of the, already buggy, code.\r\nWhen the  pending message subscription checker or job timeout checker writes  a `(id, ttl, partition, command)` tuple (I refuse to call this a quadruple!) , the new command writer would ensure that it only sends or writes the command with the given `id` if it hasn't done so since `ttl`. This would free up the checkers from keeping in-memory state to remember which commands should be sent again.\r\nTo cap the memory requirements when lot's of ids are used, the rate limiting would be done on a best-effort basis. At worst, the same command is written too often (false negative) but it is never forgotten (false positive).\r\n\r\nAs long as all writers can uniquely identify commands, this design should work for pending message subscriptions and deployment distribution. To support job timeouts, infinite TTLs and writing to the local log must also be supported.\r\n\r\nAfter diving in a quick prototype, I've decided against pursuing this for now.\r\nThe main reason is that checking for pending message subscriptions requires scanning an entire column family where potentially only a few entries are pending. I suspect that this would become a performance issue that can only be prevented by either introducing new column families or by introducing yet another in-memory state.\r\n\r\n**Conclusion**\r\nIn the interest of moving on and resolving known bugs sooner than later, I'll move forward with simple, straight forward fixes for https://github.com/camunda/zeebe/issues/12797 and https://github.com/camunda/zeebe/issues/12798 that I'll describe in each issue separately.\r\nI still think the idea outlined above is worth to revisit at some point.\r\n\r\n\n korthout: Thanks @oleschoenburg 👏 Well written. This is great!",
    "title": "Scheduled tasks should not block processing"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13024",
      "component": "Zeebe",
      "subcomponent": "zbctl",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nMany of the recently reported [CVEs](https://github.com/camunda/zeebe/issues/12649) were related to the `zbctl` in the docker image, which only used in debugging and troubleshooting of SaaS clusters.\r\n\r\n**Describe the solution you'd like**\r\nRemove `zbctl` from the docker images from 8.3 going forward. This would eliminate any CVEs reported on our docker images that are golang related. \r\n\r\n\n\n megglos: ZDP-Triage:\n- would be nice to get out of the way, might be worth combining it with #12959 \n npepinpe: The simplest approach is to remove it from the distribution entirely. Would that be alright? Otherwise, we can remove it only from the Dockerfile.\n megglos: > The simplest approach is to remove it from the distribution entirely. Would that be alright? Otherwise, we can remove it only from the Dockerfile.\r\n\r\nI guess that would be fine 🤔  we attach it as artifact on every release anyway\n npepinpe: I've opened the PR where it's removed from the Dockerfile. It's also not that big a deal to do it then, and I guess it's a smaller breaking change :shrug:\r\n\r\nI'm pretty 50/50 on this honestly.\n megglos: > I've opened the PR where it's removed from the Dockerfile. It's also not that big a deal to do it then, and I guess it's a smaller breaking change 🤷\r\n> \r\n> I'm pretty 50/50 on this honestly.\r\n\r\nif it's already done on the dockerfile that's also fine then, the smaller scope would give us more flexibility to revise that decision ^^",
    "title": "Remove zbctl from 8.3 images going forward"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13465",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\n\nWe need to validate user input before registering workers to the job stream. That will provide two main benefits:\n- We will be able to return better validation error messages to the client\n- Without validation, worker can register to the stream with a wrong job type and later no jobs will be pushed. We will prevent that with validation.\n\n**Expected validation checks:**\n- type is blank (empty string, null)\n- ~worker is blank (empty string, null)~ this is going to be optional as we do in polling mechanism\n- timeout less than 1\n",
    "title": "Validate user input before registering a worker to the job stream"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13429",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\n\r\nWith the new `StreamJobs` RPC introduced via #13351, we can now easily stream jobs from a Zeebe cluster. As a first step, we'll introduce a new command in `ZeebeClient` which will wrap the underlying gRPC call and integrate it into our client.\r\n\r\nWe'll introduce a new command, `StreamJobsCommand`, and a new API, `ZeebeClient#newStreamJobsCommand`. This will follow closely the `ActivateJobsCommand`, and have three steps:\r\n\r\n1. Set the job type (REQUIRED)\r\n2. Set a job consumer (REQUIRED)\r\n3. Set metadata (e.g. worker, job timeout, etc.) (OPTIONAL)\r\n\r\n### Long living stream\r\n\r\nSince this call is meant to be a long living stream, it diverges from our normal calls, which are all meant to eventually end.\r\n\r\n#### Request timeout\r\n\r\nFor example, the client has a default request timeout which is applied everywhere. This is counterproductive for this feature. Instead, **we will not apply the default request timeout here**. Users can still provide one which will be respected, but by default there will be no request timeout.\r\n\r\n#### Consumer\r\n\r\nSince the stream is long living, we don't want to wait for the command to complete before passing along the results. As opposed to `ActivateJobsCommand`, the `StreamJobsCommand` will take in a consumer as a mandatory build step, and results will be piped there. Future integration in the job worker can then do whatever with it, including passing it to the `JobHandler`.\r\n\r\n#### Cancellation/termination\r\n\r\nUsing the standard gRPC tools, you would normally cancel the stream either by throwing an exception (which is sent over the client's `StreamObserver#onError`), or by using `CancellableContext`. Closing the underlying transport channel would also work.\r\n\r\nThis doesn't fit the current `ZeebeClient` abstraction, and refactoring that goes beyond the scope of this issue. So instead, we'll piggyback on top of `Future#cancel(boolean)` - by calling this method, the user will be able to terminate the job stream gracefully, completing the future and notifying the server that the call is closed.\n\n npepinpe: Blocked by https://github.com/camunda/zeebe/issues/13430",
    "title": "Implement ZeebeClient wrapper for the StreamJobs RPC"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13349",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\n\r\nThe process deleting event will mark the process definition as pending deletion.\r\n\r\n- Add the `DELETING` intent to the `ProcessIntent`\r\n- Add an `EventApplier` to handle the `DELETING` intent\r\n    - The applier must change the state the `PersistedProcess` to `PENDING_DELETION`\r\n        - Don't forget to change the state in the `CoulmnFamily`, as well as in the cache.\r\n    - This means a new method must be created on the `DbProcessState`\r\n        - Include tests!\r\n\r\n**Out of scope**\r\n- Writing the event will happen in a different issue\r\n\r\n**Blocked by**\r\n#13348 \n",
    "title": "Add and handle Process Deleting event"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13348",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\n\r\nWe need to keep track of process definitions that are pending deletion. For this we will introduce a `state` on the `PersitedProcess`.\r\n\r\n- Add a `state` property on the `PersistedProcess`\r\n- State should be a new enum (`PeristedProcessState`)\r\n- As of now we will know 2 states:\r\n    - `ACTIVE` - This is the default.\r\n    - `PENDING_DELETION` - Used to mark a process for deletion\r\n\r\nIn the future the states could be extended with other states, e.g. `SUSPENDED`.\r\n\n",
    "title": "Add a `state` to `PersistedProcess`"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13342",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\nWhen a process has reached an end state we must remove it from this ColumnFamily\r\n\r\n- Add a method to remove data from this ColumnFamily\r\n    - Include tests!\r\n- When a process is completed use this method to remove it from the ColumnFamily\r\n- When a process is terminated use this method to remove it from the ColumnFamily\r\n- When a call activity is completed / terminated use this method to remove it from the ColumnFamily\r\n\r\nPlease also change the `ElementInstanceStateTest#shouldNotLeakMemoryOnRemoval`. We must change the processInstanceRecord that's generated here to be of element type PROCESS.\r\n\r\n**Blocked by**\r\n#13340 \n",
    "title": "Remove from `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13341",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\nIn #13340 we have created this ColumnFamily. We now need to fill it with data.\r\n\r\n- Add a method to insert data into this ColumnFamily\r\n    - Include tests!\r\n- Use this method to insert data when starting a new process instance\r\n    - Also when starting a process anywhere\r\n- Use this method to insert data upon calling a CallActivity\r\n\r\n**Blocked by**\r\n#13340 \n",
    "title": "Insert into the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13340",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\nWe will need a way to find out if there are any process instances for a definition key. We need this so when a process instance is terminated/completed we can check if there are other process instances still running. \r\nIf this is not the case, and the deployment is pending deletion we can write the followup events to fully delete the resource.\r\n\r\n- Create the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY ` in `ZbColumnFamililes`\r\n- Create this ColumnFamily in the `DbElementInstanceState`\r\n\r\n**Out of scope:**\r\n- Doing any actions on this ColumnFamily. Inserting and deleting data will happen in:\r\n    - #13341 \r\n    - #13342\r\n    - #13343\n",
    "title": "Add `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13335",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\n\r\n- Add a new `Intent` to the `ResourceDeletionIntent` class named `DELETING`\r\n- In the `ResourceDeletionProcessor` write the `DELETING` event\r\n    - Currently only the DMN resource are part of this processor.\r\n    - We are writing a `DELETED` event to the log, and are writing this as a response to the client.\r\n    - Before this we should write the `DELETING` event to the log.\r\n    - The response to the client should be changed to be `DELETING` as wel\r\n    - After the response we must still write the `DELETED` tot he log.\n",
    "title": "Add Resource Deleting intent"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13040",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nAs a programmer writing Zeebe Job Handlers, I want to complete a job returning only a single variable value.\r\n\r\nThe `newCompleteCommand` offers only a parameter `.variables()` where I have to wrap the value in a Map beforehand.\r\n\r\nThis is overhead.\r\n\r\n**Describe the solution you'd like**\r\n`client.newCompleteCommand(job).variable(\"name\", value)...`\r\n\r\n**Describe alternatives you've considered**\r\nPromote the Java feature with `Map.of(\"name\", value)` giving examples in the docs and Zeebe examples. \r\n\r\nOlder programmers, that learned Java with versions older than 8 are not aware of this shortcut.\r\n\r\n**Additional context**\r\nIt came up in a migration Workshop with a customer.\n\n Gireesh2002: Acdording to [javadoc.io]https://javadoc.io/doc/io.camunda/zeebe-client-java/1.2.1/index-all.html\r\nMy Suggesstion\r\nbefor\r\njobClient.newCompleteCommand(job.getKey()).variables(variables);\r\nafter\r\nSystem.out.println(job.getElementId());   // returns element Id (or) null\r\n                  Map variables = job.getVariablesAsMap();  // Get variables\r\njobClient.newCompleteCommand(job.getKey()).variables((Map.of(\"newVariable\",\"VariableFromClient\"));\n korthout: ZPA triage:\n- seems like a reasonable request\n- should be a good first issue\n- @aleksander-dytko please consider if you want the team to work on this from PM perspective\n aleksander-dytko: @korthout thanks for the mention. I don't see it as a priority at the moment - changing priority level to `Later`.\n megglos: ZDP-Triage:\n- not affecting ZDP",
    "title": "As a Zeebe Java user, I want to complete a Job with a single variable"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12975",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nWhen configuring async message TTL checking, I need to know the health of my system. A key metric here is the number of \"buffered\" messages. When this number is steadily climbing, I might need to configure the TTL checker differently to ensure that messages expire fast enough and don't accumulate.\r\n\r\n**Describe the solution you'd like**\r\nExport more message related metrics. Similar to the banned instance metrics, these should restore on recovery so that the reported counts are accurate even when brokers restart.\r\n\r\n**Describe alternatives you've considered**\r\nNot exposing metrics for this directly, instead relying on the exported record stream. This is more complicated and requires a separate application continously running.\r\n\r\n**Additional context**\r\n\r\nrelated to https://jira.camunda.com/browse/SUPPORT-17177\r\n\n\n Zelldon: :bulb: Just as a thought we might want to consider this a general metric for all column families. \r\n\n megglos: ZDP-Planning:\n- low hanging fruit that can be done shortly\n- solution needs to be discussed still though\n megglos: @rodrigo-lourenco-lopes @oleschoenburg  we likely would need to backport this (at least back to 8.1) for a particular customer to make use of, is that feasible? \n rodrigo-lourenco-lopes: > @rodrigo-lourenco-lopes @oleschoenburg we likely would need to backport this (at least back to 8.1) for a particular customer to make use of, is that feasible?\r\n\r\nYes, it should be possible :) ",
    "title": "Export number of buffered messages as metrics"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12878",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nIn the BPMN model API, a `send task` or `message throw event` should be able to reference a message definition.\r\n\r\n* a `send task` or `message throw event` can have an extension element `messageDefinition` with\r\n  * an attribute `messageName` that references the name of the message\r\n    * has either a static value or an expression\r\n    * must not be empty\r\n  * an attribute `correlationKey` that references the correlation key of the message\r\n    * has either a static value or an expression\r\n    * must not be empty\r\n  * an attribute `messageId` that references the id of the message\r\n    * has either a static value or an expression\r\n  * an attribute `timeToLive` that references the time to live of the message\r\n    * has either a static value or an expression\r\n*  a `send task` or `message throw event` must have either an extension element `messageDefinition` or `taskDefinition` but not both\r\n\r\n\r\nSample XML with static attributes:\r\n\r\n```\r\n<bpmn:sendTask id=\"send_message\">\r\n  <bpmn:extensionElements>\r\n    <zeebe:messageDefinition \r\n        messageName=\"foo\" \r\n        correlationKey=\"abc\" \r\n        messageId=\"abc\" \r\n        timeToLive=\"PT10S\"/>\r\n  </bpmn:extensionElements>  \r\n</bpmn:sendTask> \r\n\r\n\r\n<bpmn:intermediateThrowEvent id=\"Event_1nqoocy\">\r\n  <bpmn:extensionElements>\r\n    <zeebe:messageDefinition \r\n        messageName=\"foo\" \r\n        correlationKey=\"abc\" \r\n        messageId=\"abc\" \r\n        timeToLive=\"PT10S\"/>\r\n  </bpmn:extensionElements>\r\n  <bpmn:messageEventDefinition id=\"MessageEventDefinition_1wizp0b\" />\r\n</bpmn:intermediateThrowEvent>\r\n```\r\n\r\nSample XML with expression attributes:\r\n\r\n```\r\n<bpmn:sendTask id=\"send_message\">\r\n  <bpmn:extensionElements>\r\n    <zeebe:messageDefinition \r\n        messageName=\"=messageName\" \r\n        correlationKey=\"=correlationKey\" \r\n        messageId=\"=messageId\" \r\n        timeToLive=\"=timeToLive\"/>\r\n  </bpmn:extensionElements>  \r\n</bpmn:sendTask> \r\n\r\n<bpmn:intermediateThrowEvent id=\"Event_1nqoocy\">\r\n  <bpmn:extensionElements>\r\n    <zeebe:messageDefinition \r\n        messageName=\"=messageName\" \r\n        correlationKey=\"=correlationKey\" \r\n        messageId=\"=messageId\" \r\n        timeToLive=\"=timeToLive\"/>\r\n  </bpmn:extensionElements>\r\n  <bpmn:messageEventDefinition id=\"MessageEventDefinition_1wizp0b\" />\r\n</bpmn:intermediateThrowEvent>\r\n```\r\n\n\n korthout: Hi @lzgabel 👋 Sorry for the delay. I wanted to look at your [pull request](https://github.com/camunda/zeebe/pull/12879), but I want to understand the big picture before reviewing it.\r\n\r\nAs I understand, this issue is one of several parts of:\r\n- #1021?\r\n\r\nIn order to allow a user to model a process where a Send Task (or Messsage Throw Event) sends a message, we need to bind it to some Message Definition. Which is what this issue is about.\r\n\r\nIn your suggestion, the binding is achieved through a new `messageDefinition` extension element in the zeebe namespace, informing the message name, correlation key, message id and time-to-live of the message that should be published.\r\n\r\n❓ How did you come to this design? Was there a specific reason why you chose this over any other?\r\n\r\nFor example, there already exists a `MessageEventDefinition` in the BPMN spec that we could attempt to re-use for this. Here is an example of pure BPMN.\r\n\r\n```xml\r\n<bpmn:definitions ...>\r\n  <bpmn:process id=\"Process_02p7q4p\" isExecutable=\"true\">\r\n    ...\r\n    <bpmn:intermediateThrowEvent id=\"Event_145tbfx\">\r\n      <bpmn:messageEventDefinition id=\"MessageEventDefinition_0inu3y4\" messageRef=\"Message_1nb8aa6\" />\r\n    </bpmn:intermediateThrowEvent>\r\n    ...\r\n  </bpmn:process>\r\n  <bpmn:message id=\"Message_1nb8aa6\" name=\"order_placed\" />\r\n  ...\r\n</bpmn:definitions>\r\n```\n lzgabel: Hi @korthout. Thanks for you reply.  I'm sorry for starting the implementation without agreeing on this issue. :bow:\r\n\r\n> How did you come to this design? Was there a specific reason why you chose this over any other?\r\n\r\nActually, what I first thought was that this behavior is considered to be an internal implementation of the Zeebe engine, and should be create a new `messageDefinition` extension element in the zeebe namespace. 😄 \r\n\r\n> there already exists a MessageEventDefinition in the BPMN spec that we could attempt to re-use for this. Here is an example of pure BPMN.\r\n\r\nYes. You are absolutely right. 👍  I've checked the BPMN spec again, I will take your suggestion.\r\n\r\n---\r\n🤔 I plan to add an extension element under message, WDYT? :\r\n\r\n```xml\r\n<bpmn:message id=\"Message_1nb8aa6\" name=\"order_placed\">\r\n  <bpmn:extensionElements>\r\n    <zeebe:publish correlationKey=\"= orderId\" timeTolive=\"PT10S\" messageId=\"= uuid()\" />\r\n  </bpmn:extensionElements>\r\n</bpmn:message>\r\n```\r\n\r\n--- \r\nAt the same time, I think we can also support the `message end event`. 🚀 \r\n\r\n\r\n\r\n\n korthout: > I plan to add an extension element under message\r\n\r\n@lzgabel Thanks for checking with me. I don't know what is best. \r\n\r\nAn argument for putting this under the message is that this allows several tasks/events to publish the same message with the same details (correlation key, TTL, message id).\r\n\r\nBut the counter-argument is that this does not allow changing any of these for tasks/events that want to publish the same message but with different details.\r\n\r\nI'll check with our Modeling experts to see what fits best. I'll attempt to make a decision on this early next week. Is that okay for you, @lzgabel?\n lzgabel: > I'll check with our Modeling experts to see what fits best. I'll attempt to make a decision on this early next week. Is that okay for you, @lzgabel?\r\n\r\nYes. Looking forward to the conclusion.\n nikku: @korthout to mirror my internal comment here:\r\n\r\n* I'd love to clearly separate a message from \"stuff that is being done with the message, in the context of a flow / process execution\".\r\n* To support this I'd keep what is relevant in the _execution flow_ on the flow element (intermediate catch event in this case)\n barmac: Hi, I'd like to add my two cents. First, it's great that you are looking into this feature, and I appreciate it a lot that you pulled in Modeler devs :)\r\n\r\nLet's examine how we solve modeling problems with each of the solutions:\r\n\r\n| Problem | Properties defined on shared `bpmn:Message` | Properties defined on individual events via `bpmn:MessageEventDefinition`|\r\n|-|-|-|\r\n| I want to add another event which publishes message of given name with the same correlation key, TTL etc. | Create a new event and select shared message | Copy existing event |\r\n| I want to modify properties for a specific event | Create a new message with the same name but different propertie | Modify properties on the event |\r\n| I want to modify properties on all events | Modify in single place | Modify on each event |\r\n| I want to create an event with a different variable as correlation key | Create a new message with the same name | Change on individual event |\r\n\r\nSetting different properties for the same message name leads to redundancy in the diagram (multiple messages of the same name but different extension elements). I believe this is not what we want, therefore I'd support setting event-related properties on the event definition.\r\n\r\nNote that all of that can be also applied to catch event. \r\n\r\n---\r\n\r\nI started writing this before Nico's comment but we discussed this in the morning and have 100% agreement :)\r\n\r\nProposal:\r\n\r\n```xml\r\n<bpmn:endEvent id=\"Message_1nb8aa6\" name=\"order_placed\">\r\n  <bpmn:messageEventDefinition>\r\n    <bpmn:extensionElements>\r\n      <zeebe:publish correlationKey=\"= orderId\" timeTolive=\"PT10S\" />\r\n    </bpmn:extensionElements>\r\n  </bpmn:messageEventDefinition>\r\n</bpmn:endEvent>\r\n```\n barmac: Question: What would `messageId` be used for?\n nikku: > Question: What would `messageId` be used for?\r\n\r\n@barmac I asked myself the same question. It is another way to ensure idempotent message delivery ([internal ref](https://camunda.slack.com/archives/CSQ2E3BT4/p1686047258957109)).\n nikku: @barmac and as we discussed, based on https://github.com/camunda/zeebe/issues/12878#issuecomment-1578613868, imagine this (attaching subscription information to the message definition, not the `bpmn:Message` as it is currently the case):\r\n\r\n```xml\r\n<bpmn:intermediateCatchEvent id=\"Message_1nb8aa6\" name=\"order_processed\">\r\n  <bpmn:messageEventDefinition messageRef=\"order_processed_message\">\r\n    <bpmn:extensionElements>\r\n      <zeebe:subscribe correlationKey=\"= orderId\" />\r\n    </bpmn:extensionElements>\r\n  </bpmn:messageEventDefinition>\r\n</bpmn:intermediateCatchEvent>\r\n```\n lzgabel: 👋 Hi Guys. Will this [#12878 (comment)](https://github.com/camunda/zeebe/issues/12878#issuecomment-1578613868) be the final conclusion?\n korthout: Hi @lzgabel I've asked our internal Modeling experts. @nikku and @barmac gave great input, and we'll likely take that route. However, I don't want to take the opportunity away for others to voice their opinions. I set Monday, June 12th, as the deadline for input, so let's await that. I'll update this issue on Tuesday with the outcome.\n jonathanlukas: I would also prefer the approach @nikku proposes. While a message correlation surely is related to the message itself, the implementation details are more part of the event definition of each bpmn element.\r\n\r\nAlso, I think in Camunda 7, the message throw event was able to select a message element to refer to which still leads to confusion...\n korthout: Thanks everyone for your input 👏 \r\n\r\nNo opinions against the proposal were raised. Let's move forward with the following:\r\n\r\n- a new extension element that can be added to the `bpmn:messageEventDefinition` (of Intermediate Throw Event, or End Event) and/or to the `bpmn:sendTask` to specify the details of the message to publish (i.e. correlation key, time to live, message id)\r\n- let's name this extension element `zeebe:publishMessage` to make the XML easier to understand/read for humans\r\n- the message to publish can be referenced directly from the `bpmn:messageEventDefinition` and from the `bpmn:sendTask` according to BPMN spec using the `messageRef` attribute (the referenced message contains the message name).\r\n\r\nHere follows an example with three different elements all publishing the same message with different message details (i.e. correlation key, time to live, message id).\r\n\r\n```xml\r\n<bpmn:definitions ...>\r\n  <bpmn:process id=\"Process_02p7q4p\" isExecutable=\"true\">\r\n    ...\r\n    <bpmn:intermediateThrowEvent id=\"Event_145tbfx\">\r\n      <bpmn:messageEventDefinition id=\"MessageEventDefinition_0inu3y4\" messageRef=\"Message_1nb8aa6\">\r\n        <bpmn:extensionElements>\r\n          <zeebe:publishMessage correlationKey=\"= orderId\" timeTolive=\"PT10S\" messageId=\"= orderId\" />\r\n        </bpmn:extensionElements>\r\n      </bpmn:messageEventDefinition>\r\n    </bpmn:intermediateThrowEvent>\r\n    ...\r\n    <bpmn:sendTask id=\"Activity_1kgyl85\" messageRef=\"Message_1nb8aa6\">\r\n      <bpmn:extensionElements>\r\n        <zeebe:publishMessage correlationKey=\"= some_other_order_id\" />\r\n      </bpmn:extensionElements>\r\n    </bpmn:sendTask>\r\n    ...\r\n    <bpmn:endEvent id=\"Message_1nb8aa6\" name=\"order_placed\">\r\n      <bpmn:messageEventDefinition messageRef=\"Message_1nb8aa6\">\r\n        <bpmn:extensionElements>\r\n          <zeebe:publishMessage correlationKey=\"= orderId\" timeTolive=\"PT1H\" />\r\n        </bpmn:extensionElements>\r\n      </bpmn:messageEventDefinition>\r\n    </bpmn:endEvent>\r\n    ...\r\n  </bpmn:process>\r\n  <bpmn:message id=\"Message_1nb8aa6\" name=\"order_placed\" />\r\n  ...\r\n</bpmn:definitions>\r\n```\r\n\r\n\r\n\n lzgabel: @korthout. Thanks. ❤️ \r\nI'll be pushing the latest commit for this feature in the next few days.",
    "title": "Add message definition extension elements"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12696",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\n\r\nSome process models can sometimes cause whole partitions to become completely bricked. For example, process models with a combination of loops, large multi-instance collection spawning activities, or straight-through processing (e.g. FEEL script tasks, undefined tasks, manual tasks, etc.) can cause us to write a large amount of follow up records, much faster than we can process or export. This can lead to:\r\n\r\n- The exported system being overloaded, e.g. Operate cannot import fast enough, Elasticsearch runs out of disk space, etc.\r\n- Runaway process instances cannot be canceled because the command is either not accepted, or written so far down the log that it takes hours to get there, at which point we've written millions more follow up records.\r\n- Node is out of disk space, leading to all partitions stopping.\r\n\r\n**Describe the solution you'd like**\r\n\r\nWhile preventing such situations is ideal, as a first step, we would like to provide an escape hatch for when they arise by allowing users to forcefully terminate a process without having to wait for all events to be processed.\r\n\r\nThis means providing a way to bypass the causality chain of the partition by directly modifying the state projection.\r\n\r\nWe would have to:\r\n\r\n1. Provide an API endpoint (whether a management or client endpoint is to be determined) where users can forcefully terminate a PI by key\r\n2. The leader of the partition for this PI must then modify the state to mark that PI as forcefully terminated. **It may not be possible to write to disk anymore, so we may have to simply modify the state**.\r\n3. The state modification must be replicated to all followers, much like the exporter state. This is to ensure consistency between all nodes should an election occur.\r\n4. Once a PI is forcefully terminated, then all records associated it still have to be read but can be skipped/not processed/replayed. **We cannot delete them from the log - this not only breaks the append-only contract, but is much more complicated**\r\n5. The records must also be skipped by the exporters. However, we may want to notify the external systems that the PI has been forcefully terminated somehow, so we may need some solution here. Maybe we still need to write a record at the end of the log specifying it was forcefully terminated.\r\n\r\nIn terms of UX, it would be great to be able to differentiate between normal termination and forceful termination, if only for monitoring and auditing purposes.\r\n\r\n**Describe alternatives you've considered**\r\n\r\n- Static analysis of process models to detect endless loops. This may not always be possible, and there may very well be valid use cases for such. Unclear how easy that is, especially things like a process with a call activity where the called ID is a variable which could point to the process itself.\r\n- Fair/weighted process scheduling. A rather complex solution, where the processor ensures that one PI cannot overwhelm the partition at the cost of other PIs or external user commands. This could be achieved by performing more look-ahead and determining whether it's safe to continue processing a given PI or if it should yield, and writing deferred computation records to resume it.\r\n\r\n**Additional context**\r\n\r\nSeveral incidents occurred recently which were the results of users deploying looping, straight-through processes, which were producing too writing more than the system could process/export. One was writing an exponential number of follow up records, which led to an unrecoverable out of disk space, where there was nothing to compact yet, so processing could never resume.\r\n\n\n korthout: @npepinpe I worry that termination of the process instance is considered a loss of data.\r\n\r\nInstead of termination of the process instance, I'd rather see something like suspension of the process instance. For example, by creating a process instance-level [Incident](https://docs.camunda.io/docs/components/concepts/incidents/) with a new `ErrorType` that prevents any further progress unless they are attempts to repair the instance, e.g. Modification, SetVariable, Migration, etc. This aids in visibility and allows users to repair the instance.\r\n\r\nSuch a solution is also what we are investigating in relation to:\r\n- #5121 \r\n- https://github.com/camunda/product-hub/issues/686\r\n\r\nI understand if that would not make it into the first iteration, but I'd hope we can replace the termination logic with what I've described here in some future iteration.\n Zelldon: Yesterday I was also thinking about suspending process instances just for the same use case, from the operations point of view, to get my system healthy again or avoid an imbalance of exporting and processing. \r\n\r\nNot sure whether the proposed solutions above are optimal, due to the state changes without processing a command. \r\n\r\n----\r\n\r\nWhat I thought we could do is have a runtime transient toggle that can be flipped for every process instance. If we see that a process instance makes problems we could flip it and the process instance will be ignored. Kind of similar to being blacklisted, but not persistent. If we restart the instance can be executed again. \r\n\r\n**What does ignored mean?** \r\n\r\nThere could be two things either, completely skip all commands, but then you will lose the progress. OR you simply append the current command for the process instance at the end of the log, without processing it. This allows to make progress still on other instances, and later again on the PI if it is no longer suspended. (BTW IMHO commands shouldn't be exported anyway (https://github.com/camunda/zeebe/issues/6749))\r\n\r\nWe need to allow certain commands like canceling still be processed for such PI. This allows to clean up, similar to we should allow for blacklisted instances.\r\n\r\n_______\r\n\r\n\r\nTo develop this further I could imagine that we could at some point have this even automatically, like normal OS scheduling or as we know in K8. If you have X process instances and Y CPUs than a PI is only allowed to take Y/X CPU time, if more we could suspend it for a time frame (append the current command at the end such it will be processed later).\r\n\n korthout: 🤔 Interesting ideas @Zelldon. \r\n\r\nYielding the commands (appending them to the end of the log instead of processing them) would solve parts of these issues, but not the fork bomb case where the log keeps expanding. Consider a fork bomb that's been running for a few minutes unnoticed. There are likely already thousands if not millions of unprocessed commands for that instance on the log. These will all be yielded to back of the log continuously. New commands will take at least the time that it takes to yield all those commands.\r\n\r\nI'm personally more in favor of skipping the commands. This puts the process instance in a state of limbo just like a blacklisted process instance. All we need is a way to start up processing again, and to allow the user to cancel the instance. \r\n\r\n- We could persist the skipped commands. As soon as we want to continue, we write these back to the log and continue processing. This has two downsides: disk usage and fork bomb would continue. So I don't think we should do this.\r\n- We could ignore the skipped commands. As soon as we want to continue, the user can usePI modification / migration to re-start the processing, or they can simply cancel the instance. \r\n\r\nLastly, I was wondering how you'd keep track of a runtime flag for each process instance? If a user has large state (millions of running PI) then this would consume a lot of memory. Why not persist it like blacklisting?\n deepthidevaki: > Not sure whether the proposed solutions above are optimal, due to the state changes without processing a command.\r\n\r\nI think it is possible to do this correctly. When we force \"suspend\" (for lack of a better word) a process, we should bypass the normal stream processing process. That means, we do not write the command to the logstream, but submit to StreamProcessor actor as a task to be processed immediately. When processing this task, it should generate followup events  to suspend the process. The followup event could be also the error record/incident record which @korthout mentioned. The follow up event should somehow refer to the invisible command that should have been at the current processingPosition (yet to figure out how to do it.). The event get's replicated and the followers will have the same state as the leader. Send the response only after this follow up event is committed, so that we know for sure that the process is suspended.\r\n\r\nSide note:- In general, it would make sense to not write user commands to the logstream. Only follow up command and events are required for deterministic replay. \n megglos: ZDP-Triage:\n- may be a topic that affects both teams\n- if such incidents occur we can't recover in an easy way (you would need to manually update the rocksDb sate to e.g. blacklist the instance)\n\n@abbasadel would be curious on the triage outcome of your team, as this would be crucial to mitigate incidents where we experience malicious processes\n megglos: @felix-mueller this is the feature I mentioned in the stakeholder round today in order to allow us to better handle incidents that are caused by malicious processes\n megglos: ZDP-Planning:\n- ZDP is picking this up to assess solutions for this asap to allow engineers to handle incidents better going forward\n remcowesterhoud: ZPA Triage:\r\n- The solution of bypassing the log is unrelated to ZPA, as we only build things on top of the log stream.\r\n- We are interested in what happens to the process instance. The state of this PI is our responsibility\r\n- Please reach out to us when you need involvement on deciding what happens with the PI! We are happy to support 🙂 \r\n- We're not planning to work on this unless we need to support the ZDP team. As a result we will remove it from our project board.\n Zelldon: First of all, I want to thank you all for your ideas and brain dumps you have done here. :bow: \r\n\r\nI thought about this for a while and also re-read all your comments. If you're interested, I used the following [gdoc](https://docs.google.com/document/d/1NDKjq7osuzYoFd5Jtj9Xhe5_buw8rVU6nLIqBDQwJ74/edit) to summarize and better assess potential solutions. \r\n\r\nI have a potential solution in mind, let me shortly explain it to you.\r\n\r\n## Proposal:\r\n\r\nI think we can combine several ideas together to get the best out of it. \r\n\r\nIt makes sense to have a two-step process implemented, which allows to first ban the instance and secondly (if wished) to cancel the instance.\r\n\r\nWe provide an API (potentially actuator) where we can send “ban instance X” request. We access the ZeebeDB with a separate context, as we do with the Exporters, and mark the process instance as banned. I think this should bring us a quick win and feels potentially easy to implement. \r\n\r\nThe ban column family is less frequented, which should allows us to already stop the processing for a specific PI, without conflicts. It might make sense to think about the blacklisting cache again, whether we really need it or we just delete it (otherwise we have to update flag).\r\n\r\nThe banning of the instance should be confirmed with an event on the log, this allows to replicate the state change, and followers can do the same.\r\n\r\nPlease be aware as soon as the instance is banned, there is right now no way to recover. [The process instance is in limbo, as ](https://github.com/camunda/zeebe/issues/12696#:~:text=I%27m%20personally%20more%20in%20favor%20of%20skipping%20the%20commands.%20This%20puts%20the%20process%20instance%20in%20a%20state%20of%20limbo%20just%20like%20a%20blacklisted%20process%20instance.%20All%20we%20need%20is%20a%20way%20to%20start%20up%20processing%20again%2C%20and%20to%20allow%20the%20user%20to%20cancel%20the%20instance.)[Nico Korthout](mailto:nico.korthout@camunda.com) said so nicely. To handle this differently I see this out of the scope of this feature since we expect it to use only for severe issues where the cluster is otherwise not recoverable (and/or can make no progress).\r\n\r\nYou may ask why is that actually the case. Because we likely skip commands which are then lost. Furthermore, there is right now no way to remove some PI from the banned instances list. A potential follow-up could be to allow removing instances from that list and applying PI modification (to repair instances).\r\n\r\nWe could think of [persisting the skipped records into the state](https://github.com/camunda/zeebe/issues/12696#:~:text=We%20could%20persist%20the%20skipped%20commands.%20As%20soon%20as%20we%20want%20to%20continue%2C%20we%20write%20these%20back%20to%20the%20log%20and%20continue%20processing.%20This%20has%20two%20downsides%3A%20disk%20usage%20and%20fork%20bomb%20would%20continue.%20So%20I%20don%27t%20think%20we%20should%20do%20this.), but I would rather not do that, since it could cause severe other issues due to limited space, etc. It is also not clear to me whether the gains are outweighed by the costs since we can also easily run process instance modification to recover such.\r\n\r\nThe approach above is a combination of multiple others mentioned before, and I feel it is best to not make it too easy to erase customer data, which is why cancelation is/can be the second step. \r\n\r\nGenerally, cancelation involves more work, due to the child's cancelation, etc. This should happen in the processing, with a separate command. Since it might fail as well, due to large nested instances, etc, but with this approach it is fine since it is already banned in this case. \r\n\r\nIn order to allow the cancelation we need to make sure that we implement the [cancelation for banned instances.](https://github.com/camunda/zeebe/issues/12772)\r\n\r\n## Next\r\n\r\nAs a next step, I would like to spend some time doing a POC and play a bit around with that. Please feel free to raise any concerns with this approach. I'm open for discussions of course.\r\n",
    "title": "Forcefully terminate a process instance"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12382",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nCurrently the zeebe process is run by the root user in the zeebe docker image:\r\n```\r\nroot@5ce6a5346a36:/usr/local/zeebe# ps aux\r\nUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\r\nroot         1  0.3  0.0   1940   448 ?        Ss   14:26   0:00 tini -- /usr/local/bin/startup.sh\r\nroot         7  197  1.4 4216156 295668 ?      Sl   14:26   0:05 /opt/java/openjdk/bin/java -Xms128m -XX:+ExitOnOutOfMemoryError -Dfile.encoding=UTF-8 -classpath /usr/local/zeebe/config:/usr/local/zeebe/lib/* -Dapp.name=broker -Dapp.pid=7 -Dapp.repo=/usr/local/zeebe\r\nroot        42  1.0  0.0   6880  3364 pts/0    Ss   14:26   0:00 /bin/bash\r\nroot        55  0.0  0.0   8476  2808 pts/0    R+   14:26   0:00 ps aux\r\n```\r\n\r\nTo harden the security of the docker image it should **default** to run it with an unprivileged user instead, [see the OWASP recommendation](https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html#rule-2-set-a-user).\r\n\r\n**Describe the solution you'd like**\r\nThe zeebe image **should by default run with an unprivileged user.** [There is already a zeebe user setup with uid 1000](https://github.com/camunda/zeebe/blob/main/Dockerfile#L105-L111) it's just not used.\r\n\r\n**Additional context**\r\nRelates to https://github.com/camunda/product-hub/issues/717\r\nsupport for running with an unprivileged user was added with https://github.com/camunda/zeebe/issues/11784\r\nRelates to https://jira.camunda.com/browse/SUPPORT-15969\n\n mvawork: Please review my pull request.\r\n\r\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\"><pre class=\"notranslate\"><code class=\"notranslate\"># ps -aux\r\nUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\r\nzeebe        1  0.1  0.0   2504   580 pts/0    Ss   12:28   0:00 tini -- /usr/local/bin/startup.sh /bin/bash\r\nzeebe       12 35.8  1.7 12603416 455936 pts/0 Sl+  12:28   1:18 /opt/java/openjdk/bin/java -XX:+ExitOnOutOfMemoryError -Dfile.encoding=UTF-8 -classpath /usr/local/zeebe/config:/usr/local/zeeb\r\nroot       100  0.0  0.0   2612   536 pts/1    Ss   12:28   0:00 /bin/sh\r\nroot       109  0.0  0.0   8892  3308 pts/1    R+   12:32   0:00 ps -aux\r\n# \r\n</code></pre><div class=\"zeroclipboard-container position-absolute right-0 top-0\">\r\n    <clipboard-copy aria-label=\"Copy\" class=\"ClipboardButton btn js-clipboard-copy m-2 p-0 tooltipped-no-delay\" data-copy-feedback=\"Copied!\" data-tooltip-direction=\"w\" value=\"root@5ce6a5346a36:/usr/local/zeebe# ps aux\r\nUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\r\nroot         1  0.3  0.0   1940   448 ?        Ss   14:26   0:00 tini -- /usr/local/bin/startup.sh\r\nroot         7  197  1.4 4216156 295668 ?      Sl   14:26   0:05 /opt/java/openjdk/bin/java -Xms128m -XX:+ExitOnOutOfMemoryError -Dfile.encoding=UTF-8 -classpath /usr/local/zeebe/config:/usr/local/zeebe/lib/* -Dapp.name=broker -Dapp.pid=7 -Dapp.repo=/usr/local/zeebe\r\nroot        42  1.0  0.0   6880  3364 pts/0    Ss   14:26   0:00 /bin/bash\r\nroot        55  0.0  0.0   8476  2808 pts/0    R+   14:26   0:00 ps aux\" tabindex=\"0\" role=\"button\" style=\"display: inherit;\">\r\n      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-copy js-clipboard-copy-icon m-2\">\r\n    <path d=\"M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z\"></path><path d=\"M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z\"></path>\r\n</svg>\r\n      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-check js-clipboard-check-icon color-fg-success m-2 d-none\">\r\n    <path d=\"M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z\"></path>\r\n</svg>\r\n    </clipboard-copy>\r\n  </div></div>\r\n\n jessesimpson36: I proposed an [alternative PR ](https://github.com/camunda/zeebe/pull/13418)to accomplish the same goal, without gosu.  I believe there was a concern that if you set `USER zeebe` about whether you could exec into the container as root, and I did verify manually that you can run the process as root via `docker run --user root ...`  as well as `docker exec -it --user root ...`.\r\n\r\nThis change will require a helm chart change, but I'm on the team that works on that, so coordinating that change shouldn't be much of a challenge.",
    "title": "Docker: Run the zeebe process with an unprivileged user by default"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11708",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Description**\r\n\r\nIn order to register job streams to the gateway, we will need to add a new gRPC API for this. This API will be a unidirectional stream (from server to client). \r\n\r\nIt should take in as parameter the same activation properties as the job worker (minus anything related to long polling and the likes), but it will be a long-living stream. Later, when implementing it on the client side and on the gateway, it's important that its keep-alive be configured properly.\r\n\r\nIt returns a stream of single `ActivatedJob`. We can discuss if we want to keep the batching properties, as this could be a useful optimization, but I would propose to ignore it for the alpha target and measure its impact once we have the complete end-to-end pipeline.\n",
    "title": "Add gRPC job stream API"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13233",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "We've introduced a regression on deploying large payloads on multi-partition clusters with\r\n- #11661 \r\n\r\nBefore that pull request, a deployed resource was written in two follow-up events:\r\n- `Deployment:CREATED`\r\n- `Process:CREATED` (or `DecisionRequirements:CREATED` depending on resource)\r\n\r\nThe specialized `DeploymentDistribution` would take the resource from the `Deployment:CREATED` to distribute it to the other partitions.\r\n\r\nWith #11456, a new event `CommandDistribution:STARTED` was introduced to store the command for distribution. For deployments, this contains the entire Deployment incl. the resource. This event is only appended on multi-partition clusters. But, when it is appended, it further reduces the maximum payload size because the follow-up events reach the `MAX_BATCH_SIZE` restriction with a lower payload size.\r\n\r\n> **Note** The `MAX_BATCH_SIZE` is a limitation that originated from the `MAX_MESSAGE_SIZE` configuration setting but nowadays is only defined by the `LogStreamBuilder`'s `MAX_FRAGMENT_SIZE`: 4MB.\r\n\r\nThis regression lowers the maximum payload size of deployments from ~2MB down to ~1.4MB.\r\n\r\nThe regression does not exist on single partition clusters as these do not require the distribution of the deployment.\r\n\r\n---\r\n\r\nSuggested solution:\r\n- don't write the resource in the `Deployment:CREATED` event, only in the `Process:CREATED` and the `DecisionRequirements:CREATED` events (estimate: x-small)\r\n\r\nOf course, we'd need to inform users that we're no longer writing the resource for that event in the Update Guide. As this would be breaking user space. The documentation should clarify that the resource is available in the `Process:CREATED` and the `DecisionRequirements:CREATED` instead.\n\n korthout: There are several solution ideas:\n- https://github.com/camunda/zeebe/issues/11513\n  - Does not seem trivial to implement\n  - Mostly a ZDP effort\n- don't write the resource in the `Deployment:CREATED` event, only in the `Process:CREATED` and the `DecisionRequirements:CREATED` events (estimate: x-small)\n  - would require alignment with Operate and Optimize to make sure they consume the resource from `Process:CREATED` \n korthout: @sdorokhova Can you verify for me that Operate consumes the `Process:CREATED` event to get the BPMN model and not the `Deployment:CREATED` event? Likewise, does it consume the `DecisionRequirements:CREATED` event for DMN models?\r\n\r\n@RomanJRW Can you verify the same as above, but for Optimize?\n sdorokhova: Hi @korthout ,\r\nwe read processes from `process` index and decision requirements from `decision-requirements` index. You can check [here](https://github.com/camunda/operate/blob/168d22f5352c66bfd56c47678e92655a82414194/common/src/main/java/io/camunda/operate/zeebe/ZeebeESConstants.java) all the indices we're reading from. Is this what you were asking about? \n korthout: @sdorokhova Perfect! That's exactly what I mean. I should've mentioned that it concerns the ES indices `zeebe-record-process` and `zeebe-record-decision-requirements`. \r\n\r\nI was hoping you weren't reading the BPMN/DMN resources from the `zeebe-record-deployment` index.\n megglos: @korthout do I understand this correctly that this regression is only present in 8.3? And the changes that happened in 8.2 like https://github.com/camunda/zeebe/issues/11660 have not caused a regression?\n korthout: @megglos Correct, the regression only exists on `main` and if left unfixed will only be present in the upcoming 8.3. The reason is that deployment distribution has been switched over only for 8.3(-alpha*), not for 8.2.\n RomanJRW: Hey @korthout - apologies for slow response, I have had FTO. I can confirm the same for Optimize, that we read from `zeebe-record-process` and not the deployment index\n korthout: Now that both Operate and Optimize have confirmed that they don't access the resource from the `zeebe-record-deployment` index, I think it's fine to take this solution:\r\n- don't write the resource in the `Deployment:CREATED` event, only in the `Process:CREATED` and the `DecisionRequirements:CREATED` events (estimate: x-small)\r\n\r\nOf course, we'd need to inform users that we're no longer writing the resource for that event in the Update Guide. As this would be breaking user space. The documentation should clarify that the resource is available in the `Process:CREATED` and the `DecisionRequirements:CREATED` instead.",
    "title": "Regression in deploying large payloads"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13715",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "See https://github.com/camunda/zeebe/actions/runs/5709567673\r\n```\r\nThe workflow is not valid. In .github/workflows/release-main-dry-run.yml (Line: 11, Col: 11): Error from called workflow camunda/zeebe/.github/workflows/release.yml@bc32ea937f8d3650b657431c70d22bae9339ba3c (Line: 329, Col: 12): Unrecognized named-value: 'env'. Located at position 1 within expression: env.RELEASE_BRANCH\r\n```\n\n megglos: https://github.com/actions/runner/issues/2372",
    "title": "Release Dry fails because of unrecognized argument"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13650",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n`ZeebePartitionHealth` repeatedly calls listeners and logs a change of health status when the health has not actually changed.\r\nThis has existed for a long time but was made worse with #13042 where we now update the health status more frequently.\r\n\r\nThis is the bug: https://github.com/camunda/zeebe/blob/c49d14b7eb8052895aa813c9884850ae9f590a2f/broker/src/main/java/io/camunda/zeebe/broker/system/partitions/ZeebePartitionHealth.java#L60\r\n\r\nHere we should actually compare the health reports and not check for identity.\r\n\r\n**Environment:**\r\nZeebe Version: 8.2.9\n",
    "title": "`ZeebePartitionHealth` repeatedly reports change of health status"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13471",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n```\r\nINFO 2023-07-10T18:18:50.941085479Z [resource.labels.containerName: zeebe] Preparing transition from FOLLOWER on term 31 completed\r\nINFO 2023-07-10T18:18:50.941163945Z [resource.labels.containerName: zeebe] Transition to LEADER on term 31 starting\r\nINFO 2023-07-10T18:18:50.941238114Z [resource.labels.containerName: zeebe] Cancelling transition to LEADER on term 31\r\nWARNING 2023-07-10T18:18:50.941635348Z [resource.labels.containerName: zeebe] Uncaught exception in Broker-1-ZeebePartition-2.\r\n\"java.lang.NullPointerException: must specify a log stream\r\n\tat java.util.Objects.requireNonNull(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.broker.jobstream.RemoteJobStreamErrorHandlerService.onBecomingLeader(RemoteJobStreamErrorHandlerService.java:56) ~[zeebe-broker-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.broker.system.partitions.PartitionStartupAndTransitionContextImpl.lambda$notifyListenersOfBecomingLeader$4(PartitionStartupAndTransitionContextImpl.java:164) ~[zeebe-broker-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat java.util.stream.ReferencePipeline$3$1.accept(Unknown Source) ~[?:?]\r\n\tat java.util.ArrayList$ArrayListSpliterator.forEachRemaining(Unknown Source) ~[?:?]\r\n\tat java.util.stream.AbstractPipeline.copyInto(Unknown Source) ~[?:?]\r\n\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(Unknown Source) ~[?:?]\r\n\tat java.util.stream.ReduceOps$ReduceOp.evaluateSequential(Unknown Source) ~[?:?]\r\n\tat java.util.stream.AbstractPipeline.evaluate(Unknown Source) ~[?:?]\r\n\tat java.util.stream.ReferencePipeline.collect(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.broker.system.partitions.PartitionStartupAndTransitionContextImpl.notifyListenersOfBecomingLeader(PartitionStartupAndTransitionContextImpl.java:165) ~[zeebe-broker-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.broker.system.partitions.ZeebePartition.lambda$leaderTransition$6(ZeebePartition.java:243) ~[zeebe-broker-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.future.FutureContinuationRunnable.run(FutureContinuationRunnable.java:28) ~[zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94) ~[zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT] ... \r\nERROR 2023-07-10T18:18:50.942738393Z [resource.labels.containerName: zeebe] Failed to install partition 2\r\n```\r\n\r\nTransition to leader is cancelled (probably because there is a new leader already). But the `PartitionStartupAndTransitionContextImpl.lambda$notifyListenersOfBecomingLeader` is still invoked. Since the services were not installed, logstream is null and this causes NPE in the listener. \r\n\r\nPartition is inactive after that. However, the partition is still marked as healthy (See #13401) \r\n\r\n**Expected behavior**\r\n\r\nPartition listeners are only invoked if the transition is successfully completed.\r\n\r\n\r\n**Environment:**\r\n- Zeebe Version: Observed in 8.3.0-SNAPSHOT (medic-cw-27 benchmark)\r\n\n\n megglos: ZDP-Triage:\r\n- likelihood seems high - happened a couple of times in benchmarks already\r\n- bug may exist for way longer already - it was not as visible before though\r\n- 3-4 already existing issues might be caused by this\r\n- workaround would be a pod restart - but hard to identify given partition is still marked as healthy\n oleschoenburg: The issue is that listeners are called when the transition completes normally and that we complete the transition normally when it is actually cancelled: https://github.com/camunda/zeebe/blob/a9d810011935b0538ed6a97cab342b72681c54a4/broker/src/main/java/io/camunda/zeebe/broker/system/partitions/impl/PartitionTransitionProcess.java#L66-L71",
    "title": "PartitionListeners are notified even if the transition is cancelled causing NPE"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13431",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nWhen tls enabled on gateway for [secure client communication](https://docs.camunda.io/docs/next/self-managed/zeebe-deployment/security/secure-client-communication/), readiness check fails. \r\n\r\n<details><summary>Failed health status</summary>\r\n <p>\r\n\r\n```\r\n{\r\n   \"components\" : {\r\n      \"diskSpace\" : {\r\n         \"details\" : {\r\n           ....\r\n         },\r\n         \"status\" : \"UP\"\r\n      },\r\n      \"gatewayClusterAwareness\" : {\r\n         \"status\" : \"UP\"\r\n      },\r\n      \"gatewayPartitionLeaderAwareness\" : {\r\n         \"status\" : \"UP\"\r\n      },\r\n      \"gatewayResponsive\" : {\r\n         \"details\" : {\r\n            \"error\" : \"java.util.concurrent.ExecutionException: io.grpc.StatusRuntimeException: UNAVAILABLE: io exception\\nChannel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]\",\r\n            \"healthZeebeClientProperties\" : {\r\n               \"requestTimeout\" : \"PT0.5S\",\r\n               \"securityProperties\" : {}\r\n            },\r\n            \"timeOut\" : \"PT0.5S\"\r\n         },\r\n         \"status\" : \"DOWN\"\r\n      },\r\n      \"gatewayStarted\" : {\r\n         \"status\" : \"UP\"\r\n      },\r\n      \"livenessGatewayClusterAwareness\" : {\r\n         \"details\" : {\r\n            \"derivedFrom\" : \"ClusterAwarenessHealthIndicator\",\r\n            \"lastSeenDelegateHealthStatus\" : {\r\n               \"status\" : \"UP\"\r\n            },\r\n            \"maxDowntime\" : \"PT5M\",\r\n            \"wasEverUp\" : true\r\n         },\r\n         \"status\" : \"UP\"\r\n      },\r\n      \"livenessGatewayPartitionLeaderAwareness\" : {\r\n         \"details\" : {\r\n            \"derivedFrom\" : \"PartitionLeaderAwarenessHealthIndicator\",\r\n            \"lastSeenDelegateHealthStatus\" : {\r\n               \"status\" : \"UP\"\r\n            },\r\n            \"maxDowntime\" : \"PT5M\",\r\n            \"wasEverUp\" : true\r\n         },\r\n         \"status\" : \"UP\"\r\n      },\r\n      \"livenessGatewayResponsive\" : {\r\n         \"details\" : {\r\n            \"derivedFrom\" : \"ResponsiveHealthIndicator\",\r\n            \"lastSeenDelegateHealthStatus\" : {\r\n               \"details\" : {\r\n                  \"error\" : \"java.util.concurrent.ExecutionException: io.grpc.StatusRuntimeException: UNAVAILABLE: io exception\\nChannel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]\",\r\n                  \"healthZeebeClientProperties\" : {\r\n                     \"requestTimeout\" : \"PT5S\",\r\n                     \"securityProperties\" : {}\r\n                  },\r\n                  \"timeOut\" : \"PT5S\"\r\n               },\r\n               \"status\" : \"DOWN\"\r\n            },\r\n            \"maxDowntime\" : \"PT10M\",\r\n            \"wasEverUp\" : false\r\n         },\r\n         \"status\" : \"DOWN\"\r\n      },\r\n      \"livenessMemory\" : {\r\n         \"details\" : {\r\n            \"threshold\" : 0.01\r\n         },\r\n         \"status\" : \"UP\"\r\n      },\r\n      \"livenessState\" : {\r\n         \"status\" : \"UP\"\r\n      },\r\n      \"ping\" : {\r\n         \"status\" : \"UP\"\r\n      },\r\n      \"readinessState\" : {\r\n         \"status\" : \"UP\"\r\n      }\r\n   },\r\n   \"groups\" : [\r\n      \"liveness\",\r\n      \"readiness\",\r\n      \"startup\"\r\n   ],\r\n   \"status\" : \"DOWN\"\r\n}\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\nBoth `gatewayResponsive` and `livenssGatewayResponsive` fails because of some SSL error.   \r\n\r\nThis was unnoticed before because readiness check was disabled by default in camunda platform helm until 8.2. From 8.2 it is enabled by default. \r\n\r\n**To Reproduce**\r\n\r\nStart a single gateway with TLS enabled as documented [here](https://docs.camunda.io/docs/next/self-managed/zeebe-deployment/security/secure-client-communication/). And then query `http://gateway-host:9600/actuator/health`.\r\n\r\n**Expected behavior**\r\n\r\nHealth and readiness check should work with TLS enabled.\r\n\r\n**Environment:**\r\n- Zeebe Version: All versions (reproduced with 8.0.x, 8.1.5, and 8.2.7)\r\n\r\nrelated to https://jira.camunda.com/browse/SUPPORT-17529 \r\nhttps://jira.camunda.com/browse/SUPPORT-16945\n\n npepinpe: See https://github.com/camunda/zeebe/issues/11799#issuecomment-1558773952\r\n\r\nI guess it was already linked to a support issue. Workaround is to disable the check by setting the following env var:\r\n\r\n```\r\nMANAGEMENT_HEALTH_GATEWAY-RESPONSIVE_ENABLED=false\r\n```\n deepthidevaki: I will leave it open for easy visibility. I searched for open bugs for this, but couldn't find any.\r\n\r\nPlease close both issues when it is fixed. \n megglos: ZDP-Triage:\n- no functional impact on gateway itself\n- breaks the helm chart update as readiness check was recently enabled there\n- workaround is just disabling the specific check https://github.com/camunda/zeebe/issues/13431#issuecomment-1630928379\n- relates to https://github.com/camunda/zeebe/issues/11799\n- quick win could be removing this indicator from the readiness\n- may be better to execute a topologyrequest from k8s instead\n- let's revisit readiness and liveness\n- we need to check-in with the distribution team on this => we may release the workaround disabling this check asap @megglos \n\n megglos: Hey @deepthidevaki ,\r\n\r\nas I try to follow-up on this with the distribution team, the only official config exposed relating to TLS on the helm chart is enabling it on the ingress https://github.com/camunda/camunda-platform-helm/blob/main/charts/camunda-platform/values.yaml#L595-L600 .\r\n\r\nIn the linked support case TLS was enabled on the zeebe pod by the customer via setting `ZEEBE_GATEWAY_SECURITY_ENABLED` etc., so it’s kind of special and not a general issue/breaking change with the helm charts intended way to enable TLS. Where if tls is enabled on the ingress the check should still work as TLS terminates at the ingress and the health check directly accesses the pod endpoint, do I understand this correctly?\r\n\r\nI would then conclude no change to the helm chart is needed and assume likelihood of others hitting that regression is on the lower end ^^ If someone sets up TLS termination manually on the pod, they can make use of the workaround until this issue is resolved on the zeebe side.\n deepthidevaki: > the only official config exposed relating to TLS on the helm chart is enabling it on the ingress\r\n\r\n@megglos \r\nIs that the recommended way to do it? If so, we can also recommend it to the customer. Is there any documentation on it? The only [documentation](https://docs.camunda.io/docs/next/self-managed/zeebe-deployment/security/secure-client-communication/) I found is to configure it on Zeebe gateway. \r\n\r\n> I would then conclude no change to the helm chart is needed and assume likelihood of others hitting that regression is on the lower end ^^ If someone sets up TLS termination manually on the pod, they can make use of the workaround until this issue is resolved on the zeebe side.\r\n\r\n:+1:\n npepinpe: Can we just get rid of this `ResponsiveHealthIndicator`? I don't think it really purposefully demonstrates that it's responsive:\r\n\r\n1. The user can query the topology themselves to see if the service is working\r\n2. The user queries the health endpoint, which queries the topology, to see if the service is working\r\n\r\nIt just seems like extra steps. I understand that the idea is for things like load balancers to not send requests to an unresponsive gateway, but I would argue that it really doesn't bring that much value for the added complexity, since the gateway client has to be configured for all possible gateway configuration (TLS, authentication, etc.) Plus it only checks that the gRPC server serving the right service, but not things like \"can you actually send requests to brokers?\" or the likes. It doesn't even do anything with the topology itself :shrug: \n oleschoenburg: Maybe we should just implement the gRPC [health checking protocol](https://grpc.github.io/grpc/core/md_doc_health-checking.html) and then recommend [gRPC probes](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#probe-check-methods)? \n npepinpe: Surprise surprise, we already do serve the gRPC health check protocol! But it's very basic, only reporting whether the gateway started or is shutting down. But it'd be cool to extend it :+1: \n oleschoenburg: Ah TIL :+1: \r\nI guess this would not 100% replace the `ResponsiveHealthIndicator` anyway because we'd not do a topology request that way. \r\n\r\nI'm not attached to the `ResponsiveHealthIndicator` so just removing it for now also sounds good to me.\n megglos: :information_source: What determines the liveness/readiness of the Gateway as of now?\r\n\r\nreadiness: `StartedHealthIndicator`\r\n\r\n[liveness](https://github.com/camunda/zeebe/blob/main/dist/src/main/resources/application-gateway.properties#L9):\r\n`livenessGatewayResponsive` ==> ResponsiveHealthIndicator  (doing a topologyRequest on the grpc endpoint of the pod)\r\n`livenessGatewayClusterAwareness`  ==> gateway::getStatus\r\n`livenessGatewayPartitionLeaderAwareness`  ==> doing topologyManager.getTopology and checking if there is a leader known for every partition\r\n`livenessMemory`  => checking if JVM has at least 10% free memory (actually intended was >1%, according to the initial task https://github.com/camunda/zeebe/issues/4339 is that a bug? :smile: )\r\n\r\n**I see the following options going forward:**\r\n\r\n:one: Make sure users can configure the internal zeebe client to work properly in such scenarios (there is actually dedicated config present for tls and oauth config)\r\n:heavy_plus_sign: Pro:\r\n- the check’s semantics stay as they are\r\n\r\n:heavy_minus_sign: Con:\r\n- it seems still like a weird UX, I may have to configure a client for my gateway within the Gateway? Likely to be missed and causing questions and support cases\r\n- the gateway being able to connect to it’s own GRPC port does not mean that port is properly reachable from outside the pod\r\n\r\n:wrench: Effort:\r\n- improve docs, hoping users will pick that up without needing help :crossed_fingers:\r\n- potentially extending the helm chart for allowing to config the client\r\n\r\n:two: We remove the ResponsiveHealthIndicator as suggested by Nicolas \r\n\r\n:heavy_plus_sign: Pro:\r\n- less complexity in our liveness check, no side-effects of auth or transport encryption setups\r\n- grpc readiness/connectivity can be checked via a grpc_health_probe via the existing GRPC health endpoint which exists since this https://github.com/camunda/zeebe/pull/7737 (thanks to @npepinpe for raising that!)\r\n  - as of now there is no real readiness probe implemented, that was decided [here](https://github.com/camunda/zeebe/issues/4339#issuecomment-627277532), the ready endpoint is still used by e.g. the helm chart but it equals the startup probe semantics, we may follow-up using the grpc health check as readiness endpoint\r\n- cluster awareness and connectivity is already covered by the other checks\r\n\r\n:heavy_minus_sign: Con:\r\n- semantics of the actuator liveness probe changes, needs announcement\r\n- breaking change which should not be backported, we may still need to advice people to disable the check via `MANAGEMENT_HEALTH_GATEWAY-RESPONSIVE_ENABLED` if they encounter issues\r\n\r\n:wrench: Effort:\r\n- 💥 ResponsiveHealthIndicator\r\n- docs announcement + update guide\r\n- (follow-up) with controller and distribution team of using the grpc_probe as readiness\r\n",
    "title": "Gateway readiness fails when tls enabled"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13061",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nThere is currently a potential race condition which would result in a remote stream existing server side, even though the client stream has gone away.\r\n\r\nSince we register remote streams asynchronously, a remove request may be submitted client side, which will immediately remove it there. Then asynchronous removal requests are sent to the server. However, this can be interleaved with the asynchronous registration, resulting in a stream existing server side.\r\n\r\nThe impact is additional latency during a push, or possible unnecessary job activation if it was the last stream for this type. However, the stream will eventually get removed appropriately.\r\n\r\n**Expected behavior**\r\n\r\nRegistration/removal of remote streams is sequenced, such that a removal request would cancel registration attempts, and queue the removal after whatever in-flight requests were sent are finished.\r\n\r\nThere is still a slight edge case around time outs, of course, but I think this is acceptable for now. The other option would be introducing even more coordination in the protocol, and I'd rather avoid this.\n\n npepinpe: The idea is to associate a `ClientStreamRegistration` state machine to each stream, which manages the current state of the remote registration. States would be:\r\n\r\n```mermaid\r\ngraph LR;\r\n    Initial-->Adding;\r\n    Adding-->Added;\r\n    Initial-->Removing;\r\n    Adding-->Removing;\r\n    Added-->Removing;\r\n    Removing-->Removed;\r\n```\r\n\r\nWhen a client first connects, it creates a registration state machine. The initial state is `INITIAL`, before any request is sent. In this state, nothing is happening until the first request occurs. Once an add request is submitted, it transitions to `ADDING`. It may also transition to `REMOVING` immediately; for simplicity, we'll send a REMOVE request anyway at the moment. \r\n\r\nIn `ADDING`, it will send registration requests to all known brokers. It transitions to `ADDED` when it has been registered to all known brokers. If the client disconnects in this stage, it transitions to `REMOVING`.\r\n\r\nIn the `ADDED` state, it does nothing. When a new broker is discovered, it transitions to `ADDING`, and again only transitions back to `ADDED` when it is registered to all known brokers. If, however, a client disconnects in the `ADDED` state, it transitions to `REMOVING`.\r\n\r\nIn the `REMOVING` state, it first cancels any in-flight registration requests, if possible. It will also discard any responses from previous registration requests at that point. Then it sends a request to all known brokers to remove this stream. When it is disconnected from all brokers, it transitions to `REMOVED`. If a new broker is discovered in this state, nothing should happen.\r\n\r\nIn the `REMOVED` state, nothing happens, and ideally the registration is garbage collected. Any incoming responses should be discarded.\n npepinpe: The above is not quite enough to catch all edge cases however. Since requests can be processed out of order, for every request to a broker per stream, we have to wait until previous requests are finished before sending the next one. Now, we still have the time out edge case - when a request to a broker times out, we can't tell if it never made it, or if it will eventually be processed. But the likelihood of a request being processed out of order in this case is very low I think. Say you send an ADD request, which times out, then a REMOVE request. It's very unlikely that the REMOVE request will somehow be received and processed before the ADD by the remote server. Whereas if you send an ADD, then immediately a REMOVE (without waiting for the ADD), the likelihood is much higher.\n deepthidevaki: If we are not able to prevent the edge case due to REMOVE processed before ADD, we can handle it safely by ensuring that gateway sends a `NoStreamExistsException`, and the broker proactively closes the stream if it gets this error response. \n npepinpe: I think that's already the case. In a way, this issue is generally an optimization one, to avoid unnecessarily activating and pushing a job when there is nothing to receive it. It's a bit hard for me to evaluate the likelihood of this happening. I see two cases:\r\n\r\n1. A client disconnects quickly while registration is still on-going.\r\n2. A client disconnects around the same time a new broker was added to the topology.\r\n\r\nI think for case 1 this is not too likely, so the second case is probably going to be the \"common\" one (and also how I initially observed it).\r\n\r\nMy current approach is to replace the set of servers an `AggregatedClientStream` is connected to with a map of server -> registration state. The registration state will keep track of the current state (e.g. `INITIAL`, `ADDING`, etc.), transition rules, and the single in-flight request (as we don't allow more than one at a time). Note that registrations are scoped per server.\r\n\r\nThe request manager now takes in registrations and manipulates those based on their state.\r\n\r\nAn alternative was to push the communication into the registration itself as well :shrug: Another one was to keep track of registrations in the request manager, not in the client stream. The drawback was we'd have to keep track of yet another source truth, in yet another map of maps (stream -> [server -> [registration]]).\n deepthidevaki: > I think that's already the case.\r\n\r\n:+1: \r\n\r\n> My current approach is to replace the set of servers an AggregatedClientStream is connected to with a map of server -> registration state. The registration state will keep track of the current state (e.g. INITIAL, ADDING, etc.), transition rules, and the single in-flight request (as we don't allow more than one at a time). Note that registrations are scoped per server.\r\n\r\nThis sounds good. Will AggregatedClientStream also marks if the client is closed, so that it can actively close registrations in case of out-of-order Add/Remove requests? i.e if a broker moves to ADDED state after the client is closed, how do we detect this and send a REMOVE request? \n npepinpe: Correct, I ended up adding a CLOSED state to the registration as a short-circuit way to close any pending operations.",
    "title": "Cancel on-going remote stream registration on stream removal"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13046",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nIn #11460 we've implemented a truncation on error messages. This was done to prevent error messages from exceeding the maximum message size. Currently the messages are limited to 500 characters.\r\n\r\nWe've had a support case asking us where the remainder of the error message has gone. 500 characters may be too strict. A regular stack trace will quickly exceed this limit.\r\n\r\nGenerally users would have logging on their workers which should be able to show them the full message. Obviously this isn't visible in Operate and is an extra step for users. If we could provide a better UX by increasing this limit we should consider this.\r\n\r\nWe could also think about making this limit configurable.\r\n\r\n**To Reproduce**\r\n\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\nThrow an error on a Job with > 500 characters.\r\n\r\n**Expected behavior**\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nI can see more than just the first 500 characters of my error.\r\n\r\n-------\r\n\r\nhttps://jira.camunda.com/browse/SUPPORT-17090\n\n korthout: ZPA triage:\n- request makes sense\n- we could easily increase the limit much higher (e.g. 10k) without exceeding MAX_MESSAGE_SIZE\n- we could also consider ways to make this more dynamic, but this is more complex\n- the simple fix has our preference and could be backported to all supported versions easily\n- we'll mark this a bug\n- could be a good first issue for new onboarders\n akkie: The support also pointed me to this issue. I work for a consulting company and we helped a large German corporation to implement Camunda Platform 8 for their processes. In all our presentations, we showed how easy it is for the DevOps guys to see the errors directly in Operate instead of searching through the logs. Of course this was not the only selling point, but it was a feature that everyone really liked.\r\n\r\nI understand the problem for truncating the message, but as @korthout mentioned, if it's ok to increase the limit, it would be really appreciated, because currently with the 500 chars limit, it's not really useful. For us 10k would be more as enough.\n korthout: Thanks for sharing @akkie. It's very helpful to understand such perspectives 🙇 \n aleksander-dytko: Another Support issue about this: https://jira.camunda.com/browse/SUPPORT-17503\r\n\r\nIt would be great to finish this soon. Thank @nicpuppa for taking care of this one 🚀 ",
    "title": "Error message truncation may be too strict"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13041",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "Similar to https://github.com/camunda/zeebe/issues/12797, the `JobBackoffChecker` also tries to cleanup backoffs:\r\n\r\nhttps://github.com/camunda/zeebe/blob/d166007d8fee3fa6f112367ea595d35199807f4f/engine/src/main/java/io/camunda/zeebe/engine/state/instance/DbJobState.java#L308-L317\n",
    "title": "Don't mutate state through `JobBackoffChecker`"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12886",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\n\r\nZeebe offers a [Stackdriver](https://github.com/camunda/zeebe/tree/main/util/src/main/java/io/camunda/zeebe/util/logging/stackdriver) logging format that displays logs as JSON, formatted for Google Cloud Logging.\r\n\r\nNot all the logs are formatted according to this format though:\r\n\r\n* At startup, Zeebe displays a huge ASCII banner showing  `Zeebe`\r\n* Some logs (Tomcat logs?) are not formatted as JSON:\r\n  ```\r\n  May 30, 2023 6:51:02 AM org.apache.coyote.AbstractProtocol init\r\n  INFO: Initializing ProtocolHandler [\"http-nio-0.0.0.0-9600\"]\r\n  May 30, 2023 6:51:02 AM org.apache.catalina.core.StandardService startInternal\r\n  INFO: Starting service [Tomcat]\r\n  May 30, 2023 6:51:02 AM org.apache.catalina.core.StandardEngine startInternal\r\n  INFO: Starting Servlet engine: [Apache Tomcat/10.1.7]\r\n  May 30, 2023 6:51:02 AM org.apache.catalina.core.ApplicationContext log\r\n  INFO: Initializing Spring embedded WebApplicationContext\r\n  ```\r\n\r\n**Describe the solution you'd like**\r\n\r\nIdeally:\r\n\r\n* All the logs are properly formatted as JSON, instead of a mix of plain text/JSON.\r\n* The banner can be completely removed.\r\n\r\nThis would make all the logs uniform, help to classify them correctly, and would prevent spurious parsing of plain-text only logs.\r\n\r\n**Describe alternatives you've considered**\r\n\r\nNone\r\n\r\n**Additional context**\r\n\r\nA typical startup looks like this:\r\n\r\n```\r\nPicked up JAVA_TOOL_OPTIONS: -XX:MaxRAMPercentage=50.0 -XX:InitialRAMPercentage=25.0 -XX:+ExitOnOutOfMemoryError -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/usr/local/zeebe/data/java_started_1685429455.hprof\r\n  ______  ______   ______   ____    ______     ____    _____     ____    _  __  ______   _____  \r\n |___  / |  ____| |  ____| |  _ \\  |  ____|   |  _ \\  |  __ \\   / __ \\  | |/ / |  ____| |  __ \\ \r\n    / /  | |__    | |__    | |_) | | |__      | |_) | | |__) | | |  | | | ' /  | |__    | |__) |\r\n   / /   |  __|   |  __|   |  _ <  |  __|     |  _ <  |  _  /  | |  | | |  <   |  __|   |  _  / \r\n  / /__  | |____  | |____  | |_) | | |____    | |_) | | | \\ \\  | |__| | | . \\  | |____  | | \\ \\ \r\n /_____| |______| |______| |____/  |______|   |____/  |_|  \\_\\  \\____/  |_|\\_\\ |______| |_|  \\_\\\r\n                                                                                                \r\n{\"severity\":\"INFO\",\"logging.googleapis.com/sourceLocation\":{\"function\":\"logStarting\",\"file\":\"StartupInfoLogger.java\",\"line\":51},\"message\":\"Starting StandaloneBroker v8.2.3 using Java 17.0.6 with PID 7 (/usr/local/zeebe/lib/camunda-zeebe-8.2.3.jar started by root in /usr/local/zeebe)\",\"serviceContext\":{\"service\":\"zeebe\",\"version\":\"8.2.3\"},\"context\":{\"threadId\":1,\"threadPriority\":5,\"loggerName\":\"io.camunda.zeebe.broker.StandaloneBroker\",\"threadName\":\"main\"},\"timestampSeconds\":1685429459,\"timestampNanos\":42397893}\r\n{\"severity\":\"DEBUG\",\"logging.googleapis.com/sourceLocation\":{\"function\":\"logStarting\",\"file\":\"StartupInfoLogger.java\",\"line\":52},\"message\":\"Running with Spring Boot v3.0.5, Spring v6.0.8\",\"serviceContext\":{\"service\":\"zeebe\",\"version\":\"8.2.3\"},\"context\":{\"threadId\":1,\"threadPriority\":5,\"loggerName\":\"io.camunda.zeebe.broker.StandaloneBroker\",\"threadName\":\"main\"},\"timestampSeconds\":1685429459,\"timestampNanos\":128377708}\r\n{\"severity\":\"INFO\",\"logging.googleapis.com/sourceLocation\":{\"function\":\"logStartupProfileInfo\",\"file\":\"SpringApplication.java\",\"line\":638},\"message\":\"The following 1 profile is active: \\\"broker\\\"\",\"serviceContext\":{\"service\":\"zeebe\",\"version\":\"8.2.3\"},\"context\":{\"threadId\":1,\"threadPriority\":5,\"loggerName\":\"io.camunda.zeebe.broker.StandaloneBroker\",\"threadName\":\"main\"},\"timestampSeconds\":1685429459,\"timestampNanos\":130430841}\r\n{\"severity\":\"INFO\",\"logging.googleapis.com/sourceLocation\":{\"function\":\"initialize\",\"file\":\"TomcatWebServer.java\",\"line\":108},\"message\":\"Tomcat initialized with port(s): 9600 (http)\",\"serviceContext\":{\"service\":\"zeebe\",\"version\":\"8.2.3\"},\"context\":{\"threadId\":1,\"threadPriority\":5,\"loggerName\":\"org.springframework.boot.web.embedded.tomcat.TomcatWebServer\",\"threadName\":\"main\"},\"timestampSeconds\":1685429462,\"timestampNanos\":567312120}\r\nMay 30, 2023 6:51:02 AM org.apache.coyote.AbstractProtocol init\r\nINFO: Initializing ProtocolHandler [\"http-nio-0.0.0.0-9600\"]\r\nMay 30, 2023 6:51:02 AM org.apache.catalina.core.StandardService startInternal\r\nINFO: Starting service [Tomcat]\r\nMay 30, 2023 6:51:02 AM org.apache.catalina.core.StandardEngine startInternal\r\nINFO: Starting Servlet engine: [Apache Tomcat/10.1.7]\r\nMay 30, 2023 6:51:02 AM org.apache.catalina.core.ApplicationContext log\r\nINFO: Initializing Spring embedded WebApplicationContext\r\n{\"severity\":\"INFO\",\"logging.googleapis.com/sourceLocation\":{\"function\":\"prepareWebApplicationContext\",\"file\":\"ServletWebServerApplicationContext.java\",\"line\":291},\"message\":\"Root WebApplicationContext: initialization completed in 3603 ms\",\"serviceContext\":{\"service\":\"zeebe\",\"version\":\"8.2.3\"},\"context\":{\"threadId\":1,\"threadPriority\":5,\"loggerName\":\"org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext\",\"threadName\":\"main\"},\"timestampSeconds\":1685429462,\"timestampNanos\":856072455}\r\n{\"severity\":\"WARNING\",\"logging.googleapis.com/sourceLocation\":{\"function\":\"overrideDiskConfig\",\"file\":\"DataCfg.java\",\"line\":69},\"message\":\"Configuration parameter data.diskUsageCommandWatermark is deprecated. Use data.disk.freeSpace.processing instead.\",\"serviceContext\":{\"service\":\"zeebe\",\"version\":\"8.2.3\"},\"context\":{\"threadId\":1,\"threadPriority\":5,\"loggerName\":\"io.camunda.zeebe.broker.system\",\"threadName\":\"main\"},\"timestampSeconds\":1685429463,\"timestampNanos\":631599523}\r\n{\"severity\":\"WARNING\",\"logging.googleapis.com/sourceLocation\":{\"function\":\"overrideDiskConfig\",\"file\":\"DataCfg.java\",\"line\":75},\"message\":\"Configuration parameter data.diskUsageReplicationWatermark is deprecated. Use data.disk.freeSpace.replication instead.\",\"serviceContext\":{\"service\":\"zeebe\",\"version\":\"8.2.3\"},\"context\":{\"threadId\":1,\"threadPriority\":5,\"loggerName\":\"io.camunda.zeebe.broker.system\",\"threadName\":\"main\"},\"timestampSeconds\":1685429463,\"timestampNanos\":632382123}\r\n{\"severity\":\"INFO\",\"logging.googleapis.com/sourceLocation\":{\"function\":\"<init>\",\"file\":\"EndpointLinksResolver.java\",\"line\":58},\"message\":\"Exposing 8 endpoint(s) beneath base path '/actuator'\",\"serviceContext\":{\"service\":\"zeebe\",\"version\":\"8.2.3\"},\"context\":{\"threadId\":1,\"threadPriority\":5,\"loggerName\":\"org.springframework.boot.actuate.endpoint.web.EndpointLinksResolver\",\"threadName\":\"main\"},\"timestampSeconds\":1685429465,\"timestampNanos\":427691130}\r\nMay 30, 2023 6:51:05 AM org.apache.coyote.AbstractProtocol start\r\nINFO: Starting ProtocolHandler [\"http-nio-0.0.0.0-9600\"]\r\n\r\n[...]\r\n```\r\n\r\n* The banner should be removed\r\n* Non-JSON logs should be turned to JSON\r\n* Ideally, the `JAVA_TOOL_OPTIONS` should also be turned into JSON (or removed)\n\n korthout: ZPA triage:\n- relevant issue, but not a priority for us (`later`)\n- good first issue for newcomers\n megglos: ZDP-Triage:\n- spring/tomcat seems to use some different logger setup or sysout\n- JAVA_TOOL_OPTIONS output seems to come from the JVM => it cannot be suppressed => would require sout redirect, alternatively we could use other ways to pass these options\n- @multani what's the actual impact for the logging infrastructure, I would assume these logs are ignored or not properly categorized?\n- assigning later as well\n multani: > * @multani what's the actual impact for the logging infrastructure, I would assume these logs are ignored or not properly categorized?\r\n\r\nImpact is not huge: logs are analyzed as plain text without any extra metadata and are not really filterable / analyzable.\r\n\r\nFor instance: I never saw Tomcat errors in our systems, but it would be difficult to look them up as the log level is not parsed. If they were parsed correctly, errors would clearly stand out.\r\n\r\nThe banner is only displays as garbage into the logs and should be simply removed.\n multani: This is definitely not critical, but consider it as a good practice for production.\n npepinpe: re Tomcat, tbh I've been thinking we should switch to WebFlux, but there wasn't much pressure for it. but it would remove unnecessary dependencies, since webflux is built on top of netty and we already bundle netty for our own usage (and would not do this weird logging to STDERR as well).\n npepinpe: My proposal:\r\n\r\n- [x] Since we can't suppress `JAVA_TOOL_OPTIONS`, use `JAVA_OPTS` to pass options instead of `JAVA_TOOL_OPTIONS`. This would require some changes on the Helm chart and the controller though.\r\n- [x] [Switch from Tomcat to WebFlux or Undertow (preference to WebFlux). WebFlux is Spring's reactive web server based on Netty, and as our whole system is based on Netty anyway, might as well go for that](https://github.com/camunda/zeebe/pull/13539)\r\n- [x] The huge banner can be disabled via `SPRING_MAIN_BANNER-MODE=off` - again this is a change in the Helm chart/controller.\r\n\r\nSo from the Zeebe side, it would be just the second part. The others would be done in the controller.\n multani: If you can't remove the `Picked up JAVA_TOOL_OPTIONS` line, would it be possible to output that line to stdout instead of stderr?\n npepinpe: We have no control over that, that's the JVM directly writing this =/\r\n\r\nThe only way I can think of would be to redirect STDOUT to STDERR, but then that's all logs =/\r\n\r\nI think the best option is to simply not use `JAVA_TOOL_OPTIONS`. Our scripts already support `JAVA_OPTS`, so we can just make sure the controller and Helm chars both use that instead.\n multani: > We have no control over that, that's the JVM directly writing this =/\r\n\r\nAh yes, you are right, I forgot about that :facepalm: \n npepinpe: Done via https://github.com/camunda-cloud/camunda-operator/pull/1753\r\n\r\nThe Helm charts were not updated, so they will still print non JSON logs, but that can be fixed by essentially doing the same we did in https://github.com/camunda-cloud/camunda-operator/pull/1753.",
    "title": "Zeebe should only log as JSON, if configured to do so"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12007",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n```\r\nio.camunda.zeebe.stream.api.records.ExceededBatchRecordSizeException: Can't append entry: \r\n'RecordBatchEntry[recordMetadata=RecordMetadata{recordType=EVENT, valueType=JOB_BATCH, intent=ACTIVATED}, \r\nkey=4503599628576248, sourceIndex=-1, unifiedRecordValue=\r\n\r\n\"maxJobsToActivate\":13822\r\n```\r\n\r\nPartition is unhealthy. \r\n```\r\nBroker-1-StreamProcessor-2{status=UNHEALTHY, issue=HealthIssue[message=not making progress, \r\n\r\nBroker-1-StreamProcessor-1{status=UNHEALTHY, issue=HealthIssue[message=actor appears blocked, \r\n```\r\nBut StreamProcessor is not stuck. Eventhough there is high backpressure, it is not 100%. Meaning that StreamProcessor is accepting new commands and processing them. But it seems no job can be activated.\r\n\r\n**To Reproduce**\r\n\r\n\r\n\r\n**Expected behavior**\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n**Log/Stacktrace**\r\n\r\n[Link to logs](https://console.cloud.google.com/errors/detail/CIeFtI729PW2Cw;service=zeebe;time=P7D?project=camunda-cloud-240911)\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\nio.camunda.zeebe.stream.api.records.ExceededBatchRecordSizeException: Can't append entry: 'RecordBatchEntry[recordMetadata=RecordMetadata{recordType=EVENT, valueType=JOB_BATCH, intent=ACTIVATED}, key=4503599628576248, sourceIndex=-1, unifiedRecordValue={\"type\":\"***\",\"worker\":\"***\",\"timeout\":300000,\"maxJobsToActivate\":13822,\"jobKeys\":[4503599628420203,4503599628420225,4503599628420247,4503599628420269,4503599628420291,4503599628420314,4503599628420318,4503599628420340,4503599628420344,4503599628420348,4503599628420352,4503599628420356,4503599628420360,4503599628420364,4503599628420446,4503599628420450,4503599628420454,4503599628420458,4503599628420462,4503599628420466,4503599628420516,4503599628420520,4503599628420524,4503599628420528,4503599628420532,4503599628420629,4503599628420633,4503599628420650,4503599628420702,4503599628420706,4503599628420716,4503599628420726,4503599628420730,4503599628420734,4503599628420770,4503599628420790,4503599628420794,4503599628420810,4503599628420818,4503599628420822,4503599628420834,4503599628420842,4503599...' with size: 4194277 this would exceed the maximum batch size. [ currentBatchEntryCount: 0, currentBatchSize: 0]\r\n\r\nat io.camunda.zeebe.stream.impl.records.RecordBatch.appendRecord ( [io/camunda.zeebe.stream.impl.records/RecordBatch.java:67](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl.records%2FRecordBatch.java&line=67&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.BufferedProcessingResultBuilder.appendRecordReturnEither ( [io/camunda.zeebe.stream.impl/BufferedProcessingResultBuilder.java:62](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FBufferedProcessingResultBuilder.java&line=62&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.api.ProcessingResultBuilder.appendRecord ( [io/camunda.zeebe.stream.api/ProcessingResultBuilder.java:38](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.api%2FProcessingResultBuilder.java&line=38&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.streamprocessor.writers.ResultBuilderBackedEventApplyingStateWriter.appendFollowUpEvent ( [io/camunda.zeebe.engine.processing.streamprocessor.writers/ResultBuilderBackedEventApplyingStateWriter.java:40](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.streamprocessor.writers%2FResultBuilderBackedEventApplyingStateWriter.java&line=40&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.job.JobBatchActivateProcessor.activateJobBatch ( [io/camunda.zeebe.engine.processing.job/JobBatchActivateProcessor.java:125](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.job%2FJobBatchActivateProcessor.java&line=125&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.job.JobBatchActivateProcessor.activateJobs ( [io/camunda.zeebe.engine.processing.job/JobBatchActivateProcessor.java:86](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.job%2FJobBatchActivateProcessor.java&line=86&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.job.JobBatchActivateProcessor.processRecord ( [io/camunda.zeebe.engine.processing.job/JobBatchActivateProcessor.java:63](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.job%2FJobBatchActivateProcessor.java&line=63&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.Engine.process ( [io/camunda.zeebe.engine/Engine.java:127](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine%2FEngine.java&line=127&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.batchProcessing ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:340](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=340&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$2 ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:263](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=263&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run ( [io/camunda.zeebe.db.impl.rocksdb.transaction/ZeebeTransaction.java:84](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.db.impl.rocksdb.transaction%2FZeebeTransaction.java&line=84&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:263](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=263&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:222](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=222&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.readNextRecord ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:198](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=198&project=camunda-cloud-240911) )\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n\r\n- Zeebe Version: 8.2.0-alpha5\r\n\n\n deepthidevaki: @korthout Please check if this is affecting ZPA.\n megglos: The exception changed, surfacing this issue. The cause might just be the high number of jobs to activate.\nTime box it for 30m to double-check if this is an actual bug.\n korthout: Removed from our board, until further notice\n remcowesterhoud: This happened again on the release benchmark of `8.2.x`\r\n\r\nI've increased the max message size to recover the benchmark\n Zelldon: > Removed from our board, until further notice\r\n\r\nThe reasoning would be great here.\n remcowesterhoud: > > Removed from our board, until further notice\r\n> \r\n> The reasoning would be great here.\r\n\r\nI don't remember, but probably because we thought it was related to the stream platform because of the label.\r\n\r\nI'll try to do a deeper analysis this week and see if I can pinpoint the bug.\n remcowesterhoud: I've been trying to reproduce but have been unable to. I can think of 2 possible causes:\r\n\r\n1. The [check if a record of size X can be appended](https://github.com/camunda/zeebe/blob/main/logstreams/src/main/java/io/camunda/zeebe/logstreams/impl/log/Sequencer.java#L56-L63) is not 100% foolproof and in edge cases returns `true` even though it doesn't fit.\r\n2. The [expected event length](https://github.com/camunda/zeebe/blob/main/engine/src/main/java/io/camunda/zeebe/engine/processing/job/JobBatchCollector.java#L87-L96) is not fully accurate. It tries to calculate the event length precisely, but if it's slightly off, we could append it yet still exceed the batch size.\r\n\r\nIMO 2 is the more likely scenario. My proposal would be to add an extra buffer to expected event length. This means we would check if a larger than expected event would fit in the batch. If this isn't the case we won't add it. This buffer can be relatively small. For terminating in batches we used 8 KB, but I think that's excessive for job activations. Instead I'd stick with a few bytes.\r\n\r\nAs this is part of the code is ZPA's responsibility I'll remove this from the ZDP board and add it to ours.\n korthout: ZPA triage:\n- small chance, but high impact\n- small effort: let's add a small margin to the check whether another job fits the batch\n- such a margin could help with #12778 \n- let's prioritize this as `upcoming`",
    "title": "ExceededBatchRecordSizeException: Can't append entry"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13666",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "<!--\r\nIn case you have questions about our software we encourage everyone to participate in our community via the\r\n- Camunda Platform community forum https://forum.camunda.io/ or \r\n- Slack https://camunda-cloud.slack.com/ (For invite: https://camunda-slack-invite.herokuapp.com/)\r\n\r\nThere you can exchange ideas with other Zeebe and Camunda Platform 8 users, as well as the product developers, and use the search to find answer to similar questions.\r\n\r\nThis issue template is used by the Zeebe engineers to create general tasks.\r\n-->\r\n\r\n**Description**\r\n\r\nBackport PRs that are created by the backport-action could be merged automatically if they succeed the CI. At this stage the code has already been reviewed and if the CI fails there is no reason to not merge it.\r\n\r\nWe already have a similar step to merge dependabot PRs, which we can reuse by simply adding the backport-action user to it. It works by approving the PR using the `GITHUB_TOKEN` of the workflow and adding a `bors merge` comment to merge the PR.\n",
    "title": "Automatically merge backport PRs"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13645",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nIt seems like Snyk is always monitoring the 8.3.0-SNAPSHOT image for all stable releases. The action is incorrectly checking out main instead of whatever the release workflow wants it to check out.\r\n\r\nThat's pretty bad, so should be fixed ASAP\r\n\n",
    "title": "Snyk monitoring wrong Docker images for stable releases"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13601",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nIf a PR is stale we want to label these with a `stale` label. This allows us to keep track of PRs that have been inactive for a while and we can decide to close them at some point.\r\n\r\n**Describe the solution you'd like**\r\n- Mark a PR as stale after 1 month of inactivity\r\n- Document in the Contributing guide that a PR is marked as stale after 1 month of inactivity and we could delete if it stays inactive.\r\n\r\nThere is a[ GitHub Action](https://github.com/actions/stale) that can do this for us.\r\n\r\n**Describe alternatives you've considered**\r\nN/A\r\n\n",
    "title": "Mark PR as stale after 1 month"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13281",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nWe added many actors with the new job push feature, but forgot to name them properly. Having properly named actors (with the node ID and behavior) helps a lot when debugging to understand who is doing what.\r\n\n\n npepinpe: Turns out we had to backport the release changes, as the workflow definition used is from the branch under test apparently.",
    "title": "Define actor names for new remote stream actors"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13240",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "Follow up #https://github.com/camunda/zeebe/pull/12839#issuecomment-1564228116 \r\n\r\nIn  PR #12839, when we added new fields to the segment descriptor the descriptor length changed. This can cause issues, when we overwrite an existing descriptor. We have to ensure that we don't accidently overwrite the first entry. This is handled in the PR. But, when ever we make changes to the descriptor this needs to be handled correctly. It is prone to errors, so it is better we use fixed size for the descriptor. We may not use the full allocated bytes, but it allows us to add new fields without the need to change the length again.\r\n\r\nBesides, we also now have to keep track of length of each version. https://github.com/camunda/zeebe/pull/12839#discussion_r1246323206 With fixed lenght, we might be able to handle this better.\r\n\r\nNote: Better do it before 8.3, otherwise it will be more effort to make it backward compatible.\r\n\n\n deepthidevaki: @npepinpe We discussed different things previously. \r\n1. As described in the issue, use a fixed length for descriptor. There will be unused bytes so that we can freely add or remove fields in future with out changing the descriptor length. This means we will have three versions - V1 (old one), V2 (version until 8.2 where the length = sbe encoded length), V3 with fixed length.\r\n2. We also discussed about removing the need to keep track of the version length because we can determine the length while decoding sbe. In this case we will have only two version - V1 (old one) and V2. In V2 depending on sbe version we may get different lengths. We can also add new fields in future which changes the length without changing the version because sbe version is updated.\r\n\r\nI'm now thinking about which way to proceed. Any opinions?\r\nI'm leaning towards option 2. The main problem with variable length is that a new version should not overwrite an existing descriptor with old version. But, this is already handled in the code. So I don't see any other blockers. With this, I don't see any advantage of using fixed length. What do you think? \n npepinpe: The second one as well :+1:\n deepthidevaki: Closing this issue as we decided not to use fixed length descriptor. Instead PR #13618 improves the handling of version in segment descriptor.",
    "title": "Use a fixed length segment descriptor"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13214",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nRelated to https://github.com/camunda/product-hub/issues/1356 all C8 components must adhere to the [best practices](https://confluence.camunda.com/display/HAN/Dockerfile+Base+Images) by implementing them or being an (approved+documented) exception.\r\n\r\nAccording to the current [Adoption Status](https://confluence.camunda.com/display/HAN/Dockerfile+Base+Images#DockerfileBaseImages-AdoptionStatus) for Zeebe this means:\r\n\r\n- introduce [strict pinning](https://confluence.camunda.com/display/HAN/Dockerfile+Base+Images#DockerfileBaseImages-StrictVersionPinningforBaseImages) by changing https://github.com/camunda/zeebe/blob/main/Dockerfile\r\n- check whether Alpine can be adopted or provide written reasoning why it is not feasible\n\n npepinpe: We've already started work via https://github.com/camunda/zeebe/issues/12959\r\n\r\nAs a first step we'll move to the Alpine based Temurin image. Later we can investigate building our own Java base image on top of Alpine, if only to keep us more up to date with base image security fixes, as each additional layer on top of a base image adds delays for security patches to get added.\r\n\r\nI did want to challenge a few things there, if you have some time though (e.g. pro-actively apply security patches ourselves to our own Java base image at the cost of reproducibility).\n cmur2: > As a first step we'll move to the Alpine based Temurin image.\r\n\r\nNice :+1: I think if it is Alpine 3.18 based there shouldn't be too much problems in C8 SaaS but you'll probably test it carefully, anyways.\r\n\r\n> I did want to challenge a few things there, if you have some time though (e.g. pro-actively apply security patches ourselves to our own Java base image at the cost of reproducibility).\r\n\r\nSure, I'm available :) I think manually applying security patches can have more upsides than downsides in your usecase, so definitely open to that :+1:\n npepinpe: So the easy way of applying patches is simply to update installed packages to their latest version. Downside is:\r\n\r\n1. It's blindly updating everything, not just security patches. It seems Alpine doesn't support making a distinction between security updates and other kinds.\r\n2. Builds won't be reproducible anymore (unless we vendor the APKINDEX tar ball =/)\r\n\r\nFor 1, well, there isn't all that much installed in the first place, so maybe it's acceptable. For 2, I can't remember the last time we wanted to reproduce a build, except within very short time frames, so unclear how much value we get from that currently.\n megglos: moving to ready as @npepinpe is already on it\n npepinpe: Another question (though this may be more a product decision) is, do we consider moving from an Ubuntu base to Alpine a breaking change?\r\n\r\nFor example, doing this _will_ break the Helm chart, as it currently overwrites the startup command with a `bash` script - Alpine, by default, does not include `bash`, but `sh`.\r\n\r\nWe can update the Helm chart, but this may break existing user deployments, or anybody who was building on top of our own image.\n hown3d: @npepinpe adding bash in the alpine image by apk would fix that issue at first to not introduce breaking changes. \nI would expect the new alpine based image to work without changes in Kubernetes configuration.\n npepinpe: Hey @hown3d , thanks for the perspective. Bash was just an example though, there might be other changes which would break existing deployments, since ubuntu based images contain about 3 times the number of utilities, and who knows what everyone is relying on.\n\nSince one of the goals here is to reduce effort related to CVE maintenance (including informing users how certain flagged vulnerabilities are irrelevant for us), reinstalling everything is counterproductive (of course we could say only bash is important and everything else is acceptable :shrug:)\n\nAt some point I think we'll have to bite the bullet. The problem is I don't have any real data on how many builds would break by switching to an Alpine based image.\n\nMaybe it's enough to, say, have an image which works out of the box with the existing Helm chart, and we can remove bash in the future. I don't think bash is a big source of CVEs, so should be fine.\n hown3d: If it's about CVE maintenance, maybe check out the images from [chainguard](https://github.com/chainguard-images/images). They provide secure and minimal images, that contain as close to 0 CVEs. The images are based on the [Wolfi OS project](https://github.com/wolfi-dev/os).\n npepinpe: I evaluated the Chainguard images [here](https://github.com/camunda/zeebe/issues/12959). I agree they likely have less vulnerabilities, but it's also a bit of a chicken and egg, where most vulnerability scanners simply have no information about the base image and it's unclear how well it gets scanned for CVEs. Plus, extending them is somewhat painful (much like Google's Distroless) due to the steep learning curve behind apko and melange.\r\n\r\nAt any rate, the issue with breaking changes remains the same, whether it's chainguard or alpine based :shrug: \n hown3d: Indeed, it is accurate to say that extending it can pose some challenges. \r\nMy suggestion would be to utilize the [wolfi-base](https://edu.chainguard.dev/chainguard/chainguard-images/reference/wolfi-base/overview/) image, which includes **apk** and **busybox** enabling the installation of packages. Using the wolfi-base image would allow to build and extend the image by Dockerfile.\r\nAs wolfi employs the APKINDEX, security scanners should also extract data from it. \r\n\r\nA downside would be that it would be neccessary to install jre during build.\r\n\r\nIt is worth noting that there are still lingering breaking changes by those changes, as you rightly mentioned.\r\n\r\n\n npepinpe: > A downside would be that it would be neccessary to install jre during build.\r\n\r\nAt any rate, I was thinking we might do this in the long run anyway, if only to keep our base image faster up to date with security patches (since the official JRE images are often behind by up to a month). The main blocker there is enabling Renovate to detect JRE updates based on our Dockerfile and raise PRs/issues when we need to update it. This should be possible via regex matches and the [Java Version data source](https://docs.renovatebot.com/modules/datasource/java-version/)\n hown3d: I've scribbled around a bit and created a patch to use wolfi as the base image:\r\n```diff\r\ndiff --git a/Dockerfile b/Dockerfile\r\nindex c20495235a..83ab04ea78 100644\r\n--- a/Dockerfile\r\n+++ b/Dockerfile\r\n@@ -5,9 +5,9 @@\r\n ARG JVM=\"eclipse-temurin\"\r\n ARG JAVA_VERSION=\"17\"\r\n # We duplicate the JVM and JAVA_VERSION vars here as renovate will otherwise fail to properly parse\r\n-ARG BASE_IMAGE=\"eclipse-temurin:17-jre-focal\"\r\n-ARG BASE_DIGEST_AMD64=\"sha256:901eeb64e3d1e74d261e82e4158386407b95628eaf723058fb96d4efb9141b88\"\r\n-ARG BASE_DIGEST_ARM64=\"sha256:eb3488634b9b33c601be1bbee8abf59e98bf4f2493abd982baea0f4831f25b31\"\r\n+ARG BASE_IMAGE=\"cgr.dev/chainguard/wolfi-base\"\r\n+ARG BASE_DIGEST_AMD64=\"sha256:dd1801ff8675b4aa2aac0a62384be211ed678d29df4a835c039ffdb71f12de05\"\r\n+ARG BASE_DIGEST_ARM64=\"sha256:a57f3751b6ead1dab3c48ca06092ac4e7a30156f3b84b070e88e48890d77f01c\"\r\n\r\n # set to \"build\" to build zeebe from scratch instead of using a distball\r\n ARG DIST=\"distball\"\r\n@@ -93,10 +93,16 @@ LABEL io.openshift.non-scalable=\"false\"\r\n LABEL io.openshift.min-memory=\"512Mi\"\r\n LABEL io.openshift.min-cpu=\"1\"\r\n\r\n+RUN apk add ca-certificates \\\r\n+    bash \\\r\n+    openjdk-17-jre~17 \\\r\n+    libstdc++ # for rocksdb\r\n+\r\n ENV ZB_HOME=/usr/local/zeebe \\\r\n     ZEEBE_BROKER_GATEWAY_NETWORK_HOST=0.0.0.0 \\\r\n     ZEEBE_STANDALONE_GATEWAY=false \\\r\n-    ZEEBE_RESTORE=false\r\n+    ZEEBE_RESTORE=false \\\r\n+    JAVA_HOME=/usr/lib/jvm/java-17-openjdk\r\n ENV PATH \"${ZB_HOME}/bin:${PATH}\"\r\n # Disable RocksDB runtime check for musl, which launches `ldd` as a shell process\r\n # We know there's no need to check for musl on this image\r\n@@ -108,8 +114,8 @@ VOLUME /tmp\r\n VOLUME ${ZB_HOME}/data\r\n VOLUME ${ZB_HOME}/logs\r\n\r\n-RUN groupadd -g 1000 zeebe && \\\r\n-    adduser -u 1000 zeebe --system --ingroup zeebe && \\\r\n+RUN addgroup -g 1000 zeebe && \\\r\n+    adduser -u 1000 zeebe -S -G zeebe && \\\r\n     chmod g=u /etc/passwd && \\\r\n     # These directories are to be mounted by users, eagerly creating them and setting ownership\r\n     # helps to avoid potential permission issues due to default volume ownership.\r\n```\r\n\n npepinpe: Happy to get feedback on the PR. I created it as a PR, but I actually meant to make it a draft PR to get some early feedback :sweat_smile: ",
    "title": "Align with Docker base image best practices"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12959",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nMany container scanning tools regularly flag several medium and low CVEs in our official Docker image. While we investigate them, it's a time consuming process, and it's poorly communicated to the outside world. Additionally, many of these come from dependencies that we make no use of, and end up being either just noise (e.g. memory overflow in freetype), or unnecessary attack vectors (e.g. having `nc` or `sshd` installed)\r\n\r\n> **Note**\r\n> Consider this a first step towards a vulnerability free container image.\r\n\r\nOne recurring suggestion to switch base image. This would hopefully mean we can only focus on the vulnerabilities that would be introduced directly by us, over which we have full control.\r\n\r\nThis issue is to investigate this and other possible solutions to modify our official Docker image to reduce not only the number of CVEs currently listed, but reduce the overall amount of effort needed to keep it low.\r\n\n\n npepinpe: # Changing base images\r\n\r\nLet's start with the biggest bang for our buck, changing base image. I've spent some time to analyze the following images:\r\n\r\n- Distroless - [gcr.io/distroless/java17-debian11](https://gcr.io/distroless/java17-debian11)\r\n- Temurin Ubuntu - [eclipse-temurin-17-jre-focal](https://hub.docker.com/layers/library/eclipse-temurin/17-jre-focal/images/sha256-22f133769ce2b956d150ab749cd4630b3e7fbac2b37049911aa0973a1283047c?context=explore)\r\n- Temurin Alpine - [eclipse-temurin:17-jre-alpine](https://hub.docker.com/layers/library/eclipse-temurin/17-jre-alpine/images/sha256-d69f8cf3526fd75992366d2e96348682dfbc04c5d321c08d084e1fc26980d81d)\r\n- Chainguard - [cgr.dev/chainguard/jre:latest](https://github.com/chainguard-images/images/tree/main/images/jre)\r\n- Liberica JRE - [bellsoft/liberica-runtime-container](https://hub.docker.com/r/bellsoft/liberica-runtime-container)\r\n- Zulu Distroless - [azul/zulu-openjdk:17-distroless](https://hub.docker.com/r/azul/zulu-openjdk)\r\n\r\nFor each image, I evaluated the following:\r\n\r\n- Number of existing CVEs\r\n- Release cadence\r\n- Architectures supported\r\n- Governance (i.e. who maintains/supports it)\r\n- Longevity (i.e. how long it's been around)\r\n- DevEx (e.g. tooling support, learning curve, extensibility, etc.)\r\n\r\n## Not considered\r\n\r\nI didn't look deeply into the following images:\r\n\r\n- Amazon Corretto: the official image is Alpine based, and Corretto being an OpenJDK variant, quick scans showed very similar results as the Temurin/Alpine images.\r\n- Azul Zulu Alpine: similarly, these images seem very similar to the Temurin ones, where the vulnerability comes from the base image.\r\n\r\n## Distroless\r\n\r\nImage: [gcr.io/distroless/java17-debian11](https://gcr.io/distroless/java17-debian11)\r\n\r\n### TL;DR\r\n\r\n| CVEs | Release | Arch | Governance | Longevity | DevEx | Size |\r\n|------|---------|------|------------|-----------|-------|------|\r\n| :warning: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :warning: | 231 MB |\r\n\r\n### Evaluation\r\n\r\n[Distroless](https://github.com/GoogleContainerTools/distroless) is actually a set of images provided by Google, which are not, in fact, distro-less. They're based on debian, but a very, very stripped down version of debian.\r\n\r\nEach image is offered in various variants: `nonroot`, `debug`, and `debug-nonroot`. The `nonroot` variants are self explanatory, using a pre-made, non-root user, and the debug variants simply add [busybox](https://busybox.net/) to the image. They're all built for several architectures, but in our case, the important part is they support both `amd64`, `arm64`.\r\n\r\nImages typically built on top of each other. So the Java image has the following hierarchy:\r\n\r\n- gcr.io/distroless/static\r\n- gcr.io/distroless/base\r\n- gcr.io/distroless/java17-debian11\r\n\r\nThe static image contains a stripped down version of debian (version configurable), `ca-certificates`, a `/etc/passwd` entry for a root user, a `/tmp` directory, and time zones (`tzdata`). The base then adds `glibc`, `libssl`, and `openssl`. Then nonroot and debug variants add additional entries and packages, as mentioned above.\r\n\r\n> **Note**\r\n> To fully understand how the distro is stripped down, you have to read the [Bazel](https://bazel.build/) build files. It's fairly understand as these are not complex ones, but it's still a little of a learning curve.\r\n\r\nThe Java image uses debian11 as the base distro, and adds a JDK or a JRE on top. Currently, only images for the LTS versions of the JDK/JRE are produced (11 and 17). These are installed from the official Debian packages.\r\n\r\nIn addition to the JDK/JRE package, it adds several other packages required for a full deployment: `libcrypt`, `libfreetype6`, `libpcre3`, etc. The Bazel file contains an exhaustive list.\r\n\r\n> **Note**\r\n> While I say images are built on top of each other, they're built using Bazel. Meaning in the end, the Java image has no base image at all.\r\n\r\nScanning it with Snyk reveals 33 issues: 1 high, and 2 medium. Scanning with Trivy actually failed at times, and when not, returned 2 high, 5 medium, and 27 low vulnerabilities. Release cadence seems to be fairly regular, with new images pushed whenever new patches are available on for the software included (as obtained via APT). The images have been around for about 6 years at this point, so they're fairly well honed.\r\n\r\nThe biggest downside here is the developer experience. Bazel is used to build the images, which results in an image with no Dockerfile. This means it's less straightforward to understand exactly how it was built, as you need to parse Bazel files. Thankfully these are small enough, so it's not too big of a learning curve, but still. It also means while their Bazel build is made to be extensible, it's a bit more difficult to extend the images via Docker. Finally, their recommended production images (the non-debug variants) are meant to be as stripped as possible. While this reduces possible attack vectors and sources of noisy CVEs, it means developers need to get used to having shell-less, utility-less (e.g. no `ls`, no `df`, etc.) tools, and must use things such as ephemeral containers on Kubernetes (if available), or manually import their tooling (e.g. copy over busybox's statically linked uclibc variant).\r\n\r\n## Temurin Ubuntu\r\n\r\nImage: [eclipse-temurin:17-jre-focal](https://hub.docker.com/_/eclipse-temurin)\r\n\r\n> **Note**\r\n> This is currently the base image for [camunda/zeebe](https://hub.docker.com/camunda/zeebe)\r\n\r\n### TL;DR\r\n\r\n| CVEs | Release | Arch | Governance | Longevity | DevEx | Size |\r\n|------|---------|------|------------|-----------|-------|------|\r\n| :warning: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | 267 MB |\r\n\r\n### Evaluation\r\n\r\nThis image is built by the [Eclipse foundation](https://projects.eclipse.org/projects/adoptium.temurin), and provided via the [Adoptium project](https://adoptium.net/). This is a working group backed by Microsoft, IBM, RedHat, Google, Azul, and others. It's essentially a pre-built OpenJDK distributed by them. As such, there is no change between this and any other OpenJDK implementation to be expected.\r\n\r\n> **Note**\r\n> Eclipse Temurin was formerly the [AdoptOpenJDK project](https://adoptium.net/blog/2021/08/adoptium-celebrates-first-release/).\r\n\r\nThe image is built directly on top of the latest [ubuntu-focal](https://hub.docker.com/_/ubuntu) - see the [official Dockerfile](https://github.com/adoptium/containers/blob/920efae8fe37e2b8f2b288b5f7f9e67134ecad1d/17/jre/ubuntu/focal/Dockerfile.releases.full) here. It contains everything from that image, plus the `tzdata`, `curl`, `wget`, `ca-certificates`, `fontconfig`, `locales`, and `binutils` packages.\r\n\r\n> **Note**\r\n> I don't really see why they add `curl`, and `wget`, other than for convenience and usage of `wget` in their Dockerfile, since Docker supports a remote `ADD` command.\r\n\r\nThe JRE installed is fetched directly as a tar ball from their own servers, and unpacked.\r\n\r\nThe image is released roughly on a monthly basis from experience, keeping up with security patches from the base image. That said, as of now, a Snyk scan of the image results in 31 CVEs, of which 9 are medium. **One major caveat to this**: the latest official Ubuntu base image (Mantic or 23) is vulnerability free, and the last Debian base image (buster or 11, which is what the Temurin image is based on) only has low severity CVEs. One alternative would then be us using the Ubuntu base image and unpacking the official Java tar ball ourselves.\r\n\r\nRunning a Snyk scan returns 31 vulnerabilities, of which 9 are medium, and 22 are low.\r\n\r\nRunning a Trivy scan returns 25 medium, and 57 low vulnerabilities.\r\n\r\n## Temurin Alpine\r\n\r\nImage: [eclipse-temurin:17-jre-alpine](https://hub.docker.com/_/eclipse-temurin)\r\n\r\n### TL;DR\r\n\r\n| CVEs | Release | Arch | Governance | Longevity | DevEx | Size |\r\n|------|---------|------|------------|-----------|-------|------|\r\n| :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :warning: | :heavy_check_mark: | :heavy_check_mark: | 162 MB |\r\n\r\n### Evaluation\r\n\r\nThis image is the [Linux Alpine](https://www.alpinelinux.org/) variant of the Temurin family of images. The main difference with the previous one is its reliance on the [musl libc](https://musl.libc.org/) implementation instead of the more common [glibc](https://www.gnu.org/software/libc/). This sometimes has an impact on native libraries which can make usage of the standard C library. For example, in Zeebe, we previously could not use any Alpine based images because of our dependency on RocksDB, which at the time, did not distribute `musl` variants.\r\n\r\nRunning a Snyk scan of the image returns a single low severity vulnerability, which is impressive. Running a Trivy scan returned a single medium vulnerability. However, keep in mind that Alpine is entirely community driven, and it's difficult to evaluate if the fact it has less CVEs in general is because it *is* more secure, or simply that it's less scrutinized/used. \r\n\r\n> **Warning**\r\n> I haven't been able to find any conclusive evidence that Alpine as a distribution is more secure than Debian. If anyone is aware of anything, I'd be very interested to learn more about it.\r\n\r\nOne major drawback of switching to Alpine is our use of native libraries. When using native libraries, the differences between `glibc` and `musl` may be subtle, and it can be difficult to pinpoint errors. If the libraries are not explicitly testing against `musl`, this could be a major downside. Speaking of the differences, [here is a nice comparison table of the main differences](https://www.etalabs.net/compare_libcs.html).\r\n\r\nThe image is built directly on top of the latest [alpine:3.18](https://hub.docker.com/_/alpine) - see the [official Dockerfile](https://github.com/adoptium/containers/blob/main/17/jre/alpine/Dockerfile.releases.full) here. It contains everything from that image, plus the `fontconfig`, `libretls`, `musl-locales`, `musl-locales-lang`, `ttf-dejavu`, `tzdata`, and `zlib` packages.\r\n\r\n# Chainguard/Wolfi\r\n\r\nImage: [cgr.dev/chainguard/jre:latest](https://github.com/chainguard-images/images/tree/main/images/jre)\r\n\r\n## TL;DR\r\n\r\n| CVEs | Release | Arch | Governance | Longevity | DevEx | Size |\r\n|------|---------|------|------------|-----------|-------|------|\r\n| :question: | :heavy_check_mark: | :heavy_check_mark: | :warning: | :warning: | :warning: | 273 MB |\r\n\r\n## Evaluation\r\n\r\n[Chainguard](https://www.chainguard.dev) is a fairly new startup (founded in 2021) which focuses on software supply chain solutions. They provide their own variant of a [distroless image, which they dub \"Undistro\"](https://www.chainguard.dev/unchained/introducing-wolfi-the-first-linux-un-distro), called [Wolfi](https://github.com/wolfi-dev/os).\r\n\r\nOn the plus side, Chainguard images are rebuilt daily from source, meaning they keep up with CVEs fairly well. While they use the APK format, they aren't an Alpine variant, but really a different Linux distro (or as they would say, undistro, as they don't package a kernel along with their OS, relying on the host's kernel). As they use two in-house tools to build their images - [apko](https://github.com/chainguard-dev/apko) and [melange](https://github.com/chainguard-dev/melange) - their images result in a single fat layer. This makes extending the JRE image, for example, a bit complicated, unless you're willing to pick up knowledge of both tools. Extending the base Wolfi image would certainly be easier.\r\n\r\nSimilar to the distroless images, the Chainguard images tend towards minimal, and typically won't include shells, or sshd, ftpd, etc. Additionally, they don't include a package manager, which removes one additional attack vector (e.g. an attacker gaining access to your image, downloading curl, then downloading their own malicious application). However, the downside is it's harder to debug such images. To circumvent this, much like the distroless images, [Chainguard provides `dev` variants which include many utilities](https://edu.chainguard.dev/chainguard/chainguard-images/debugging-distroless-images/).\r\n\r\nRunning a Snyk scan on the image results in no vulnerabilities found, but Snyk does emit a warning:  \"Note that we currently do not have vulnerability information for Wolfi 20230201, which we detected in your image.\"\r\n\r\nRunning a Trivy scan returns no vulnerabilities, with Wolfi being correctly detected without any warnings this time.\r\n\r\n# Liberica\r\n\r\nImage: [bellsoft/liberica-runtime-container](https://hub.docker.com/r/bellsoft/liberica-runtime-container)\r\n\r\n## TL;DR\r\n\r\n| CVEs | Release | Arch | Governance | Longevity | DevEx | Size |\r\n|------|---------|------|------------|-----------|-------|------|\r\n| :question: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :warning: | :heavy_check_mark: | 124 MB |\r\n\r\n[Liberica JDK](https://bell-sw.com/libericajdk/) is an OpenJDK distribution built by [BellSoft](https://bell-sw.com/), who are notable OpenJDK, having pushed hard for the Alpine port, for example. \r\n\r\n> **Note**\r\n> I haven't had much experience with Liberica JDK. It's supposedly a cloud optimized flavor of OpenJDK, with a focus on a smaller size via module compression. It should otherwise be essentially an OpenJDK clone, functionality wise. \r\n\r\nThe Liberica Runtime Container (which is a JRE image) is made of two layers. The base, BellSoft's Alpine variant called [Alpaquita Linux](https://bell-sw.com/alpaquita-linux/):\r\n\r\n> Alpaquita is distinguished by multiple performance and security optimizations and LTS releases optimal for enterprise use. Alpaquita comes with three additional malloc for various workloads and best Java support. In addition, we enhanced stock musl libc. Our musl perf is similar or superior to glibc performance. We also provide two Alpaquita variants based on musl perf or glibc.\r\n\r\nFull disclosure, I have no idea whether or not these claims are true, and this would definitely require some testing. \r\n\r\nRunning a Snyk scan reveals no vulnerabilities, however it prints a similar warning as with Wolfi, that it doesn't recognize the Linux distribution. A Trivy scan also returned no vulnerabilities.\r\n\r\nIn terms of DevEx, the Liberica image comes with BusyBox, the `apk` package manager, and all that entails.\r\n\r\n# Zulu Distroless\r\n\r\nImage: [azul/zulu-openjdk:17-distroless](https://hub.docker.com/r/azul/zulu-openjdk)\r\n\r\n## TL;DR\r\n\r\n| CVEs | Release | Arch | Governance | Longevity | DevEx | Size |\r\n|------|---------|------|------------|-----------|-------|------|\r\n| :heavy_check_mark: | :heavy_check_mark: | :x: | :heavy_check_mark: | :warning: | :warning: | 192 MB |\r\n\r\n## Evaluation\r\n\r\nAzul is a fairly well known player in the Java sphere, having provided their Zulu JDK for quite a long time now. Their JDK differs substantially from the OpenJDK (e.g. they have their own GC offering), however I included it here because they recently started offering a true distroless variant, containing literally only the binaries necessary to run Java programs. Here are the contents of the image:\r\n\r\n```\r\n─────── ┃ ● Current Layer Contents ┣━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\r\nLayer   Permission     UID:GID       Size  Filetree\r\n   0    drwxr-xr-x         0:0     3.5 MB  ├── lib\r\n   1    drwxr-xr-x         0:0     3.5 MB  │   └── x86_64-linux-gnu\r\n   2    -rw-r--r--         0:0     2.2 MB  │       ├── libc.so.6\r\n   3    -rw-r--r--         0:0      14 kB  │       ├── libdl.so.2\r\n        -rw-r--r--         0:0     941 kB  │       ├── libm.so.6\r\n        -rw-r--r--         0:0      14 kB  │       ├── libnss_dns.so.2\r\n        -rw-r--r--         0:0      14 kB  │       ├── libnss_files.so.2\r\n        -rw-r--r--         0:0      21 kB  │       ├── libpthread.so.0\r\n        -rw-r--r--         0:0      69 kB  │       ├── libresolv.so.2\r\n        -rw-r--r--         0:0      15 kB  │       ├── librt.so.1\r\n        -rw-r--r--         0:0     109 kB  │       ├── libz.so.1\r\n        -rw-r--r--         0:0     109 kB  │       └── libz.so.1.2.11\r\n        drwxr-xr-x         0:0     241 kB  ├── lib64\r\n        -rwxr-xr-x         0:0     241 kB  │   └── ld-linux-x86-64.so.2\r\n        drwxr-xr-x         0:0        0 B  ├── tmp\r\n        drwxr-xr-x         0:0     190 MB  └── usr\r\n        drwxr-xr-x         0:0     190 MB      └── lib\r\n        drwxr-xr-x         0:0     190 MB          └── jvm\r\n        drwxr-xr-x         0:0     190 MB              └─⊕ zulu17\r\n```\r\n\r\nUnsurprisingly, running scans with both Snyk and Trivy here returned no vulnerabilities. This image contains no package manager, no extraneous utilities, it is the most barebones JVM image I can think of. This means a steep learning curve when it comes to debugging, but the same techniques that apply to all distroless images apply here as well.\r\n\r\nIt seems this is distributed only for AMD64 as well.\r\n\r\n# Base Distro\r\n\r\nOne final alternative is selecting our own preferred base distro as an image - whether Ubuntu, Alpine, Alpaquita, or what have you - and deploying a pre-built OpenJDK (e.g. Eclipse Temurin) on it. As evidenced by the Dockerfiles of the Eclipse Temurin project, this is not particularly difficult. The one caveat is hooking our dependency updates to flag OpenJDK updates in order to rebuild our image.\n npepinpe: # Shell-less/utility-less images\r\n\r\nMany small images still container a basic set of utilities, typically something like BusyBox. The distroless variants, however, don't, and this is often advertised as an advantage from a security point of view.\r\n\r\nSome of these utilities rely on things like `openssl`, `libcrypto`, `glibc`, etc., which may have open CVEs (`openssl` being a usual culprit). So removing them and having only your application may indeed reduce the CVE incidence rate.\r\n\r\nAdditionally, some utilities may be vector of attacks in themselves. As a simple example, take [nc](https://en.wikipedia.org/wiki/Netcat), which is packaged with BusyBox. It allows you to open TCP/UDP tunnels, scan ports, send data, and many other things. Now take assume an attacker gains access to a running container with `nc`. They can possibly download some malicious payload via `nc`, and execute it. If there was no shell, no utility, etc., they would be hard pressed to do much.\r\n\r\nOf course, with our Java based image, the point is kind of lost; attackers have access to a complete JVM, which gives them an incredible sandbox to do whatever they want if they have access to the container. But for SCRATCH images which only contain a single binary, then there is some benefit to dropping all the extra stuff.\n npepinpe: ## Zeebe Examples\r\n\r\nHere's a set of sample Zeebe Docker images based on the above base images.\r\n\r\n### Zeebe Alpine\r\n\r\nI wrote two Dockerfiles which results in a Zeebe image based on Alpine. One is essentially based on Alpine and installs OpenJDK via `apk`. The other is the same, but installs the Eclipse Temurin pre-built binary (so much like the `eclipse-temurin` image).\r\n\r\nThere reason I did this and not use the `eclipse-temurin` Alpine image directly is because it's not based on the latest Alpine, and is based on one version which has several DNS issues.\r\n\r\nPre-built image: gcr.io/zeebe-io/zeebe:alpine\r\n<details><summary>Zeebe Alpine OpenJDK</summary>\r\n\r\n```Dockerfile\r\n# syntax=docker/dockerfile:1.4\r\n# This Dockerfile requires BuildKit to be enabled, by setting the environment variable\r\n# DOCKER_BUILDKIT=1\r\n# see https://docs.docker.com/build/buildkit/#getting-started\r\nARG BASE_IMAGE=\"alpine:3.18.0\"\r\nARG BASE_DIGEST_AMD64=\"sha256:c0669ef34cdc14332c0f1ab0c2c01acb91d96014b172f1a76f3a39e63d1f0bda\"\r\nARG BASE_DIGEST_ARM64=\"sha256:30e6d35703c578ee703230b9dc87ada2ba958c1928615ac8a674fcbbcbb0f281\"\r\n\r\n# set to \"build\" to build zeebe from scratch instead of using a distball\r\nARG DIST=\"distball\"\r\n\r\n### Extract zeebe from distball ###\r\nFROM ${BASE_IMAGE} as distball\r\nWORKDIR /zeebe\r\nARG DISTBALL=\"dist/target/camunda-zeebe-*.tar.gz\"\r\nCOPY --link ${DISTBALL} zeebe.tar.gz\r\nRUN mkdir camunda-zeebe && tar xfvz zeebe.tar.gz --strip 1 -C camunda-zeebe\r\n\r\n### Image containing the zeebe distribution ###\r\n# hadolint ignore=DL3006\r\nFROM ${DIST} as dist\r\n\r\n### AMD64 base image ###\r\n# BASE_DIGEST_AMD64 is defined at the top of the Dockerfile\r\n# hadolint ignore=DL3006\r\nFROM ${BASE_IMAGE}@${BASE_DIGEST_AMD64} as base-amd64\r\nARG BASE_DIGEST_AMD64\r\nARG BASE_DIGEST=\"${BASE_DIGEST_AMD64}\"\r\n\r\n### ARM64 base image ##\r\n# BASE_DIGEST_ARM64 is defined at the top of the Dockerfile\r\n# hadolint ignore=DL3006\r\nFROM ${BASE_IMAGE}@${BASE_DIGEST_ARM64} as base-arm64\r\nARG BASE_DIGEST_ARM64\r\nARG BASE_DIGEST=\"${BASE_DIGEST_ARM64}\"\r\n\r\n### Application Image ###\r\n# TARGETARCH is provided by buildkit\r\n# https://docs.docker.com/engine/reference/builder/#automatic-platform-args-in-the-global-scope\r\n# hadolint ignore=DL3006\r\nFROM base-${TARGETARCH} as app\r\n# leave unset to use the default value at the top of the file\r\nARG BASE_IMAGE\r\nARG BASE_DIGEST\r\nARG VERSION=\"\"\r\nARG DATE=\"\"\r\nARG REVISION=\"\"\r\n\r\n# OCI labels: https://github.com/opencontainers/image-spec/blob/main/annotations.md\r\nLABEL org.opencontainers.image.base.digest=\"${BASE_DIGEST}\"\r\nLABEL org.opencontainers.image.base.name=\"docker.io/library/${BASE_IMAGE}\"\r\nLABEL org.opencontainers.image.created=\"${DATE}\"\r\nLABEL org.opencontainers.image.authors=\"zeebe@camunda.com\"\r\nLABEL org.opencontainers.image.url=\"https://zeebe.io\"\r\nLABEL org.opencontainers.image.documentation=\"https://docs.camunda.io/docs/self-managed/zeebe-deployment/\"\r\nLABEL org.opencontainers.image.source=\"https://github.com/camunda/zeebe\"\r\nLABEL org.opencontainers.image.version=\"${VERSION}\"\r\n# According to https://github.com/opencontainers/image-spec/blob/main/annotations.md#pre-defined-annotation-keys\r\n# and given we set the base.name and base.digest, we reference the manifest of the base image here\r\nLABEL org.opencontainers.image.ref.name=\"${BASE_IMAGE}\"\r\nLABEL org.opencontainers.image.revision=\"${REVISION}\"\r\nLABEL org.opencontainers.image.vendor=\"Camunda Services GmbH\"\r\nLABEL org.opencontainers.image.licenses=\"(Apache-2.0 AND LicenseRef-Zeebe-Community-1.1)\"\r\nLABEL org.opencontainers.image.title=\"Zeebe\"\r\nLABEL org.opencontainers.image.description=\"Workflow engine for microservice orchestration\"\r\n\r\n# OpenShift labels: https://docs.openshift.com/container-platform/4.10/openshift_images/create-images.html#defining-image-metadata\r\nLABEL io.openshift.tags=\"bpmn,orchestration,workflow\"\r\nLABEL io.k8s.description=\"Workflow engine for microservice orchestration\"\r\nLABEL io.openshift.non-scalable=\"false\"\r\nLABEL io.openshift.min-memory=\"512Mi\"\r\nLABEL io.openshift.min-cpu=\"1\"\r\n\r\nENV ZB_HOME=/usr/local/zeebe \\\r\n    ZEEBE_BROKER_GATEWAY_NETWORK_HOST=0.0.0.0 \\\r\n    ZEEBE_STANDALONE_GATEWAY=false \\\r\n    ZEEBE_RESTORE=false\r\nENV PATH \"${ZB_HOME}/bin:${PATH}\"\r\n# Disable RocksDB runtime check for musl, which launches `ldd` as a shell process\r\nENV ROCKSDB_MUSL_LIBC=true\r\n\r\nWORKDIR ${ZB_HOME}\r\nEXPOSE 26500 26501 26502\r\nVOLUME /tmp\r\nVOLUME ${ZB_HOME}/data\r\nVOLUME ${ZB_HOME}/logs\r\n\r\nWORKDIR /zeebe\r\nRUN addgroup -g 1000 zeebe && \\\r\n    adduser -u 1000 zeebe --system --ingroup zeebe && \\\r\n    chmod g=u /etc/passwd && \\\r\n    # These directories are to be mounted by users, eagerly creating them and setting ownership\r\n    # helps to avoid potential permission issues due to default volume ownership.\r\n    mkdir ${ZB_HOME}/data && \\\r\n    mkdir ${ZB_HOME}/logs && \\\r\n    chown -R 1000:0 ${ZB_HOME} && \\\r\n    chmod -R 0775 ${ZB_HOME}\r\n\r\nRUN --mount=type=cache,target=/var/cache/apk,id=apk-musl,sharing=locked \\\r\n    ln -s /var/cache/apk /etc/apk/cache && \\\r\n    apk add tini libstdc++ libgcc musl-dev openjdk17-jre-headless\r\n\r\nCOPY --link --chown=1000:0 docker/utils/startup.sh /usr/local/bin/startup.sh\r\nCOPY --from=dist --chown=1000:0 /zeebe/camunda-zeebe ${ZB_HOME}\r\n\r\nENTRYPOINT [\"tini\", \"--\", \"/usr/local/bin/startup.sh\"]\r\n```\r\n\r\n</details>\r\n\r\nPre-built image: gcr.io/zeebe-io/zeebe:temurin-alpine-libressl\r\n\r\n<details><summary>Zeebe Alpine Eclipse Temurin</summary>\r\n\r\n```Dockerfile\r\n# syntax=docker/dockerfile:1.5-labs\r\n# This Dockerfile requires BuildKit to be enabled, by setting the environment variable\r\n# DOCKER_BUILDKIT=1\r\n# see https://docs.docker.com/build/buildkit/#getting-started\r\nARG BASE_IMAGE=\"alpine:3.18.0\"\r\nARG BASE_DIGEST_AMD64=\"sha256:c0669ef34cdc14332c0f1ab0c2c01acb91d96014b172f1a76f3a39e63d1f0bda\"\r\nARG BASE_DIGEST_ARM64=\"sha256:30e6d35703c578ee703230b9dc87ada2ba958c1928615ac8a674fcbbcbb0f281\"\r\n\r\n# set to \"build\" to build zeebe from scratch instead of using a distball\r\nARG DIST=\"distball\"\r\n\r\n### Extract zeebe from distball ###\r\nFROM ${BASE_IMAGE} as distball\r\nWORKDIR /zeebe\r\nARG DISTBALL=\"dist/target/camunda-zeebe-*.tar.gz\"\r\nCOPY --link ${DISTBALL} zeebe.tar.gz\r\nRUN mkdir camunda-zeebe && tar xfvz zeebe.tar.gz --strip 1 -C camunda-zeebe\r\n\r\n### Image containing the zeebe distribution ###\r\n# hadolint ignore=DL3006\r\nFROM ${DIST} as dist\r\n\r\n### AMD64 base image ###\r\n# BASE_DIGEST_AMD64 is defined at the top of the Dockerfile\r\n# hadolint ignore=DL3006\r\nFROM ${BASE_IMAGE}@${BASE_DIGEST_AMD64} as base-amd64\r\nARG BASE_DIGEST_AMD64\r\nARG BASE_DIGEST=\"${BASE_DIGEST_AMD64}\"\r\n\r\n### ARM64 base image ##\r\n# BASE_DIGEST_ARM64 is defined at the top of the Dockerfile\r\n# hadolint ignore=DL3006\r\nFROM ${BASE_IMAGE}@${BASE_DIGEST_ARM64} as base-arm64\r\nARG BASE_DIGEST_ARM64\r\nARG BASE_DIGEST=\"${BASE_DIGEST_ARM64}\"\r\n\r\n### Java Alpine base image\r\n# TARGETARCH is provided by buildkit\r\nFROM base-${TARGETARCH} as java\r\n\r\nARG JAVA_URL=\"https://github.com/adoptium/temurin17-binaries/releases/download/jdk-17.0.7%2B7/OpenJDK17U-jre_x64_alpine-linux_hotspot_17.0.7_7.tar.gz\"\r\nARG SUM=\"sha256:711f837bacf8222dee9e8cd7f39941a4a0acf869243f03e6038ca3ba189f66ca\"\r\n\r\n# Default to UTF-8 file.encoding\r\nENV LANG='en_US.UTF-8' LANGUAGE='en_US:en' LC_ALL='en_US.UTF-8'\r\n\r\n# Setup JAVA_HOME and binaries in the path\r\nENV JAVA_HOME /opt/java/openjdk\r\nENV JAVA_VERSION jdk-17.0.7+7\r\nENV PATH $JAVA_HOME/bin:$PATH\r\n\r\n# Install dependencies, but skip installing fonts and so on as we don't need any front-end\r\n# Additionally, replace OpenSSL with LibreSSL to avoid the vulnerabilities plaguing OpenSSL\r\nRUN --mount=type=cache,target=/var/cache/apk,id=apk-musl,sharing=locked \\\r\n    ln -s /var/cache/apk /etc/apk/cache && \\\r\n    apk del openssl openssl-dev && apk add libressl libressl-dev && \\\r\n    apk add musl-locales musl-locales-lang tzdata zlib\r\n\r\n# Download and install the Java distribution\r\nADD --checksum=${SUM} ${JAVA_URL} /tmp/openjdk.tar.gz\r\nRUN set -eux; \\\r\n\t  mkdir -p \"$JAVA_HOME\"; \\\r\n\t  tar --extract \\\r\n\t      --file /tmp/openjdk.tar.gz \\\r\n\t      --directory \"$JAVA_HOME\" \\\r\n\t      --strip-components 1 \\\r\n\t      --no-same-owner && \\\r\n    rm -f /tmp/openjdk.tar.gz ${JAVA_HOME}/lib/src.zip;\r\n\r\n### Application Image ###\r\n# https://docs.docker.com/engine/reference/builder/#automatic-platform-args-in-the-global-scope\r\n# hadolint ignore=DL3006\r\nFROM java as app\r\n\r\n# leave unset to use the default value at the top of the file\r\nARG BASE_IMAGE\r\nARG BASE_DIGEST\r\nARG VERSION=\"\"\r\nARG DATE=\"\"\r\nARG REVISION=\"\"\r\n\r\n# OCI labels: https://github.com/opencontainers/image-spec/blob/main/annotations.md\r\nLABEL org.opencontainers.image.base.digest=\"${BASE_DIGEST}\"\r\nLABEL org.opencontainers.image.base.name=\"docker.io/library/${BASE_IMAGE}\"\r\nLABEL org.opencontainers.image.created=\"${DATE}\"\r\nLABEL org.opencontainers.image.authors=\"zeebe@camunda.com\"\r\nLABEL org.opencontainers.image.url=\"https://zeebe.io\"\r\nLABEL org.opencontainers.image.documentation=\"https://docs.camunda.io/docs/self-managed/zeebe-deployment/\"\r\nLABEL org.opencontainers.image.source=\"https://github.com/camunda/zeebe\"\r\nLABEL org.opencontainers.image.version=\"${VERSION}\"\r\n# According to https://github.com/opencontainers/image-spec/blob/main/annotations.md#pre-defined-annotation-keys\r\n# and given we set the base.name and base.digest, we reference the manifest of the base image here\r\nLABEL org.opencontainers.image.ref.name=\"${BASE_IMAGE}\"\r\nLABEL org.opencontainers.image.revision=\"${REVISION}\"\r\nLABEL org.opencontainers.image.vendor=\"Camunda Services GmbH\"\r\nLABEL org.opencontainers.image.licenses=\"(Apache-2.0 AND LicenseRef-Zeebe-Community-1.1)\"\r\nLABEL org.opencontainers.image.title=\"Zeebe\"\r\nLABEL org.opencontainers.image.description=\"Workflow engine for microservice orchestration\"\r\n\r\n# OpenShift labels: https://docs.openshift.com/container-platform/4.10/openshift_images/create-images.html#defining-image-metadata\r\nLABEL io.openshift.tags=\"bpmn,orchestration,workflow\"\r\nLABEL io.k8s.description=\"Workflow engine for microservice orchestration\"\r\nLABEL io.openshift.non-scalable=\"false\"\r\nLABEL io.openshift.min-memory=\"512Mi\"\r\nLABEL io.openshift.min-cpu=\"1\"\r\n\r\nENV ZB_HOME=/usr/local/zeebe \\\r\n    ZEEBE_BROKER_GATEWAY_NETWORK_HOST=0.0.0.0 \\\r\n    ZEEBE_STANDALONE_GATEWAY=false \\\r\n    ZEEBE_RESTORE=false\r\nENV PATH \"${ZB_HOME}/bin:${PATH}\"\r\n# Disable RocksDB runtime check for musl, which launches `ldd` as a shell process\r\nENV ROCKSDB_MUSL_LIBC=true\r\n\r\nWORKDIR ${ZB_HOME}\r\nEXPOSE 26500 26501 26502\r\nVOLUME /tmp\r\nVOLUME ${ZB_HOME}/data\r\nVOLUME ${ZB_HOME}/logs\r\n\r\nWORKDIR /zeebe\r\nRUN addgroup -g 1000 zeebe && \\\r\n    adduser -u 1000 zeebe --system --ingroup zeebe && \\\r\n    chmod g=u /etc/passwd && \\\r\n    # These directories are to be mounted by users, eagerly creating them and setting ownership\r\n    # helps to avoid potential permission issues due to default volume ownership.\r\n    mkdir ${ZB_HOME}/data && \\\r\n    mkdir ${ZB_HOME}/logs && \\\r\n    chown -R 1000:0 ${ZB_HOME} && \\\r\n    chmod -R 0775 ${ZB_HOME}\r\n\r\n# Install tini and RocksDB dependencies\r\nRUN --mount=type=cache,target=/var/cache/apk,id=apk-musl,sharing=locked \\\r\n    apk add tini libstdc++ libgcc musl-dev\r\n\r\nCOPY --link --chown=1000:0 docker/utils/startup.sh /usr/local/bin/startup.sh\r\nCOPY --from=dist --chown=1000:0 /zeebe/camunda-zeebe ${ZB_HOME}\r\n\r\nENTRYPOINT [\"tini\", \"--\", \"/usr/local/bin/startup.sh\"]\r\n```\r\n\r\n</details>\r\n\r\n> **Note**\r\n> With the latter one, one of the build stage is just building an own Alpine/Temurin base image, which also replaces OpenSSL with LibreSSL (the OpenBSD variant), hopefully further reducing CVEs\r\n> The idea here is that we can better keep up with the latest Alpine, but would need Renovate to figure out that there are Java updates :+1: \r\n\r\n### Zeebe Liberica\r\n\r\nI also built two Liberica variants, one based on musl and one on glibc (remember the Liberica image is based on Alpaquita, which is an Alpine variant, supporting both glibc and musl).\r\n\r\nPre-built image: gcr.io/zeebe-io/zeebe:liberica-glibc\r\n\r\n<details><summary>Zeebe Liberica GLIBC</summary>\r\n\r\n```Dockerfile\r\n# syntax=docker/dockerfile:1.4\r\n# This Dockerfile requires BuildKit to be enabled, by setting the environment variable\r\n# DOCKER_BUILDKIT=1\r\n# see https://docs.docker.com/build/buildkit/#getting-started\r\nARG BASE_IMAGE=\"bellsoft/liberica-runtime-container:jre-17.0.7_7-slim-glibc\"\r\nARG BASE_DIGEST_AMD64=\"sha256:e31bf84e791405ceb7336647896724b38649557e7ee485568cd151f388bbaf6b\"\r\n\r\n# set to \"build\" to build zeebe from scratch instead of using a distball\r\nARG DIST=\"distball\"\r\n\r\n### Init image containing tini and the startup script ###\r\nFROM bellsoft/alpaquita-linux-base:stream-glibc as init\r\nWORKDIR /zeebe\r\nRUN --mount=type=cache,target=/var/cache/apk,sharing=locked \\\r\n    ln -s /var/cache/apk /etc/apk/cache && \\\r\n    apk add tini libstdc++ libgcc && cp /sbin/tini .\r\nCOPY --link --chown=1000:0 docker/utils/startup.sh .\r\n\r\n### Extract zeebe from distball ###\r\nFROM bellsoft/alpaquita-linux-base:stream-glibc as distball\r\nWORKDIR /zeebe\r\nARG DISTBALL=\"dist/target/camunda-zeebe-*.tar.gz\"\r\nCOPY --link ${DISTBALL} zeebe.tar.gz\r\nRUN mkdir camunda-zeebe && tar xfvz zeebe.tar.gz --strip 1 -C camunda-zeebe\r\n\r\n### Image containing the zeebe distribution ###\r\n# hadolint ignore=DL3006\r\nFROM ${DIST} as dist\r\n\r\n### AMD64 base image ###\r\n# BASE_DIGEST_AMD64 is defined at the top of the Dockerfile\r\n# hadolint ignore=DL3006\r\nFROM ${BASE_IMAGE}@${BASE_DIGEST_AMD64} as base-amd64\r\nARG BASE_DIGEST_AMD64\r\nARG BASE_DIGEST=\"${BASE_DIGEST_AMD64}\"\r\n\r\n### Application Image ###\r\n# TARGETARCH is provided by buildkit\r\n# https://docs.docker.com/engine/reference/builder/#automatic-platform-args-in-the-global-scope\r\n# hadolint ignore=DL3006\r\nFROM base-${TARGETARCH} as app\r\n# leave unset to use the default value at the top of the file\r\nARG BASE_IMAGE\r\nARG BASE_DIGEST\r\nARG VERSION=\"\"\r\nARG DATE=\"\"\r\nARG REVISION=\"\"\r\n\r\n# OCI labels: https://github.com/opencontainers/image-spec/blob/main/annotations.md\r\nLABEL org.opencontainers.image.base.digest=\"${BASE_DIGEST}\"\r\nLABEL org.opencontainers.image.base.name=\"docker.io/library/${BASE_IMAGE}\"\r\nLABEL org.opencontainers.image.created=\"${DATE}\"\r\nLABEL org.opencontainers.image.authors=\"zeebe@camunda.com\"\r\nLABEL org.opencontainers.image.url=\"https://zeebe.io\"\r\nLABEL org.opencontainers.image.documentation=\"https://docs.camunda.io/docs/self-managed/zeebe-deployment/\"\r\nLABEL org.opencontainers.image.source=\"https://github.com/camunda/zeebe\"\r\nLABEL org.opencontainers.image.version=\"${VERSION}\"\r\n# According to https://github.com/opencontainers/image-spec/blob/main/annotations.md#pre-defined-annotation-keys\r\n# and given we set the base.name and base.digest, we reference the manifest of the base image here\r\nLABEL org.opencontainers.image.ref.name=\"${BASE_IMAGE}\"\r\nLABEL org.opencontainers.image.revision=\"${REVISION}\"\r\nLABEL org.opencontainers.image.vendor=\"Camunda Services GmbH\"\r\nLABEL org.opencontainers.image.licenses=\"(Apache-2.0 AND LicenseRef-Zeebe-Community-1.1)\"\r\nLABEL org.opencontainers.image.title=\"Zeebe\"\r\nLABEL org.opencontainers.image.description=\"Workflow engine for microservice orchestration\"\r\n\r\n# OpenShift labels: https://docs.openshift.com/container-platform/4.10/openshift_images/create-images.html#defining-image-metadata\r\nLABEL io.openshift.tags=\"bpmn,orchestration,workflow\"\r\nLABEL io.k8s.description=\"Workflow engine for microservice orchestration\"\r\nLABEL io.openshift.non-scalable=\"false\"\r\nLABEL io.openshift.min-memory=\"512Mi\"\r\nLABEL io.openshift.min-cpu=\"1\"\r\n\r\nENV ZB_HOME=/usr/local/zeebe \\\r\n    ZEEBE_BROKER_GATEWAY_NETWORK_HOST=0.0.0.0 \\\r\n    ZEEBE_STANDALONE_GATEWAY=false \\\r\n    ZEEBE_RESTORE=false\r\nENV PATH \"${ZB_HOME}/bin:${PATH}\"\r\n# Disable RocksDB runtime check for musl, which launches `ldd` as a shell process\r\nENV ROCKSDB_MUSL_LIBC=false\r\n\r\nWORKDIR ${ZB_HOME}\r\nEXPOSE 26500 26501 26502\r\nVOLUME /tmp\r\nVOLUME ${ZB_HOME}/data\r\nVOLUME ${ZB_HOME}/logs\r\n\r\nRUN addgroup -g 1000 zeebe && \\\r\n    adduser -u 1000 zeebe --system --ingroup zeebe && \\\r\n    chmod g=u /etc/passwd && \\\r\n    # These directories are to be mounted by users, eagerly creating them and setting ownership\r\n    # helps to avoid potential permission issues due to default volume ownership.\r\n    mkdir ${ZB_HOME}/data && \\\r\n    mkdir ${ZB_HOME}/logs && \\\r\n    chown -R 1000:0 ${ZB_HOME} && \\\r\n    chmod -R 0775 ${ZB_HOME}\r\n\r\nCOPY --from=init /lib/libstdc++.so.6.0.30 /lib/libstdc++.so.6.0.30\r\nCOPY --from=init /lib/libstdc++.so.6 /lib/libstdc++.so.6\r\nCOPY --from=init /lib/libgcc_s.so.1 /lib/libgcc_s.so.1\r\nCOPY --from=init --chown=1000:0 /zeebe/tini ${ZB_HOME}/bin/\r\nCOPY --from=init --chown=1000:0 /zeebe/startup.sh /usr/local/bin/startup.sh\r\nCOPY --from=dist --chown=1000:0 /zeebe/camunda-zeebe ${ZB_HOME}\r\n\r\nENTRYPOINT [\"tini\", \"--\", \"/usr/local/bin/startup.sh\"]\r\n```\r\n\r\n</details>\r\n\r\nPre-built image: gcr.io/zeebe-io/zeebe:liberica-musl\r\n\r\n<details><summary>Zeebe Liberica MUSL</summary>\r\n\r\n```Dockerfile\r\n# syntax=docker/dockerfile:1.4\r\n# This Dockerfile requires BuildKit to be enabled, by setting the environment variable\r\n# DOCKER_BUILDKIT=1\r\n# see https://docs.docker.com/build/buildkit/#getting-started\r\nARG BASE_IMAGE=\"bellsoft/liberica-runtime-container:jre-17.0.7_7-slim\"\r\nARG BASE_DIGEST_AMD64=\"sha256:e31bf84e791405ceb7336647896724b38649557e7ee485568cd151f388bbaf6b\"\r\n\r\n# set to \"build\" to build zeebe from scratch instead of using a distball\r\nARG DIST=\"distball\"\r\n\r\n### Init image containing tini and the startup script ###\r\nFROM bellsoft/alpaquita-linux-base:stream as init\r\nWORKDIR /zeebe\r\nRUN --mount=type=cache,target=/var/cache/apk,id=apk-musl,sharing=locked \\\r\n    ln -s /var/cache/apk /etc/apk/cache && \\\r\n    apk add tini libstdc++ libgcc musl-dev\r\nCOPY --link --chown=1000:0 docker/utils/startup.sh .\r\n\r\n### Extract zeebe from distball ###\r\nFROM bellsoft/alpaquita-linux-base:stream as distball\r\nWORKDIR /zeebe\r\nARG DISTBALL=\"dist/target/camunda-zeebe-*.tar.gz\"\r\nCOPY --link ${DISTBALL} zeebe.tar.gz\r\nRUN mkdir camunda-zeebe && tar xfvz zeebe.tar.gz --strip 1 -C camunda-zeebe\r\n\r\n### Image containing the zeebe distribution ###\r\n# hadolint ignore=DL3006\r\nFROM ${DIST} as dist\r\n\r\n### AMD64 base image ###\r\n# BASE_DIGEST_AMD64 is defined at the top of the Dockerfile\r\n# hadolint ignore=DL3006\r\nFROM ${BASE_IMAGE}@${BASE_DIGEST_AMD64} as base-amd64\r\nARG BASE_DIGEST_AMD64\r\nARG BASE_DIGEST=\"${BASE_DIGEST_AMD64}\"\r\n\r\n### Application Image ###\r\n# TARGETARCH is provided by buildkit\r\n# https://docs.docker.com/engine/reference/builder/#automatic-platform-args-in-the-global-scope\r\n# hadolint ignore=DL3006\r\nFROM base-${TARGETARCH} as app\r\n# leave unset to use the default value at the top of the file\r\nARG BASE_IMAGE\r\nARG BASE_DIGEST\r\nARG VERSION=\"\"\r\nARG DATE=\"\"\r\nARG REVISION=\"\"\r\n\r\n# OCI labels: https://github.com/opencontainers/image-spec/blob/main/annotations.md\r\nLABEL org.opencontainers.image.base.digest=\"${BASE_DIGEST}\"\r\nLABEL org.opencontainers.image.base.name=\"docker.io/library/${BASE_IMAGE}\"\r\nLABEL org.opencontainers.image.created=\"${DATE}\"\r\nLABEL org.opencontainers.image.authors=\"zeebe@camunda.com\"\r\nLABEL org.opencontainers.image.url=\"https://zeebe.io\"\r\nLABEL org.opencontainers.image.documentation=\"https://docs.camunda.io/docs/self-managed/zeebe-deployment/\"\r\nLABEL org.opencontainers.image.source=\"https://github.com/camunda/zeebe\"\r\nLABEL org.opencontainers.image.version=\"${VERSION}\"\r\n# According to https://github.com/opencontainers/image-spec/blob/main/annotations.md#pre-defined-annotation-keys\r\n# and given we set the base.name and base.digest, we reference the manifest of the base image here\r\nLABEL org.opencontainers.image.ref.name=\"${BASE_IMAGE}\"\r\nLABEL org.opencontainers.image.revision=\"${REVISION}\"\r\nLABEL org.opencontainers.image.vendor=\"Camunda Services GmbH\"\r\nLABEL org.opencontainers.image.licenses=\"(Apache-2.0 AND LicenseRef-Zeebe-Community-1.1)\"\r\nLABEL org.opencontainers.image.title=\"Zeebe\"\r\nLABEL org.opencontainers.image.description=\"Workflow engine for microservice orchestration\"\r\n\r\n# OpenShift labels: https://docs.openshift.com/container-platform/4.10/openshift_images/create-images.html#defining-image-metadata\r\nLABEL io.openshift.tags=\"bpmn,orchestration,workflow\"\r\nLABEL io.k8s.description=\"Workflow engine for microservice orchestration\"\r\nLABEL io.openshift.non-scalable=\"false\"\r\nLABEL io.openshift.min-memory=\"512Mi\"\r\nLABEL io.openshift.min-cpu=\"1\"\r\n\r\nENV ZB_HOME=/usr/local/zeebe \\\r\n    ZEEBE_BROKER_GATEWAY_NETWORK_HOST=0.0.0.0 \\\r\n    ZEEBE_STANDALONE_GATEWAY=false \\\r\n    ZEEBE_RESTORE=false\r\nENV PATH \"${ZB_HOME}/bin:${PATH}\"\r\n# Disable RocksDB runtime check for musl, which launches `ldd` as a shell process\r\nENV ROCKSDB_MUSL_LIBC=true\r\n\r\nWORKDIR ${ZB_HOME}\r\nEXPOSE 26500 26501 26502\r\nVOLUME /tmp\r\nVOLUME ${ZB_HOME}/data\r\nVOLUME ${ZB_HOME}/logs\r\n\r\nRUN addgroup -g 1000 zeebe && \\\r\n    adduser -u 1000 zeebe --system --ingroup zeebe && \\\r\n    chmod g=u /etc/passwd && \\\r\n    # These directories are to be mounted by users, eagerly creating them and setting ownership\r\n    # helps to avoid potential permission issues due to default volume ownership.\r\n    mkdir ${ZB_HOME}/data && \\\r\n    mkdir ${ZB_HOME}/logs && \\\r\n    chown -R 1000:0 ${ZB_HOME} && \\\r\n    chmod -R 0775 ${ZB_HOME}\r\n\r\nCOPY --from=init /lib/libc.musl-x86_64.so.1 /lib/libc.musl-x86_64.so.1\r\nCOPY --from=init /lib/libstdc++.so.6.0.30 /lib/libstdc++.so.6.0.30\r\nCOPY --from=init /lib/libstdc++.so.6 /lib/libstdc++.so.6\r\nCOPY --from=init /lib/libgcc_s.so.1 /lib/libgcc_s.so.1\r\nCOPY --from=init --chown=1000:0 /sbin/tini ${ZB_HOME}/bin/\r\nCOPY --from=init --chown=1000:0 /zeebe/startup.sh /usr/local/bin/startup.sh\r\nCOPY --from=dist --chown=1000:0 /zeebe/camunda-zeebe ${ZB_HOME}\r\n\r\nENTRYPOINT [\"tini\", \"--\", \"/usr/local/bin/startup.sh\"]\r\n```\r\n\r\n</details>\n megglos: ZDP-Triage:\n- would be good to get out of the way sooner\n- ",
    "title": "Reduce container CVE maintenance effort"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12859",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "Zeebe 8.3 is [supposed to](https://confluence.camunda.com/display/HAN/Camunda+8+Supported+Environments) support Elasticsearch 8.7+\r\n\r\nThe curator is no longer compatible with ES8, so we've added the ability to configure an ILM policy:\r\n- https://github.com/camunda/zeebe/pull/12147\r\n- https://github.com/camunda/zeebe/issues/12132\r\n\r\nWhat is still left over is to update the Elasticsearch client to the latest 8.7 version.\r\n\r\nThis is not updated automatically, due to our [dependabot configuration](https://github.com/camunda/zeebe/blob/main/.github/dependabot.yml#L20C1-L23), so we'll need to update it manually:\r\n- https://github.com/camunda/zeebe/pull/9308\r\n- https://github.com/camunda/zeebe/pull/11563\r\n\r\nAlso see, the last dependabot PR for this: \r\n- https://github.com/camunda/zeebe/issues/9287\n\n korthout: ZPA Triage:\n- should be easy: \n  - update the dependency\n  - make sure the CI passes (`mvn test` or `mvn verify` locally)\n  - manually test to see that we can export into ES 8.7 with it\n- should be completed before 8.3 release",
    "title": "Update Elasticsearch client to 8.7  "
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12601",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "<!--\r\nIn case you have questions about our software we encourage everyone to participate in our community via the\r\n- Camunda Platform community forum https://forum.camunda.io/ or \r\n- Slack https://camunda-cloud.slack.com/ (For invite: https://camunda-slack-invite.herokuapp.com/)\r\n\r\nThere you can exchange ideas with other Zeebe and Camunda Platform 8 users, as well as the product developers, and use the search to find answer to similar questions.\r\n\r\nThis issue template is used by the Zeebe engineers to create general tasks.\r\n-->\r\n\r\n**Description**\r\n\r\nCame up in https://github.com/camunda-cloud/alerts/pull/91, raised by @multani \r\n\r\nRight now our UIDs are not human readibly see https://github.com/camunda/zeebe/blob/11c54b23ab2d567c6933cf7925327481562d0b32/monitor/grafana/zeebe.json#L14934 which means it is hard to reference them and understand what the request dashboard is about.\r\n\r\nBenefit references like this https://github.com/camunda-cloud/alerts/blob/e2245885e6010d08de2924d0da4750be9d33bb32/generated/camunda-cloud-240911/development-worker-1/monitoring.coreos.com_v1_prometheusrule_zeebe-crashlooping.yaml#L17 would be more clear\r\n\r\n\r\nChanging the UID would improve this, I think this would be a simple and straight forward change. Similar to https://github.com/camunda-cloud/grafana-dashboards/issues/21\r\n\n\n npepinpe: Let's do it sooner than later before more things reference the old UIDs and it becomes more of a pain to change them.\n\nPlease remember to update the references (e.g. Cloud Bot, alerts - check with SREs or do a GitHub search of the ID in the camunda and camunda-cloud orgs).\n koevskinikola: ZPA triage:\r\n@Zelldon is something expected from ZPA on this issue?\n Zelldon: @koevskinikola benchmarks and grafana dashboard is seen as shared responsibility so yes you could happily do this as well :)",
    "title": "Make dashboard UIDs human-readable"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12427",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "<!--\r\nIn case you have questions about our software we encourage everyone to participate in our community via the\r\n- Camunda Platform community forum https://forum.camunda.io/ or \r\n- Slack https://camunda-cloud.slack.com/ (For invite: https://camunda-slack-invite.herokuapp.com/)\r\n\r\nThere you can exchange ideas with other Zeebe and Camunda Platform 8 users, as well as the product developers, and use the search to find answer to similar questions.\r\n\r\nThis issue template is used by the Zeebe engineers to create general tasks.\r\n-->\r\n\r\n**Description**\r\n\r\nI recently came across this library: https://github.com/acm19/aws-request-signing-apache-interceptor\r\nThis provides a request interceptor to take care AWS request signing. Currently we are maintaining our own interceptor. It would be great to switch it for this maintained library so we can forget about it ourselves.\r\n\n\n korthout: ZPA triage:\n- we like the idea, this was suggested by someone from AWS\n- pretty low-hanging fruit (just switch out the used interceptor and remove our own code)\n- marking as `later` not needed immediately but could be done when we work on opensearch again",
    "title": "Switch custom AwsSignHttpRequestInterceptor in Opensearch exporter to library"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12390",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nIn order for the brokers to push jobs all the way back to clients, whenever a client starts a new job stream RPC, a `ClientStreamConsumer` adapter must be registered with the given `ClientStreamer`. Since you now have to convert the `JobActivationProperties`, it's likely you will need to move this and its implementation into the `protocol-impl` module. \r\n\r\nAs part of the endpoint implementation, it should:\r\n\r\n- [ ] Convert the `ActivateJobsRequest` into serializable `JobActivationProperties`\r\n- [ ] Register the consumer along with its job type as stream type, the job properties as metadata, and the consumer adapter for this observer\r\n- [ ] Ensure that when the observer is cancelled or closed (using `ServerStreamObserver#setOnCloseHandler` and `ServerStreamObserver#setOnCancelledHandler`), that the consumer is removed using the UUID obtained on registration\r\n\r\nSince the `ClientStreamer` is an actor, it's likely you will also need to handle registration and pushes in an actor context, similarly to how we deal with the long polling job activation. Don't hesitate to ask if you have questions. I would advise against simply calling `ActorFuture#join` on registration to get the UUID, as you'd be blocking the gRPC thread.\r\n\r\nAdditionally, you may want to check on whether it's necessary to register the consumer only when the observer is ready. When a client is slow, it may not be ready to receive messages for a while, so it's pointless to register it until it's ready. That said, evaluate if it's worth the trade off in terms of complexity/UX.\r\n\n\n npepinpe: FYI @berkaycanbc, I re-read the gRPC docs, and we shouldn't use the `setOnReadyHandler` for stream registration:\r\n\r\n> Set a Runnable that will be executed every time the stream isReady() state changes from false to true. While it is not guaranteed that the same thread will always be used to execute the Runnable, it is guaranteed that executions are serialized with calls to the 'inbound' StreamObserver.\r\n> May only be called during the initial call to the application, before the service returns its StreamObserver.\r\n> Because there is a processing delay to deliver this notification, it is possible for concurrent writes to cause isReady() == false within this callback. Handle \"spurious\" notifications by checking isReady()'s current value instead of assuming it is now true. If isReady() == false the normal expectations apply, so there would be another onReadyHandler callback.\r\n\r\nSo essentially the stream may become ready multiple over its lifetime. One obvious case is what we expected, during the initial set up. Another one is when gRPC's backpressure kicks in - then the stream is not ready anymore, and becomes ready again only when the client wants to accept new messages.\r\n\r\nWe could use it to detect when the client is ready to accept new messages, however, but we will integrate this in our flow control solution in the next phase.\r\n\r\nSee [this](https://github.com/grpc/grpc-java/tree/master/examples/src/main/java/io/grpc/examples/manualflowcontrol) for more on gRPC's built in flow control.",
    "title": "Register ClientStreamConsumer adapters for every gRPC stream observer"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12389",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nThe ZDP provided `ClientStreamer` allows registering `ClientStreamConsumer` instances. These consumers will receive serialized `ActivatedJob` instances. These serialized payloads are owned by the ZPA team, and ideally should reuse the same type, so will likely have to be put in something like the `protocol-impl` module, along with the interface that currently lives in the `workflow-engine` module.\r\n\r\n- [ ] Deserialize & validate the payload\r\n- [ ] Convert it to the appropriate gRPC type\r\n- [ ] Forward it to the gRPC stream observer\r\n\r\nIf an error occurs at any point, the consumer should throw an appropriate exception. The caller of the `push` method will take care of retrying or yielding the job back on error. **Note that if the error occurs when calling the `StreamObserver#onNext` method, the consumer implementation must call `StreamObserver#onError` as per the gRPC API**.\r\n\r\n\r\n\n\n deepthidevaki: Nicolas and me had some questions, and here is what we found out. It might be relevant when you implement the consumer.\r\n\r\n- If we sent 10 jobs back to back on a Grpc stream, but the 5th job failed to sent, what happens to the rest of it?\r\n  What I learned is that Grpc stream guarantees ordered delivery. That would mean, if the 5th job failed jobs 6-10 is not sent.\r\n [ref1](https://grpc.io/docs/what-is-grpc/core-concepts/) [ref2](https://groups.google.com/g/grpc-io/c/f37hmkJrWHY)\r\n  > Server streaming RPCs where the client sends a request to the server and gets a stream to read a sequence of messages back. The client reads from the returned stream until there are no more messages. gRPC guarantees message ordering within an individual RPC call.\r\n\r\n\r\n\r\n- Can we know when the 5th job is failed?\r\n   So far it is not guaranteed. Implementation of `onNext`can be non-blocking. So it might fail later and we will know about it when we call `onNext` next time, which might be when we sent the 7th job. So there are chances that we assume the job is sent successfully, but it was not. I couldn't find an api where we can wait on each message sent via `onNext`. ",
    "title": "Implement ClientStreamConsumer adapter to forward jobs to gRPC clients"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11799",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "<!--\r\nIn case you have questions about our software we encourage everyone to participate in our community via the\r\n- Camunda Platform community forum https://forum.camunda.io/ or \r\n- Slack https://camunda-cloud.slack.com/ (For invite: https://camunda-slack-invite.herokuapp.com/)\r\n\r\nThere you can exchange ideas with other Zeebe and Camunda Platform 8 users, as well as the product developers, and use the search to find answer to similar questions.\r\n\r\nThis issue template is used by the Zeebe engineers to create general tasks.\r\n-->\r\n\r\n**Description**\r\n\r\nIt looks like we using the Zeebe client in the gateway, which is somehow weird. It add an unnecessary dependency to the client, and makes unnecessary requests. \r\n\r\nThis was recently discovered here https://github.com/camunda/zeebe/pull/11599#discussion_r1109846523 \r\n\r\nFurthermore, the impact was discovered also in one of the chaos days https://zeebe-io.github.io/zeebe-chaos/2023/02/23/Recursive-call-activity/\r\n\r\n\r\n![general](https://user-images.githubusercontent.com/2758593/220933420-06a44b51-a549-4b71-8611-6aa9d1b2d691.png)\r\n\r\n> One interesting fact is that the topology request rate is also up to 0.400 per second, so potentially every 2.5 seconds we send a topology request to the gateway. But there is no application deployed that does this. [I have recently found out again](https://github.com/camunda/zeebe/pull/11599#discussion_r1109846523), that we have the Zeebe client usage in the gateway to request the topology. Might be worth investigating whether this is an issue.\r\n\r\n\r\nRight now I'm not sure how problematic it is, whether it is just toil or whether this might be an actual bigger issue.\r\n\r\nrelates to https://jira.camunda.com/browse/SUPPORT-16945\r\n\n\n korthout: We're unsure what this health check's expected behavior should be. We can discuss this with SRE, as they are the users of this API in SaaS. Marking this as `later` for now.\n\n//cc @aleksander-dytko \n npepinpe: Support related now - see https://jira.camunda.com/browse/SUPPORT-16945?focusedCommentId=285709&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel\r\n\r\nHaving to configure authentication, security, etc., just overly complicates things. Then factor in things like hostname verification checks and so on - let's just get rid of it and hook in the topology manager directly.\n npepinpe: imo this would then fall to the ZDP team to do\n megglos: this actually overlaps with https://github.com/camunda/zeebe/issues/13431 , let's not work on it until that one is clarified",
    "title": "Investigate usage of client in gateway"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11710",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nOnce #11707, #11708, #11709, and #11713, we should integrate them together and test the complete job stream lifecycle.\r\n\r\n- [x] A client can register a new job stream worker all the way to the broker\r\n- [x] Two stream workers with equivalent properties, on the same gateway, become one on the broker\r\n- [x] Two streams with different properties, on the same gateway, are two different workers on the broker\r\n- [x] When a client call is closed (gracefully or not), and there are no other logically equivalent clients on the gateway, the gateway will remove the stream worker from the broker\r\n- [x] When a client call is closed (gracefully or not), and there is at least one other logically equivalent client on the gateway, the gateway will not remove the stream worker from the broker\r\n- [x] When a gateway shuts down gracefully, it should remove all associated stream workers from the broker.\r\n\r\nI opted against testing with multiple gateways at the QA level. One thing is that our test infrastructure doesn't support multiple gateways (yet). Another is that we already test with multiple members at the transport level (but using integration test), and from a client point of view, it doesn't really matter.\r\n\n",
    "title": "Integrate end-to-end job stream lifecycle management"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11590",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "Examples:\r\n- https://github.com/camunda/zeebe/actions/runs/4134636764/jobs/7146014786\r\n- https://github.com/camunda/zeebe/actions/runs/4130548513/jobs/7137350957\r\n\r\nCould be related to MTU or GCP network interference. See https://github.com/zeebe-io/infra/blob/main/gcp/zeebe-io/zeebe-ci-1/runner-arm64-4.yml for the configuration we use for these runners\n\n megglos: Even after runner downgrade (https://github.com/zeebe-io/infra/pull/32 ) we experienced the network issues \r\nhttps://github.com/camunda/zeebe/actions/runs/4173174037/jobs/7228803794\r\n\r\n```\r\n k describe pod actions-runner-arm64-4-fjmsf-rl828                                                                                              \r\n    Image:          summerwind/actions-runner-dind@sha256:c2fff4d42cd3cd456038dbb3372846a588dca5d26da18ac8718cb2195b767d80\r\n```\r\n\r\nAlso checked the network interfaces mtu, actually looked fine:\r\n```\r\nrunner@actions-runner-arm64-4-fjmsf-rl828:/$ ifconfig\r\ndocker0: flags=4099<UP,BROADCAST,MULTICAST>  mtu 1460\r\n...\r\n\r\neth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1460\r\n...\r\n```",
    "title": "ARM64 CI Jobs frequently fail due to connection resets"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/13376",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5035",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4965",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4940",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4993",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4893",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4877",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4868",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4869",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4702",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4826",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4849",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5020",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5028",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5033",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5000",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5008",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4944",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4954",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4947",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4934",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4941",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4907",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4912",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4879",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5063",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5030",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5037",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5048",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5047",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5039",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5013",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4992",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5012",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4975",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4943",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4901",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4974",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4988",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4982",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5004",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4999",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4939",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/5001",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4976",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4996",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4962",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4959",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4957",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4911",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4970",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4956",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4956",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4955",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4958",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4946",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4814",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4816",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4833",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4952",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4950",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4935",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4917",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4918",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4885",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4865",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4924",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4904",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4905",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4899",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4902",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4921",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4923",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4830",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4881",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4897",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4890",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4883",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4878",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4876",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4838",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4837",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4781",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4829",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3266",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3256",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3233",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3219",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3184",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3168",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3169",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3251",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3235",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3232",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3218",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3192",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3216",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3187",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3188",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3177",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3262",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3242",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3243",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3228",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3223",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3222",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3207",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3212",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3217",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3191",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3196",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3197",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3198",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3194",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2816",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3183",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3171",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3181",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3185",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3178",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3170",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3165",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1962",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1961",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1963",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1878",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1837",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1830",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1823",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1827",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1818",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1816",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1791",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1789",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1749",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1953",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1941",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1940",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1933",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1932",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1931",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1921",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1912",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1899",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1907",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1890",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1881",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1889",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1886",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1879",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1876",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1874",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1814",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1825",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1824",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1804",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1780",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1775",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1768",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1758",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1757",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1756",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1737",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1964",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1958",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1951",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1954",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1952",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1947",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1949",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1948",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1946",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1945",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1944",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1942",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1943",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1934",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1939",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1937",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1935",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1925",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1924",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1908",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1923",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1919",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1922",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1918",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1916",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1917",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1914",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1913",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1904",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1911",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1909",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1910",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1896",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1906",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1897",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1902",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1877",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1895",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1893",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1894",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1891",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1892",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1849",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1883",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1888",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1882",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1875",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1865",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1863",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1864",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1861",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1862",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1856",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1854",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1852",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1834",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1850",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1839",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1829",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1838",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1833",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1836",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1832",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1826",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1828",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1820",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1822",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1808",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1806",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1807",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1805",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1798",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1795",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1800",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1533",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1796",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1794",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1781",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1782",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1792",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1777",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1778",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1776",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1759",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1772",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1769",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1764",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1765",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1761",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1747",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1755",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1754",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1753",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1752",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1735",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha4",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1736",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12655",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\nAfter a release we must send a list of fixed issues related to support tickets. For this we look at the `support` label on issues. It's easy to forget to add this label.\n\n**Describe the solution you'd like**\nIntroduce a GitHub action that checks new issues and comments in issues if the text contains `SUPPORT-XXXX`. If it find any the action should add the `support` label.\n\n**Describe alternatives you've considered**\nN/A\n\n**Additional context**\nN/A\n\n\n remcowesterhoud: @abbasadel fyi 🙂 \n megglos: ZDP-Triage:\n- would be great to be done for all C8 teams actually\n- @megglos will take this over as part of the support/engineering collaboration\n",
    "title": "Automatically add `support` label to support related issues"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13254",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nWhen the command to deploy processes failed with, for example, `EXCEEDED_BATCH_RECORD_SIZE`, any processes - that have been cached during the deployment process - are still kept in cache afterward. That way, clients can start process instances of these processes. Once a leader change happened (or the broker restarted), the Stream Process fails to replay these started process instances:\r\n\r\n```\r\njava.lang.RuntimeException: java.lang.IllegalStateException: Expected to find a process deployed with key '2251799813685405' but not found.\r\n\tat io.camunda.zeebe.stream.impl.ReplayStateMachine.lambda$replayNextEvent$4(ReplayStateMachine.java:169) ~[zeebe-stream-platform-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.future.FutureContinuationRunnable.run(FutureContinuationRunnable.java:33) ~[zeebe-scheduler-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94) ~[zeebe-scheduler-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) [zeebe-scheduler-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) [zeebe-scheduler-8.2.4.jar:8.2.4]\r\nCaused by: java.lang.IllegalStateException: Expected to find a process deployed with key '2251799813685405' but not found.\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.getFlowElement(DbProcessState.java:288) ~[zeebe-workflow-engine-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.engine.state.appliers.ProcessInstanceElementActivatingApplier.createEventScope(ProcessInstanceElementActivatingApplier.java:261) ~[zeebe-workflow-engine-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.engine.state.appliers.ProcessInstanceElementActivatingApplier.applyState(ProcessInstanceElementActivatingApplier.java:51) ~[zeebe-workflow-engine-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.engine.state.appliers.ProcessInstanceElementActivatingApplier.applyState(ProcessInstanceElementActivatingApplier.java:32) ~[zeebe-workflow-engine-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.engine.state.appliers.EventAppliers.applyState(EventAppliers.java:302) ~[zeebe-workflow-engine-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.engine.Engine.replay(Engine.java:110) ~[zeebe-workflow-engine-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.stream.impl.ReplayStateMachine.lambda$replayEvent$7(ReplayStateMachine.java:230) ~[zeebe-stream-platform-8.2.4.jar:8.2.4]\r\n\tat java.util.Optional.ifPresent(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.stream.impl.ReplayStateMachine.replayEvent(ReplayStateMachine.java:230) ~[zeebe-stream-platform-8.2.4.jar:8.2.4]\r\n\tat java.util.Iterator.forEachRemaining(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.stream.impl.ReplayStateMachine.lambda$tryToReplayBatch$5(ReplayStateMachine.java:208) ~[zeebe-stream-platform-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run(ZeebeTransaction.java:84) ~[zeebe-db-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.stream.impl.ReplayStateMachine.tryToReplayBatch(ReplayStateMachine.java:206) ~[zeebe-stream-platform-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.stream.impl.ReplayStateMachine.lambda$replayNextEvent$3(ReplayStateMachine.java:165) ~[zeebe-stream-platform-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.retry.ActorRetryMechanism.run(ActorRetryMechanism.java:28) ~[zeebe-scheduler-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.retry.RecoverableRetryStrategy.run(RecoverableRetryStrategy.java:48) ~[zeebe-scheduler-8.2.4.jar:8.2.4]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) ~[zeebe-scheduler-8.2.4.jar:8.2.4]\r\n\t... 5 more\r\n```\r\n\r\nBasically, when processing a deployment command the following happens\r\n\r\n1. `DeploymentCreateProcessor#processRecord()` transforms the given resources accordingly\r\nhttps://github.com/camunda/zeebe/blob/e4c88b2a20f48e764118caefa5052199e12ad15e/engine/src/main/java/io/camunda/zeebe/engine/processing/deployment/DeploymentCreateProcessor.java#L129-L135\r\n2. If succeeded, the engine will create timers for any timer start event\r\nhttps://github.com/camunda/zeebe/blob/e4c88b2a20f48e764118caefa5052199e12ad15e/engine/src/main/java/io/camunda/zeebe/engine/processing/deployment/DeploymentCreateProcessor.java#L137-L142\r\n3. When executing the `#createTimerIfTimerStartEvent()` it will try to get the process definition by key:\r\nhttps://github.com/camunda/zeebe/blob/e4c88b2a20f48e764118caefa5052199e12ad15e/engine/src/main/java/io/camunda/zeebe/engine/processing/deployment/DeploymentCreateProcessor.java#L193-L203\r\n4. Getting the process definition by key will eventually put the process definition into a map `processesByProcessIdAndVersion` for caching purposes:\r\nhttps://github.com/camunda/zeebe/blob/e4c88b2a20f48e764118caefa5052199e12ad15e/engine/src/main/java/io/camunda/zeebe/engine/state/deployment/DbProcessState.java#L201-L211\r\n5. Once `#createTimerIfTimerStartEvent()` finishes, the engine will write a follow-up event to indicate the deployment has been created\r\nhttps://github.com/camunda/zeebe/blob/e4c88b2a20f48e764118caefa5052199e12ad15e/engine/src/main/java/io/camunda/zeebe/engine/processing/deployment/DeploymentCreateProcessor.java#L146\r\n6. Writing this follow-up event might fail with `EXCEEDED_BATCH_RECORD_SIZE`\r\n\r\nAs a consequence, the RocksDB transaction gets rolled back, meaning, the process definitions are not stored in RocksDB. But the not successful deployed process definitions are still present in the map `processesByProcessIdAndVersion`. That way, when a client starts a process by `bpmnProcessId`, the engine will get the actually non-existing process definition from the map `processesByProcessIdAndVersion` and start a process instance eventually. And as long as the leader keeps the same and Zeebe does not restart, process instances of the non-existing process definition can get started.\r\n\r\nAdditionally, the `ProcessVersionManager#versionCache` caches the latest version of a process definition. The same applies here, when the deployment fails, the cached latest version has been already increased and stays present in the cache. Basically, since `processesByProcessIdAndVersion` contains a non-existing process definition (for example, with version `12`) and the `versionCache` points to the non-existing version `12` of that process definition, the engine will return the process definition from `processesByProcessIdAndVersion`\r\n\r\nhttps://github.com/camunda/zeebe/blob/e4c88b2a20f48e764118caefa5052199e12ad15e/engine/src/main/java/io/camunda/zeebe/engine/state/deployment/DbProcessState.java#L214-L231\r\n\r\nBefore `versionCache` have been introduced, the engine would get the latest version from RocksDB which would not be `12` for sure. That way, the engine ensured that it always returns a process definition that actually exists.\r\n\r\n**To Reproduce**\r\n1. Deploy many processes at once (containing a process with `bpmnProcessId=foo` )\r\n2. Ensure that the deployment fails with `EXCEEDED_BATCH_RECORD_SIZE`\r\n3. Start a process instance by `bpmnProcessId=foo`  => The process instance gets started accordingly.\r\n4. Stop the broker\r\n5. Start the broker\r\n\r\n**Expected behavior**\r\n* Whenever a deployment fails, the client cannot start any process instance of any of those process definitions.\r\n* To be more concrete, any process definitions that get cached when processing the deployment command must be removed when the deployment failed.\r\n* Or as an alternative, don't cache any process definitions during deployment.\r\n* The same applies to `ProcessVersionManager#versionCache`\r\n\r\n**What is the impact of the issue?**\r\n* Any follower cannot replay any events of process instances belonging to a non-existing process definition. That way, the stream processor will stop replaying anything. The follower won't create any new snapshots, and so on.\r\n* If there is a failover, the new leader will fail to replay the events when recovering from the last snapshot. It will again stop the stream processor. No snapshots are taken anymore. The back pressure goes up to 100%. The partition gets unhealthy.\r\n* It may result in process definitions that are available on partition >= 2 but not on partition 1. For example:\r\n  1. Deploy the first version of a process `foo` that succeeds\r\n  2. Deploy a new version of the process `foo` that fails with `EXCEEDED_BATCH_RECORD_SIZE` (try doing multiple times)\r\n  3. Then redeploy the first version once again (and let it succeed)\r\n  4. => It will result in a process definition with version `x` (`x > 1`) which gets distributed to other partitions but on partition 1 this process definition won't get persisted in RocksDB.\r\n  5. So on partition > 1, there are running process instances referencing process definition with version `x`. But these process instances cannot be opened in Operate, because the process definition was never exported and imported.\r\n* There is no way to recover from that. Only purging the clusters resolves the issue.\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.2.4\r\n\r\n---\r\n\r\nrelated to #5723 \r\nrelated to [SUPPORT-17483](https://jira.camunda.com/browse/SUPPORT-17483)\r\nrelated to #12873\n\n Zelldon: **Sidenote**: this might affect also other places where we have a transient state (cached data in the state) since we never roll back such cached data on errors. \n Zelldon: Small assessment from my side:\n\nRight now, it is hard to detect from the engine perspective when a transaction is rolled back. \n\nWe could better handle the exceeding batch size, of course, but even then, it might be already part of our cache.\n\nI haven't checked the code yet. It might make sense to avoid the cache here, but I'm not sure whether this is possible.\n\nAlternatively, we could split the caches and store it in a second cache during processing, and when we are at the end of the command processing, we can put everything in the actual one.\n\nSimilarly, we could do this on the stream platform side. We have a primary cache object and a secondary cache object which is a copy which we give as an argument to the engine. On rollback, the secondary cache is cleared on success we persist the cache in our primary cache. \n\nThis involves a lot of copying which might be suboptimal if you have cached a lot of data but would resolve our issue in a more general fashion which we can use also for other places.\n\nRight now, i have no other or better idea yet. Except remove all caches.\n\n korthout: So far, we know the following solutions may work:\r\n1. only cache the processes when deployment successful\r\n  - requires reworking the logic\r\n2. when an exception is thrown, clear cache\r\n  - may slow down regular processing slightly as cache needs to be rebuild\r\n  - this also happens on leader changes\r\n3. allowing the stream processor to invalidate new cache entries similar to state transaction rollbacks\r\n  - more involved, but likely the most elegant solution\r\n  - will allow us to add caches more easily in the future\r\n\r\nAs a quick solution to overcome this problem fast, I suggest using option 2 for the next patch release. \r\n\r\n@Zelldon @romansmirnov What do you think?\n romansmirnov: @korthout, option 2 sounds like a good approach to get it fixed quickly. I share your assessment.\n Zelldon: Agree :+1: \n korthout: I was thinking a bit about how this case could've happened. We have several guarantees on our persisted state (e.g. all state changes must be covered by an event on the log), including some relational consistency checks. We do not have a foreign key relation from process instance to the process definition it belongs to. \r\n\r\nIf we had such a check, the corrupted state would've been avoided. The affected partition would've been `DEAD` but recoverable after a patch. Such a check would avoid all errors (including the unknown ones) related to process instance creation where the process definition is actually not stored in state.\r\n\r\nKeeping track of this foreign key relation would come at the cost of performance (two additional IO operations on process instance creation: one to store the additional entry in a new column family, and one to check the foreign key relation when the instance is being created in the state). \r\n\r\nIn the past, we've made decisions to go for safety over speed, but here we'd have to consider that decision again.\n korthout: Closing as fixed by:\r\n- #13256 \r\n- #13327 \r\n- #13280 \r\n- #13328 \r\n\r\nHowever, we should follow up with:\r\n- https://github.com/camunda/zeebe/issues/13259\r\n- https://github.com/camunda/zeebe/issues/13254#issuecomment-1617550522",
    "title": "When the deployment of a process definition failed, clients can still start new process instances of the actually non-existing process definition"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13164",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nA segfault can occur when one of the experimental feature flags is enabled and the other disabled:\r\n- `ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLEMESSAGETTLCHECKERASYNC`\r\n- `ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLETIMERDUEDATECHECKERASYNC`\r\n\r\nThis same error does not occur when both are enabled, or both are disabled.\r\n\r\n**To Reproduce**\r\n\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\nEnable either of, and disable the other:\r\n- `ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLEMESSAGETTLCHECKERASYNC`\r\n- `ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLETIMERDUEDATECHECKERASYNC`\r\n\r\n**Expected behavior**\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\nNo segfault occurs.\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\nCurrent thread (0x00007f4dbd280db0):  JavaThread \"-zb-actors-7\" [_thread_in_native, id=15747, stack(0x00007f4d1222d000,0x00007f4d1232e000)]\r\n\r\nStack: [0x00007f4d1222d000,0x00007f4d1232e000],  sp=0x00007f4d1232c340,  free space=1020k\r\nNative frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)\r\nC  [librocksdbjni14514272448621065586.so+0x5e2a98]  rocksdb::Arena::~Arena()+0x188\r\nC  [librocksdbjni14514272448621065586.so+0x877aa6]  rocksdb::WriteBatchWithIndex::Rep::ClearIndex()+0x16\r\nC  [librocksdbjni14514272448621065586.so+0x851e59]  rocksdb::TransactionBaseImpl::Clear()+0x349\r\nC  [librocksdbjni14514272448621065586.so+0x83afca]  rocksdb::OptimisticTransaction::Rollback()+0x2a\r\nC  [librocksdbjni14514272448621065586.so+0x2def2e]  Java_org_rocksdb_Transaction_rollback+0x1e\r\nJ 13789  org.rocksdb.Transaction.rollback(J)V (0 bytes) @ 0x00007f4dad4fa05b [0x00007f4dad4f9fa0+0x00000000000000bb]\r\nJ 14913 c1 org.rocksdb.Transaction.rollback()V (60 bytes) @ 0x00007f4da5bf2fbc [0x00007f4da5bf2e00+0x00000000000001bc]\r\nJ 14285 c2 io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.ensureInOpenTransaction(Lio/camunda/zeebe/db/impl/rocksdb/transaction/TransactionConsumer;)V (29 bytes) @ 0x00007f4dad398614 [0x00007f4dad398280+0x0000000000000394]\r\nJ 21410 c1 io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.forEach(Ljava/util/function/BiConsumer;)V (24 bytes) @ 0x00007f4da5fb5a94 [0x00007f4da5fb56a0+0x00000000000003f4]\r\nj  io.camunda.zeebe.engine.state.distribution.DbDistributionState.foreachPendingDistribution(Lio/camunda/zeebe/engine/state/immutable/DistributionState$PendingDistributionVisitor;)V+49\r\nj  io.camunda.zeebe.engine.processing.distribution.CommandRedistributor.runRetryCycle()V+30\r\nj  io.camunda.zeebe.engine.processing.distribution.CommandRedistributor$$Lambda$727+0x00000008015c5c08.run()V+4\r\nJ 18846 c1 io.camunda.zeebe.stream.api.scheduling.SimpleProcessingScheduleService.lambda$runAtFixedRate$0(Ljava/lang/Runnable;Ljava/time/Duration;)V (49 bytes) @ 0x00007f4da57d5274 [0x00007f4da57d5160+0x0000000000000114]\r\nJ 18845 c1 io.camunda.zeebe.stream.api.scheduling.SimpleProcessingScheduleService$$Lambda$720+0x00000008015c4698.run()V (18 bytes) @ 0x00007f4da5f4ecac [0x00007f4da5f4eb40+0x000000000000016c]\r\nJ 15127 c2 io.camunda.zeebe.scheduler.ActorJob.execute(Lio/camunda/zeebe/scheduler/ActorThread;)V (351 bytes) @ 0x00007f4dad5606ac [0x00007f4dad560540+0x000000000000016c]\r\nJ 18627 c2 io.camunda.zeebe.scheduler.ActorTask.execute(Lio/camunda/zeebe/scheduler/ActorThread;)Z (350 bytes) @ 0x00007f4dad8730f4 [0x00007f4dad872840+0x00000000000008b4]\r\nJ 22040 c2 io.camunda.zeebe.scheduler.ActorThread.doWork()V (285 bytes) @ 0x00007f4dadbb264c [0x00007f4dadbb2240+0x000000000000040c]\r\nJ 11375 c2 io.camunda.zeebe.scheduler.ActorThread.run()V (103 bytes) @ 0x00007f4dacf579e0 [0x00007f4dacf578e0+0x0000000000000100]\r\nv  ~StubRoutines::call_stub\r\nV  [libjvm.so+0x825045]  JavaCalls::call_helper(JavaValue*, methodHandle const&, JavaCallArguments*, JavaThread*)+0x315\r\nV  [libjvm.so+0x82683b]  JavaCalls::call_virtual(JavaValue*, Handle, Klass*, Symbol*, Symbol*, JavaThread*)+0x1cb\r\nV  [libjvm.so+0x8f14f3]  thread_entry(JavaThread*, JavaThread*)+0xa3\r\nV  [libjvm.so+0xe62564]  JavaThread::thread_main_inner()+0x184\r\nV  [libjvm.so+0xe65c10]  Thread::call_run()+0xc0\r\nV  [libjvm.so+0xc1aa21]  thread_native_entry(Thread*)+0xe1\r\n```\r\n\r\n</p>\r\n</details>\n\n korthout: ❗ Since merging #13008, this segfault can also occur when both feature flags are enabled. We should fix this soon! Luckily the fix is easy and already coming up.\r\n\r\nThe segfault occurs because of the following situation:\r\n- the stream processor has its own transaction context\r\n- the scheduled tasks have their own shared transaction context\r\n- the scheduled tasks can be run on the stream processor's actor, or async on their scheduled tasks actor\r\n- the problem occurs when two scheduled tasks run on different actors while reusing the shared transaction context\n korthout: Since this issue only exists in a few places and those don't affect users, we'll look into it after the company retreat:\r\n- on released versions (`8.2.7-` and `8.1.13-`, but not on any `8.3` alphas) when only one of these experimental feature flags are enabled and the other is disabled:\r\n  - `ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLEMESSAGETTLCHECKERASYNC`\r\n  - `ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLETIMERDUEDATECHECKERASYNC`\r\n- on `main` branch when one or both feature flags are enabled\r\n- on `main` branch in tests (because both feature flags are enabled)\r\n\r\nWe should fix this before the next alpha release: `8.3.0-alpha3`.",
    "title": "Segfault on enabling async scheduled task"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13123",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nCurrently, the `JobTimeOutProcessor` assumes that the job it is supposed to time out is in the state. This is wrong, as it's possible to have the following sequence on the log: Job.COMPLETE, Job.TIME_OUT. The first command will remove the job from the state, and when processing the time out command, the state will not return any job.\r\n\r\n**To Reproduce**\r\n\r\nNew test in `JobTimeOutTest`:\r\n\r\n```java\r\n@Test\r\n  public void shouldRejectIfJobDoesNotExist() {\r\n    // given\r\n    final var jobKey = ENGINE.createJob(jobType, PROCESS_ID).getKey();\r\n    final var job = ENGINE.getProcessingState().getJobState().getJob(jobKey);\r\n    final var partitionId = Protocol.decodePartitionId(jobKey);\r\n\r\n    // when\r\n    ENGINE.pauseProcessing(partitionId);\r\n    ENGINE.writeRecords(\r\n        RecordToWrite.command().job(JobIntent.COMPLETE, job),\r\n        RecordToWrite.command().job(JobIntent.TIME_OUT, job));\r\n    ENGINE.resumeProcessing(partitionId);\r\n    Awaitility.await(\"until everything processed\").until(ENGINE::hasReachedEnd);\r\n\r\n    // then activated again\r\n    final List<Record<JobRecordValue>> jobEvents =\r\n        jobRecords()\r\n            .withRecordKey(jobKey)\r\n            .withIntent(JobIntent.TIME_OUT)\r\n            .withRecordType(RecordType.COMMAND_REJECTION)\r\n            .limit(1)\r\n            .toList();\r\n    assertThat(jobEvents).isNotEmpty();\r\n  }\r\n```\r\n\r\n**Expected behavior**\r\n\r\nThe command is rejected when the job to time out does not exist.\r\n\r\n**Log/Stacktrace**\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.lang.NullPointerException: Cannot invoke \"io.camunda.zeebe.protocol.impl.record.value.job.JobRecord.getDeadline()\" because \"job\" is null\r\n\tat io.camunda.zeebe.engine.processing.job.JobTimeOutProcessor.processRecord(JobTimeOutProcessor.java:53) ~[zeebe-workflow-engine-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.engine.Engine.process(Engine.java:143) ~[zeebe-workflow-engine-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.batchProcessing(ProcessingStateMachine.java:347) ~[zeebe-stream-platform-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$2(ProcessingStateMachine.java:268) ~[zeebe-stream-platform-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run(ZeebeTransaction.java:84) ~[zeebe-db-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand(ProcessingStateMachine.java:268) ~[zeebe-stream-platform-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord(ProcessingStateMachine.java:227) ~[zeebe-stream-platform-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.readNextRecord(ProcessingStateMachine.java:203) ~[zeebe-stream-platform-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:203) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n```\r\n\r\n</p>\r\n</details>\r\n\n\n koevskinikola: Just adding for transparency:\r\n\r\n*Observed behavior*:\r\nThe process instance related to the job is banned, making it unrecoverable.\r\n\r\n*Workarounds*:\r\n- There are no workarounds to fix this issue if it has already happened. As a result, this bug has a high severity.",
    "title": "NPE when processing Job.TIME_OUT command"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13038",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nWhen trying/retrying a push from the broker side, we every time pick the next random stream consumer to push to. However, it's possible that during a retry all consumers are removed. Right now, this throws an error because we try to generate a random index from 0 to 0 :smile: \r\n\r\n**Expected behavior**\r\n\r\nWe should cope with streams having no consumers (on both the gateway/broker) even during retries, and bail out early.\r\n\n\n npepinpe: Alright, so `AggregatedRemoteStream` is not thread safe, but it's also not immutable right now.\r\n\r\nSo we have two options: making it thread-safe and handle mutation, or making it immutable and potentially handle more errors.\r\n\r\nMaking it thread safe means turning the consumer list into a `CopyOnWriteArrayList` (as we likely are reading from it more often than writing to it), and handling mutations when picking a random stream by first grabbing a copy of the list before doing any operations on it.\r\n\r\nMaking it immutable for now would simply be copying the `AggregatedRemoteStream` record when picking the target, and passing this around downstream. I would lean towards the second option as it's simpler to handle, and in the common case we still wouldn't be handling more errors than usual.",
    "title": "Handle stream consumers changes during push"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13036",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nWhen registering a new client job stream from the gateway to any broker, the brokers are successfully registering the streams, but not responding to the gateway, causing the gateway to retry over and over.\r\n\r\nWhile this still allows pushing, it creates a lot of noise and the impression that the streams are not successfully registered.\r\n\r\nThe underlying cause is the usage of the `ClusterCommunicationService#subscribe(String, Function<byte[], M>, BiConsumer<MemberId, M>, Executor executor)` - any subscriber which is a consumer will never send a response back. However, the client in this case expects a response.\r\n\r\n**Expected behavior**\r\n\r\nClient streams are properly registered on both sides.\r\n\n\n npepinpe: One hurdle here is there's no easy way to introspect the registered client streams. So our tests were passing because we just check if the server has registered the stream (which it has), but we have no real way to catch that the client is in an endless loop.",
    "title": "Endless client job stream registration"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12957",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nSince 8.2, it is possible to deploy processes with [Undefined Task](https://docs.camunda.io/docs/next/components/modeler/bpmn/undefined-tasks/), which the workflow engine processes as a straight-through activity. When placed in a loop without a wait state (i.e. element in the process that requires the engine to wait), the workflow engine may process faster than the exporters can export the produced records.\r\n\r\nWhen the workflow engine is faster than the exporters, the log grows, and Zeebe's disk space usage increases. Eventually, Zeebe Brokers may take too long to start up because the log is too large. This may lead to the unavailability of Zeebe.\r\n\r\nWe've encountered this on the Elasticsearch exporter, where ES may reject exported records. In that case, it can even result in the unavailability of Operate and Zeebe.\r\n\r\nAs a temporary workaround:\r\n- increase the CPU/memory resources of Elasticsearch to support the higher influx of records\r\n- increase the failure threshold Zeebe Brokers are allowed as start-up duration\r\n- cancel the process instance with the straight-through processing loop\r\n\r\n**To Reproduce**\r\n\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\n> The following only works on clusters exporting to Elasticsearch with specific resource allocations. You may need to reduce the available Elasticsearch resources.\r\n\r\n- deploy a process with a loop of undefined tasks\r\n- create an instance of this process\r\n- notice the number of not exported records increasing\r\n\r\n**Expected behavior**\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\nProcess instances of processes with a straight-through processing loop should not cause problems.\r\n\r\n**Solution proposals**\r\n\r\nA solution has already been proposed to resolve this issue:\r\n- #12696 \r\n\r\nAdditionally, we suggest to:\r\n- add a feature flag to allow/disallow undefined tasks and/or processes without wait states\r\n  - allows Play to use them in the Modeling/Play iteration; while it can be disabled on SaaS production-like clusters\r\n  - flag could also disallow creating instance of such processes if they are already deployed\r\n  - flag could also stop execution of process instances of such processes if already created\r\n- raise an [Incident](https://docs.camunda.io/docs/next/components/concepts/incidents/) when a straight-through processing loop is detected during process execution\r\n\r\n**Considered alternatives**\r\n\r\nI also considered interrupting a process instance (and its child process instances) immediately when canceling it instead of the current recursive and async termination flow. However, that solution would be better fitting to kill fork bombs. A simple straight-through processing loop does not produce a log of unprocessed records.\r\n\n\n Zelldon: > IMO, we should not reject deployments of processes without wait states because these may be valuable in Modeling/Play iteration when first designing a process.\r\n\r\nI think this can be differentiated. We consider Play our new tool for users to play around with there models and here I agree it makes sense to allow it, but not on production-like clusters.\r\n\r\nWe could have a feature flag that allows us to disable certain features, like undefined tasks or other activities. We could then reject models (with a well-written message) which contain such elements on SaaS clusters, and reference to Play to try out processes. This would also allow us to prevent problematic scenarios in production environments.\r\n\r\nIn my opinion, this would be straightforward and with minimal effort to implement. \r\n\r\nAlternative and much more complex would be of course detection mechanism, and in general better handling of suspending/cancelation of bad behavior instances.\r\n\r\n\r\n\n korthout: Good point @Zelldon! I'll adjust the description accordingly.\n megglos: Thanks for raising this @korthout! I wanted to follow-up on this topic this week. I would love us to have a short-term mitigation, like e.g. disabling undefined tasks until we have a proper way of at least recovering or even better detecting these situations and suspend such instances.\r\n\r\nFor short term I see these options:\r\n\r\n1. disable undefined task processing by default on SaaS, only offer a flag to opt-in => assuming the use case is covered by  Zeebe Play (for which it is enabled)\r\nPro:\r\nno such incidents on SaaS anymore\r\noptional: we can make this transparent to the user by the engine rejecting processes that use undefined tasks\r\nCon:\r\nthe feature is effectively removed from SaaS\r\n\r\n2. option 1 but only for all non-trial clusters => prevent an incident where a real production cluster gets impacted by such a process\r\nPro:\r\nwe prevent this from happening on a production cluster\r\nCon:\r\nthe noise caused by trials will not disappear\r\n\r\n3. Assess a fist iteration to detect loops of undefined tasks and reject their deployment => even if it may not cover all potential edge-cases may a first iteration be feasible that already is good-enough to prevent a majority of such processes to cause trouble?\r\nPro:\r\nmay be suffice to reduce the frequency of such incidents heavily already\r\nerror message can make this transparent to the user\r\nmay better fit a 8.2 patch scope => prevent users from running into this issue while keeping the previous default of undefined tasks being allowed\r\nCon:\r\neffort seems higher\r\nstill a risk of an edge-case not covered causing such loop (a call-activity of a process that contains undefined tasks only 🙃 ?)\r\n\r\nMy personal favorite is option 3 and if that's too complicated option 1 asap. (I'm fine doing an out of schedule patch, for the sake of preventing this issue asap)\r\nOption 2 is not really helping on the trial noise, meaning it is not good enough of a proper mitigation from that perspective,  and introduced feature imbalance for trial that we didn't have yet. right?\r\n\r\nwdyt?\r\n\r\n\n Zelldon: \r\n![giphy](https://github.com/camunda/zeebe/assets/2758593/16eeb955-6c44-4f78-831f-9071bb92e104)\r\n\r\nStop the bleeding with one, and work on a better solution e.g. three. \r\n\r\nExamples: Reject deployments which contain only no wait states, or detect loops which contains no wait states (https://stackoverflow.com/questions/261573/best-algorithm-for-detecting-cycles-in-a-directed-graph) \r\n\n megglos: The issue we have with 1 & 2 is that it effectively disables a feature of 8.2, that's why I would like to favor 3 first and take 1 as last resort + the communication overhead to announce this breaking change.\n megglos: @korthout as a mitigation to this would help us to avoid repeated incidents with manual effort I would suggest to bump severity to high, wdyt?\n korthout: @megglos The `severity` label is clearly defined. This bug does not have high severity because a workaround is available. \r\n\r\nI do see the need to prioritize it as `🚧 upcoming` (intend to work on it soon) or even as `⛔ blocker` (stop-the-world) due to the impact it has on us (incidents, alert noise, etc).\n megglos: another option for a potential mitigation that was raised by @oleschoenburg \r\n\r\n4. What about having an artificial delay for undefined and manual tasks, e.g. 1s?\r\nThat would throttle such loops significantly by introducing a wait state and thus offsetting disk resource issues significantly.\r\nFrom a user perspective that might be acceptable to unnoticeable. We could make it configurable to cover cases where a self-managed user cares about low latency and still wants to keep and undefined/manual task in the process\r\nPro:\r\nmight it be more straightforward to implement compared to option 3 => basically applying timer event behavior\r\nCon:\r\nDoesn't prevent this situation but offsets any resulting resource problems\r\n\r\nSo we could decide on doing either 4 or 3 first. While 3 would prevent such situations (but not all of them) 4 would offset any resulting issues of such processes by a significant amount of time. Both together appear already good enough to mitigate the pain of such looping processes.\r\n\r\nWith these in place we may be able to close this issue and eventually shift towards https://github.com/camunda/zeebe/issues/12696 that would enable us to at least manually intervene with any rogue process.\r\n\r\nwdyt @korthout @Zelldon ?\r\n\r\n\n korthout: Interesting idea\r\n\r\n>artificial delay for undefined and manual tasks\r\n\r\nI assume we mean non-blocking for the stream processor. So schedule a delayed task that appends the command to complete the task.\n Zelldon: Not sure how this should look like, since if you start to process it you need to commit all changes to it before starting on the next command, the processing of commands is in a serial order. I can only imagine you put it back to the end of the log, other than that I see no way to do that ?\r\n\n oleschoenburg: > Not sure how this should look like, since if you start to process it you need to commit all changes to it before starting on the next command\r\n\r\nSimilar to how job timeouts works for example: after the manual task is `ACTIVATED` and after some period of time has passed we write a `COMPLETE` command to the log.\r\nThat's how I thought it could work anyway, maybe that doesn't work?\n korthout: @Zelldon is right; it wouldn't be safe to do so. Unless we add a way to write the `COMPLETE` command for any currently `ACTIVATED` undefined/manual tasks, on recovery.\n oleschoenburg: But every `ACTIVATED` undefined/manual task should already have a `COMPLETE` follow-up command on the log. Otherwise, how would this task ever complete?\n korthout: >But every ACTIVATED undefined/manual task should already have a COMPLETE follow-up command on the log. \r\n\r\nThat's not correct. The proposal was to add the delay async, so when processing `ACTIVATE_ELEMENT` we append `ELEMENT_ACTIVATING` and `ELEMENT_ACTIVATED`, and also schedule a post-commit task to append `COMPLETE_ELEMENT` after a delay (this may be lost).\n oleschoenburg: Ah sorry, I misunderstood and thought you were talking about the upgrade from old to new behavior.\r\n\r\nOf course you are right and this would require persistence of activated manual/undefined tasks so that they can be completed eventually. Again, similar to job timeouts.\n Zelldon: So to summarize the proposal (just that I get it right):\r\n\r\n * There will be another checker/consumer which is scheduled via the ProcessingScheduleService (in order to be decoupled of the processing). Lets call it U-COMPLETER\r\n * The processing of undefined tasks is split up, it will only produce ACTIVATE_ACTIVATED on processing\r\n * During activation we need to store the record and all necessary data in a new column family (for the U-COMPLETER)\r\n * The U-COMPLETER will write based on the data in the column family the complete commands, in order to continue the processing of the instance.\n korthout: ZPA triage:\n- let's split up this issue into the different solution proposals\n- we like the proposal to reject deployments of processes with an undefined task loop but there are simpler solutions, so we'll focus on those first\n- we like the proposal to slow down undefined task, but it's unclear to us whether we should do this sync or async.\n  - sync: this blocks the stream processor a bit but is a simpler solution\n  - async: adds additional complexity over sync because we have to care about the unreliability of scheduled post-commit tasks\n  - @camunda/zeebe-distributed-platform, do you have an opinion about the performance loss?\n- we should check what impact our solutions have on Zeebe Play\n- we will focus on the quick fix in the upcoming iteration\n megglos: > sync: this blocks the stream processor a bit but is a simpler solution\r\n> async: adds additional complexity over sync because we have to care about the unreliability of scheduled post-commit tasks\r\n\r\nsync would not be an option due to the impact on overall processing latency/throughput\r\n\n megglos: I'm also in all in for rolling out a 8.2.X patch & a 8.2.0-alpha2.1 (did we ever do an out of cadence alpha before?) as soon as a mitigation is available\n korthout: >sync would not be an option due to the impact on overall processing latency/throughput\r\n\r\nIs Undefined Task really an element used in production where performance (latency/throughput matters?\r\n\r\nI imagine Undefined Task only has use in Zeebe Play, or as a lonely element in a production process where a small latency (~10-50ms) doesn't really matter.\n remcowesterhoud: I've created https://github.com/camunda/zeebe/issues/12993 as a separate issue to delay undefined tasks.\n korthout: Thanks @remcowesterhoud, marking this issue as `later` priority, as #12993 is split out with higher prio.\n oleschoenburg: > I imagine Undefined Task only has use in Zeebe Play, or as a lonely element in a production process where a small latency (~10-50ms) doesn't really matter.\r\n\r\nI disagree that it's fine for production workloads. If a user \"accidentally\" deploys and starts such as process that loops on an undefined task, it could seriously impact the performance of an entire partition because the partition will repeatedly sleep. Even worse with batch processing enabled where it's not just one sleep but many (like 10 or 100).\n megglos: ZDP-Triage:\n- moving to backlog now as @remcowesterhoud is looking into this\n remcowesterhoud: As the quick-fix is done and getting released as I type this, I'm going to remove my assignment from this issue. We'll still need to make a proper solution, so I will move this back to the teams inbox to triage it again.\n Zelldon: Great work @remcowesterhoud \n korthout: ZPA triage:\n- with a quick fix available, we think other issues have higher priority than this\n- we'll increase priority if this does occur again\n- marking as `later`\n megglos: ZDP-Triage:\n- loop detection was a first band-aid and we still lack measures to handle situations not covered by validation\n- measures from the steam platform might be possible, e.g. detect high processing load on the same key, it needs to be discussed if this is the right direction though\n- most solutions likely require collab between both teams\n megglos: https://github.com/camunda/zeebe/issues/12696 is related to this as well, will put focus on that one first\n megglos: ZDP-Planning:\n- is related to #12560 and would be mitigated if processing is rate limited\n- we have alerts on noop loops => we can cancel such instances proactively",
    "title": "Straight-through processing loop may cause problems"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12933",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nWith job push enabled, when a job is failed with remaining retries and no backoff, the job is first activated and then immediately marked as failed again. This results in pushing out jobs that are in failed state and presumably some data inconsistencies because the job lifecycle is not followed. \r\n\r\n**To Reproduce**\r\n\r\nRun `ActivatableJobsPushTest#shouldPushAfterJobFailed` and inspect the log.\r\n\r\n**Expected behavior**\r\n\r\nWhen a job is failed with remaining retries and no backoff, the job should first transition to failed before being activated again.\r\n\r\n**Log/Stacktrace**\r\n\r\n<details><summary>Compact log representation</summary>\r\n <p>\r\n\r\n```\r\nC DPLY      CREATE            - #01-> -1  -1 - \r\nE PROC      CREATED           - #02->#01 K01 - process.bpmn -> \"process\" (version:1)\r\nE DPLY      CREATED           - #03->#01 K02 - process.bpmn\r\nE DPLY      FULLY_DISTRIBUTED - #04->#01 K02 - \r\nC CREA      CREATE            - #05-> -1  -1 - new <process \"process\"> (default start)  with variables: {a=valA, b=valB, c=valC}\r\nE VAR       CREATED           - #06->#05 K04 - b->\"valB\" in <process [K03]>\r\nE VAR       CREATED           - #07->#05 K05 - a->\"valA\" in <process [K03]>\r\nE VAR       CREATED           - #08->#05 K06 - c->\"valC\" in <process [K03]>\r\nC PI        ACTIVATE          - #09->#05 K03 - PROCESS \"process\" in <process \"process\"[K03]>\r\nE CREA      CREATED           - #10->#05 K07 - new <process \"process\"> (default start)  with variables: {a=valA, b=valB, c=valC}\r\nE PI        ACTIVATING        - #11->#05 K03 - PROCESS \"process\" in <process \"process\"[K03]>\r\nE PI        ACTIVATED         - #12->#05 K03 - PROCESS \"process\" in <process \"process\"[K03]>\r\nC PI        ACTIVATE          - #13->#05  -1 - START_EVENT \"start\" in <process \"process\"[K03]>\r\nE PI        ACTIVATING        - #14->#05 K08 - START_EVENT \"start\" in <process \"process\"[K03]>\r\nE PI        ACTIVATED         - #15->#05 K08 - START_EVENT \"start\" in <process \"process\"[K03]>\r\nC PI        COMPLETE          - #16->#05 K08 - START_EVENT \"start\" in <process \"process\"[K03]>\r\nE PI        COMPLETING        - #17->#05 K08 - START_EVENT \"start\" in <process \"process\"[K03]>\r\nE PI        COMPLETED         - #18->#05 K08 - START_EVENT \"start\" in <process \"process\"[K03]>\r\nE PI        SEQ_FLOW_TAKEN    - #19->#05 K09 - SEQUENCE_FLOW \"sequenc..7fb902a\" in <process \"process\"[K03]>\r\nC PI        ACTIVATE          - #20->#05 K10 - SERVICE_TASK \"task\" in <process \"process\"[K03]>\r\nE PI        ACTIVATING        - #21->#05 K10 - SERVICE_TASK \"task\" in <process \"process\"[K03]>\r\nE JOB       CREATED           - #22->#05 K11 - K11 \"id-ae6f81e8-dfdb-436e-a86a-d681b855a326\" @\"task\"[K10] 3 retries, in <process \"process\"[K03]> (no vars)\r\nE JOB_BATCH ACTIVATED         - #23->#05 K12 - \"id-ae6f81e8-dfdb-436e-a86a-d681b855a326\" 1/-1\r\n                K11 \"id-ae6f81e8-dfdb-436e-a86a-d681b855a326\" @\"task\"[K10] 3 retries, in <process \"process\"[K03]> (no vars)\r\nE PI        ACTIVATED         - #24->#05 K10 - SERVICE_TASK \"task\" in <process \"process\"[K03]>\r\nC JOB       FAIL              - #25-> -1 K11 - K11 5 retries, in <process ?[?]> (no vars)\r\nE JOB_BATCH ACTIVATED         - #26->#25 K13 - \"id-ae6f81e8-dfdb-436e-a86a-d681b855a326\" 1/-1\r\n                K11 \"id-ae6f81e8-dfdb-436e-a86a-d681b855a326\" @\"task\"[K10] 5 retries, in <process \"process\"[K03]> (no vars)\r\nE JOB       FAILED            - #27->#25 K11 - K11 \"id-ae6f81e8-dfdb-436e-a86a-d681b855a326\" @\"task\"[K10] 5 retries, in <process \"process\"[K03]> (no vars)\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n\r\nThis is currently a (soft) blocker for https://github.com/camunda/zeebe/issues/12797\n\n oleschoenburg: I think the issue is related to the _very confusing_ interplay between the `JobFailProcessor` (a `CommandProcessor`), the `CommandControl` and the `BpmnJobActivationBehavior`.\r\nThe code _seemingly_ does the right thing, first transitioning to `FAILED` and then handing over to `jobActivationBehavior` which should transition to `ACTIVATED`:\r\nhttps://github.com/camunda/zeebe/blob/5bd2e1f830d8de2efb6869dc0a29d47896b51800/engine/src/main/java/io/camunda/zeebe/engine/processing/job/JobFailProcessor.java#L137-L143\r\n\r\nIf I understand correctly, the mistakes is that the commandControl does not immediately write the follow-up event but only later when the `ACTIVATED` event is already written.\n npepinpe: Do we really need to mark the job as failed? Could we skip it entirely?\r\n\r\nAt any rate, if we have to, the log would ideally look like: ACTIVATED -> FAIL -> FAILED -> ACTIVATED. While the push has to happen as a side effect, the new ACTIVATED record should be the last one written in terms of state changes.\n koevskinikola: This bug will also be present when pushing jobs on timeout and recur, as the corresponding processors implement the `CommandProcessor` interface as well.\r\n\r\n1. The quick fix would be to move the job push to the `CommandProcessor#afterAccept` method.\r\n1. The nicer fix, which would make these Processors more readable, would be to refactor them to use the `TypedRecordProcessor` interface, where it's more clear when the events are written.\n npepinpe: :+1: for the second one\n koevskinikola: @npepinpe sorry for not replying to:\r\n> Do we really need to mark the job as failed? Could we skip it entirely?\r\n\r\nI think it would be good to have the following sequence of job events on the log: ACTIVATED -> FAIL -> FAILED -> ACTIVATED.\r\n\r\nIt's clearer to users that the `FAIL` command resulted in a `FAILED` event, and the job was then `ACTIVATED`.\r\n\r\nHaving ACTIVATED -> FAIL -> ACTIVATED is a bit confusing.",
    "title": "Failing jobs with job push enabled breaks the job lifecycle"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12915",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "While working on #12839, it was observed that it is not possible to change the journal record schema without breaking backward compatability. (See [comment](https://github.com/camunda/zeebe/pull/12839#issuecomment-1568545921))\r\n\r\nWhen journal record schema was moved to SBE, one of the main motivator was to allow extending it without breaking changes. \r\n\r\nProblems:\r\n- A broker on newer version cannot receive any events via raft replication, if the event was written by a leader at older version.\r\n   - This is a deal breaker. As we cannot upgrade a running system to a new version.\r\n- A broker on older version cannot receive any events via raft replication, of the event was written by a leader at newer version.\r\n   - This is less problematic, as it might cause some unavailability during rolling update.\r\n\r\nGoal:\r\nEnsure we can extend journal and raft record schema with out breaking compatibility. \r\n\r\nTo limit the scope, it might be ok if we could ensure backward compatibility. That is, ensure brokers at newer versions can work with record written with old version. It is ok, if old versions cannot read records from new version as long as it can detect it and do not cause any inconsistency. \r\n\r\n\r\nblocking #12839 \r\n\n\n deepthidevaki: Summary of discussion with @npepinpe \r\n\r\nIt is better to send the serialized journal record in AppendRequest. This would require some changes in the journal api and raft replication handling. We have to figure out how to change this, without breaking compatibility. During rolling update, it might be acceptible if follower's on older version cannot receive any events from a leader on newer version. But we should ensure that, followers on newer version can receive events from leaders on older version. Otherwise this can block both rolling update and recreating with new version.\r\n\r\nOne idea is to check the version of the sender, and interpret the request accordingly. Right now, we don't have a concept of raft-protocol-version. But we can probably add that, and not having it can be interpreted as old version. When raft follower on new version, receives a request from old leader, it writes the record using the old sbe version and the checksum calculated will match that of the original one. If the request is from the new leader, it uses the new logic to handle the serialized journal record.\n megglos: ZDP-Triage:\n- the checksum is affected by the schema version\n- we need a general mechanism to handle such changes\n- blocks any changes to the journal schema => makes sense to work on it asap\n- also relates to properly supporting updates in general\n- the solution will have an effect on rolling updates going forward https://github.com/camunda/product-hub/issues/256",
    "title": "Allow extending journal record format with out breaking compatibility"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12875",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Summary**\r\n\r\nRun `ModifyProcessInstanceRejectionTest#shouldRejectActivationWhenAncestorScopeIsNotFlowScope` test failed on Windows 10.\r\n\r\n**Logs**\r\n\r\n\r\n<details><summary>Logs</summary>\r\n<pre>\r\nE PI   SEQ_FLOW_TAKEN    - #63->#05 K21 - SEQUENCE_FLOW \"sequenc..b588006\" in <process \"process\"[K03]>\r\nC PI   ACTIVATE          - #64->#05 K22 - PARALLEL_GATEWAY \"join\" in <process \"process\"[K03]>\r\nR PI   ACTIVATE          - #65->#05 K22 - PARALLEL_GATEWAY \"join\" in <process \"process\"[K03]> !INVALID_STATE (Expected to be able to activate parallel gateway 'join', but not all sequence flows have been taken.)\r\nC MOD  MODIFY            - #66-> -1 K03 - <activate \"A\" no vars> <activate \"B\" no vars> <activate \"A\" no vars> \r\nR MOD  MODIFY            - #67->#66 K03 - <activate \"A\" no vars> <activate \"B\" no vars> <activate \"A\" no vars>  !INVALID_ARGUMENT (Expected to modify instance of process 'process' but it contains one or more activate instructions with an ancestor scope key that is not an ancestor of the element to activate:\r\n- instance '2251799813685257' of element 'subProcess2' is not an ancestor of element 'A'\r\n- instance '2251799813685263' of element 'B' is not an ancestor of element 'A')\r\n\r\n\r\n\r\njava.lang.AssertionError: [Expect that subProcess2 cannot be selected as ancestor of task A] \r\nExpecting rejectionReason of:\r\n  <{\"valueType\":\"PROCESS_INSTANCE_MODIFICATION\",\"key\":2251799813685251,\"position\":67,\"timestamp\":1685115125924,\"recordType\":\"COMMAND_REJECTION\",\"intent\":\"MODIFY\",\"partitionId\":1,\"rejectionType\":\"INVALID_ARGUMENT\",\"rejectionReason\":\"Expected to modify instance of process 'process' but it contains one or more activate instructions with an ancestor scope key that is not an ancestor of the element to activate:\\r\\n- instance '2251799813685257' of element 'subProcess2' is not an ancestor of element 'A'\\r\\n- instance '2251799813685263' of element 'B' is not an ancestor of element 'A'\",\"brokerVersion\":\"8.3.0\",\"value\":{\"processInstanceKey\":2251799813685251,\"ancestorScopeKeys\":[],\"terminateInstructions\":[],\"activateInstructions\":[{\"ancestorScopeKeys\":[],\"elementId\":\"A\",\"ancestorScopeKey\":2251799813685257,\"variableInstructions\":[]},{\"ancestorScopeKeys\":[],\"elementId\":\"B\",\"ancestorScopeKey\":2251799813685257,\"variableInstructions\":[]},{\"ancestorScopeKeys\":[],\"elementId\":\"A\",\"ancestorScopeKey\":2251799813685263,\"variableInstru...>\r\nto be:\r\n  <Expected to modify instance of process 'process' but it contains one or more activate instructions with an ancestor scope key that is not an ancestor of the element to activate:\r\n- instance '2251799813685257' of element 'subProcess2' is not an ancestor of element 'A'\r\n- instance '2251799813685263' of element 'B' is not an ancestor of element 'A'>\r\nbut was:\r\n  <Expected to modify instance of process 'process' but it contains one or more activate instructions with an ancestor scope key that is not an ancestor of the element to activate:\r\n- instance '2251799813685257' of element 'subProcess2' is not an ancestor of element 'A'\r\n- instance '2251799813685263' of element 'B' is not an ancestor of element 'A'>\r\n\r\n\tat io.camunda.zeebe.engine.processing.processinstance.ModifyProcessInstanceRejectionTest.shouldRejectActivationWhenAncestorScopeIsNotFlowScope(ModifyProcessInstanceRejectionTest.java:664)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n</pre>\r\n</details>\r\n\r\n```\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Detecting the operating system and CPU architecture\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] os.detected.name: windows\r\n[INFO] os.detected.arch: x86_64\r\n[INFO] os.detected.bitness: 64\r\n[INFO] os.detected.version: 10.0\r\n[INFO] os.detected.version.major: 10\r\n[INFO] os.detected.version.minor: 0\r\n[INFO] os.detected.classifier: windows-x86_64\r\n\r\n```\r\n\r\n\n\n remcowesterhoud: @skayliu I assume this test always fails for you and is not flaky? I believe it has to do with the line breaks I can see in your error message: `\\r\\n`\n skayliu: @remcowesterhoud, Yes, It always failed not flaky.\n remcowesterhoud: Perfect, thanks! Time to dust off my old Windows laptop 😄 ",
    "title": "`ModifyProcessInstanceRejectionTest#shouldRejectActivationWhenAncestorScopeIsNotFlowScope` test fails on Windows"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12837",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\nUsing an error catch event (boundary or start event in Subprocess) with an empty `errorCode` results in incident with `errorType=UNHANDLED_ERROR_EVENT`. \r\nAlso only error events with a non-empty errorCode will be reported like this: `Available error events are [boundary]`.\r\n\r\n\r\n**To Reproduce**\r\n\r\n1. Create a BPMN process with error catch event as boundary event or as start event in subprocess.\r\n2. Deploy, start the process. In a job client throw an error command with an unhandled errorCode.\r\n3. Zeebe creates an incident instead of catching the error \r\n\r\nHere an example process [errorProcessCatchAll.bpmn.txt](https://github.com/camunda/zeebe/files/11544062/errorProcessCatchAll.bpmn.txt)\r\n\r\n[Slack thread](https://camunda.slack.com/archives/CSQ2E3BT4/p1684841521468159)\r\n\r\n**Expected behavior**\r\n\r\nThe error is caught and no incident is created.\r\n\r\n**Log/Stacktrace**\r\n\r\nExample error message from Zeebe:\r\n```\r\nerrorType=UNHANDLED_ERROR_EVENT, errorMessage='Expected to throw an error event with the code 'unknown' with message 'Process error', but it was not caught. Available error events are [boundary]\r\n```\r\n\r\n**Environment:**\r\n- OS: MacOS\r\n- Zeebe Version: `8.2.0-alpha3` and later\r\n- Configuration: ElasticsearchExporter\r\n\r\n\n\n lzgabel: Hi @korthout. I think I found the problem, please assign this task to  me, I will submit a PR to fix this problem soon. Thanks :heart:\n lzgabel: BTW. As a workaround, user can catch all error events by removing the `errorEventDefinition`.\r\n",
    "title": "Catch all error events by using empty 'errorCode' does not work"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12833",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nThere was an incident in one of the chaos test models, where a none end event had an output mapping referencing a non-existent variable `source` to be mapped to `target`. I resolved the incident by adding a dummy variable (the mapping should not have been there anyway), but then the process was stuck on the none end event.\r\n\r\nSee https://bru-2.operate.camunda.io/eeef5734-cfd6-47a5-a2ed-5fe13269e589/processes/2251799815205221\r\n\r\n**To Reproduce**\r\n\r\nAdd the following test case to `io.camunda.zeebe.engine.processing.incidentOutputMappingIncidentTest`:\r\n\r\n```java\r\n          {\r\n            \"None end event\",\r\n            ENGINE\r\n                .deployment()\r\n                .withXmlResource(\r\n                    Bpmn.createExecutableProcess(PROCESS_ID)\r\n                        .startEvent()\r\n                        .endEvent(\"endEventId\", b -> b.zeebeOutputExpression(\"foo\", \"bar\"))\r\n                        .done()),\r\n            \"endEventId\",\r\n            false\r\n          },\r\n```\r\n\r\n**Expected behavior**\r\n\r\nI can resolve incidents on a none end event.\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.2.3\r\n- Configuration: SaaS - G3 S\r\n\n\n korthout: As a workaround, users can use [Process Instance Modification](https://docs.camunda.io/docs/next/components/concepts/process-instance-modification/) to re-activate the end event\n korthout: ZPA triage:\r\n- should be simple to resolve `good-first-issue` (just implement the `onComplete` method on `NoneEndEventBehavior` in the `EndEventProcessor`)\r\n- since it's low-hanging fruit and may effect users we would like to tackle it sooner than later.",
    "title": "Cannot resolve output mapping incident on a none end event"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13299",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nSpring displays a warning at startup if both `spring-jcl` and `commons-logging` are found in the same class path:\r\n\r\n```\r\nStandard Commons Logging discovery in action with spring-jcl: please remove commons-logging.jar from classpath in order to avoid potential conflicts\r\n```\r\n\r\n[As per their own Spring Data project (which also uses the Elasticsearch client)](https://github.com/spring-projects/spring-data-elasticsearch/pull/1993), it seems fine to simply exclude `commons-logging`.\r\n\n",
    "title": "Remove Spring warning about spring-jcl and commons-logging"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13161",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "With #12764 we can now append different versions of records. By default, we append version 1 of a record. However, that's not easy to maintain. As we add more places where records are appended we have to remember all these and update them when we want to create a new version.\r\n\r\nInstead, it makes more sense to append the latest version of a record by default, and allow changing this in specific cases. Adding a new version of a record is then achieved automatically in all places.\n",
    "title": "Append latest version of records by default"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13057",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "The Command Distribution records are not yet logged in compact form by the compact record logger.\n",
    "title": "Implement compact record logger for Command Distribution"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12955",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "<!--\r\nIn case you have questions about our software we encourage everyone to participate in our community via the\r\n- Camunda Platform community forum https://forum.camunda.io/ or \r\n- Slack https://camunda-cloud.slack.com/ (For invite: https://camunda-slack-invite.herokuapp.com/)\r\n\r\nThere you can exchange ideas with other Zeebe and Camunda Platform 8 users, as well as the product developers, and use the search to find answer to similar questions.\r\n\r\nThis issue template is used by the Zeebe engineers to create general tasks.\r\n-->\r\n\r\n**Description**\r\n\r\n**In progress** \r\n\r\n|                      | [SST Partitioning](https://github.com/camunda/zeebe/pull/12483) | [Column family separation ](https://github.com/camunda/zeebe/pull/12675)|\r\n|------------------|------------------------|----------------------------------------|\r\n| Description | Enabling the experimental feature flag, to separate the SST creation based on a column family prefix. |Separate some of our high-frequently used virtual column families into physical ones. |\r\n| Impact | The SST files in RocksDB are split based on virtual column family prefixes, which might cause to create more SST files in the end. In general, the configuration is for compaction, which might compact SST files earlier ( or more often). We have seen that even with a large state in RocksDB we can iterate/seek/access data in almost similar performance as with small data sets. | Separating column families, would allow to separate hot data from colder ones to not influence too much different data sets. Allows to increase performance in seeking and accessing certain data. Since we use more physically separated column families this would mean more memory and space for RocksDB is needed. It would allow configuring column families differently, which might help in performance but as consequence also increases complexity. |\r\n| Upgradability | Upgrading should have no major impact. It was tested more in-depth in two chaos days. [[1](https://zeebe-io.github.io/zeebe-chaos/2023/05/15/SST-Partitioning-toggle), [2](https://zeebe-io.github.io/zeebe-chaos/2023/05/19/Continuing-SST-Partitioning-toggle)] and it was confirmed [here](https://groups.google.com/g/rocksdb/c/Ys-yZIznZwU). | Splitting the column families, is no easy-pick and involves some engineering effort. We need to run certain migration steps on startup in order to migrate data from virtual column families in to new physical column families. |\r\n| Risk |  The configuration is marked as experimental in RocksDB. [In a related Google group, it was described as stable and is used by other products](https://groups.google.com/g/rocksdb/c/Ys-yZIznZwU).  | There could be new bugs introduced with separating of column families and migrating data from one to multiple column families. There might be a loss of data (depending on bugs). Migration might be only one way possible, otherwise would increase complexity.  |\r\n| Implementation complexity | **Small.** We provided just a configuration in order to enable this feature in RocksDB. | **Mid**. The feature itself is not complicated ([see PR](https://github.com/camunda/zeebe/pull/12675/files)), but based on the benchmark it is also not 100% clear whether this works without issues. The migration procedure might be a bit more problematic, especially when we want to support going back to one CF. Furthermore, testing and how to configure certain column families, default values etc. can be challenging. | \r\n| **Benchmarks** |  | |\r\n| [JMH](https://github.com/camunda/zeebe/issues/12241) | 656.639 ± 91.394  ops/s  | 681.872  ± 77.924  ops/s |\r\n| Normal | ![general-sst-normal-bench](https://github.com/camunda/zeebe/assets/2758593/41b3127e-3493-498d-a61d-3795effca5bf) Worked without major issues. | ![general-cf-normal-bench](https://github.com/camunda/zeebe/assets/2758593/3764293c-c8d6-46da-ad92-4213e0da8ecd) No major issues, but the changes seem to have some issues with disk usage. |\r\n| Max throughput | ![general-sst-max-bench](https://github.com/camunda/zeebe/assets/2758593/e8f7295f-b427-48fc-8269-cb94a14aa3de) The maximum throughput was similar to a base benchmark. | ![general-cf-max-bench](https://github.com/camunda/zeebe/assets/2758593/7694ffd0-ada8-444f-9280-f158718fbb79) The solution suffered at the begin on degraded performance, but later watched a bit up. |\r\n| Large state |![general-sst-large-bench](https://github.com/camunda/zeebe/assets/2758593/67312278-8a02-4cbb-be5a-d73bf3c68f80) Running for around four days, reaching 80 gig of Snapshot and filling 500 gig of disk. Disk size seems to be the only limit. | ![general-cf-large-bench](https://github.com/camunda/zeebe/assets/2758593/c414371e-d8ff-4a0c-b872-5cd5dabd4168) Survives longer than our base (dies normally after one hour), but is still not enough. |\r\n| Latency  |  There was no specific latency benchmark, but on normal benchmark looks similar to base.|  There was no specific latency benchmark, but on normal benchmark looks similar to base. |\r\n\r\n\r\n\r\nrelates #12033 \n\n Zelldon: Based on the results above I will conclude this comparison and mark SST partitioning as the preferred solution for now.\r\n\r\nIt gives a lot of gains, like performance improvement in a larger state (which was the goal) with smaller risks and effort to implement compared to splitting up the column families.\r\n\r\nWe can always revise the decision later, but for now, it looks like the better and simpler solution to move forward.",
    "title": "Compare SST partitioning with real Column Family split"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12818",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nIn case on a snapshot inconsistency the broker fails to startup and logs the IllegalArgumentException that contains the information on the index mismatch of the log and the snapshot, it would be helpful to also include the partition id.\r\n\r\n```\r\n[Broker-7-Startup] [Broker-7-zb-actors-2] WARN \r\n        io.camunda.zeebe.broker.system - Aborting startup process due to exception during step Partition Manager\r\n  java.util.concurrent.CompletionException: java.lang.IllegalStateException: Expected to find a snapshot at index >= log's first index 161591, but found snapshot 25. A previous snapshot is most likely corrupted.\r\n```\r\n\r\n<details>\r\n  <summary>Logs</summary>\r\n\r\n  ```\r\n  2023-05-21 14:44:55.018 [Broker-7-Startup] [Broker-7-zb-actors-2] WARN \r\n        io.camunda.zeebe.broker.system - Aborting startup process due to exception during step Partition Manager\r\n  java.util.concurrent.CompletionException: java.lang.IllegalStateException: Expected to find a snapshot at index >= log's first index 161591, but found snapshot 25. A previous snapshot is most likely corrupted.\r\n\t  at java.util.concurrent.CompletableFuture.encodeThrowable(Unknown Source) ~[?:?]\r\n\t  at java.util.concurrent.CompletableFuture.uniApplyNow(Unknown Source) ~[?:?]\r\n\t  at java.util.concurrent.CompletableFuture.uniApplyStage(Unknown Source) ~[?:?]\r\n\t  at java.util.concurrent.CompletableFuture.thenApply(Unknown Source) ~[?:?]\r\n\t  at io.atomix.raft.partition.RaftPartition.open(RaftPartition.java:90) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t  at io.atomix.raft.partition.RaftPartitionGroup.lambda$join$2(RaftPartitionGroup.java:174) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t  at java.util.stream.ReferencePipeline$3$1.accept(Unknown Source) ~[?:?]\r\n\t  at java.util.HashMap$KeySpliterator.forEachRemaining(Unknown Source) ~[?:?]\r\n\t  at java.util.stream.AbstractPipeline.copyInto(Unknown Source) ~[?:?]\r\n\t  at java.util.stream.AbstractPipeline.wrapAndCopyInto(Unknown Source) ~[?:?]\r\n\t  at java.util.stream.AbstractPipeline.evaluate(Unknown Source) ~[?:?]\r\n\t  at java.util.stream.AbstractPipeline.evaluateToArrayNode(Unknown Source) ~[?:?]\r\n\t  at java.util.stream.ReferencePipeline.toArray(Unknown Source) ~[?:?]\r\n\t  at io.atomix.raft.partition.RaftPartitionGroup.join(RaftPartitionGroup.java:175) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t  at io.atomix.primitive.partition.impl.DefaultPartitionService.start(DefaultPartitionService.java:63) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t  at io.camunda.zeebe.broker.partitioning.PartitionManagerImpl.start(PartitionManagerImpl.java:125) ~[zeebe-broker-8.1.9.jar:8.1.9]\r\n\t  at java.util.concurrent.CompletableFuture$AsyncSupply.run(Unknown Source) ~[?:?]\r\n\t  at java.util.concurrent.CompletableFuture$AsyncSupply.exec(Unknown Source) ~[?:?]\r\n\t  at java.util.concurrent.ForkJoinTask.doExec(Unknown Source) ~[?:?]\r\n\t  at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source) ~[?:?]\r\n\t  at java.util.concurrent.ForkJoinPool.scan(Unknown Source) ~[?:?]\r\n\t  at java.util.concurrent.ForkJoinPool.runWorker(Unknown Source) ~[?:?]\r\n\t  at java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source) ~[?:?]\r\n  Caused by: java.lang.IllegalStateException: Expected to find a snapshot at index >= log's first index 161591, but found snapshot 25. A previous snapshot is most likely corrupted.\r\n\t  at io.atomix.raft.utils.StateUtil.verifySnapshotLogConsistent(StateUtil.java:32) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t  at io.atomix.raft.impl.RaftContext.<init>(RaftContext.java:204) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t  at io.atomix.raft.impl.DefaultRaftServer$Builder.build(DefaultRaftServer.java:263) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t  at io.atomix.raft.impl.DefaultRaftServer$Builder.build(DefaultRaftServer.java:237) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t  at io.atomix.raft.partition.impl.RaftPartitionServer.buildServer(RaftPartitionServer.java:185) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t  at io.atomix.raft.partition.impl.RaftPartitionServer.initServer(RaftPartitionServer.java:151) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t  at io.atomix.raft.partition.impl.RaftPartitionServer.start(RaftPartitionServer.java:110) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t  ... 19 more\r\n  2023-05-21 14:44:55.025 [] [main] ERROR\r\n        io.camunda.zeebe.broker.system - Failed to start broker 7!\r\n  java.util.concurrent.ExecutionException: Startup failed in the following steps: [Partition Manager]. See suppressed exceptions for details.\r\n\t  at io.camunda.zeebe.scheduler.future.CompletableActorFuture.get(CompletableActorFuture.java:142) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t  at io.camunda.zeebe.scheduler.future.CompletableActorFuture.get(CompletableActorFuture.java:109) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t  at io.camunda.zeebe.scheduler.FutureUtil.join(FutureUtil.java:21) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t  at io.camunda.zeebe.scheduler.future.CompletableActorFuture.join(CompletableActorFuture.java:198) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t  at io.camunda.zeebe.broker.Broker.internalStart(Broker.java:101) ~[zeebe-broker-8.1.9.jar:8.1.9]\r\n\t  at io.camunda.zeebe.util.LogUtil.doWithMDC(LogUtil.java:23) ~[zeebe-util-8.1.9.jar:8.1.9]\r\n\t  at io.camunda.zeebe.broker.Broker.start(Broker.java:83) ~[zeebe-broker-8.1.9.jar:8.1.9]\r\n\t  at io.camunda.zeebe.broker.StandaloneBroker.run(StandaloneBroker.java:92) ~[camunda-zeebe-8.1.9.jar:8.1.9]\r\n\t  at org.springframework.boot.SpringApplication.callRunner(SpringApplication.java:771) ~[spring-boot-2.7.4.jar:2.7.4]\r\n\t  at org.springframework.boot.SpringApplication.callRunners(SpringApplication.java:755) ~[spring-boot-2.7.4.jar:2.7.4]\r\n\t  at org.springframework.boot.SpringApplication.run(SpringApplication.java:315) ~[spring-boot-2.7.4.jar:2.7.4]\r\n\t  at io.camunda.zeebe.broker.StandaloneBroker.main(StandaloneBroker.java:82) ~[camunda-zeebe-8.1.9.jar:8.1.9]\r\n  Caused by: io.camunda.zeebe.scheduler.startup.StartupProcessException: Startup failed in the following steps: [Partition Manager]. See suppressed exceptions for details.\r\n\t  at io.camunda.zeebe.scheduler.startup.StartupProcess.aggregateExceptionsSynchronized(StartupProcess.java:282) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t  at io.camunda.zeebe.scheduler.startup.StartupProcess.completeStartupFutureExceptionallySynchronized(StartupProcess.java:183) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t  at io.camunda.zeebe.scheduler.startup.StartupProcess.lambda$proceedWithStartupSynchronized$3(StartupProcess.java:167) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t  at io.camunda.zeebe.scheduler.future.FutureContinuationRunnable.run(FutureContinuationRunnable.java:33) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t  at io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t  at io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t  at io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t  at io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t  at io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t  at io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t  Suppressed: io.camunda.zeebe.scheduler.startup.StartupProcessStepException: Bootstrap step Partition Manager failed\r\n\t\t  at io.camunda.zeebe.scheduler.startup.StartupProcess.completeStartupFutureExceptionallySynchronized(StartupProcess.java:185) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t\t  at io.camunda.zeebe.scheduler.startup.StartupProcess.lambda$proceedWithStartupSynchronized$3(StartupProcess.java:167) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t\t  at io.camunda.zeebe.scheduler.future.FutureContinuationRunnable.run(FutureContinuationRunnable.java:33) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t\t  at io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t\t  at io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t\t  at io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t\t  at io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t\t  at io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t\t  at io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) ~[zeebe-scheduler-8.1.9.jar:8.1.9]\r\n\t  Caused by: java.util.concurrent.CompletionException: java.lang.IllegalStateException: Expected to find a snapshot at index >= log's first index 161591, but found snapshot 25. A previous snapshot is most likely corrupted.\r\n\t\t  at java.util.concurrent.CompletableFuture.encodeThrowable(Unknown Source) ~[?:?]\r\n\t\t  at java.util.concurrent.CompletableFuture.uniApplyNow(Unknown Source) ~[?:?]\r\n\t\t  at java.util.concurrent.CompletableFuture.uniApplyStage(Unknown Source) ~[?:?]\r\n\t\t  at java.util.concurrent.CompletableFuture.thenApply(Unknown Source) ~[?:?]\r\n\t\t  at io.atomix.raft.partition.RaftPartition.open(RaftPartition.java:90) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t\t  at io.atomix.raft.partition.RaftPartitionGroup.lambda$join$2(RaftPartitionGroup.java:174) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t\t  at java.util.stream.ReferencePipeline$3$1.accept(Unknown Source) ~[?:?]\r\n\t\t  at java.util.HashMap$KeySpliterator.forEachRemaining(Unknown Source) ~[?:?]\r\n\t\t  at java.util.stream.AbstractPipeline.copyInto(Unknown Source) ~[?:?]\r\n\t\t  at java.util.stream.AbstractPipeline.wrapAndCopyInto(Unknown Source) ~[?:?]\r\n\t\t  at java.util.stream.AbstractPipeline.evaluate(Unknown Source) ~[?:?]\r\n\t\t  at java.util.stream.AbstractPipeline.evaluateToArrayNode(Unknown Source) ~[?:?]\r\n\t\t  at java.util.stream.ReferencePipeline.toArray(Unknown Source) ~[?:?]\r\n\t\t  at io.atomix.raft.partition.RaftPartitionGroup.join(RaftPartitionGroup.java:175) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t\t  at io.atomix.primitive.partition.impl.DefaultPartitionService.start(DefaultPartitionService.java:63) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t\t  at io.camunda.zeebe.broker.partitioning.PartitionManagerImpl.start(PartitionManagerImpl.java:125) ~[zeebe-broker-8.1.9.jar:8.1.9]\r\n\t\t  at java.util.concurrent.CompletableFuture$AsyncSupply.run(Unknown Source) ~[?:?]\r\n\t\t  at java.util.concurrent.CompletableFuture$AsyncSupply.exec(Unknown Source) ~[?:?]\r\n\t\t  at java.util.concurrent.ForkJoinTask.doExec(Unknown Source) ~[?:?]\r\n\t\t  at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source) ~[?:?]\r\n\t\t  at java.util.concurrent.ForkJoinPool.scan(Unknown Source) ~[?:?]\r\n\t\t  at java.util.concurrent.ForkJoinPool.runWorker(Unknown Source) ~[?:?]\r\n\t\t  at java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source) ~[?:?]\r\n\t  Caused by: java.lang.IllegalStateException: Expected to find a snapshot at index >= log's first index 161591, but found snapshot 25. A previous snapshot is most likely corrupted.\r\n\t\t  at io.atomix.raft.utils.StateUtil.verifySnapshotLogConsistent(StateUtil.java:32) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t\t  at io.atomix.raft.impl.RaftContext.<init>(RaftContext.java:204) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t\t  at io.atomix.raft.impl.DefaultRaftServer$Builder.build(DefaultRaftServer.java:263) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t\t  at io.atomix.raft.impl.DefaultRaftServer$Builder.build(DefaultRaftServer.java:237) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t\t  at io.atomix.raft.partition.impl.RaftPartitionServer.buildServer(RaftPartitionServer.java:185) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t\t  at io.atomix.raft.partition.impl.RaftPartitionServer.initServer(RaftPartitionServer.java:151) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t\t  at io.atomix.raft.partition.impl.RaftPartitionServer.start(RaftPartitionServer.java:110) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t\t  at io.atomix.raft.partition.RaftPartition.open(RaftPartition.java:90) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t\t  at io.atomix.raft.partition.RaftPartitionGroup.lambda$join$2(RaftPartitionGroup.java:174) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t\t  at java.util.stream.ReferencePipeline$3$1.accept(Unknown Source) ~[?:?]\r\n\t\t  at java.util.HashMap$KeySpliterator.forEachRemaining(Unknown Source) ~[?:?]\r\n\t\t  at java.util.stream.AbstractPipeline.copyInto(Unknown Source) ~[?:?]\r\n\t\t  at java.util.stream.AbstractPipeline.wrapAndCopyInto(Unknown Source) ~[?:?]\r\n\t\t  at java.util.stream.AbstractPipeline.evaluate(Unknown Source) ~[?:?]\r\n\t\t  at java.util.stream.AbstractPipeline.evaluateToArrayNode(Unknown Source) ~[?:?]\r\n\t\t  at java.util.stream.ReferencePipeline.toArray(Unknown Source) ~[?:?]\r\n\t\t  at io.atomix.raft.partition.RaftPartitionGroup.join(RaftPartitionGroup.java:175) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t\t  at io.atomix.primitive.partition.impl.DefaultPartitionService.start(DefaultPartitionService.java:63) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n\t\t  at io.camunda.zeebe.broker.partitioning.PartitionManagerImpl.start(PartitionManagerImpl.java:125) ~[zeebe-broker-8.1.9.jar:8.1.9]\r\n\t\t  at java.util.concurrent.CompletableFuture$AsyncSupply.run(Unknown Source) ~[?:?]\r\n\t\t  at java.util.concurrent.CompletableFuture$AsyncSupply.exec(Unknown Source) ~[?:?]\r\n\t\t  at java.util.concurrent.ForkJoinTask.doExec(Unknown Source) ~[?:?]\r\n\t\t  at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source) ~[?:?]\r\n\t\t  at java.util.concurrent.ForkJoinPool.scan(Unknown Source) ~[?:?]\r\n\t\t  at java.util.concurrent.ForkJoinPool.runWorker(Unknown Source) ~[?:?]\r\n\t\t  at java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source) ~[?:?]\r\n  ```\r\n</details>\r\n\n",
    "title": "StateUtil.verifySnapshotLogConsistent message does not indicate which partition is affected"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12798",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "There is an in-memory state tracked in `MutablePendingMessageSubscriptionState`. Usage of this is scheduled in tasks through multiple places. This shared mutable state makes it impossible to run these tasks async from the stream processor.\r\n\r\nWe need to make sure that `MutablePendingMessageSubscriptionState` does not share mutable state. \r\n\r\nThis is a blocker for:\r\n- #12302 \n",
    "title": "Don't share mutable state through `MutablePendingMessageSubscriptionState`"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12764",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "<!--\r\nIn case you have questions about our software we encourage everyone to participate in our community via the\r\n- Camunda Platform community forum https://forum.camunda.io/ or \r\n- Slack https://camunda-cloud.slack.com/ (For invite: https://camunda-slack-invite.herokuapp.com/)\r\n\r\nThere you can exchange ideas with other Zeebe and Camunda Platform 8 users, as well as the product developers, and use the search to find answer to similar questions.\r\n\r\nThis issue template is used by the Zeebe engineers to create general tasks.\r\n-->\r\n\r\n**Description**\r\n\r\nWe need to introduce a way to version our Event Appliers. \r\n\r\nThis is required for:\r\n- #11456 \r\n\r\nCurrently, the `Deployment.CREATED` event applier inserts a deployment record into the state. This is used to be able to re-distribute the deployment later in case distribution failed. This deployment record is deleted (cleaned up) by the `Deployment.FULLY_DISTRIBUTED` event applier.\r\n\r\nWith generalized record distribution, we will still write the `Deployment.CREATED` events, but there is no need anymore for this deployment record because it is stored for distribution in a generalized way (see #11456). In addition, we no longer write the `Deployment.FULLY_DISTRIBUTED` event. So, we cannot clean up the state anymore if we would continue to insert a deployment record on `Deployment.CREATED`.\r\n\r\nAfter updating to a version with generalized record distribution, unprocessed deployment distribution commands may exist on the log, and deployment distribution events must be replayed in the same way as they were when they were written. So, in order to stay backward compatible, both the existing Deployment Distribution mechanism and the Generalized Record Distribution need to co-exist. \r\n\r\nTo allow the co-existence of these mechanisms, we need to have a way to determine how the event should be applied. This can be done by versioning our event appliers.\r\n\r\n**Considered alternatives**\r\n\r\n1. **Continue to write the `Deployment.FULLY_DISTRIBUTED` event for the Generalized Record Distribution of deployments**. This would allow us to keep the `Deployment.CREATED` event applier as is, continuing to insert the deployment record into the state. This alternative has some downsides:\r\n- we are inserting the deployment record twice into the state (once redundantly), which is a performance regression (it was only once before).\r\n- the `Deployment.FULLY_DISTRIBUTED` event leaks the distribution responsibility into deployments, which is what we were trying to remove with generalized record distribution.\r\n- there is no possibility to remove the `Deployment.FULLY_DISTRIBUTED` event in future releases, exactly for the problem above. We'd need to change the `Deployment.CREATED` event applier, which is not possible without versioning.\r\n\r\n2. **Replace `Deployment.CREATED` with a `Deployment.CREATED_V2` intent**. Intents are the standard way to change the behavior of processing/applying records. This alternative has some downsides:\r\n- it's not pretty to have a version number in the intent. We could avoid this by carefully thinking about alternative intent names, like `DEPLOYED` but it would never fit the `CREATE` command.\r\n- users might depend on consuming the `Deployment.CREATED` event in an exporter to extract the deployed resources.\r\n\r\n3. **Stop writing the `Deployment.CREATED` event altogether**. The event applier can stay as is, to replay previously written events. This alternative has some downsides:\r\n- each command must be followed up by a follow-up record to indicate that it was processed. A `Deployment.CREATE` command without resources is rejected. Those with acceptable resources will always produce `Process.CREATED` or `Decision.CREATED` events. However, it is not so nice if the `Deployment.CREATE` command does not have `Deployment.CREATED` follow-up event.\r\n- users might depend on consuming the `Deployment.CREATED` event in an exporter to extract the deployed resources.\n",
    "title": "Version event appliers"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12667",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nCurrently, on startup, we scan all segments to detect corruption and to re-build journal index. This result in a high startup duration.\r\n\r\nFor example, in the following experiment, a broker took almost 2 minutes to startup when there were around 60 segments per partition.\r\n![image](https://user-images.githubusercontent.com/1997478/236187198-da42768f-d7f6-4ba7-9c2b-a003a8e95c5c.png)\r\n\r\nA partition can end up with large number of segments usually if the exporting is slow or not making progress. Other unexpected bugs in processing or snapshot can also lead to the situation. Although it is important to fix the root cause, we should also try to reduce the impact of it on startup duration. We have observed that high startup duration make things worse, because during that time the broker is not ready, it may not receive events until it has scanned all segments, and as a result needs longer time to catch up with the leader. If more than one broker is restarted, the cluster is unavailable for a long time. This has resulted in a few incidents in production. \r\n\r\nTo improve this situation, we should remove the scan of all segments during startup. This has the following impact.\r\n1. We cannot detect corruption on startup. But this might be ok, because we will detect it later when the corrupted entry is read. If it is never read, then it would be ok to ignore it. An entry is never read again if it was already replicated to all brokers, exported, and processed, which means it will be compacted soon.\r\n2. We cannot rebuilt the journal index, which means the seeks might be slow. We should verify how much impact it has, and see if we can improve it. Some ideas - a. persist the index b. dynamically build it when we think it is needed. c. build it async in background. \n\n deepthidevaki: I did a [spike](https://github.com/camunda/zeebe/tree/dd-spike-remove-segment-scan) to remove scanning of all entries on startup. To have some integrity checks, in the spike I stored the the position and index of the last entry of a segment in it's segment descriptor. This helps to verify if the segments are ordered correctly, and if there are any missing segments. \r\n\r\nThe startup duration reduced to 30s or lower.\r\n![image](https://user-images.githubusercontent.com/1997478/236194716-ba2c99fb-30c4-4a6c-a489-a79f5a805da4.png)\r\n\n npepinpe: Let's time box that and evaluate the impact of building the index as we go instead of at startup.\n deepthidevaki: I wrote a [jmh benchmark](https://github.com/camunda/zeebe/commit/0fe01e524c989e168e3e755deae0583f97f39206) to evaluate the impact on seek if there is no index. To simulate a journal with no index, indexDensity is set to a high value. The benchmark evaluates the time to seek to the last entry of the segment.\r\n\r\n```\r\n\r\nBenchmark                                       (indexDensity)  (segmentSizeInMB)  Mode  Cnt          Score          Error  Units\r\nSegmentedJournalReaderJmhTest.seekToLast             100                128  avgt    5        502.369 ±       15.029  ns/op\r\nSegmentedJournalReaderJmhTest.seekToLast             100                512  avgt    5       1913.527 ±      161.392  ns/op\r\nSegmentedJournalReaderJmhTest.seekToLast      1000000000                128  avgt    5  122563053.457 ±  9138218.052  ns/op\r\nSegmentedJournalReaderJmhTest.seekToLast      1000000000                512  avgt    5  472341864.454 ± 40436424.711  ns/op\r\n\r\n```",
    "title": "Reduce startup time when there are many segments"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12655",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\nAfter a release we must send a list of fixed issues related to support tickets. For this we look at the `support` label on issues. It's easy to forget to add this label.\n\n**Describe the solution you'd like**\nIntroduce a GitHub action that checks new issues and comments in issues if the text contains `SUPPORT-XXXX`. If it find any the action should add the `support` label.\n\n**Describe alternatives you've considered**\nN/A\n\n**Additional context**\nN/A\n\n\n remcowesterhoud: @abbasadel fyi 🙂 \n megglos: ZDP-Triage:\n- would be great to be done for all C8 teams actually\n- @megglos will take this over as part of the support/engineering collaboration\n",
    "title": "Automatically add `support` label to support related issues"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12418",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "For `main` we run the following : \r\n- Weekly benchmarks (rolling 4)\r\n- E2E tests (weekly)\r\n- QA/Chaos tests (daily) \r\n\r\nFor stable branches we are only running daily chaos tests.\r\n\r\nSince we don't do any additional QA during patch release, we could run atleast a weekly benchmark one per stable branch. In addition we can also run the e2e tests weekly on the stable branch.\n\n npepinpe: Great idea, I would be fine with running the E2E weekly on stable and nothing for the patch releases :+1: This puts a little bit more work on the medic potentially, but I guess that's just putting pressure on us to make sure things are stable ;)\n korthout: ZPA triage: doesn't sound urgent, could be done by a medic during a quiet week.\n megglos: ZDP triage: would be great to get out of the way asap, should be a low hanging fruit.\n\ne2e: more urgent => to simplify the patch release process\nqa: already done\nweekly: to be done, but less urgent",
    "title": "Improve QA of stable branches"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11914",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "We need to be able to redistribute pending command distributions. This will work in a similar way as the [DeploymentRedistributor](https://github.com/camunda/zeebe/blob/main/engine/src/main/java/io/camunda/zeebe/engine/processing/deployment/distribute/DeploymentRedistributor.java).\r\n\r\n**Always**\r\n- Create `CommandRedistributor`\r\n- Remove `@Ignore` in `MultiPartitionDeploymentLifecycleTest.shouldRejectCompleteDeploymentDistributionWhenAlreadyCompleted`\r\n\r\n**Option 1**\r\n- Deprecate `DeploymentRedistributor`, we need to keep it to stay backwards compatible\r\n- See if we can disable the `DeploymentRedistributor` from running if there are no pending deployments\r\n\r\n**Option 2**\r\n- Migrate pending deployment distributions to generalised distribution\n\n remcowesterhoud: Option 1 is the least amount of work. However, it leaves a bunch of unused code in the codebase. With option 2 we don't have this issue. The data should be relatively easy to migrate.\n\nWith option 2 we can remove a few things, but not everything:\n- `DbDeploymentState` is no longer required. After migration there won't be data in these column families. The column families itself will remain as they will be used for the migration.\n- `EventAppliers` and `Processors` must remain. There might still be an old command on the log that isn't processed yet. We have to make sure we stay backwards compatible.\n- `Redistributor` can be removed. This looks into the migrated column families, which should be empty after migration. Instead any redistributing will happen by the new `CommandRedistributor`.\n- **Open questions**\n    - The old processors/appliers are still populating the migrated column families. What do we do here? Can we modify these so they use the generalised way of distributing?",
    "title": "Create a CommandRedistributor"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11661",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "- Switching should be backward compatible\r\n- We don't delete the existing processors for `Deployment` commands\r\n- We don't delete the existing appliers for `Deployment` events\r\n- Switching happens by changing the processor's logic\r\n- ~~We also need to switch the logic in the `DeploymentRedistributor`~~\r\n- `DeploymentCreateProcessor` should be idempotent\r\n\r\nDepends on\r\n- #11660 \n\n remcowesterhoud: Moving this back to the backlog or now as it got deprioritised",
    "title": "Switch `Deployment:Create` processor to new record distribution behavior"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/9245",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nWe should integrate Snyk into our CI to check for dependency vulnerabilities and licensing issues.\r\n\r\nFor licensing issues, refer to https://confluence.camunda.com/pages/viewpage.action?pageId=76485290 to create an appropriate policy.\r\n\r\nFor vulnerabilities, we want a check which fails when introducing vulnerabilities with `Medium` or higher vulnerabilities. To allow for exceptions when the vulnerability does not affect us, you can use a `.snyk` file which you can create/edit with the Snyk CLI.\r\n\r\nWe can use the [Snyk GitHub actions](https://github.com/snyk/actions) to help us out here, as they will be more flexible than the basic GitHub integration.\n\n npepinpe: Hey @megglos, I see the checks are already on PR - is it in testing or is this done? :thinking: \n megglos: Hey @npepinpe ,\r\n\r\nthanks for poking here. As a first iteration I just enabled the PR support from the Web UI to raise awareness.\r\n\r\nWhat is missing is:\r\n- [ ] assess the current license policy in place, haven't digged into it yet, the snyk setup relies within the responsibility of the infosec team now, might be a matter of communication only\r\n- [ ] customize fail conditions, right now the options are based on what is configurable in the [web ui of snyk](https://app.snyk.io/org/team-zeebe/manage/integrations/github) it will fail on any vulnerabilities\r\n- [ ] define how we handle vulnerabilities\n npepinpe: Is it only running PR checks or is it running daily checks? Security vulnerabilities can be found after the fact, so we need both. Last I remember, setting up the daily check via the web UI was very manual and cumbersome :(\n megglos: ZDP-Triage:\n- needs to be updated, @npepinpe will take care of this\n- @npepinpe is already working on this as part of the Snyk work he does currently\n npepinpe: Repurposing this old issue :smiley: \r\n\r\n### Goals\r\n\r\nOne core value we have in terms of DevEx is that we want to automate our workflows as much as possible. If it's not automated, chances are we will either forget, or make an error.\r\n\r\nSo we want to:\r\n\r\n1. Automate vulnerability checks during CI to avoid including known vulnerabilities as early as possible\r\n2. Automate license checking during CI to avoid including commercially problematic licenses such as GPL\r\n3. Continuously monitor our distributed artifacts to be notified of new vulnerabilities \r\n\r\n> **Note**\r\n> In the future, having automated fixes for these would be best, but for now we will rely on our normal dependency update features.\r\n\r\nLet's flesh this out a bit. We want to monitor the following:\r\n\r\n- The HEAD of every supported version branch (aka stable) as the development version for that version\r\n- The HEAD of main as the development version for the next version\r\n- For each supported version, the actual release as the production variant of the version; this includes the Docker image, the Java distribution, the clients, and all modules specified in the Maven BOM.\r\n\r\nWe distinguish between the \"production\" and \"development\" mainly because both can be out of sync, and distinguishing between each is useful as we can then be notified of vulnerabilities  on the production variant, that were automatically fixed via dependabot on the development variant, and require a release.\r\n\r\n### Automation\r\n\r\nThere are three main ways to import projects to Snyk. The Web UI (via the GitHub integration), the CLI, and the REST API. The Web UI is not really automate-able, as it is, well, a UI. So we're left with two options: CLI vs REST API.\r\n\r\n> **NOTE**\r\n> The REST API is an enterprise plan only feature, but we are on one, so as long as we remain on it, all good.\r\n\r\nThe [REST API](https://snyk.docs.apiary.io/#introduction/rest-api) allows us to import projects via one of the integrations (e.g. GitHub). The main advantage there is its tight integration with automatic PR fixes, and automated PR checks. The downside with it is there is no client library for it. We have to manually write our own calls with our own authentication. However, perhaps this is a good use case for our REST connector, and we simply integrate this as part of the Zeebe release process. The REST API allows us to import and delete projects, so we could potentially automate everything through it, including deleting projects once a minor version drops off.\r\n\r\nThe [CLI](https://docs.snyk.io/snyk-cli) is much more straightforward, but is also more limited in what it can do. The CLI lets you test specific projects to automatically obtain SARIF reports about existing vulnerabilities, and also lets you `monitor` your projects. Monitoring will upload a snapshot of your dependency graph (per project) to Snyk, and essentially is the equivalent of the Web UI project import. Snyk will then notify you (via any of the channels) about new vulnerabilities in your dependencies. The advantage of the CLI is it's very easy to integrate into our CI and release process. The downside is that it does not integrate with automated PR checks, fixes, and so on. You can also not delete projects with the CLI, only import them (via the `monitor`) command. Finally, the CLI supports the `.snyk` file, a way to commit your Snyk ignore, exclusion, and trust policies directly with your code. This offers a portable way of propagating ignore rules, which is quite nice, as the web UI does not allow this.\r\n\r\nHere is a sample workflow which would test using the CLI:\r\n\r\n<details><summary>.github/workflows/snyk.yml</summary>\r\n\r\n```yaml\r\nname: Snyk License & Vulnerability Scan\r\n\r\non:\r\n  workflow_dispatch:\r\n    inputs:\r\n      version:\r\n        description: The project version; defaults to the version as defined by the root POM\r\n        required: false\r\n        type: string\r\n      target:\r\n        description: Allows overriding the project target reference directly; defaults to the current branch\r\n        required: false\r\n        type: string\r\n      monitor:\r\n        description: Upload Snyk snapshot instead of test\r\n        required: false\r\n        type: boolean\r\n        default: false\r\n  workflow_call:\r\n    inputs:\r\n      version:\r\n        description: The project version; defaults to the version as defined by the root POM\r\n        required: false\r\n        type: string\r\n      target:\r\n        description: Allows overriding the project target reference directly; defaults to the current branch\r\n        required: false\r\n        type: string\r\n      monitor:\r\n        description: Upload Snyk snapshot instead of test\r\n        required: false\r\n        type: boolean\r\n        default: false\r\n\r\ndefaults:\r\n  run:\r\n    # use bash shell by default to ensure pipefail behavior is the default\r\n    # see https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#exit-codes-and-error-action-preference\r\n    shell: bash\r\n\r\njobs:\r\n  scan:\r\n    name: Snyk Scan\r\n    # Run on self-hosted to make building Zeebe much faster\r\n    runs-on: [ self-hosted, linux, amd64, \"16\" ]\r\n    permissions:\r\n      security-events: write # required to upload SARIF files\r\n    steps:\r\n      - name: Install Snyk CLI\r\n        uses: snyk/actions/setup@master\r\n      - uses: actions/checkout@v3\r\n      - uses: ./.github/actions/setup-zeebe\r\n        with:\r\n          maven-cache-key-modifier: snyk\r\n          secret_vault_secretId: ${{ secrets.VAULT_SECRET_ID }}\r\n          secret_vault_address: ${{ secrets.VAULT_ADDR }}\r\n          secret_vault_roleId: ${{ secrets.VAULT_ROLE_ID }}\r\n      # We need to build the Docker image (and thus the distribution) to scan it\r\n      - uses: ./.github/actions/build-zeebe\r\n        id: build-zeebe\r\n      - uses: ./.github/actions/build-docker\r\n        id: build-docker\r\n        with:\r\n          distball: ${{ steps.build-zeebe.outputs.distball }}\r\n      # Prepares the bash environment for the step which will actually run Snyk, to avoid mixing too\r\n      # much the GitHub Action contexts/syntax and bash itself.\r\n      - name: Build Snyk Environment\r\n        id: info\r\n        run: |\r\n          set -x\r\n          export TARGET=$([[ ! -z '${{ inputs.target }}' ]] && echo '${{ inputs.target }}' || echo \"${GITHUB_HEAD_REF:-${GITHUB_REF#refs/heads/}}\")\r\n          export VERSION=$([[ ! -z '${{ inputs.version }}' ]] && echo '${{ inputs.version }}' || ./mvnw -q -Dexec.executable=echo -Dexec.args='${project.version}' --non-recursive exec:exec 2>/dev/null)\r\n          export VERSION_TAG=$([[ \"${VERSION}\" == *-SNAPSHOT ]] && echo 'development' || echo \"${VERSION}\")\r\n          export LIFECYCLE=$([[ \"${VERSION}\" == *-SNAPSHOT ]] && echo 'development' || echo 'production')\r\n          echo \"SNYK_ARGS=\" \\\r\n            \"--show-vulnerable-paths=all\" \\\r\n            \"--severity-threshold=high\" \\\r\n            \"--org=team-zeebe\" \\\r\n            \"--project-lifecycle=${LIFECYCLE}\" \\\r\n            \"--project-tags=version=${VERSION_TAG}\" \\\r\n            \"--target-reference=${TARGET}\" >> $GITHUB_ENV\r\n          echo \"SNYK_COMMAND=${{ (inputs.monitor && 'monitor') || 'test' }}\" >> $GITHUB_ENV\r\n          echo \"SARIF=${{ (inputs.monitor && '') || 'true' }}\" >> $GITHUB_ENV\r\n          echo \"DOCKER_IMAGE=${{ steps.build-docker.outputs.image }}\" >> $GITHUB_ENV\r\n      - name: Run Snyk\r\n        env:\r\n          SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}\r\n        # The script is set up to scan all distributable artifacts: the projects listed in the bom,\r\n        # the Go client, and the official Docker image.\r\n        # If we add things to the BOM, for example, we should also add them here to the\r\n        # JAVA_PROJECTS variable.\r\n        run: |\r\n          # To avoid exiting on the first failure, we instead flip this to 1 as soon as one of the\r\n          # command fails, and return this at the end; anything non 0 will cause the step to fail\r\n          exitCode=0\r\n          JAVA_PROJECTS=(bom bpmn-model clients/java dist exporter-api gateway-protocol-impl protocol)\r\n          # Remember that when called from a sub-shell, the environment/globals are different\r\n          function output() {\r\n            local sarif=\"$1\"\r\n            local name=\"$2\"\r\n            [ \"${sarif}\" == 'true' ] && echo \"--sarif-file-output=sarif-results/${name}.sarif\"\r\n          }\r\n          # Print out command if debug logging is enabled\r\n          set -x\r\n          snyk \"${SNYK_COMMAND}\" --file=clients/go/go.mod --project-name=clients/go ${SNYK_ARGS} \"$(output \"${SARIF}\" 'go')\" || exitCode=1\r\n          for project in \"${JAVA_PROJECTS[@]}\"; do\r\n          snyk \"${SNYK_COMMAND}\" --file=${project}/pom.xml --project-name=${project} ${SNYK_ARGS} \"$(output \"${SARIF}\" \"${project/\\//-}\")\" || exitCode=1\r\n          done\r\n          snyk container \"${SNYK_COMMAND}\" \"${DOCKER_IMAGE}\" --file=Dockerfile --project-name=camunda/zeebe --exclude-app-vulns ${SNYK_ARGS} \"$(output \"${SARIF}\" 'docker')\" || exitCode=1\r\n          exit \"${exitCode}\"\r\n      # This makes the result of our test available in GitHub Code Scanning (look for the Snyk tools)\r\n      # You can filter by your PR ID, by branch, etc.\r\n      # This step is only executed if we're testing, as otherwise no SARIF files are emitted\r\n      - name: Upload Snyk results to GitHub Code Scanning\r\n        if: ${{ ! inputs.monitor }}\r\n        uses: github/codeql-action/upload-sarif@v2\r\n        with:\r\n          sarif_file: sarif-results/\r\n      - name: Code Scanning summary\r\n        if: ${{ ! inputs.monitor }}\r\n        run: |\r\n          export PR_NUMBER=$(echo $GITHUB_REF | awk 'BEGIN { FS = \"/\" } ; { print $3 }')\r\n          cat >> $GITHUB_STEP_SUMMARY <<EOF\r\n            ## Result Links\r\n            - [Code scanning (PR)](https://github.com/camunda/zeebe/security/code-scanning?query=pr:${PR_NUMBER}+tool:\"Snyk+Container\",\"Snyk+Open+Source\"+is:open)\r\n            - [Code scanning (branch)](https://github.com/camunda/zeebe/security/code-scanning?query=branch:${{ github.ref_name }}+tool:\"Snyk+Container\",\"Snyk+Open+Source\"+is:open)\r\n            - [Snyk projects](https://app.snyk.io/org/team-zeebe/projects)\r\n          EOF\r\n```\r\n\r\n</details>\r\n\r\n\r\n### REST API examples\r\n\r\nImporting a project via the REST API follows this flow:\r\n\r\n- Submit an import job for the projects you want, under a specific integration (e.g. GitHub, Docker Hub) (one REST call)\r\n- Poll the import job status until failed or successful (one REST call on each poll)\r\n- Add the required version tag to each project (one REST call per project)\r\n- Apply the required attributes to each project (one REST call per project)\r\nAs for the REST API\r\nAnd here is a sample REST command (with the credentials omitted obviously) to import a project:\r\n\r\n<details><summary>Import Minor Version</summary>\r\n\r\n```\r\ncurl -XPOST https://api.snyk.io/v1/org/team-zeebe/integrations/<GITHUB_INTEGRATION_ID>/import  \\\r\n  -H 'Content-Type: application/json; charset=utf-8' -H \"Authorization: token $SNYK_TOKEN\" \\\r\n  --data-binary @- <<EOF\r\n  {\r\n    \"target\": {\r\n      \"owner\": \"camunda\",\r\n      \"name\": \"zeebe\",\r\n      \"branch\": \"stable/8.0\"\r\n    },\r\n    \"exclusionGlobs\": \"target,vendor\",\r\n    \"files\": [\r\n      { \"path\": \"protocol/pom.xml\" },\r\n      { \"path\": \"bom/pom.xml\" },\r\n      { \"path\": \"clients/java/pom.xml\" },\r\n      { \"path\": \"clients/go/go.mod\" },\r\n      { \"path\": \"exporter-api/pom.xml\" },\r\n      { \"path\": \"dist/pom.xml\" },\r\n      { \"path\": \"bpmn-model/pom.xml\" }\r\n    ]\r\n  }\r\nEOF\r\ncurl -XPOST https://api.snyk.io/v1/org/team-zeebe/integrations/<DOCKER_INTEGRATION_ID>/import \\\r\n  -H 'Content-Type: application/json; charset=utf-8' -H \"Authorization: token $SNYK_TOKEN\" \\\r\n  -d '{ \"target\": { \"name\": \"camunda/zeebe:8.3.0\" }] }'\r\n```\r\n\r\n</details>\r\n\r\nAnd here's a sample script which would perform the complete import for code projects (i.e. it's missing the Docker import, which would have to be another job):\r\n\r\n<details><summary>snyk-import.sh</summary>\r\n\r\n```shell\r\n#!/bin/bash -eux\r\n\r\n# Secrets to inject\r\nSNYK_TOKEN=\"\"\r\nGITHUB_INTEGRATION_ID=\"\"\r\n\r\nfunction importCodeProjects() {\r\n  curl -s -D - -XPOST \"https://api.snyk.io/v1/org/team-zeebe/integrations/$GITHUB_INTEGRATION_ID/import\"  \\\r\n  -H 'Content-Type: application/json; charset=utf-8' -H \"Authorization: token $SNYK_TOKEN\" \\\r\n  --data-binary @- <<EOF | grep -i '^Location:' | cut -d: -f2- | tr -d \"[:space:]\"\r\n  {\r\n    \"target\": {\r\n      \"owner\": \"camunda\",\r\n      \"name\": \"zeebe\",\r\n      \"branch\": \"stable/8.0\"\r\n    },\r\n    \"exclusionGlobs\": \"target,vendor\",\r\n    \"files\": [\r\n      { \"path\": \"protocol/pom.xml\" },\r\n      { \"path\": \"bom/pom.xml\" },\r\n      { \"path\": \"clients/java/pom.xml\" },\r\n      { \"path\": \"clients/go/go.mod\" },\r\n      { \"path\": \"exporter-api/pom.xml\" },\r\n      { \"path\": \"dist/pom.xml\" },\r\n      { \"path\": \"bpmn-model/pom.xml\" }\r\n    ]\r\n  }\r\nEOF\r\n}\r\n\r\nfunction pollImport() {\r\n  local jobUrl=\"$1\"\r\n  local status=\"pending\"\r\n\r\n  while [ \"${status}\" != \"complete\"  ]; do\r\n    status=$(curl -s ${jobUrl} -H 'Accept: application/json' -H \"Authorization: token $SNYK_TOKEN\" | jq -r '.status')\r\n    if [ \"${status}\" == \"aborted\" ] || [ \"${status}\" == \"failed\" ]; then\r\n      echo \"Import job failed; see ${jobUrl} for more\"\r\n      return 1\r\n    fi\r\n    [ \"${status}\" != \"complete\" ] && sleep 5 # avoid getting throttled\r\n  done\r\n\r\n  return 0\r\n}\r\n\r\nfunction addProjectTag() {\r\n  local projectUrl=\"$1\"\r\n  local version=\"${2:-development}\"\r\n  \r\n  curl -s -XPOST \"${projectUrl}/tags\" \\\r\n    -H 'Accept: application/json' -H 'Content-Type: application/json; charset=utf-8' -H \"Authorization: token $SNYK_TOKEN\" \\\r\n    -d \"{\\\"key\\\": \\\"version\\\", \\\"value\\\": \\\"${version}\\\"}\"\r\n}\r\n\r\nfunction applyProjectAttributes() {\r\n  local projectUrl=\"$1\"\r\n  local lifecycle=\"${2:-development}\"\r\n  \r\n  curl -s -XPOST \"${projectUrl}/attributes\" \\\r\n    -H 'Accept: application/json' -H 'Content-Type: application/json; charset=utf-8' -H \"Authorization: token $SNYK_TOKEN\" \\\r\n    -d \"{\\\"lifecycle\\\": [\\\"${lifecycle}\\\"]}\"\r\n}\r\n\r\nCODE_JOB_URL=$(importCodeProjects \"\")\r\npollImport \"${CODE_JOB_URL}\"\r\n\r\nPROJECT_URLS=$(curl -s ${CODE_JOB_URL} -H 'Accept: application/json' -H \"Authorization: token $SNYK_TOKEN\" | jq -r '.logs[].projects[].projectUrl')\r\nfor project in $PROJECT_URLS; do\r\n  addProjectTag \"${project}\"\r\n  applyProjectAttributes \"${project}\"\r\ndone\r\n```\r\n\r\n</details>\r\n\r\n> **Warning**\r\n> Right now, it seems only Group administrators can add tags/attributes. I'm an organization administrator, but I keep getting a 403. Go figure.\r\n\r\nDeleting projects would be similar and would require us to fetch the list of projects for a specific grouping (e.g. stable/8.0) and delete them.\r\n\r\n### Summary\r\n\r\nAll in all, neither CLI or REST API is an ideal approach. And I still need to flesh out which approach is best for what we want to achieve.\n npepinpe: ## Workflow for CLI integration\r\n\r\nWith the CLI integration, as mentioned, automated PR checks would be more effort to implement. We would have to manually implement a check that compares the base branch results to the target branch result, and extract only the information about new vulnerabilities. Otherwise, we would be failing a PR based on vulnerabilities that are not related to the PR. This could cause tons of PRs to fail simultaneously for the same cause.\r\n\r\nAs such, we would drop checking every PR. Instead, the workflow would be the following:\r\n\r\n- For every stable branch, on every SNAPSHOT deploy, we would `monitor` this version. This would be the `development` variant of that particular supported version. For `main`, this would be our development head.\r\n- On every release (except alphas?), we would also `monitor` the release version, under the appropriate group. This would be our `production` variant for that version.\r\n- The Slack integration would send us notifications about new CVEs every day for each of these variants, so we would at most figure out a PR introduced a vulnerability after 24h (or 72h for a whole weekend).\r\n\r\nThe automation would be done entirely via GitHub actions by hooking into our release workflow as a post-release task.\r\n\r\n> **Warning**\r\n> Deleting a project (or multiple projects) is not do-able via CLI, so we would anyway have to rely on the REST API for that.\r\n\r\nThe advantages of the CLI approach is that it's very easy to implement (one liners), and very easy to understand/maintain. Additionally, we can make use of the `.snyk` file to check in our Snyk configuration, which includes ignore rules, trust policies, etc. The downsides are that we can't use the automated PR checks/fixes available in the GitHub integration, and we can't delete projects (so we still need either a manual task to do so, or using the REST API).\r\n\r\n## Workflow with the REST API\r\n\r\nFor the REST API, integration would be done via Zeebe, using our release process. A new BPMN process would be created to model the process of importing Snyk projects, and deleting the previous, now unsupported version.\r\n\r\n> **Note**\r\n> Ideally we can mostly use the REST connector, but I believe for the import job it might not be possible, as the result is not returned via the response by via the `Location` header.\r\n\r\nHere is what the import process could look like:\r\n\r\n![image](https://github.com/camunda/zeebe/assets/43373/a725a0d2-17d1-45fc-be27-75b0c1d2b2fc)\r\n\r\n> **Warning**\r\n> Currently there are some permission issues with tagging, but I will try to sort this out with our admins.\r\n\r\nWe would hook into the main release process and add the above as a post-release call activity. Additionally, we would also add a manual task which would trigger the deletion (via another call activity) to delete the previous, now unsupported minor version.\r\n\r\n> **Note**\r\n> Whether we pick CLI or REST, we likely need to do the deletion via REST anyway.\r\n\r\nSince this would import projects using the GitHub and Docker Hub integrations, there is no need to run something on every SNAPSHOT push. Instead, importing the projects once is enough, and Snyk will refresh them regularly. Additionally, the automated PR checks will work with the given projects.\r\n\r\n> **Warning**\r\n> Project names are mangled often in the PR check, seems it cannot handle which base branch to use. But it will scan the appropriate project, so I think it's OK.\r\n\r\nAdvantages with this approach is that it's easy to understand via the BPMN, and all tasks are confined to the release process. We also get access to automated PR checks and fixes. Downside here is fiddling with a poorly documented API, there is no available client, and it's harder to test locally obviously as you need to use the deployed Zeebe process and workers. We also can't make use of the `.snyk` file, meaning that ignores and the likes are manual and not transparent :(",
    "title": "Use Snyk to check for license issues or dependency vulnerabilities"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/8846",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nAs we use SpotBugs ourselves, and the FindBugs project is dormant, we should replace any and all usage of its annotations with SpotBugs. This means replacing:\r\n\r\n```xml\r\n<dependency>\r\n  <groupId>com.google.code.findbugs</groupId>\r\n  <artifactId>jsr305</artifactId>\r\n</dependency>\r\n```\r\n\r\nWith:\r\n\r\n```xml\r\n<dependency>\r\n  <groupId>com.github.spotbugs</groupId>\r\n  <artifactId>spotbugs-annotations</artifactId>\r\n</dependency>\r\n\r\n<dependency>\r\n  <groupId>net.jcip</groupId>\r\n  <artifactId>jcip-annotations</artifactId>\r\n</dependency>\r\n```\r\n\r\nThe source here is the SpotBugs documentation itself. And as we do rely on SpotBugs, I would encourage us exploring making better use of their annotations to leverage the tool.\r\n\r\nNOTE: this might also mean replacing usages of any other `javax.annotation` or JSR305 implementations with SpotBugs's own, if possible. I don't think we use any other, but I could be wrong.\n\n menski: Potential topic for the weeks around the next release for someone to pick-up, no urgency right now, therefore I removed it from the board.",
    "title": "Replace usages of JSR305 from FindBugs with SpotBugs annotations"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13058",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Documentation"
    },
    "gitHubIssueText": "As we're switching the Deployment Distribution over on the Generalized Record Distribution (aka Command Distribution), we should document how the logic of distribution works. This description should be general, but also include a section on resource deployments.\n\n remcowesterhoud: I question the value of including a section on resource deployments. Distribution is generic, it will work the same for all commands. I don't see a need to touch upon specific cases.\r\nI could add a full example and use resource deployment for this, but other than that I wouldn't go deeper into it.",
    "title": "Document new Deployment Distribution logic"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/13168",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/13069",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4794",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4551",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4708",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4621",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4706",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4677",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4647",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4840",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4286",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4793",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4755",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4775",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4710",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4760",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4762",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4763",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/3962",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4719",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4847",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4828",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4812",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4827",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4831",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4824",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4823",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4822",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4821",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4820",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4809",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4790",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4803",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4796",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4788",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4185",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4797",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4795",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4791",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4633",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4468",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4601",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4656",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4785",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4773",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4553",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4759",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4756",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4752",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4774",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4757",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4749",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4750",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4751",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4743",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4731",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4728",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4732",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4742",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4733",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4727",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4735",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4729",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4724",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4716",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4715",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4714",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4713",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4705",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4679",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4666",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4639",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4703",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4683",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4698",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4687",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4676",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4651",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4684",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/3753",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4650",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4649",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4665",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4645",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3139",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3098",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3095",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3039",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3180",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3176",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3125",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3105",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3120",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3115",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3081",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3099",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3061",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3091",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3080",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3069",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3085",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3084",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3083",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3082",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3079",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3077",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3073",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3070",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3182",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3159",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3166",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3163",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3162",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3161",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3155",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3132",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3141",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3140",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3129",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3131",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3123",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3110",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3117",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3121",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3101",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3113",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3114",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3111",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3109",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3103",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3087",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3100",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3092",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3067",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3068",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3064",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3078",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3088",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3075",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3076",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3036",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3029",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1878",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1837",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1830",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1823",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1827",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1818",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1816",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1791",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1789",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1749",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1912",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1899",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1907",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1890",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1881",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1889",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1886",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1879",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1876",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1874",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1814",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1825",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1824",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1804",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1780",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1775",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1768",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1758",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1757",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1756",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1737",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1904",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1911",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1909",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1910",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1896",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1906",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1897",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1902",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1877",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1895",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1893",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1894",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1891",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1892",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1849",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1883",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1888",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1882",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1875",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1865",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1863",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1864",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1861",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1862",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1856",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1854",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1852",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1834",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1850",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1839",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1829",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1838",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1833",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1836",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1832",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1826",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1828",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1820",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1822",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1808",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1806",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1807",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1805",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1798",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1795",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1800",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1533",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1796",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1794",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1781",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1782",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1792",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1777",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1778",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1776",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1759",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1772",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1769",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1764",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1765",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1761",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1747",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1755",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1754",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1753",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1752",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1735",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha3",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1736",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/8263",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nCame up in a recent [incident review ](https://docs.google.com/document/d/1E03kB3UCfM3l8X0xU7RjzqvOnX8dk4GtKAmH5-_vY60/edit) that it would be nice to see whether instances are blacklisted or not, maybe also the rate of blacklisting.\r\n\r\nI realized that we added a while ago the metrics for that, but never added this to the dashboard, see https://github.com/camunda-cloud/zeebe/pull/6715. \r\n\r\n**Describe the solution you'd like**\r\n\r\nEither show a graph of blacklisted instance count or just some indication whether instances are blacklisted.\r\n\r\nI had yesterday a **hard time**  to find a good visualization, since the count is only exported if there was a new blacklisted instance. If the pod is restarted the metric is not exported. _It might make sense to refill the metric on restart_ OR we need to work with it, but this would mean we can only show limited data, see below.\r\n\r\n**Example A - Show an indication that instances are blacklisted**\r\n\r\n![gauge-blacklisted](https://user-images.githubusercontent.com/2758593/143187841-3f9d4a4c-50b9-49de-95b0-43650d479103.png)\r\n![gauge-blacklisted-general](https://user-images.githubusercontent.com/2758593/143187843-2c6c629b-0687-463a-82de-970f39a4e372.png)\r\n\r\nIn general I like this, since it shows directly whether there is something wrong.\r\nThe problem here is if the time frame is smaller (were no instances are blacklisted) than this indication is green. :-1: \r\n\r\n**Example B - Graph**\r\n\r\nShowing a graph is not that fruitful, since as described above the metric is not always exported. \r\n![graph-blacklisted](https://user-images.githubusercontent.com/2758593/143187948-f6d36e3e-1b15-48a6-82ba-872d58592ca5.png)\r\n\r\nShowing zero for null values\r\n![graph-zero-blacklisted](https://user-images.githubusercontent.com/2758593/143188265-1b85b331-2306-40d2-94df-0132255c767c.png)\r\n\r\n**Example C - Rate**\r\n\r\nThe rate of such metric is also not really useful, since the change is too rare.\r\n\r\n![graph-rate-blacklisted](https://user-images.githubusercontent.com/2758593/143188214-3b44348e-3b0c-4464-85ed-5ccbb62ed60f.png)\r\n\r\n\r\n**Example D - Table**\r\n![table-blacklisted](https://user-images.githubusercontent.com/2758593/143188772-b439d832-b192-47ed-8b6e-dbe31842a7b9.png)\r\n\r\nOther alternative would be to show in a table the recent count, but this is also very limited (to the time frame) and other\r\n\r\n**Describe alternatives you've considered**.\r\n\r\nIdeally we would export the metric always, this would simplify things enormous. Then we can choose better one or more of the possible visualization from above.\r\n\r\n**Additional context**\r\n\r\nBlacklisting always shows that something problematic happened in the process execution (processing). An exception was thrown during the execution, which mostly an indication of a bug. \r\n\r\nI would like to find a good way how we can visualize it and hope someone has some comments, opinion or ideas.\r\n\r\n\\cc @pihme\r\n\n\n pihme: What about exposing the metric as a rate and visualizing it with a heat map? \n npepinpe: Prioritized as planned for now under the assumption we won't work on a better long term alternative to blacklisting in the next quarter, and this will already be an improvement in terms of visibility. Before opening a PR for this, please discuss and decide on the visualization/metric that we want.\n Zelldon: The question for me is really what we want to achieve with the metric.\r\n\r\nDo we want to know really how many (exact) are blacklisted? Then Prometheus might be not the best fit, but if we just want to have an indicator that SOMETHING is blacklisted then this could work:\r\n \r\n![blacklist](https://user-images.githubusercontent.com/2758593/148549724-9548b692-0f78-44a6-9783-0692e0bd9919.png)\r\n\r\nHere we could change the colors and shown values to something like Blacklisted (if x > 0) and nothing (if x <= 0).I think this is similar what I have shown above with example A. This would potentially already help.\r\n\n pihme: What about exposing the metric as a rate and visualizing it with a heat map?\n Zelldon: This wouldn't help since the metric is not exported all the time. Plus what should tell me the heatmap would be the question?\n pihme: I would expect to see a change in the blacklisting rate.\r\n\r\nKinda like here:\r\n![image](https://user-images.githubusercontent.com/14032870/148551718-06e323ce-d576-4e46-bd78-130a9e97a4d5.png)\r\n\r\nIf it is not exported for some time, I will just see a black column, but I can always make the time scale wider and then I should see data pretty much for all the time, because each export would be aggregated in the rate (I hope).\r\n\r\nThen I could look at a big enough time scale and if my blacklisting suddenly jumps from 2 per day to 200 per day, that would be my signal. And if the jump is correlated to e.g. the point there was an update or a pod restart, that would also be interesting.\r\n\r\nThis is all speculation though, one would have to see what it actually looks like.\n Zelldon: Thanks @pihme \r\n\r\nyeah so it would look like this\r\n\r\n15 mins\r\n![heatmap](https://user-images.githubusercontent.com/2758593/148553374-792367bc-f419-43cf-a476-70b9f9244280.png)\r\n\r\n90 days\r\n![heatmap2](https://user-images.githubusercontent.com/2758593/148553377-ae5ce979-9e40-4134-a8c1-76a636c77043.png)\r\n\r\nProblem is that we not store data long enough on our prometheus server. So there can be data deleted which contained some blacklisting, which is why I ask what we want to see. The real count or just an indication all in all I think our current metric doesn't work well. Ideally we would report always or on bootstrap with the current value of blacklisted instances.\r\n\n pihme: Thanks @Zelldon. Yeah I regularly forget we forget data. I think 90 days would be good enough, and yes, the visualization looks like what I had in mind. But if the history is 30 days or less, it becomes less useful. So I see your point.\n Zelldon: I think the best would be as described as alternative above:\r\n\r\n> **Describe alternatives you've considered.**\r\n> Ideally we would export the metric always, this would simplify things enormous. Then we can choose better one or more of the possible visualization from above.\r\n\r\nThis means at least on restart we export the metric at least once. \n Zelldon: @remcowesterhoud since you mentioned to me that you had to look at a cluster with zdb to find out whether something was blacklisted might be useful for you to just have this in the metrics :) \n menski: Right now we don't see value to implement this, we have other ways to identify blacklisted instances, which also allow us to see the corresponding process instance keys, i.e. logs or zdb.\n\nHopefully at one point we can get rid of blacklisting\n korthout: ZPA triage:\r\n- we want to gain more insights into the frequency of blacklisting before choosing to replace the concept\n remcowesterhoud: Attaching this to the \"Blacklisting replacement\" as having insights in these metrics may be useful for this epic.\n Zelldon: Reopening since we still need panels on the dashboard. \n koevskinikola: ZPA triage:\r\n- @Zelldon you mentioned [HERE](https://github.com/camunda/zeebe/pull/12606#issuecomment-1528862192) that you'll add the Grafana dashboards. Is this still your plan, or ZPA should do something from our side?\r\n  - ZPA also has this issue marked as `upcoming` so we would do it soon either way.",
    "title": "Show blacklisting in the Grafana Dashboard"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12796",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nWhen pushing a job fails, we should trigger the `JobYieldProcessor` to make the related job available for long polling.\r\n\r\n**Describe the solution you'd like**\r\n`JobIntent.YIELD` command should be appended when job push fails. This will be achieved through registering the error handler to `RemoteJobStreamErrorHandlerService` inside `JobStreamServiceStep`.\r\n\r\n**Describe alternatives you've considered**\r\n/\r\n\r\n**Additional context**\r\nBlocked by:\r\n* https://github.com/camunda/zeebe/issues/12541\r\n\n",
    "title": "Provide Error Handler implementation for Job Streamer"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12793",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\n\r\nWe can configure a raft request timeout that is applicable to all requests send between raft leaders and followers. InstallRequest send snapshot files which are considerably larger than other raft requests. As a result sometimes it takes longer to send the request and get a response, especially on networks with higher latency between brokers. This can result in timeout exception and the snapshot replication will have to restart from the beginning (See #11496). \r\n\r\nTo workaround, we can increase raft request timeout. But increasing this can affect how fast failures are detected, how fast requests are retried etc. So it would be better to be able to set a higher request timeout just of InstallRequest.\r\n\r\n**Describe the solution you'd like**\r\n\r\nExpose a configuration to set request timeout for InstallRequest. This should be different from the existing raft request timeout configuration.\r\n\r\n**Describe alternatives you've considered**\r\n\r\n- Fix #11496. This will not solve the issue, but reduces the impact if the request timesout.\r\n- Instead of sending the files as it is, split into to smaller requests. This would be a better solution, but requires more effort. https://github.com/camunda/zeebe/issues/12795 \r\n\r\n**Additional context**\r\nRelated to https://jira.camunda.com/browse/SUPPORT-16901 \r\n\n\n megglos: Triage:\n- it allows us to provide a better workaround in cases where the snapshot transmission takes a longer time (depending on the size of a single snapshot chunk up to 64MB) => we could just increase the timeout for installs but not everything\n- me moved it into ready already to get to it asap for the next patch (we have to do it next week)",
    "title": "Allow configuring request timeout for InstallRequest"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12575",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\n\r\nI think that it is not a good idea to collect snapshot files in the list. \r\nCheck it here: https://github.com/camunda/zeebe/blob/02d1df8ac1b6f5484f1e9d4c1f19c8a5712176b1/snapshot/src/main/java/io/camunda/zeebe/snapshots/impl/SnapshotChecksum.java#L41. \r\nBecause, after a while, the amount of snapshot files should grow. So, we could instead call the `.forEachOrdered` method to calculate the snapshot. I will provide the PR soon to see this in action and benchmark this.\r\n\r\n**Describe the solution you'd like**\r\nWe should call `Stream.forEachOrdered` instead of collecting snapshots `File`s in the list.\r\n\r\n**Describe alternatives you've considered**\r\nWe could use `Stream.forEach` but as I can understand the order is important, so we shouldn't do this.\n\n aivinog1: @Zelldon Hello 👋 \nI think that this could be addressed to the ZDP Team (sorry if I am wrong 😅)\nCould we assign this to someone, please? In our load test environment, we see a little degradation in creating snapshots and we are suspecting this part of the code.\nThank you in advance 🙂\n Zelldon: Looks like @deepthidevaki will look into your PR, thanks for providing it :)",
    "title": "Improve the traversing of snapshot files"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12548",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nI understand that [the Actor metrics](https://github.com/camunda/zeebe/blob/a3d2002a0eb74cc701572b4e02abfeb16d6a48a8/scheduler/src/main/java/io/camunda/zeebe/scheduler/ActorMetrics.java) are in the experimental phase, but it would be nice if there exists the Actor dashboard out of the box 🙂\r\n\r\n**Describe the solution you'd like**\r\nA separate file, called, for example, actor.json in [the Grafana dashboards](https://github.com/camunda/zeebe/tree/a3d2002a0eb74cc701572b4e02abfeb16d6a48a8/monitor/grafana) folder.\r\n\r\n**Describe alternatives you've considered**\r\nAdd these metrics to the existing dashboard, for example, [zeebe.json](https://github.com/camunda/zeebe/blob/a3d2002a0eb74cc701572b4e02abfeb16d6a48a8/monitor/grafana/zeebe.json). But I think that it would be more convenient if we have this in a separate dashboard (some users may decide to not enable these metrics thus the dashboard is not needed for them)\n\n megglos: ZDP-Triage:\r\n- seems like a good first issue to pickup during an upcoming onboarding",
    "title": "Provide Grafana Dashboards for the Actor metrics"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12541",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThe following `Processor` classes are able to push an activated job to a client.\r\n\r\n- [JobWorkerTaskProcessor](https://github.com/camunda/zeebe/blob/319813a684325c97556fb59013eb8b88ea27b3f2/engine/src/main/java/io/camunda/zeebe/engine/processing/bpmn/task/JobWorkerTaskProcessor.java#L54) / [BpmnJobBehavior](https://github.com/camunda/zeebe/blob/4d46a4947e6d3ac72cb4e0af324f4c978b591989/engine/src/main/java/io/camunda/zeebe/engine/processing/bpmn/behavior/BpmnJobBehavior.java#L81-L92)\r\n- [JobTimeOutProcessor](https://github.com/camunda/zeebe/blob/ad1d5c92a3d4d6009da3a9d968238b83b9dd5c5c/engine/src/main/java/io/camunda/zeebe/engine/processing/job/JobTimeOutProcessor.java#L20)\r\n- [JobFailProcessor](https://github.com/camunda/zeebe/blob/813c86f780f54496fcd089810704c1a4e4958141/engine/src/main/java/io/camunda/zeebe/engine/processing/job/JobFailProcessor.java)\r\n- [JobRecurProcessor](https://github.com/camunda/zeebe/blob/813c86f780f54496fcd089810704c1a4e4958141/engine/src/main/java/io/camunda/zeebe/engine/processing/job/JobRecurProcessor.java)\r\n- [ResolveIncidentProcessor](https://github.com/camunda/zeebe/blob/18dd3c5e8df9bd2164e4d0fc73f5429c9d38b05c/engine/src/main/java/io/camunda/zeebe/engine/processing/incident/ResolveIncidentProcessor.java)\r\n\r\n**Describe the solution you'd like**\r\nThe `BpmnJobActivationBehavior` class is able to use the `JobStreamer` API to push jobs. The following steps need to be performed:\r\n\r\n- [x] For an available `JobStream` get `JobActivationProperties`\r\n- [x] Set `deadline` for `JobRecord` (using `JobActivationProperties`)\r\n- [x] Set `variables` for `JobRecord` (using `JobActivationProperties`) \r\n- [x] Set `worker` for `JobRecord` (using `JobActivationProperties`)\r\n- [x] Activate job using a `JobBatchRecord`/`JobBatchIntent.ACTIVATE`\r\n- [x] Push `JobRecord` on the `JobStream` through a `SideEffectProducer`\r\n\r\n**Describe alternatives you've considered**\r\n/\r\n\r\n**Additional context**\r\nBlocked by:\r\n* https://github.com/camunda/zeebe/issues/12083\r\n\n",
    "title": "Jobs are pushed from relevant processors"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12539",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\n\r\nCreate a processor to process `ProcessInstanceBatch.TERMINATE` commands.\r\n\r\n- Add a method to the BpmnStateBehavior to get a defined amount of child elements, starting at a specific key.\r\n- Create the processor\r\n- Write a `ProcessInstance.TERMINATE` command for each of the element instance keys in the list\r\n- If the `index` is available:\r\n    - Get the next `BATCH_SIZE` + 1 child instances of the `batchElementInstanceKey`\r\n    - Create a new `ProcessInstanceBatch` record\r\n    - Write a new `ProcessInstanceBatch.TERMINATE` command\r\n- If no `index` is available:\r\n    - Have a 🍪 Nothing else to do\n",
    "title": "Create `ProcessInstanceBatch.TERMINATE` processor"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12538",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\n\r\nContainer elements (process, subprocess, etc.) all terminate their child instances when `onTerminate` is called. For all these elements this method calls the `BpmnStateTransitionBehavior#terminateChildInstances` method. This method needs to be changed to make use of the new `ProcessInstanceBatch` command with the `TERMINATE` intent.\r\n\r\n- Modify the `BpmnStateTransitionBehavior#terminateChildInstances `:\r\n    - Create a `ProcessInstanceBatch` record\r\n        - `batchElementInstanceKey` will be the key of the container element\r\n        - `index` will be empty as this is the first batch command\r\n    - Write the a `ProcessInstanceBatch.TERMINATE` command using the created record\n",
    "title": "Use the `ProcessInstanceBatch Command` when terminating container elements"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12085",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\nA job pushed to the `JobStreamer` API may fail to be handed over to the client due to client failure.\r\n\r\n## AC\r\n* A new `JobIntent.YIELD` intent is added.\r\n* A new `JobYield` processor is added. The processor will perform logic similar to the `JobFailProcessor`, i.e. make a job `ACTIVATABLE` again.\r\n\r\n## Additional context\r\n* The implementation of this issue should then be used in the job push `ErrorHandler` implementation.\n",
    "title": "Job Yield Processor is implemented to be used for Job Push fallback"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/10031",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThe BPMN symbol for the inclusive gateway is supported with\r\n- #9747 \r\n\r\nHowever, only the diverging behavior was added. Users may want to use the converging behavior as well.\r\n\r\n**Describe the solution you'd like**\r\n- remove the validation that restricts the number of incoming sequence flows to an inclusive gateway to max 1.\r\n- only activate the inclusive gateway when (see discussion https://github.com/camunda/zeebe/pull/9747#discussion_r925770530):\r\n  - there are no active children of its flow scope instance (we can check this already),\r\n  - or if all incoming sequence flows were taken at least once (we can check this already),\r\n  - or if no path can be found from any of the active children to the inclusive gateway (but we can't check this yet).\r\n\r\n**Describe alternatives you've considered**\r\nUsers can use a combination of parallel and exclusive gateways to build similar joining logic.\r\n\r\n**Additional context**\r\nSpec: https://www.omg.org/spec/BPMN/2.0.2/PDF\r\n\r\nProduct-Hub:\r\n- https://github.com/camunda/product-hub/issues/364\r\n\n\n lzgabel: Hi @remcowesterhoud. I'm working on this recently, please assign this task to me. Thanks. ❤️\n lzgabel: Hi @remcowesterhoud. Sorry to bother you, :bow:  because I'm currently implementing this feature, and I need to get all child instances in the scope, but when I rebase the changes on top of  `main`, I found that you removed this `BpmnStateBehavior#getChildInstances` method by this [commit](https://github.com/camunda/zeebe/pull/12604/commits/0f3f458221a72b2fae27e9a1f5bc7dad890c4395), so I will add it back.\n remcowesterhoud: > Hi @remcowesterhoud. Sorry to bother you, :bow:  because I'm currently implementing this feature, and I need to get all child instances in the scope, but when I rebase the changes on top of  `main`, I found that you removed this `BpmnStateBehavior#getChildInstances` method by this [commit](https://github.com/camunda/zeebe/pull/12604/commits/0f3f458221a72b2fae27e9a1f5bc7dad890c4395), so I will add it back.\n\nIt was unused 😄\n\nOf course, go ahead and put it back if you need it!\n remcowesterhoud: @koevskinikola assigning you on the issue, so it's clear on our board you are reviewing it\n aisong: I'm really looking forward to this feature. I wonder when it will be released?\n lzgabel: Hi @aisong. This feature is available after version `8.3.0-alpha2`. It also brings another problem, which I am currently solving: #13070 \r\n\r\nBTW. `8.3.0` will be released this October.\n aisong: > Hi @aisong. This feature is available after version `8.3.0-alpha2`. It also brings another problem, which I am currently solving: #13070\r\n> \r\n> BTW. `8.3.0` will be released this October.\r\n\r\nOK,thank you. I'll be waiting for your good product.\n jschulenklopper: Looking forwards to this. This is related to the error message in the Modeler that reads \"An <Inclusive Gateway> with more than one incoming <Sequence Flow> is not supported by Camunda 8.2\", right? And delivery of this feature will deprecate the warning that's on https://docs.camunda.io/docs/components/modeler/bpmn/inclusive-gateways/?\r\n\r\nAnd for the modeller, the missing feature will be delivered per https://github.com/camunda/camunda-modeler/issues/3613 ?\r\n\r\n\n korthout: Great to hear you're all excited about this feature 🚀 \r\n\r\n@jschulenklopper That's all correct! You can already tryout the feature in `8.3.0-alpha2`, but Modeler does not take this into account yet (which is why it raises the warning). As @lzgabel mentioned, this is planned to be fully available in `8.3.0` (release planned for October 2023)\n korthout: Due to several limitations, we had to revert the implementation of the converging inclusive gateway:\r\n- https://github.com/camunda/zeebe/issues/13640\r\n\r\nTherefore, I'm re-opening this issue.\n lzgabel: Hi @korthout. I'll continue to look into this issue in the next few days. If you have a better solution, please let me know. 🙇 \n korthout: @lzgabel Sorry, I haven't spent time on this topic in a while, and I don't yet have ideas other than those that I wrote [here](https://github.com/camunda/zeebe/issues/13070#issuecomment-1649387794). So, we need some way to store in the state that a sequence flow is active and expect to process a command to activate the target element at some point in the future. If you want to work on this feature, I think we need to tackle this part first.",
    "title": "Support BPMN Converging Inclusive Gateway"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/2890",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nAt a multi-instance activity, I can define the input collection to iterate over. If the collection holds a large number of elements, the broker might fail to spawn the inner instances. Such a case can only be fixed, by manually decreasing the collection variable.\r\n\r\n**Describe the solution you'd like**\r\n* I can spawn as many instances as defined by the input collection\r\n* instead of spawning the instances all at once on activating the body (current behavior), spawn the instances step-wise until all instances are created (e.g. spawn 10 instances and write a record to spawn the next 10)\r\n* the instance spawning can be interrupted by an event or a terminate command\r\n\r\n**Describe alternatives you've considered**\r\n* increasing buffers and record max length - see #2880 \r\n* splitting the creation into multiple steps (e.g. loop or nested multi-instance) or control it externally -  \r\n\r\nRelates to\r\nhttps://jira.camunda.com/browse/SUPPORT-16653\r\nhttps://jira.camunda.com/browse/SUPPORT-16549\r\n\n\n saig0: Using Zeebe 0.21.0, we can execute a multi-instance activity with an input collection of ~ 5.000 elements (depending on the variables). By increasing the max message size, we could process even larger collections. \r\n\r\nSince this is ok for now, we will postpone the issue.\n npepinpe: Just a side note, let's postpone at least until the new engine is done (i.e. at least Q2 2021). I can see us working on reducing the max message size next year, so we will need to implement this kind of \"chunking\" - obviously can't guarantee, but it sounds plausible to me.\n saig0: **Update:** I tested it again with version `1.0.0-alpha7`. We can now iterate over an input collection with up to ~12.000 elements :tada: \n korthout: Marking priority as `later` because 'multi-instance for large collections' is not the main concern for the process automation team right now. However, we should probably work on this when we work on\r\n- #8687\r\n\r\nNote that the impact of this bug is reduced once we've resolved\r\n- #5221\r\n\r\nPlease comment if you think this should have a higher priority.\n epollum: Hello team, I want to mention that we have a customer who requires the ability to use large multi-instance activities. Please see https://jira.camunda.com/browse/SUPPORT-16499\r\nAnd https://github.com/camunda/zeebe/issues/11355\r\n\r\nThank you!\n daniel-ewing: Hi team, here is another one: https://jira.camunda.com/browse/SUPPORT-16653\n npepinpe: And another: https://jira.camunda.com/browse/SUPPORT-16549\n abbasadel: Team meeting: we changed the priority to \"Upcoming\" to pick this up when we have time\n felix-mueller: Hey @remcowesterhoud \r\nI saw you worked on this item, there is one more item in the backlog: https://github.com/camunda/zeebe/issues/8687\r\n\r\nCould you perhaps elaborate if this is fixed now as well or is there a case which is not covered? \r\nCould you perhaps explain which case is not covered for #8687?\r\n\r\nThanks\r\nFelix\n remcowesterhoud: @felix-mueller from what I understand after reading #8687 it is a different issue. This one had to do with instance banning and the original author of the issue [explicitly mentioned](https://github.com/camunda/zeebe/issues/8687#issuecomment-1026023644) his instances weren't getting banned.\r\n\r\nWhether this is fixed, I am not sure as I'm struggling to understand what is happening in the other issue 😅 @korthout do you have any ideas? It reads to me like it's more about job activation than multi-instance input collections.\r\n\r\nIf it's stale we can consider closing it and seeing if it ever occurs again.\n korthout: @remcowesterhoud I've had a look. IMO, that issue is not specifically related to multi-instance, but rather to activating many jobs with large variables in the Go client.\r\n\r\nI've provided my thoughts on that issue [here](https://github.com/camunda/zeebe/issues/8687#issuecomment-1665950474)",
    "title": "I can spawn inner instances for a large input collection"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12780",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n```\r\nERROR 2023-05-16T00:30:04.104402170Z [resource.labels.containerName: zeebe] Unexpected error on writing CREATE command Failed to write request to logstream\r\n```\r\n\r\nThe error is from CommandAPIHandler when it tries to write a user request to the leader's logstream. This happened while the leader is transition to follower, and the logstream has already closed. Before this error we see that Sequencer rejects the record because it is closed. \r\n\r\nThis is a new error message introduced in https://github.com/camunda/zeebe/pull/12676. Previously this error was ignored. So we never got the error message.\r\n\r\n[logs](https://console.cloud.google.com/errors/detail/CNy21ZD_7pnW3AE;service=zeebe;time=P7D?project=zeebe-io)\r\n\r\n**Expected behavior**\r\n\r\n- Reduce the log level to warn/debug\r\n- logstream#tryWrite should return specific error code instead of -1, and use that to log more meaningful message. \r\n- If we can recognize that this is during the leader transition we can chose to not log the error. Instead return a PARTITION_LEADER_MISMATCH code back to the gateway so that it can retry the command with the new leader before sending an error to the client.\r\n\n\n megglos: ZDP-Triage:\n- mostly noise\n- it's expected and shouldn't be logged as error in the particular scenario\n- as it's new (last or next patch) it can be considered a regression => could be confusing after update\n megglos: ZDP-Planning:\n- we will look into it before the next release\n- also affects 8.2,8.1,8.0 due to a backporrt\n Zelldon: I feel this is not 100% resolved. [We see a lot of errors messages also in the gateway](https://console.cloud.google.com/logs/query;query=resource.type%3D%22k8s_container%22%0Aresource.labels.project_id%3D%22zeebe-io%22%0Aresource.labels.location%3D%22europe-west1-b%22%0Aresource.labels.cluster_name%3D%22zeebe-cluster%22%0Aresource.labels.namespace_name%3D%22medic-y-2023-cw-20-d2345cc-benchmark%22%0Alabels.k8s-pod%2Fapp%3D%22camunda-platform%22%0Alabels.k8s-pod%2Fapp_kubernetes_io%2Fcomponent%3D%22zeebe-gateway%22%0Alabels.k8s-pod%2Fapp_kubernetes_io%2Finstance%3D%22medic-y-2023-cw-20-d2345cc-benchmark%22%0Alabels.k8s-pod%2Fapp_kubernetes_io%2Fmanaged-by%3D%22Helm%22%0Alabels.k8s-pod%2Fapp_kubernetes_io%2Fname%3D%22zeebe-gateway%22%0Alabels.k8s-pod%2Fapp_kubernetes_io%2Fpart-of%3D%22camunda-platform%22;cursorTimestamp=2023-06-07T10:45:04.209515605Z?project=zeebe-io), which is also in this case a lot of noise.\r\n\r\nExample of a current medic benchmark\r\n```\r\nio.camunda.zeebe.gateway.cmd.BrokerErrorException: Received error from broker (INTERNAL_ERROR): Failed writing request: Failed to write request to logstream\r\n\tat io.camunda.zeebe.gateway.impl.broker.BrokerRequestManager.handleResponse(BrokerRequestManager.java:194) ~[zeebe-gateway-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.gateway.impl.broker.BrokerRequestManager.lambda$sendRequestInternal$2(BrokerRequestManager.java:143) ~[zeebe-gateway-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.future.FutureContinuationRunnable.run(FutureContinuationRunnable.java:28) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) [zeebe-scheduler-8.3.0-SNAPSHOT.jar:8.3.0-SNAPSHOT]\r\n\"\r\n```\r\n\r\n[Where at this time a role change happens](https://grafana.dev.zeebe.io/d/NzsO1mUnk/zeebe-overview?orgId=1&var-DS_PROMETHEUS=Prometheus&var-cluster=All&var-namespace=medic-y-2023-cw-20-d2345cc-benchmark&var-pod=All&var-partition=All&from=1686132065352&to=1686136683285)\r\n![role](https://github.com/camunda/zeebe/assets/2758593/66f672f3-3af8-44f0-8cb3-7d3cb5029dd5)\r\n\n deepthidevaki: @Zelldon That is an old benchmark before the bug fix.\n Zelldon: Ups thanks @deepthidevaki you're right :+1: ",
    "title": "Failing to write to logstream during stepdown is logged as error"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12754",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nRelated to #12374 \r\n\r\nA customer observed another case where the startup failed with the error:\r\n```\r\n\"Expected to find a snapshot at index >= log's first index X, but found snapshot Y. A previous snapshot is most likely corrupted.\"\r\n```\r\nI could not verify it, but our assumption currently is that this is a false positive. The following might have happened:\r\n1. Follower received snapshot\r\n2. Before committing the snapshot, it reset segments\r\n3. While deleting the segments, the node was shutdown.\r\n4. After restart, it has the old snapshot plus partially deleted segments.\r\n\r\nI think this is plausible as the reset/deleting segments is not and atomic operation. We might not be handling it correctly.\r\n\r\n**Expected behavior**\r\n\r\nThe reset or snapshot commit process should not result in an invalid intermediate state which is detected as corruption.\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.1.*\r\n\r\nRelated to [support](https://jira.camunda.com/browse/SUPPORT-16905) \r\n\n\n deepthidevaki: A potential solution might be to delete the segments in the reverse order, combined with the PR that fixed #12374.\r\n\r\nIn a related issue, I discussed with @npepinpe a different approach. I was planning to create a separate issue for it, but I'm just dumping my thoughts here, as I think it is also relevant for this bug.\r\n\r\n------\r\nTerminologies\r\n\r\nIn-Sync replicas - The followers that are in sync with leaders and participating in the commit. In-sync replicas forms the quorum.\r\n\r\nOut-of-sync follower - The follower which is receiving the event potentially after it is already committed.\r\n\r\nIn a setup with replicationFactor 3, there will be 2 in-sync replicas, out of which one will be the leader. The third one can be an out-of-sync follower.\r\n\r\nRaft guarantees that any committed event is available in all in-sync replicas. The events are replicated to the out-of-sync follower eventually. But there is no guarantee when they are replicated. Ideally, they are replicated immediately. But it is possible the follower is slow, or the network is slow, resulting the out-of-sync follower to lag behind the leader by a large amount. \r\n\r\nAt some point, the leader sends a snapshot instead of the events, because the follower is lagging behind. At this point, the current behavior is as follows:\r\n\r\nFollower resets the log (delete all segments)\r\nReceives and commits the new snapshot\r\nDelete the old snapshot\r\nContinue receiving the events after the snapshot index\r\n\r\nThe above process can lead to the following scenarios:\r\n\r\nStep 1 and 2 are not atomic. If the replica crashes in between, then it restarts with an empty state.\r\nIf it restarts before step 4, then it starts with a snapshot  that is not use-able. Snapshot is valid only if the event corresponding to lastFollowUpEventPosition is in the log.\r\n\r\nIn a normal scenario, the above situation is not problematic because the in-sync replicas are healthy and it is guaranteed that only one of the in-sync replica becomes the leader.\r\n\r\nHowever, in a disaster scenario where all in-sync replicas are gone, it might be acceptable to continue functioning with whatever data is available in the out-of-sync replica. It might not have up-to-date state, but for some use-cases it is ok to lose last X amount of data. In such cases, we want to enable users to recover the cluster from the state of the out-of-sync replica. However, in the above scenario the out-of-sync replica might be in a state where it's state is empty or the snapshot is not useable. \r\n\r\nSo, it would be good if we could ensure that the state in the out-of-sync follower is always in a valid state. To acheive  that we can\r\n\r\nDo not delete old snapshot and reset log immediately when it receives a snapshot.\r\nInstead keep the old snapshot and the logs until the new snapshot is committed and its follow up event is received.\r\n\r\nSolution 1:\r\n\r\nWe keep the old snapshot + logs in a different folder, the new snapshot and new logs in a different folder. On restarts, it attempts to use the new snapshot + log and fall back to the old state if necessary. \r\n\r\nSolution 2:\r\n\r\nDo not reset the logs, when a new snapshot is received. Instead, add a marker record in the journal to indicate that there is a snapshot at the position. The readers must know how to handle these “gaps”. For example, if stream processor reader hits this marker record, it has to throw away its state and replace with the snapshot. If the raft leader reads this record, it should replicate the snapshot instead of the event etc.\r\n\r\nThe compaction logic should take this into account, and compact only if there is one valid snapshot with its followup event in the log. So the follower can rollback to an old valid state if necessary.\r\n\r\nThis would also help in some issue that we observed recently where restarting the node during resetting the log can incorrectly lead to detecting it as corrupted log.\n npepinpe: With #12868, we implemented a quick fix to prevent the specific case we're aware of. By deleting the segments in reverse order during reset, we can ensure that there are no gaps in the log/snapshot, and thus no corruption. Data loss is acceptable here as it was the desired outcome of the reset operation.\r\n\r\nHowever, we agreed not to close the issue - this quick fix only solves a specific thing, and not the root cause, which is that the install operation on the follower is not atomic, specifically persisting the snapshot/clearing the log/updating the meta store/getting the first record for the snapshot/etc. are all independent operations and the node may stop at any point in between, leaving us in a weird state.\n megglos: ZDP-Planning:\r\n- while we are at it, it would be worth to assess potential solutions to get an understanding of how much effort it is to change\r\n- needs prototyping\r\n- back to backlog as upcoming for now to be picked up going forward\r\n- relates to upcoming resilience epic",
    "title": "Journal reset resulting in an intermediate state which is detected as corruption"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12623",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nAfter restoring from backup, if the leader that took the original backup is no more the leader of the partition, this can result in a duplicate backup of the partition with the same backup id. The new backup is logically equivalent to the old backup. So it doesn't matter which backup we use later. However, re-taking the backup is unnecessary as it wastes resources.\r\n\r\n**To Reproduce**\r\nRestore zeebe from a backup, and observe the logs or inspect the backup store.\r\n\r\n**Expected behavior**\r\nA partition should not re-take a backup after restore.\r\n\n",
    "title": "After restoring from backup, a partition re-takes the backup"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12622",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nSometime, a partition may attempt to take a backup again when a backup for the same id already exists. This case is rare, but can happen sometimes if there is a leader change while taking a backup. If this happens, list backup fails with an error message:\r\n```\r\n{\r\n    \"message\": \"Duplicate key 1 (attempted merging values BackupStatus[backupId=1, partitionId=1, status=COMPLETED, failureReason=, brokerVersion=8.2.2, createdAt=...])\"\r\n}\r\n```\r\n\r\n**Expected behavior**\r\nList backup should be able to handle duplicate backup ids for a partition.\r\n\n",
    "title": "List backup fails when a partition has same backup taken by multiple nodes"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12597",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nTrying to list all available backups will always fail without a useful error message.\r\nThe gateway distributes a list request to all brokers which then list all of their backups and try to respond with a `BackupListResponse`:\r\n\r\nhttps://github.com/camunda/zeebe/blob/4854b6606a803926ed9cadabfc2edb4aede18cb4/protocol/src/main/resources/cluster-management-protocol.xml#L64-L76\r\n\r\nThe `groupSizeEncoding` is defined by us:\r\n\r\nhttps://github.com/camunda/zeebe/blob/c861aac736376e1cc20aa558979c6d9c289b4a1f/protocol/src/main/resources/common-types.xml#L16-L19\r\n\r\nIt uses a `unit8` to represent the number of entries. When trying to write a `BackupListResponse` with more than 255 entries, the encoder rejects it:\r\n```\r\njava.lang.IllegalArgumentException: count outside allowed range: count=774\r\n\tat io.camunda.zeebe.protocol.management.BackupListResponseEncoder$BackupsEncoder.wrap(BackupListResponseEncoder.java:137) ~[zeebe-protocol-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.protocol.management.BackupListResponseEncoder.backupsCount(BackupListResponseEncoder.java:114) ~[zeebe-protocol-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.protocol.impl.encoding.BackupListResponse.write(BackupListResponse.java:100) ~[zeebe-protocol-impl-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.broker.transport.backupapi.BackupApiResponseWriter.write(BackupApiResponseWriter.java:71) ~[zeebe-broker-8.2.2.jar:8.2.2]\r\n....\r\n\r\n```\r\n**To Reproduce**\r\n\r\nTake 256 backups, then query all backups via `GET actuator/backups`.\r\n\r\n**Expected behavior**\r\n\r\n1. Zeebe supports much more backups than 255 (for example by using a `uint16`, thus supporting 65535 backups)\r\n2. The number of listed backups should be limited to a reasonable number. Querying the backup store to list, say 1000, available backups is likely to result in timeouts and makes the backup API unusable.\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.lang.IllegalArgumentException: count outside allowed range: count=774\r\n\tat io.camunda.zeebe.protocol.management.BackupListResponseEncoder$BackupsEncoder.wrap(BackupListResponseEncoder.java:137) ~[zeebe-protocol-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.protocol.management.BackupListResponseEncoder.backupsCount(BackupListResponseEncoder.java:114) ~[zeebe-protocol-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.protocol.impl.encoding.BackupListResponse.write(BackupListResponse.java:100) ~[zeebe-protocol-impl-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.broker.transport.backupapi.BackupApiResponseWriter.write(BackupApiResponseWriter.java:71) ~[zeebe-broker-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.transport.impl.ServerResponseImpl.write(ServerResponseImpl.java:50) ~[zeebe-transport-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.transport.impl.AtomixServerTransport.sendResponse(AtomixServerTransport.java:154) ~[zeebe-transport-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.broker.transport.backupapi.BackupApiResponseWriter.tryWriteResponse(BackupApiResponseWriter.java:51) ~[zeebe-broker-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.broker.transport.AsyncApiRequestHandler.lambda$handleRequest$1(AsyncApiRequestHandler.java:123) ~[zeebe-broker-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.future.FutureContinuationRunnable.run(FutureContinuationRunnable.java:28) ~[zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94) ~[zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) [zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) [zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- Zeebe Version: >= 8.1\r\n\n",
    "title": "Listing backups fails if more than 255 backups are available"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12591",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nAn attempt to upload oversized BPMN (other the segment limit) causes unrecoverable failure of Zeebe:\r\n* further BPMN upload of proper sizes are not possible\r\n* eventually partitions becomes unhealthy and not recovered\r\n\r\nIt is degradation from version 8.1.6 that just rejected incorrect BPMN without further problems.\r\n\r\n**To Reproduce**\r\n\r\nUpload the BMPN bigger than configured \r\n      maxMessageSize: 64KB\r\n\r\n\r\n**Expected behavior**\r\n\r\nBMPN Upload is rejected\r\n\r\n**Log/Stacktrace**\r\n```\r\n2023-04-27 17:03:37.989 [Broker-0-StreamProcessor-1] [Broker-0-zb-actors-1] ERROR\r\n      io.camunda.zeebe.broker.process - Unexpected error while processing resource 'f6c0b39d-8357-40ea-8d79-7e2611a89677.bpmn'\r\nio.camunda.zeebe.stream.api.records.ExceededBatchRecordSizeException: Can't append entry: 'RecordBatchEntry[recordMetadata=RecordMetadata{recordType=EVENT, valueType=PROCESS, intent=CREATED}, key=2251799852104669, sourceIndex=-1, unifiedRecordValue={\"bpmnProcessId\":\"id_f6c0b39d-8357-40ea-8d79-7e2611a89677\",\"version\":1,\"processDefinitionKey\":2251799852104669,\"resourceName\":\"f6c0b39d-8357-40ea-8d79-7e2611a89677.bpmn\",\"checksum\":\"uG1QH8XcklrgFGYZI3HhPg==\",\"resource\":\"PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0idXRmLTgiPz4NCjxicG1uOmRlZmluaXRpb25zIHhtbG5zOmJwbW5kaT0iaHR0cDovL3d3dy5vbWcub3JnL3NwZWMvQlBNTi8yMDEwMDUyNC9ESSIgeG1sbnM6ZGM9Imh0dHA6Ly93d3cub21nLm9yZy9zcGVjL0RELzIwMTAwNTI0L0RDIiB4bWxuczp6ZWViZT0iaHR0cDovL2NhbXVuZGEub3JnL3NjaGVtYS96ZWViZS8xLjAiIHhtbG5zOmRpPSJodHRwOi8vd3d3Lm9tZy5vcmcvc3BlYy9ERC8yMDEwMDUyNC9ESSIgeG1sbnM6eHNpPSJodHRwOi8vd3d3LnczLm9yZy8yMDAxL1hNTFNjaGVtYS1pbnN0YW5jZSIgaWQ9ImlkXzZmNTk2NTlmLWE3OGMtNDMyMy04M2VmLTdlODMwYmJlNTUwNSIgdGFyZ2V0TmFtZXNwYWNlPSJodHRwOi8vYnBtbi5pby9zY2hlbWEvYnBtbiIgZXhwb3J0ZXI9IkNvbmZpcm1pdCBCUE1OIEJ1aWxkZXIiIGV4cG9ydGVyVmVyc2lvbj0iMS4wLjAuMCIgeG1sbnM6YnBtbj0iaHR0cD...' with size: 1010867 this would exceed the maximum batch size. [ currentBatchEntryCount: 0, currentBatchSize: 0]\r\n\tat io.camunda.zeebe.stream.impl.records.RecordBatch.appendRecord(RecordBatch.java:67) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.BufferedProcessingResultBuilder.appendRecordReturnEither(BufferedProcessingResultBuilder.java:62) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.api.ProcessingResultBuilder.appendRecord(ProcessingResultBuilder.java:38) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.streamprocessor.writers.ResultBuilderBackedEventApplyingStateWriter.appendFollowUpEvent(ResultBuilderBackedEventApplyingStateWriter.java:40) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.transformProcessResource(BpmnResourceTransformer.java:162) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.lambda$transformResource$0(BpmnResourceTransformer.java:77) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.util.Either$Right.map(Either.java:355) ~[zeebe-util-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.lambda$transformResource$1(BpmnResourceTransformer.java:75) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.util.Either$Right.flatMap(Either.java:366) ~[zeebe-util-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.transformResource(BpmnResourceTransformer.java:65) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.DeploymentTransformer.transformResource(DeploymentTransformer.java:122) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.DeploymentTransformer.transform(DeploymentTransformer.java:98) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.DeploymentCreateProcessor.processRecord(DeploymentCreateProcessor.java:87) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.Engine.process(Engine.java:142) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.batchProcessing(ProcessingStateMachine.java:346) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$2(ProcessingStateMachine.java:268) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run(ZeebeTransaction.java:84) ~[zeebe-db-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand(ProcessingStateMachine.java:268) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord(ProcessingStateMachine.java:227) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.readNextRecord(ProcessingStateMachine.java:203) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\r\n\r\n```\r\n\r\n**Environment:**\r\n- OS: Windows\r\n- Zeebe Version: 8.2.3\r\n\r\nIn version 8.1.6 the behavior is correct. Here's stack traces from this version:\r\n\r\n```\r\n2023-04-27 17:52:05.506 [io.camunda.zeebe.broker.transport.commandapi.CommandApiRequestHandler] [Broker-0-zb-actors-1] ERROR\r\n      io.camunda.zeebe.broker.transport - Unexpected error on writing CREATE command\r\njava.lang.IllegalArgumentException: Expected to claim segment of size 1010866, but can't claim more than 65536 bytes.\r\n\tat io.camunda.zeebe.dispatcher.Dispatcher.offer(Dispatcher.java:207) ~[zeebe-dispatcher-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.dispatcher.Dispatcher.claimSingleFragment(Dispatcher.java:143) ~[zeebe-dispatcher-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.logstreams.impl.log.LogStreamWriterImpl.claimLogEntry(LogStreamWriterImpl.java:165) ~[zeebe-logstreams-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.logstreams.impl.log.LogStreamWriterImpl.tryWrite(LogStreamWriterImpl.java:124) ~[zeebe-logstreams-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.broker.transport.commandapi.CommandApiRequestHandler.writeCommand(CommandApiRequestHandler.java:141) ~[zeebe-broker-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.broker.transport.commandapi.CommandApiRequestHandler.handleExecuteCommandRequest(CommandApiRequestHandler.java:114) ~[zeebe-broker-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.broker.transport.commandapi.CommandApiRequestHandler.handle(CommandApiRequestHandler.java:58) ~[zeebe-broker-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.broker.transport.commandapi.CommandApiRequestHandler.handleAsync(CommandApiRequestHandler.java:49) ~[zeebe-broker-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.broker.transport.commandapi.CommandApiRequestHandler.handleAsync(CommandApiRequestHandler.java:27) ~[zeebe-broker-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.broker.transport.AsyncApiRequestHandler.handleRequest(AsyncApiRequestHandler.java:110) ~[zeebe-broker-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.broker.transport.AsyncApiRequestHandler.lambda$onRequest$0(AsyncApiRequestHandler.java:75) ~[zeebe-broker-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n2023-04-27 17:52:05.508 [io.camunda.zeebe.gateway.impl.broker.BrokerRequestManager] [Broker-0-zb-actors-0] ERROR\r\n      io.camunda.zeebe.gateway - Expected to handle gRPC request, but received an internal error from broker: BrokerError{code=INTERNAL_ERROR, message='Failed writing response: java.lang.IllegalArgumentException: Expected to claim segment of size 1010866, but can't claim more than 65536 bytes.'}\r\nio.camunda.zeebe.gateway.cmd.BrokerErrorException: Received error from broker (INTERNAL_ERROR): Failed writing response: java.lang.IllegalArgumentException: Expected to claim segment of size 1010866, but can't claim more than 65536 bytes.\r\n\tat io.camunda.zeebe.gateway.impl.broker.BrokerRequestManager.handleResponse(BrokerRequestManager.java:194) ~[zeebe-gateway-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.gateway.impl.broker.BrokerRequestManager.lambda$sendRequestInternal$2(BrokerRequestManager.java:143) ~[zeebe-gateway-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.future.FutureContinuationRunnable.run(FutureContinuationRunnable.java:28) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) ~[zeebe-scheduler-8.1.6.jar:8.1.6]\r\n\r\n```\r\n\r\n\r\n\r\n\r\n\n\n Zelldon: Hey @sergeylebed thanks for reporting this!\r\n\r\nLooks like a regression @megglos \n Zelldon: @sergeylebed can you confirm that you didn't get an error response in the client? :thinking: \r\n\r\nIt looks like, based on the stacktrace that it just failed on a different place before. \r\n\r\n```\r\nat io.camunda.zeebe.broker.transport.commandapi.CommandApiRequestHandler.writeCommand(CommandApiRequestHandler.java:141) ~[zeebe-broker-8.1.6.jar:8.1.6]\r\n```\r\n\r\nIs at the CommandAPI, when receiving the Command and writing to the dispatcher (before replicating and processing). We replaced the dispatcher in 8.2. Meaning it is now possible to write larger entries, but as you see processing is still not possible. But I think at least I would expect you get an error response. Did you?\n sergeylebed: I got an error on the client but \r\na) it is a generic error about timeout\r\nb) the system becomes inoperable \r\n\r\n```\r\nGrpc: 'DeadlineExceeded' 'Status(StatusCode=\\\"DeadlineExceeded\\\", Detail=\\\"Time out between gateway and broker: Request timed out after PT15S\\\", DebugException=\\\"Grpc.Core.Internal.CoreErrorDetailException: {\\\"created\\\":\\\"@1682543232.485000000\\\",\\\"description\\\":\\\"Error received from peer ipv6:[::1]:26500\\\",\\\"file\\\":\\\"..\\\\..\\\\..\\\\src\\\\core\\\\lib\\\\surface\\\\call.cc\\\",\\\"file_line\\\":953,\\\"grpc_message\\\":\\\"Time out between gateway and broker: Request timed out after PT15S\\\",\\\"grpc_status\\\":4}\\\")'\",\"Exception\":\"Grpc.Core.RpcException: Status(StatusCode=\\\"DeadlineExceeded\\\", Detail=\\\"Time out between gateway and broker: Request timed out after PT15S\\\", DebugException=\\\"Grpc.Core.Internal.CoreErrorDetailException: {\\\"created\\\":\\\"@1682543232.485000000\\\",\\\"description\\\":\\\"Error received from peer ipv6:[::1]:26500\\\",\\\"file\\\":\\\"..\\\\..\\\\..\\\\src\\\\core\\\\lib\\\\surface\\\\call.cc\\\",\\\"file_line\\\":953,\\\"grpc_message\\\":\\\"Time out between gateway and broker: Request timed out after PT15S\\\",\\\"grpc_status\\\":4}\\\")\\r\\n   at Zeebe.Client.Impl.Commands.DeployProcessCommand.Send(Nullable`1 timeout, CancellationToken token)\\r\\n   at  \r\n```\n sergeylebed: The very first error in the log:\r\n\r\n```\r\n2023-04-26 12:49:17.323 [Broker-0-StreamProcessor-1] [Broker-0-zb-actors-1] ERROR\r\n      io.camunda.zeebe.broker.process - Unexpected error while processing resource 'f6c0b39d-8357-40ea-8d79-7e2611a89677.bpmn'\r\nio.camunda.zeebe.stream.api.records.ExceededBatchRecordSizeException: Can't append entry: 'RecordBatchEntry[recordMetadata=RecordMetadata{recordType=EVENT, valueType=PROCESS, intent=CREATED}, key=2251799852104669, sourceIndex=-1, unifiedRecordValue={\"bpmnProcessId\":\"id_f6c0b39d-8357-40ea-8d79-7e2611a89677\",\"version\":1,\"processDefinitionKey\":2251799852104669,\"resourceName\":\"f6c0b39d-8357-40ea-8d79-7e2611a89677.bpmn\",\"checksum\":\"uG1QH8XcklrgFGYZI3HhPg==\",\"resource\":\"PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0idXRmLTgiPz4NCjxicG1uOmRlZmluaXRpb25zIHhtbG5zOmJwbW5kaT0iaHR0cDovL3d3dy5vbWcub3JnL3NwZWMvQlBNTi8yMDEwMDUyNC9ESSIgeG1sbnM6ZGM9Imh0dHA6Ly93d3cub21nLm9yZy9zcGVjL0RELzIwMTAwNTI0L0RDIiB4bWxuczp6ZWViZT0iaHR0cDovL2NhbXVuZGEub3JnL3NjaGVtYS96ZWViZS8xLjAiIHhtbG5zOmRpPSJodHRwOi8vd3d3Lm9tZy5vcmcvc3BlYy9ERC8yMDEwMDUyNC9ESSIgeG1sbnM6eHNpPSJodHRwOi8vd3d3LnczLm9yZy8yMDAxL1hNTFNjaGVtYS1pbnN0YW5jZSIgaWQ9ImlkXzZmNTk2NTlmLWE3OGMtNDMyMy04M2VmLTdlODMwYmJlNTUwNSIgdGFyZ2V0TmFtZXNwYWNlPSJodHRwOi8vYnBtbi5pby9zY2hlbWEvYnBtbiIgZXhwb3J0ZXI9IkNvbmZpcm1pdCBCUE1OIEJ1aWxkZXIiIGV4cG9ydGVyVmVyc2lvbj0iMS4wLjAuMCIgeG1sbnM6YnBtbj0iaHR0cD...' with size: 1010867 this would exceed the maximum batch size. [ currentBatchEntryCount: 0, currentBatchSize: 0]\r\n\tat io.camunda.zeebe.stream.impl.records.RecordBatch.appendRecord(RecordBatch.java:67) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.BufferedProcessingResultBuilder.appendRecordReturnEither(BufferedProcessingResultBuilder.java:62) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.api.ProcessingResultBuilder.appendRecord(ProcessingResultBuilder.java:38) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.streamprocessor.writers.ResultBuilderBackedEventApplyingStateWriter.appendFollowUpEvent(ResultBuilderBackedEventApplyingStateWriter.java:40) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.transformProcessResource(BpmnResourceTransformer.java:162) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.lambda$transformResource$0(BpmnResourceTransformer.java:77) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.util.Either$Right.map(Either.java:355) ~[zeebe-util-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.lambda$transformResource$1(BpmnResourceTransformer.java:75) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.util.Either$Right.flatMap(Either.java:366) ~[zeebe-util-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.transformResource(BpmnResourceTransformer.java:65) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.DeploymentTransformer.transformResource(DeploymentTransformer.java:122) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.DeploymentTransformer.transform(DeploymentTransformer.java:98) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.DeploymentCreateProcessor.processRecord(DeploymentCreateProcessor.java:87) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.engine.Engine.process(Engine.java:142) ~[zeebe-workflow-engine-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.batchProcessing(ProcessingStateMachine.java:346) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$2(ProcessingStateMachine.java:268) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run(ZeebeTransaction.java:84) ~[zeebe-db-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand(ProcessingStateMachine.java:268) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord(ProcessingStateMachine.java:227) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.readNextRecord(ProcessingStateMachine.java:203) ~[zeebe-stream-platform-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) [zeebe-scheduler-8.2.3.jar:8.2.3]\r\n```\n sergeylebed: In version 8.1.6 the client error is different\r\n\r\n```\r\nStatus(StatusCode=\\\"Internal\\\", Detail=\\\"Unexpected error occurred between gateway and broker (code: INTERNAL_ERROR)\\\", DebugException=\\\"Grpc.Core.Internal.CoreErrorDetailException: {\\\"created\\\":\\\"@1682632476.120000000\\\",\\\"description\\\":\\\"Error received from peer ipv6:[::1]:26500\\\",\\\"file\\\":\\\"..\\\\..\\\\..\\\\src\\\\core\\\\lib\\\\surface\\\\call.cc\\\",\\\"file_line\\\":953,\\\"grpc_message\\\":\\\"Unexpected error occurred between gateway and broker (code: INTERNAL_ERROR)\\\",\\\"grpc_status\\\":13}\\\")\",\"Exception\":\"Grpc.Core.RpcException: Status(StatusCode=\\\"Internal\\\", Detail=\\\"Unexpected error occurred between gateway and broker (code: INTERNAL_ERROR)\\\", DebugException=\\\"Grpc.Core.Internal.CoreErrorDetailException: {\\\"created\\\":\\\"@1682632476.120000000\\\",\\\"description\\\":\\\"Error received from peer ipv6:[::1]:26500\\\",\\\"file\\\":\\\"..\\\\..\\\\..\\\\src\\\\core\\\\lib\\\\surface\\\\call.cc\\\",\\\"file_line\\\":953,\\\"grpc_message\\\":\\\"Unexpected error occurred between gateway and broker (code: INTERNAL_ERROR)\\\",\\\"grpc_status\\\":13}\\\")\\r\\n   at Zeebe.Client.Impl.Commands.DeployProcessCommand.Send(Nullable`1 timeout, CancellationToken token)\r\n```\n npepinpe: Under the assumption that doing so bricks your partition unrecoverably, we'll prioritize it as a blocker/critical issue.\n deepthidevaki: I was able to reproduce this by setting maxMessageSize to 1MB in the test `CreateDeploymentTest::shouldRejectDeployIfResourceIsTooLarge()` https://github.com/camunda/zeebe/blob/main/qa/integration-tests/src/test/java/io/camunda/zeebe/it/client/command/CreateDeploymentTest.java#L28 \r\n\r\nI see multiple issues here:\r\n\r\n1. CommandAPI is not rejecting requests which exceeds maxMessageSize. This result in oversized command to be written to the log stream.\r\n2. When engine cannot write the follow up event because it is above batch size limit, it attempts to write a rejection record which contains the whole command. Since the command is already above batch size it cannot write rejection to the log stream. This result in a loop in the processing machine where it tries to handle this error endlessly.\r\n\r\nFor fixing this issue, I propose to reject the request in CommandAPI. So it is never written to the logstream. However, I would also suggest to revisit if we really have to write the whole command in the rejection record.\n sergeylebed: It does not seem to be fixed in 8.2.5. Do you think it can be merged into the latest versions?\n deepthidevaki: @sergeylebed The fix will be included in 8.2.6.",
    "title": "8.2.3 Degradation: Creating an oversized BPMN causes unrecoverable failure "
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12374",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nA Zeebe broker is crash looping. The broker tries to start up but failed with the following error message. \r\n\r\n```\r\nio.camunda.zeebe.journal.CorruptedJournalException: Expected to read the version byte from segment 'raft-partition-partition-1-1.log' but got EOF instead\r\n```\r\n\r\nThe broker was part of a cluster with three brokers. The other brokers were healthy and continued processing. \r\n\r\nTo mitigate the issue, we did a fresh restart of the broker. We removed all data and restarted the broker. After the restart, the broker was healthy again and joined the cluster.\r\n\r\n**To Reproduce**\r\n\r\nUnknown. \r\n\r\nThe broker was forced to shut down (`Shutdown was called with context: ...`). The broker created a new snapshot 30 seconds before. The log contains no warnings or suspicious behavior before or after the shutdown.\r\n\r\n_EDIT:_\r\n\r\nIt seems `zeebe-2` was restarted while receiving a snapshot and resetting the log:\r\n```\r\nINFO 2023-04-06T15:37:07.317296306Z [jsonPayload.context.partitionId: 1] [resource.labels.containerName: zeebe] RaftServer{raft-partition-partition-1}{role=FOLLOWER} - Started receiving new snapshot FileBasedReceivedSnapshot{directory=/usr/local/zeebe/data/raft-partition/partitions/1/pending/14080296-246-15599729-15599732-1, snapshotStore=Broker-2-SnapshotStore-1, metadata=FileBasedSnapshotId{index=14080296, term=246, processedPosition=15599729, exporterPosition=15599732}} from 1\r\nINFO 2023-04-06T15:37:07.490944632Z [jsonPayload.context.partitionId: 1] [resource.labels.containerName: zeebe] RaftServer{raft-partition-partition-1}{role=FOLLOWER} - Delete existing log (lastIndex '14080107') and replace with received snapshot (index '14080296'). First entry in the log will be at index 14080297\r\nERROR 2023-04-06T15:39:35.994311846Z [resource.labels.containerName: zeebe] + export SPRING_CONFIG_LOCATION=classpath:/,file:./config/zeebe.cfg.yaml\r\n```\r\nThese are the last logs from that broker, and immediately it was restarted.\r\n\r\nMost likely, the log segment on the disk is an intermediate state where the previous segments have been deleted, the new one is only partially created - which is detected as corruption after the restart. \r\n\r\n**Expected behavior**\r\n\r\nThe broker can handle this corruption failure. For example, by removing (or archiving) the corrupted data and fetching the latest data from the cluster.\r\n\r\n**Log/Stacktrace**\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.util.concurrent.ExecutionException: Startup failed in the following steps: [Partition Manager]. See suppressed exceptions for details.\r\n\tat io.camunda.zeebe.scheduler.future.CompletableActorFuture.get(CompletableActorFuture.java:142) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.future.CompletableActorFuture.get(CompletableActorFuture.java:109) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.FutureUtil.join(FutureUtil.java:21) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.future.CompletableActorFuture.join(CompletableActorFuture.java:198) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.broker.Broker.internalStart(Broker.java:101) ~[zeebe-broker-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.util.LogUtil.doWithMDC(LogUtil.java:23) ~[zeebe-util-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.broker.Broker.start(Broker.java:83) ~[zeebe-broker-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.broker.StandaloneBroker.run(StandaloneBroker.java:92) ~[camunda-zeebe-8.1.3.jar:8.1.3]\r\n\tat org.springframework.boot.SpringApplication.callRunner(SpringApplication.java:771) ~[spring-boot-2.7.4.jar:2.7.4]\r\n\tat org.springframework.boot.SpringApplication.callRunners(SpringApplication.java:755) ~[spring-boot-2.7.4.jar:2.7.4]\r\n\tat org.springframework.boot.SpringApplication.run(SpringApplication.java:315) ~[spring-boot-2.7.4.jar:2.7.4]\r\n\tat io.camunda.zeebe.broker.StandaloneBroker.main(StandaloneBroker.java:82) ~[camunda-zeebe-8.1.3.jar:8.1.3]\r\nCaused by: io.camunda.zeebe.scheduler.startup.StartupProcessException: Startup failed in the following steps: [Partition Manager]. See suppressed exceptions for details.\r\n\tat io.camunda.zeebe.scheduler.startup.StartupProcess.aggregateExceptionsSynchronized(StartupProcess.java:282) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.startup.StartupProcess.completeStartupFutureExceptionallySynchronized(StartupProcess.java:183) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.startup.StartupProcess.lambda$proceedWithStartupSynchronized$3(StartupProcess.java:167) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.future.FutureContinuationRunnable.run(FutureContinuationRunnable.java:33) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tSuppressed: io.camunda.zeebe.scheduler.startup.StartupProcessStepException: Bootstrap step Partition Manager failed\r\n\t\tat io.camunda.zeebe.scheduler.startup.StartupProcess.completeStartupFutureExceptionallySynchronized(StartupProcess.java:185) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.scheduler.startup.StartupProcess.lambda$proceedWithStartupSynchronized$3(StartupProcess.java:167) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.scheduler.future.FutureContinuationRunnable.run(FutureContinuationRunnable.java:33) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) ~[zeebe-scheduler-8.1.3.jar:8.1.3]\r\n\tCaused by: java.util.concurrent.CompletionException: io.camunda.zeebe.journal.CorruptedJournalException: Expected to read the version byte from segment 'raft-partition-partition-1-1.log' but got EOF instead.\r\n\t\tat java.util.concurrent.CompletableFuture.encodeThrowable(Unknown Source) ~[?:?]\r\n\t\tat java.util.concurrent.CompletableFuture.uniApplyNow(Unknown Source) ~[?:?]\r\n\t\tat java.util.concurrent.CompletableFuture.uniApplyStage(Unknown Source) ~[?:?]\r\n\t\tat java.util.concurrent.CompletableFuture.thenApply(Unknown Source) ~[?:?]\r\n\t\tat io.atomix.raft.partition.RaftPartition.open(RaftPartition.java:104) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.partition.RaftPartitionGroup.lambda$join$7(RaftPartitionGroup.java:201) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat java.util.stream.ReferencePipeline$3$1.accept(Unknown Source) ~[?:?]\r\n\t\tat java.util.HashMap$KeySpliterator.forEachRemaining(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.AbstractPipeline.copyInto(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.AbstractPipeline.evaluate(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.AbstractPipeline.evaluateToArrayNode(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.ReferencePipeline.toArray(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.ReferencePipeline.toArray(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.ReferencePipeline.toList(Unknown Source) ~[?:?]\r\n\t\tat io.atomix.raft.partition.RaftPartitionGroup.join(RaftPartitionGroup.java:203) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.primitive.partition.impl.DefaultPartitionService.start(DefaultPartitionService.java:63) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.broker.partitioning.PartitionManagerImpl.start(PartitionManagerImpl.java:125) ~[zeebe-broker-8.1.3.jar:8.1.3]\r\n\t\tat java.util.concurrent.CompletableFuture$AsyncSupply.run(Unknown Source) ~[?:?]\r\n\t\tat java.lang.Thread.run(Unknown Source) ~[?:?]\r\n\tCaused by: io.camunda.zeebe.journal.CorruptedJournalException: Expected to read the version byte from segment 'raft-partition-partition-1-1.log' but got EOF instead.\r\n\t\tat io.camunda.zeebe.journal.file.SegmentLoader.readVersion(SegmentLoader.java:173) ~[zeebe-journal-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.journal.file.SegmentLoader.readDescriptor(SegmentLoader.java:128) ~[zeebe-journal-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.journal.file.SegmentLoader.loadExistingSegment(SegmentLoader.java:87) ~[zeebe-journal-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.journal.file.SegmentsManager.loadSegments(SegmentsManager.java:292) ~[zeebe-journal-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.journal.file.SegmentsManager.open(SegmentsManager.java:238) ~[zeebe-journal-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.journal.file.SegmentedJournal.<init>(SegmentedJournal.java:61) ~[zeebe-journal-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.journal.file.SegmentedJournalBuilder.build(SegmentedJournalBuilder.java:158) ~[zeebe-journal-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.storage.log.RaftLogBuilder.build(RaftLogBuilder.java:136) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.storage.RaftStorage.openLog(RaftStorage.java:194) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.impl.RaftContext.<init>(RaftContext.java:194) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.impl.DefaultRaftServer$Builder.build(DefaultRaftServer.java:258) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.impl.DefaultRaftServer$Builder.build(DefaultRaftServer.java:232) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.partition.impl.RaftPartitionServer.buildServer(RaftPartitionServer.java:189) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.partition.impl.RaftPartitionServer.initServer(RaftPartitionServer.java:155) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.partition.impl.RaftPartitionServer.start(RaftPartitionServer.java:114) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.partition.RaftPartition.open(RaftPartition.java:104) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.raft.partition.RaftPartitionGroup.lambda$join$7(RaftPartitionGroup.java:201) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat java.util.stream.ReferencePipeline$3$1.accept(Unknown Source) ~[?:?]\r\n\t\tat java.util.HashMap$KeySpliterator.forEachRemaining(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.AbstractPipeline.copyInto(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.AbstractPipeline.evaluate(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.AbstractPipeline.evaluateToArrayNode(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.ReferencePipeline.toArray(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.ReferencePipeline.toArray(Unknown Source) ~[?:?]\r\n\t\tat java.util.stream.ReferencePipeline.toList(Unknown Source) ~[?:?]\r\n\t\tat io.atomix.raft.partition.RaftPartitionGroup.join(RaftPartitionGroup.java:203) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.atomix.primitive.partition.impl.DefaultPartitionService.start(DefaultPartitionService.java:63) ~[zeebe-atomix-cluster-8.1.3.jar:8.1.3]\r\n\t\tat io.camunda.zeebe.broker.partitioning.PartitionManagerImpl.start(PartitionManagerImpl.java:125) ~[zeebe-broker-8.1.3.jar:8.1.3]\r\n\t\tat java.util.concurrent.CompletableFuture$AsyncSupply.run(Unknown Source) ~[?:?]\r\n\t\tat java.lang.Thread.run(Unknown Source) ~[?:?]\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\nSee more in the downloaded [log file](https://drive.google.com/file/d/1V89vBHeXytmm_lviRhMjojBhdeoz7mJs/view?usp=share_link).\r\n\r\n**Environment:**\r\n- OS: Camunda SaaS\r\n- Zeebe Version: `8.1.3`\r\n- Configuration: `prod-worker-3`\r\n\n\n SeanAda: I have the same exeption on a self hosted Zeebe (Version 8.1.4).\r\nThe file `raft-partition-partition-1-1.log` is empty.\r\n\r\nIs there any chance to recover the data or copy it from other nodes?\n saig0: > Is there any chance to recover the data or copy it from other nodes?\r\n\r\n@SeanAda it depends on your setup. With a replication factor of 3, there is a good chance that the other two nodes takes over and continue processing. \r\n\r\nIn this case, you can mitigate the issue by doing a fresh restart of the broker. So, remove all data and restart the broker. After the restart, the broker should become healthy again and join the cluster. The broker should receive the missing data from the other nodes of the cluster.\n deepthidevaki: I had a quick look at the logs of the affected cluster. It seems `zeebe-2` was restarted while receiving a snapshot and resetting the log:\r\n```\r\nINFO 2023-04-06T15:37:07.317296306Z [jsonPayload.context.partitionId: 1] [resource.labels.containerName: zeebe] RaftServer{raft-partition-partition-1}{role=FOLLOWER} - Started receiving new snapshot FileBasedReceivedSnapshot{directory=/usr/local/zeebe/data/raft-partition/partitions/1/pending/14080296-246-15599729-15599732-1, snapshotStore=Broker-2-SnapshotStore-1, metadata=FileBasedSnapshotId{index=14080296, term=246, processedPosition=15599729, exporterPosition=15599732}} from 1\r\nINFO 2023-04-06T15:37:07.490944632Z [jsonPayload.context.partitionId: 1] [resource.labels.containerName: zeebe] RaftServer{raft-partition-partition-1}{role=FOLLOWER} - Delete existing log (lastIndex '14080107') and replace with received snapshot (index '14080296'). First entry in the log will be at index 14080297\r\nERROR 2023-04-06T15:39:35.994311846Z [resource.labels.containerName: zeebe] + export SPRING_CONFIG_LOCATION=classpath:/,file:./config/zeebe.cfg.yaml\r\n```\r\nThese are the last logs from that broker, and immediately it was restarted.\r\n\r\nMost likely, the log segment on the disk is an intermediate state where the previous segments have been deleted, the new one is only partially created - which is detected as corruption after the restart. We should handle this case better as it is not an actual corruption.\r\n\n SeanAda: Removing all the data and restarting the broker did work.\r\n\r\nThank you!\n saig0: @deepthidevaki great finding. :rocket: I added your comment to the description. \n codingman1990: But our the other two node become unhealthy too.How should i do something to recover it?\n npepinpe: Hi @codingman1990, what is the state of your two nodes? Are all brokers reporting the same error for the same partition?\n codingman1990: camunda-zeebe-0 and camunda-zeebe-1 be unhealthy.And camunda-zeebe-2 can't restart.\r\nBut camunda-zeebe-1 zeebe.log like this:\r\n\r\n```\r\n2023-04-24 15:44:14.259 [] [raft-server-0-raft-partition-partition-3] WARN \r\n      io.atomix.raft.roles.FollowerRole - RaftServer{raft-partition-partition-3}{role=FOLLOWER} - Poll request to 2 failed: java.net.ConnectException: Expected to send a message with subject 'raft-partition-partition-3-poll' to member '2', but member is not known. Known members are '[Member{id=1, address=camunda-zeebe-1.camunda-zeebe.default.svc:26502, properties={brokerInfo=EADJAAAAAwABAAAAAwAAAAMAAAADAAAAAAABCgAAAGNvbW1hbmRBcGkvAAAAY2FtdW5kYS16ZWViZS0xLmNhbXVuZGEtemVlYmUuZGVmYXVsdC5zdmM6MjY1MDEFAAMBAAAAAQIAAAABAwAAAAEMAAAFAAAAOC4xLjkFAAMBAAAAAAIAAAAAAwAAAAA=}}, Member{id=camunda-zeebe-gateway-7db56f85d6-v2jb7, address=172.20.8.149:26502, properties={event-service-topics-subscribed=KIIDAGpvYnNBdmFpbGFibOU=}}, Member{id=camunda-zeebe-gateway-7db56f85d6-z69qr, address=172.20.3.169:26502, properties={event-service-topics-subscribed=KIIDAGpvYnNBdmFpbGFibOU=}}, Member{id=0, address=camunda-zeebe-0.camunda-zeebe.default.svc:26502, properties={brokerInfo=EADJAAAAAwAAAAAAAwAAAAMAAAADAAAAAAABCgAAAGNvbW1hbmRBcGkvAAAAY2FtdW5kYS16ZWViZS0wLmNhbXVuZGEtemVlYmUuZGVmYXVsdC5zdmM6MjY1MDEFAAMBAAAAAQIAAAABAwAAAAEMAAAFAAAAOC4xLjkFAAMBAAAAAQIAAAABAwAAAAE=}}]'\r\n```\r\n\r\nAnd one zeebe_error6.log like this:\r\n\r\n```\r\n#\r\n# A fatal error has been detected by the Java Runtime Environment:\r\n#\r\n#  SIGBUS (0x7) at pc=0x00007f38d75053cc, pid=6, tid=64\r\n#\r\n# JRE version: OpenJDK Runtime Environment Temurin-17.0.6+10 (17.0.6+10) (build 17.0.6+10)\r\n# Java VM: OpenJDK 64-Bit Server VM Temurin-17.0.6+10 (17.0.6+10, mixed mode, sharing, tiered, compressed oops, compressed class ptrs, g1 gc, linux-amd64)\r\n# Problematic frame:\r\n# v  ~StubRoutines::updateBytesCRC32C\r\n#\r\n# Core dump will be written. Default location: /usr/local/zeebe/core.6\r\n#\r\n# If you would like to submit a bug report, please visit:\r\n#   https://github.com/adoptium/adoptium-support/issues\r\n#\r\n\r\n---------------  S U M M A R Y ------------\r\n\r\nCommand Line: -XX:+HeapDumpOnOutOfMemoryError -XX:InitialHeapSize=2147483648 -XX:MaxHeapSize=2147483648 -XX:HeapDumpPath=/usr/local/zeebe/data -XX:ErrorFile=/usr/local/zeebe/data/zeebe_error%p.log -XX:+ExitOnOutOfMemoryError -Xms128m -XX:+ExitOnOutOfMemoryError -Dfile.encoding=UTF-8 -Dapp.name=broker -Dapp.pid=6 -Dapp.repo=/usr/local/zeebe/lib -Dapp.home=/usr/local/zeebe -Dbasedir=/usr/local/zeebe io.camunda.zeebe.broker.StandaloneBroker\r\n\r\nHost: Intel(R) Xeon(R) CPU E5-2682 v4 @ 2.50GHz, 8 cores, 7G, Ubuntu 20.04.5 LTS\r\nTime: Mon Apr 24 05:25:16 2023 CST elapsed time: 151584.214926 seconds (1d 18h 6m 24s)\r\n\r\n---------------  T H R E A D  ---------------\r\n\r\nCurrent thread (0x00007f38080ce950):  JavaThread \"raft-server-0-raft-partition-partition-2\" [_thread_in_Java, id=64, stack(0x00007f38746dd000,0x00007f38747de000)]\r\n\r\nStack: [0x00007f38746dd000,0x00007f38747de000],  sp=0x00007f38747dc060,  free space=1020k\r\nNative frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)\r\nv  ~StubRoutines::updateBytesCRC32C\r\n\r\n\r\nsiginfo: si_signo: 7 (SIGBUS), si_code: 2 (BUS_ADRERR), si_addr: 0x00007f3744000011\r\n```\r\ncamunda-zeebe-1 zeebe.log no log content has been for 2 hours.\n codingman1990: ![image](https://user-images.githubusercontent.com/12196018/233933546-e37706d0-17a9-4573-b44c-98f7a99849cc.png)\r\nThe grafana page shows we have last camunda-zeebe-2 promethus data.\n codingman1990: After 1 day,camunda be unhealthy again.\r\n![image](https://user-images.githubusercontent.com/12196018/234449827-d98659b8-4f49-4947-adb9-afac4a8bcc83.png)\r\nHere is the  zeebe log.\r\n`2023-04-26 09:23:16.742 [] [raft-server-0-raft-partition-partition-1] ERROR\r\n      io.atomix.utils.concurrent.SingleThreadContext - Shutting down because we can't recover from JVM errors. Consider restarting this broker if it is a temporary issue.\r\njava.lang.InternalError: a fault occurred in a recent unsafe memory access operation in compiled Java code\r\n        at io.camunda.zeebe.journal.file.MessageHeaderEncoder.blockLength(MessageHeaderEncoder.java:85) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.camunda.zeebe.journal.file.SegmentDescriptorEncoder.wrapAndApplyHeader(SegmentDescriptorEncoder.java:80) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.camunda.zeebe.journal.file.SegmentDescriptor.copyTo(SegmentDescriptor.java:289) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.camunda.zeebe.journal.file.UninitializedSegment.initializeForUse(UninitializedSegment.java:35) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.camunda.zeebe.journal.file.SegmentsManager.getNextSegment(SegmentsManager.java:133) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.camunda.zeebe.journal.file.SegmentedJournal.getNextSegment(SegmentedJournal.java:203) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.camunda.zeebe.journal.file.SegmentedJournalWriter.createNewSegment(SegmentedJournalWriter.java:110) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.prometheus.client.Histogram$Child.timeWithExemplar(Histogram.java:273) ~[simpleclient-0.16.0.jar:?]\r\n        at io.prometheus.client.Histogram$Child.time(Histogram.java:260) ~[simpleclient-0.16.0.jar:?]\r\n        at io.camunda.zeebe.journal.file.JournalMetrics.observeSegmentCreation(JournalMetrics.java:71) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.camunda.zeebe.journal.file.SegmentedJournalWriter.append(SegmentedJournalWriter.java:58) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.camunda.zeebe.journal.file.SegmentedJournal.append(SegmentedJournal.java:76) ~[zeebe-journal-8.1.9.jar:8.1.9]\r\n        at io.atomix.raft.storage.log.RaftLog.append(RaftLog.java:139) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n        at io.atomix.raft.roles.LeaderRole.tryToAppend(LeaderRole.java:515) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n        at io.atomix.raft.roles.LeaderRole.append(LeaderRole.java:487) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n        at io.atomix.raft.roles.LeaderRole.safeAppendEntry(LeaderRole.java:564) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n        at io.atomix.raft.roles.LeaderRole.lambda$appendEntry$8(LeaderRole.java:541) ~[zeebe-atomix-cluster-8.1.9.jar:8.1.9]\r\n        at io.atomix.utils.concurrent.SingleThreadContext$WrappedRunnable.run(SingleThreadContext.java:171) ~[zeebe-atomix-utils-8.1.9.jar:8.1.9]\r\n        at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) ~[?:?]\r\n        at java.util.concurrent.FutureTask.run(Unknown Source) ~[?:?]\r\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source) ~[?:?]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]\r\n        at java.lang.Thread.run(Unknown Source) ~[?:?]\r\n2023-04-26 09:23:16.758 [Broker-0-Startup] [Broker-0-zb-actors-0] INFO \r\n      io.camunda.zeebe.broker.system - Shutdown Admin API\r\n2023-04-26 09:23:16.759 [Broker-0-Startup] [Broker-0-zb-actors-0] INFO \r\n      io.camunda.zeebe.broker.system - Shutdown Partition Manager`\r\nCan anyone help solve this problem.\n npepinpe: I will first focus on the original issue, as I think your issue @codingman1990 is not quite the same. It seems like you have some memory issues (crashing due to a SIGBUS), so it looks like an issue with the underlying storage medium and our usage of mmap. OTOH, I would say, if you're using network storage that would explain the increased likelihood, and I would advise against using network storage (e.g. NFS, Samba) with Zeebe for now.\r\n\r\nGoing back to the original issue. It's unclear what the solution would be. In this state, we've already updated the lastFlushedIndex to be the snapshot's index, but we possibly have no snapshot. So we have no snapshot, we have a segment with possibly no descriptor or a partial descriptor, and we have a lastFlushedIndex which is likely quite high.\r\n\r\nOne case with no descriptor, since we pre-allocate the segments, we could simply check if the descriptor portion is all zeros. Then we know we never wrote anything in there, so it's not really corrupted.\r\n\r\nThat leaves us with a partial descriptor case. How do we distinguish partially written descriptor from a corrupted one? Right now, we rely on the last position of the previous segment to determine this, but what if we have no segments?\r\n\r\nOne option would be to initially write a magic byte signifying that we're about to write the descriptor. Flush. Then write the descriptor. Flush.\r\n\r\nSo we have the following lifecycle: \r\n\r\n- Descriptor is all zero (including initial magic byte) => no descriptor, can be deleted\r\n- First byte is the magic WILL_WRITE_DESCRIPTOR byte => no descriptor, can be deleted\r\n- Descriptor is present but corrupted => corruption, cannot be deleted automatically\r\n\r\nOf course, this solution is prone to bitrot, where some bit flip turns the first byte of the descriptor into the WILL_WRITE_DESCRIPTOR magic byte and we erroneously detect it as safe to delete.\r\n\r\nOne other option is piggyback on top of our meta store, and also keep track of the last initialized segment. So whenever we write the descriptor, our segment is now ready for use, and we update the meta store. Additionally, on load, we may need to update the last initialized segment with the latest value in case we pick up a segment which was initialized but we had crashed/shutdown before updating the meta store.\r\n\r\nSo if we have an initialized segment index of 5, and we cannot read the descriptor of 5, then we know this is real corruption. If we have an initialized segment index of 5, and we cannot read the descriptor of 6, then we know this is segment was never actually used.\n npepinpe: So, it came to my mind that this is an over-engineered solution. A simple fix would be to reset the last flushed index to a null value whenever the log is reset, but before the next segment is created.\r\n\r\nThe error only affects the case where we have no previous segment, i.e. when we've reset the log. In all other cases - e.g. a new segment is created but we crash before writing its descriptor - we have a previous segment and a lastFlushedIndex to compare to. When a new segment is created, it will not be written to until initialized, so we can use the lastFlushedIndex to distinguish that.\n saig0: :information_source: I saw a similar error message that seems to be related: \r\n\r\n```\r\njava.util.concurrent.CompletionException: io.camunda.zeebe.journal.CorruptedJournalException: Couldn't read or recognize version of segment 'raft-partition-partition-1-1.log'.\r\n\tat java.util.concurrent.CompletableFuture.encodeThrowable(Unknown Source) ~[?:?]\r\n\tat java.util.concurrent.CompletableFuture.uniApplyNow(Unknown Source) ~[?:?]\r\n\tat java.util.concurrent.CompletableFuture.uniApplyStage(Unknown Source) ~[?:?]\r\n\tat java.util.concurrent.CompletableFuture.thenApply(Unknown Source) ~[?:?]\r\n\tat io.atomix.raft.partition.RaftPartition.open(RaftPartition.java:91) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.partition.RaftPartitionGroup.lambda$join$2(RaftPartitionGroup.java:176) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat java.util.stream.ReferencePipeline$3$1.accept(Unknown Source) ~[?:?]\r\n\tat java.util.HashMap$KeySpliterator.forEachRemaining(Unknown Source) ~[?:?]\r\n\tat java.util.stream.AbstractPipeline.copyInto(Unknown Source) ~[?:?]\r\n\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(Unknown Source) ~[?:?]\r\n\tat java.util.stream.AbstractPipeline.evaluate(Unknown Source) ~[?:?]\r\n\tat java.util.stream.AbstractPipeline.evaluateToArrayNode(Unknown Source) ~[?:?]\r\n\tat java.util.stream.ReferencePipeline.toArray(Unknown Source) ~[?:?]\r\n\tat io.atomix.raft.partition.RaftPartitionGroup.join(RaftPartitionGroup.java:177) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.primitive.partition.impl.DefaultPartitionService.start(DefaultPartitionService.java:63) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.broker.partitioning.PartitionManagerImpl.start(PartitionManagerImpl.java:129) ~[zeebe-broker-8.2.3.jar:8.2.3]\r\n\tat java.util.concurrent.CompletableFuture$AsyncSupply.run(Unknown Source) ~[?:?]\r\n\tat java.lang.Thread.run(Unknown Source) ~[?:?]\r\nCaused by: io.camunda.zeebe.journal.CorruptedJournalException: Couldn't read or recognize version of segment 'raft-partition-partition-1-1.log'.\r\n\tat io.camunda.zeebe.journal.file.SegmentLoader.readDescriptor(SegmentLoader.java:182) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentLoader.loadExistingSegment(SegmentLoader.java:116) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentsManager.loadSegments(SegmentsManager.java:346) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentsManager.open(SegmentsManager.java:267) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentedJournal.<init>(SegmentedJournal.java:56) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentedJournalBuilder.build(SegmentedJournalBuilder.java:174) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.storage.log.RaftLogBuilder.build(RaftLogBuilder.java:151) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.storage.RaftStorage.openLog(RaftStorage.java:197) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.impl.RaftContext.<init>(RaftContext.java:194) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.impl.DefaultRaftServer$Builder.build(DefaultRaftServer.java:243) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.impl.DefaultRaftServer$Builder.build(DefaultRaftServer.java:217) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.partition.impl.RaftPartitionServer.buildServer(RaftPartitionServer.java:184) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.partition.impl.RaftPartitionServer.initServer(RaftPartitionServer.java:150) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.partition.impl.RaftPartitionServer.start(RaftPartitionServer.java:109) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\t... 14 more\r\nCaused by: io.camunda.zeebe.journal.file.UnknownVersionException: Expected version byte to be one [1 2] but got 0 instead.\r\n\tat io.camunda.zeebe.journal.file.SegmentDescriptor.getEncodingLengthForVersion(SegmentDescriptor.java:225) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentLoader.readDescriptor(SegmentLoader.java:155) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentLoader.loadExistingSegment(SegmentLoader.java:116) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentsManager.loadSegments(SegmentsManager.java:346) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentsManager.open(SegmentsManager.java:267) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentedJournal.<init>(SegmentedJournal.java:56) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.camunda.zeebe.journal.file.SegmentedJournalBuilder.build(SegmentedJournalBuilder.java:174) ~[zeebe-journal-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.storage.log.RaftLogBuilder.build(RaftLogBuilder.java:151) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.storage.RaftStorage.openLog(RaftStorage.java:197) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.impl.RaftContext.<init>(RaftContext.java:194) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.impl.DefaultRaftServer$Builder.build(DefaultRaftServer.java:243) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.impl.DefaultRaftServer$Builder.build(DefaultRaftServer.java:217) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.partition.impl.RaftPartitionServer.buildServer(RaftPartitionServer.java:184) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.partition.impl.RaftPartitionServer.initServer(RaftPartitionServer.java:150) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\tat io.atomix.raft.partition.impl.RaftPartitionServer.start(RaftPartitionServer.java:109) ~[zeebe-atomix-cluster-8.2.3.jar:8.2.3]\r\n\t... 14 more\r\n```\r\nEnvironment: SaaS, version `8.2.3`\n codingman1990: > \r\n\r\nAs you mentioned network storage problem,is there any chance we can recover it?We are using k8s and helm to deploy camunda in production.If the error mentioned above occurs,we can't restart camunda zeebe any more.\n npepinpe: So SIGBUS errors can happen of course even without network storage, but typically that will be due to a bug or misuse, e.g. a file gets truncated while it was mmap'd. With network storage, they can happen simply due to network issues, which makes them unsuitable for any Java program using memory mapped files, where it's not possible to trap and handle such signals.\r\n\r\nIf you're not running on network storage however, this is likely a bug, and I'd be interested in how this happened. Do you perhaps have programs which try to reap `.log` files indiscriminately? `.log` files in Zeebe aren't traditional log files (e.g. for logging), but represent the actual application data, and shouldn't never be modified externally.\r\n\r\nAs for recovery, if at least one node has the right data, the simplest way is to delete the data of the other nodes, and manually copy over the \"good\" data to them. If however all nodes fail to start up, I'm afraid you'll have to rely [on your backups (if any) to restore](https://docs.camunda.io/docs/self-managed/backup-restore/backup-and-restore/).\n codingman1990: > So SIGBUS errors can happen of course even without network storage, but typically that will be due to a bug or misuse, e.g. a file gets truncated while it was mmap'd. With network storage, they can happen simply due to network issues, which makes them unsuitable for any Java program using memory mapped files, where it's not possible to trap and handle such signals.\r\n> \r\n> If you're not running on network storage however, this is likely a bug, and I'd be interested in how this happened. Do you perhaps have programs which try to reap `.log` files indiscriminately? `.log` files in Zeebe aren't traditional log files (e.g. for logging), but represent the actual application data, and shouldn't never be modified externally.\r\n> \r\n> As for recovery, if at least one node has the right data, the simplest way is to delete the data of the other nodes, and manually copy over the \"good\" data to them. If however all nodes fail to start up, I'm afraid you'll have to rely [on your backups (if any) to restore](https://docs.camunda.io/docs/self-managed/backup-restore/backup-and-restore/).\r\n\r\nThanks for your tips about `.log` file.Our production machine really has a timing clearing old and `big` files task,maybe that's the root cause.We will trying close it.",
    "title": "CorruptedJournalException: Fail to read version byte from segment"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11578",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\nMulti-Instance doesn't get started if it contains a Message Event-based Subprocess that uses the inputElement as correlationKey. Instead an Incident gets created.\r\n\r\n\r\n\r\n\r\n**To Reproduce**\r\n**Update:** For better understanding I created a GitHub Repo with a minimalistic example: https://github.com/j-lindner/multi-instance-with-message-subprocess-bug\r\n\r\n**Expected behavior**\r\nMulti-Instance should get started, get processed and should listen to possible Messages on the event-based subprocess.\r\n\r\n**Log/Stacktrace**\r\n\r\nIncident Info in Operate:\r\n```\r\nfailed to evaluate expression 'myObject.myId': no variable found for name 'myObject'\r\n```\r\n\r\n**Environment:**\r\n- OS: WebModeler or alternatively zeebe-process-test-extension-testcontainer\r\n- Zeebe Version: 8.1.7\r\n- Configuration: -\r\n\r\n**Support:** \r\nhttps://jira.camunda.com/browse/SUPPORT-17059\n\n remcowesterhoud: Thanks for reporting and reproducing @j-lindner! This is very helpful 🚀 \r\n\r\n I will have chat within the team to see if this is expected behaviour or not. From there we will prioritise it accordingly.\n korthout: Thanks for reporting this @j-lindner 👍 It was easy to reproduce in one of Zeebe's engine test cases as well. I think I have a fix, but let's await the review.\n korthout: @j-lindner Bug is fixed and will be patched in upcoming releases of `8.0.15`, `8.1.13`, `8.2.6`, and `8.3.0`. You should notice that this issue will be labeled accordingly during those releases.",
    "title": "Multi-Instance with messageevent-based subprocess that uses inputElement as correlationKey fails"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11355",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nWe got reports of crash looping Zeebe brokers on prod, it looks like the process which is running does some nesting or looping over certain activities. TODO: I will add the process model later.\r\n\r\nThe user tried to cancel the corresponding process instance but [this failed because](https://console.cloud.google.com/logs/query;query=%0AlogName:%22stdout%22%0Aresource.type%3D%22k8s_container%22%0Aresource.labels.location%3D%22us-central1%22%0Aresource.labels.namespace_name%3D%228dca781e-03c0-4a15-9b88-1832c5d60b19-zeebe%22%0Aresource.labels.cluster_name%3D%22prod-worker-3%22%0Aresource.labels.container_name%3D%22zeebe%22%0Aresource.labels.pod_name%3D%22zeebe-0%22%0Aresource.labels.project_id%3D%22camunda-cloud-240911%22;timeRange=2023-01-02T09:49:55.253Z%2F2023-01-02T10:49:55.253Z;cursorTimestamp=2023-01-02T10:19:25.169339248Z?project=camunda-cloud-240911) there were too many activities to terminate. \r\n\r\n```\r\nExpected to write one or more follow-up records for record 'LoggedEvent [type=0, version=0, streamId=2, position=299792, key=4503599627371681, timestamp=1672654759877, sourceEventPosition=297539] RecordMetadata{recordType=COMMAND, intentValue=255, intent=TERMINATE_ELEMENT, requestStreamId=-2147483648, requestId=-1, protocolVersion=3, valueType=PROCESS_INSTANCE, rejectionType=NULL_VAL, rejectionReason=, brokerVersion=8.2.0}' without errors, but exception was thrown.\r\n```\r\n\r\nError group: https://console.cloud.google.com/errors/detail/COWzpqvwz4Cg0wE;service=zeebe;time=P7D?project=camunda-cloud-240911\r\n<!-- A clear and concise description of what the bug is. -->\r\n> **Note:** Even though we replaced the dispatcher this error will still happen since we have this max message size limit.\r\n\r\nI put the severity to high since I see no workaround. BTW due to the loop and which causes the pod crash looping the cluster was in this case unusable.\r\n\r\n**To Reproduce**\r\nHave a process instance with a lot of activities active, and terminate the corresponding process instance.\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\n**Expected behavior**\r\nTermination of instances takes into account the batch size, and terminates activities batch-wise, similar issue as to activitate multi instances.\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.lang.IllegalArgumentException: Expected to claim segment of size 4481608, but can't claim more than 4194304 bytes.\r\n\tat io.camunda.zeebe.dispatcher.Dispatcher.offer(Dispatcher.java:207) ~[zeebe-dispatcher-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.dispatcher.Dispatcher.claimFragmentBatch(Dispatcher.java:164) ~[zeebe-dispatcher-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.logstreams.impl.log.LogStreamBatchWriterImpl.claimBatchForEvents(LogStreamBatchWriterImpl.java:235) ~[zeebe-logstreams-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.logstreams.impl.log.LogStreamBatchWriterImpl.tryWrite(LogStreamBatchWriterImpl.java:212) ~[zeebe-logstreams-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$writeRecords$9(ProcessingStateMachine.java:354) ~[zeebe-stream-platform-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.retry.ActorRetryMechanism.run(ActorRetryMechanism.java:28) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.retry.AbortableRetryStrategy.run(AbortableRetryStrategy.java:45) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\"\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- OS: <!-- [e.g. Linux] -->\r\n- Zeebe Version: 8.2.0-alpha2 <!-- [e.g. 0.20.0] -->\r\n- Configuration: Production G3-S<!-- [e.g. exporters etc.] -->\r\n\r\nrelates to https://jira.camunda.com/browse/SUPPORT-16499\r\n\n\n Zelldon: Another but related error occured on PROD:\r\n\r\n```\r\nio.camunda.zeebe.stream.api.records.ExceededBatchRecordSizeException: Can't append entry: 'RecordBatchEntry[key=2251799813801783, sourceIndex=-1, recordMetadata=RecordMetadata{recordType=COMMAND, intentValue=10, intent=TERMINATE_ELEMENT, requestStreamId=-2147483648, requestId=-1, protocolVersion=3, valueType=PROCESS_INSTANCE, rejectionType=NULL_VAL, rejectionReason=, brokerVersion=8.2.0}, unifiedRecordValue={\"bpmnProcessId\":\"Process_372fbfc7-9a4a-4f0b-aee5-bd96ed3e3e5d\",\"version\":1,\"processDefinitionKey\":2251799813685320,\"processInstanceKey\":2251799813685333,\"elementId\":\"Activity_0vhm20h\",\"flowScopeKey\":2251799813685333,\"bpmnElementType\":\"USER_TASK\",\"bpmnEventType\":\"UNSPECIFIED\",\"parentProcessInstanceKey\":-1,\"parentElementInstanceKey\":-1}]' with size: 335 this would exceed the maximum batch size. [ currentBatchEntryCount: 11814, currentBatchSize: 3957709]\r\n\r\nat io.camunda.zeebe.stream.impl.records.RecordBatch.appendRecord ( [io/camunda.zeebe.stream.impl.records/RecordBatch.java:66](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl.records%2FRecordBatch.java&line=66&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.BufferedProcessingResultBuilder.appendRecordReturnEither ( [io/camunda.zeebe.stream.impl/BufferedProcessingResultBuilder.java:62](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FBufferedProcessingResultBuilder.java&line=62&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.api.ProcessingResultBuilder.appendRecord ( [io/camunda.zeebe.stream.api/ProcessingResultBuilder.java:38](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.api%2FProcessingResultBuilder.java&line=38&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.streamprocessor.writers.ResultBuilderBackedTypedCommandWriter.appendRecord ( [io/camunda.zeebe.engine.processing.streamprocessor.writers/ResultBuilderBackedTypedCommandWriter.java:37](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.streamprocessor.writers%2FResultBuilderBackedTypedCommandWriter.java&line=37&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.streamprocessor.writers.ResultBuilderBackedTypedCommandWriter.appendFollowUpCommand ( [io/camunda.zeebe.engine.processing.streamprocessor.writers/ResultBuilderBackedTypedCommandWriter.java:32](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.streamprocessor.writers%2FResultBuilderBackedTypedCommandWriter.java&line=32&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnStateTransitionBehavior.lambda$terminateChildInstances$3 ( [io/camunda.zeebe.engine.processing.bpmn.behavior/BpmnStateTransitionBehavior.java:332](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn.behavior%2FBpmnStateTransitionBehavior.java&line=332&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnStateTransitionBehavior.terminateChildInstances ( [io/camunda.zeebe.engine.processing.bpmn.behavior/BpmnStateTransitionBehavior.java:330](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn.behavior%2FBpmnStateTransitionBehavior.java&line=330&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.container.ProcessProcessor.onTerminate ( [io/camunda.zeebe.engine.processing.bpmn.container/ProcessProcessor.java:85](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn.container%2FProcessProcessor.java&line=85&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.container.ProcessProcessor.onTerminate ( [io/camunda.zeebe.engine.processing.bpmn.container/ProcessProcessor.java:27](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn.container%2FProcessProcessor.java&line=27&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.processEvent ( [io/camunda.zeebe.engine.processing.bpmn/BpmnStreamProcessor.java:122](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn%2FBpmnStreamProcessor.java&line=122&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.lambda$processRecord$0 ( [io/camunda.zeebe.engine.processing.bpmn/BpmnStreamProcessor.java:95](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn%2FBpmnStreamProcessor.java&line=95&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.util.Either$Right.ifRightOrLeft ( [io/camunda.zeebe.util/Either.java:381](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.util%2FEither.java&line=381&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.processRecord ( [io/camunda.zeebe.engine.processing.bpmn/BpmnStreamProcessor.java:92](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn%2FBpmnStreamProcessor.java&line=92&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.Engine.process ( [io/camunda.zeebe.engine/Engine.java:128](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine%2FEngine.java&line=128&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$3 ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:264](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=264&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run ( [io/camunda.zeebe.db.impl.rocksdb.transaction/ZeebeTransaction.java:84](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.db.impl.rocksdb.transaction%2FZeebeTransaction.java&line=84&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:260](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=260&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:209](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=209&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.readNextRecord ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:185](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=185&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.scheduler.ActorJob.invoke ( [io/camunda.zeebe.scheduler/ActorJob.java:92](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorJob.java&line=92&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.scheduler.ActorJob.execute ( [io/camunda.zeebe.scheduler/ActorJob.java:45](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorJob.java&line=45&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.scheduler.ActorTask.execute ( [io/camunda.zeebe.scheduler/ActorTask.java:119](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorTask.java&line=119&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask ( [io/camunda.zeebe.scheduler/ActorThread.java:106](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorThread.java&line=106&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.scheduler.ActorThread.doWork ( [io/camunda.zeebe.scheduler/ActorThread.java:87](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorThread.java&line=87&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.scheduler.ActorThread.run ( [io/camunda.zeebe.scheduler/ActorThread.java:198](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorThread.java&line=198&project=camunda-cloud-240911) )\r\n```\r\n\r\nError group, https://console.cloud.google.com/errors/detail/CJujpJmq_NqemgE;service=zeebe;time=P7D?project=camunda-cloud-240911\n saig0: :information_source: Currently, the `cancel` command is excluded from blacklisting (see [here](https://github.com/camunda/zeebe/blob/main/protocol/src/main/java/io/camunda/zeebe/protocol/record/intent/ProcessInstanceIntent.java#L22)). As a result, the process instance continues with processing.\n Zelldon: :warning: Happened again this week, and caused another incident\r\n\r\nHappened on 8.1.3 https://console.cloud.google.com/errors/detail/CKvjvtrYm_SiuwE;service=zeebe;time=P7D?project=camunda-cloud-240911 \n Zelldon: I would request to re-evaluate the priority of this by @camunda/zeebe-process-automation \r\n\r\nIncidents shouldn't happen twice. This seems to be an issue that people seem to run into easily, and there is no good way to resolve it.\n korthout: Triage summary:\r\n- Create an EPIC to tackle this problem correctly: support cancelling instances with many tokens (@aleksander-dytko )\r\n- Provide a quick and dirty solution to avoid this producing further incidents.\r\n\r\nLet's continue working on this issue by providing this quick and dirty solution\n aleksander-dytko: @korthout could you please check if I have summarized all the details in https://github.com/camunda/product-hub/issues/1067 ? \r\nThanks!\n korthout: @aleksander-dytko Thanks for creating the EPIC. I think you cover all the details.\n npepinpe: This happened again, except this time the number of child element instances is so great it causes the nodes to first slow down to a crawl due to very high GC times, then be killed due to OOM.\r\n\r\nIncident link: https://camunda.slack.com/archives/C051HA4V63D\r\nData link (incl. heap dump, process BPMN, and the complete node state): https://drive.google.com/drive/folders/1VkseQsD8Czi33dQi_kE_vV-YnfOTuJgu?usp=share_link\r\n\r\nIn case of investigation with this data, the key of the command is `4503599643148887` and its position is `93582578`. It is a `ProcessInstance.TERMINATE_ELEMENT` command.\r\n\r\nAffected version is 8.1.9, though I imagine most versions are affected.\r\n\r\nFrom the heap dump:\r\n\r\n![image](https://user-images.githubusercontent.com/43373/229531128-3fdd7686-3840-4c26-a42b-002a51142bfe.png)\r\n\r\n> The thread io.camunda.zeebe.scheduler.ActorThread @ 0xab7760e8 Broker-2-zb-actors-2 keeps local variables with total size 1.90 GB (98.54%) bytes.\r\nThe memory is accumulated in one instance of java.lang.Object[], loaded by <system class loader>, which occupies 1.90 GB (98.52%) bytes.\r\nThe stacktrace of this Thread is available. See stacktrace. See stacktrace with involved local variables.\r\n>\r\n> Keywords\r\n> - java.lang.Object[]\r\n> - io.camunda.zeebe.engine.state.instance.DbElementInstanceState.lambda$getChildren$2(Ljava/util/List;Lio/camunda/zeebe/db/impl/DbCompositeKey;Lio/camunda/zeebe/db/impl/DbNil;)V\r\nDbElementInstanceState.java:258\r\n> - io.camunda.zeebe.engine.state.instance.DbElementInstanceState.getChildren(J)Ljava/util/List;\r\n> - DbElementInstanceState.java:254\r\n\r\nMemory metrics:\r\n\r\n![image](https://user-images.githubusercontent.com/43373/229530936-c7b62201-eae5-4091-8051-09ab681e5bae.png)\r\n\r\n\r\nIn our case, the cluster was also unusable, and likely the only way to recover it is to give it [ludicrous](https://www.youtube.com/watch?v=oApAdwuqtn8) amounts of memory.\n npepinpe: Relevant support issue: https://jira.camunda.com/browse/SUPPORT-16499\r\n\r\nAnd clusters which run into this are likely to be affected by https://github.com/camunda/zeebe/issues/12239 as well (relevant support issue: https://jira.camunda.com/browse/SUPPORT-16394).\r\n\r\nPlease update the support team once these issues are fixed with a patch ETA :pray: \n remcowesterhoud: I've renamed this issue as the descriptions are not related to deep-nesting. They are related to a process instance which contains many active elements instances.\r\n\r\nFor the deep-nesting we have another issue: \r\n- https://github.com/camunda/zeebe/issues/8955\r\n\r\nI've created an epic to do a proper task breakdown https://github.com/camunda/zeebe/issues/12485",
    "title": "Not possible to cancel process instance with many active element instances"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12836",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\n\nSince the merge of https://github.com/camunda/zeebe/pull/12001 zeebe depends on identity for all 8.2.0 release and later.\nAs an identity release may happen right before the zeebe release we need to add a step to the release process to make sure the identity version is the same as the zeebe version or if not update it.\n\nRight now patch level version of zeebe and identity are aligned, so if zeebe 8.2.5 is released the identity version should be 8.2.5 as well. This could be automated.\n\n\n```[tasklist]\n### Tasks\n- [ ] https://github.com/camunda/zeebe/issues/12920\n- [ ] https://github.com/zeebe-io/zeebe-engineering-processes/issues/312\n```\n\n\n\n megglos: Release process now covers checking for an existing release and or awaiting the needed release based on a Github webhook. \r\n![image](https://github.com/camunda/zeebe/assets/209518/69f40751-3da0-4b51-8ef9-44e6b143a8b3)\r\n\r\nto be tested the next days, together with @oleschoenburg \n abbasadel: ZPA Planning: \n- @megglos is there anything still needed form ZPA to close this?\n remcowesterhoud: @megglos I see all tasks here are closed. Can this issue be closed as well?\n megglos: Two releases went fine with this change, closing.",
    "title": "Release: Integrate update of identity version into the release process"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12695",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "<!--\r\nIn case you have questions about our software we encourage everyone to participate in our community via the\r\n- Camunda Platform community forum https://forum.camunda.io/ or \r\n- Slack https://camunda-cloud.slack.com/ (For invite: https://camunda-slack-invite.herokuapp.com/)\r\n\r\nThere you can exchange ideas with other Zeebe and Camunda Platform 8 users, as well as the product developers, and use the search to find answer to similar questions.\r\n\r\nThis issue template is used by the Zeebe engineers to create general tasks.\r\n-->\r\n\r\n**Description**\r\n\r\nWhile implementing #12085 (PR https://github.com/camunda/zeebe/pull/12694) duplicated code occurred in job command precondition implementations. Further, this duplication already exists with previous implementations.\r\n\r\n**Expected Outcome**\r\nThe expected outcome of refactoring is to eliminate duplications in job command preconditions and make it easier to add new preconditions in the future.\r\n\n",
    "title": "Refactor Job Command Preconditions"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12577",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nFor main, stable/8.1, probably also stable/8.2 renovate seems to have issues updating the digest.\r\n\r\n```\r\nDEBUG: Digest is not updated(packageFile=\"Dockerfile\", branch=\"renovate/stable/8.1-eclipse-temurin-17.x\")\r\n{\r\n  \"baseBranch\": \"stable/8.1\",\r\n  \"manager\": \"dockerfile\",\r\n  \"expectedValue\": \"sha256:22f133769ce2b956d150ab749cd4630b3e7fbac2b37049911aa0973a1283047c\",\r\n  \"foundValue\": \"sha256:b10df4660e02cf944260b13182e4815fc3e577ba510de7f4abccc797e93d9106\"\r\n}\r\nWARN: Error updating branch: update failure(branch=\"renovate/stable/8.1-eclipse-temurin-17.x\")\r\n{\r\n  \"baseBranch\": \"stable/8.1\"\r\n}\r\n```\r\n\r\nSee further logs at https://app.renovatebot.com/dashboard#github/camunda/zeebe/\r\n\n\n megglos: turns out the recent introduction of JVM and JAVA_VERSION broke the updates\r\n```\r\n          {\r\n            \"autoReplaceStringTemplate\": \"{{depName}}{{#if newValue}}:{{newValue}}{{/if}}{{#if newDigest}}@{{newDigest}}{{/if}}\",\r\n            \"datasource\": \"docker\",\r\n            \"depType\": \"stage\",\r\n            \"replaceString\": \"${JVM}:${JAVA_VERSION}-jre-focal@sha256:54f64f1cf8e9b984a92d06d3ad5c10fbbb9e9869144f1f45decdf530d64a4163\",\r\n            \"skipReason\": \"invalid-name\",\r\n            \"updates\": []\r\n          },\r\n```\n koevskinikola: ZPA triage:\n- @megglos it looks like ZDP is already working on this issue. Is there any reason why the issue is on the ZPA board as well?\n- ZPA will remove it from the team board. If there is anything we need to do, please re-add it to the ZPA board.\n megglos: changes are applied, I'm monitoring it till the next updates are applied to all branches",
    "title": "Renovate fails updating docker digest"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12388",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nBefore the ZPA team gets started on their side in the gateway, they should already have access to a provisioned `ClientStreamer` which they can use to add/remove streams in order to forward activated jobs.\r\n\r\nIt's perfectly fine if the streamer is initially unused, as we'll likely get to this before them, and the goal here is simply to make sure they're unblocked.\r\n\n",
    "title": "Provide ClientStreamer instance to the GatewayGrpcService/EndpointManager"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12386",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "## Description\r\nThe current error handling strategy in the job push pipeline is to yield the job back to partition if there are any observed errors until it reaches the client.\r\n\r\nThis can have a significant performance impact, since it means writing a new command - this command has to be written, replicated, committed, then processed, and the events must be committed, and then the job is activated and sent out.\r\n\r\nTo speed things up, if there is another logically equivalent stream (whether on the gateway or broker), we should try pushing to that one instead.\r\n\r\nFor the first phase, we can do this naively, ignoring any flow control or optimum work distribution.\n\n npepinpe: @deepthidevaki - this makes me think, maybe the gateway should never yield the job back directly, and instead the broker will always be the one doing this. \r\n\r\nSince a retry strategy could be:\r\n\r\nBroker pushes job to gateway 1/stream A -> gateway 1 fails to forward job, tries with stream B -> gateway 1 fails to forward job, sends it back to the broker -> broker pushes to gateway 2/stream A' -> success\n deepthidevaki: > this makes me think, maybe the gateway should never yield the job back directly, and instead the broker will always be the one doing this.\r\n> \r\n> Since a retry strategy could be:\r\n> \r\n> Broker pushes job to gateway 1/stream A -> gateway 1 fails to forward job, tries with stream B -> gateway 1 fails to forward job, sends it back to the broker -> broker pushes to gateway 2/stream A' -> success\r\n\r\nMakes sense. We could start with this approach.\r\n\r\n",
    "title": "Try pushing activated job to next logical stream on failure before yielding"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11884",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nI think that this needs to be done for basically two reasons:\r\n1. Consistency. We will start to use a single class to gain the info and in case we need to modify the caller part - we just modify `JobClient`\r\n2. Metrics. Source: https://camunda-platform.slack.com/archives/C6WGNHV2A/p1677234921004609. So, this continues the previous point - we need to have a single place to introduce metrics - I bet that the `JobClient` and `ZeebeClient` would be better ones.\r\n\r\n**Describe the solution you'd like**\r\nWe implement the method `public ActivateJobsCommandStep1 newActivateJobsCommand()` in the `JobClient` and use it in the `JobPoller`.\r\n\r\n**Describe alternatives you've considered**\r\nWe could just use the same method in the `ZeebeClient` but it feels not right to me 😅. Let all job calls will be in the `JobClient`.\r\n\r\n**Additional context**\r\nI could take this issue if you are okay with it. Also, I would like to take worker's metrics as well.\n\n aivinog1: @npepinpe Do you mind if I take it and https://github.com/camunda/zeebe/issues/4700?\n npepinpe: No, go for it. Ideally we should expose the same metrics as with the Go client.\n\nIt might be interesting to use a well established metrics facade here, e.g. OpenTelemetry, but I'll let someone from the automation team decide, as that would add a new dependency to our public API.\n korthout: I think this is a great idea @aivinog1 👍 Thanks for raising it.\r\n\r\nI see no reason to use the gateway stub directly. Note that the JobRunnerFactory already [uses the `jobClient` to fail the job](https://github.com/camunda/zeebe/blob/main/clients/java/src/main/java/io/camunda/zeebe/client/impl/worker/JobRunnableFactory.java#L56) when an uncaught exception is thrown in the handler. So your suggestion would be in line with other implementation details.\r\n\r\n👍 Feel free to go ahead with this refactoring.\r\n\r\nI can see that this would be useful to collect metrics on the job client in a single place, but I wonder what metrics would be collected here. Would this only measure requests made with the client, or can we also use this for the metric mentioned in #4700 specifically:\r\n>monitor the amount of jobs currently enqueued\r\n\r\n@aivinog1 Let me know if you already have ideas to tackle that part.\r\n\r\n@npepinpe You raise a good point about using an established metrics facade. I think I would also prefer it, but I need to read up on this topic first. @aivinog1, if you already have a  design in mind, please share it with me.\n Zelldon: @korthout regarding metrics we have this https://github.com/camunda/zeebe/issues/11570 I guess this cover this pretty much. I hope we tackle this soon (that I have time for it soon)\n korthout: @aivinog1 I've assigned you. Please go ahead if you want to work on this 👍 \n\nI've also marked this issue as `later later` as it is not among our team's priorities.",
    "title": "Migrate from `GatewayStub` to `JobClient` in the `JobPoller`"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/9970",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nIn `BrokerAdminServiceImpl` we distinguish between leaders and followers. For leaders:\r\nhttps://github.com/camunda/zeebe/blob/6720e2e25e9432901c478b62491608d8048343f3/broker/src/main/java/io/camunda/zeebe/broker/system/management/BrokerAdminServiceImpl.java#L202-L208\r\nFor followers:\r\nhttps://github.com/camunda/zeebe/blob/6720e2e25e9432901c478b62491608d8048343f3/broker/src/main/java/io/camunda/zeebe/broker/system/management/BrokerAdminServiceImpl.java#L164-L165\r\n\r\nI believe this is not necessary since followers replay and all information that we return for leaders is also available on followers.\r\n\r\nThis would make writing some integration tests easier, for example for https://github.com/camunda/zeebe/pull/9954\n",
    "title": "Followers should return the full partition status for the `/partitions` actuator"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/6034",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\n\r\nGo 1.16 will have native support for embedding files so, after updating, we can use that and remove go-bindata as a dependency.\r\nhttps://tip.golang.org/doc/go1.16\n\n Zelldon: I think this should be a quick thing to do, we already moved to go 1.17\n abbasadel: We already moved to 1.19 in #12633. Does this issue still make sense?\n npepinpe: Yes :) The idea is to migrate off of using the go-bindata utility and use the built-in compiler capabilities via the `go:embed` annotation. Definitely worth doing to reduce our dependencies on external projects which may eventually prevent us from, say, migrating to another Go version.\n npepinpe: Actually, I might take a crack a it, because I think we can solve this without either go-bindata or go:embed, and just via build flags.\r\n\r\nEDIT: nevermind, I remember now why we couldn't do this - then all users would need to pass the right build flag whenever they build their own application :smile: ",
    "title": "Remove go-bindata after Go1. 16"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/6003",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nTo help tune the config parameters of Swim protocol, it would be useful to add some metrics. \r\nExamples:\r\n- Probe latency - RTT for a probe request\r\n- Gossip latency - how long until a metadata change on a node is propagated to another node.\r\n\r\nRelated to https://github.com/zeebe-io/zeebe/issues/4827#issuecomment-740624583 https://github.com/zeebe-io/zeebe/issues/4827#issuecomment-740768778\r\n\r\n\n\n Zelldon: I feel with adding messaging service metrics we this covered as well https://github.com/camunda/zeebe/pull/11353 we have labels for the message types, which allows us to see also probe and sync req-response latencies etc.\r\n\r\nwdyt @npepinpe \n Zelldon: I could add a separate section to the dashboard for swim using these metrics\n rodrigo-lourenco-lopes: Since we are broadcasting the gossip of these updates with no answers, how could we go about measuring this gossip latency?\r\nHow about implementing a response for these broadcasts along with the metric and putting everything behind a feature flag?\r\nOr ideally, there is a more straightforward way to measure this.\r\n\r\nhttps://github.com/camunda/zeebe/blob/5280440c43122cf2da5b4b83dbff76a644273e7c/atomix/cluster/src/main/java/io/atomix/cluster/protocol/SwimMembershipProtocol.java#LL753C1-L758C4\r\n\r\nwdyt? @npepinpe @Zelldon \n npepinpe: Honestly, it sounds to me like distributed tracing would be the tool we want to have here. Start an operation somewhere, measure when it completes somewhere else, possibly with hops.\r\n\r\nAs we don't have that yet, I would postpone this. However, there may be other metrics we'd like just from the local node?\n rodrigo-lourenco-lopes: The other thing we could measure perhaps is sync() we have a response for this one.\n oleschoenburg: We could also try to just export the current state as metrics. Then we could _derive_ additional properties such as \"how long does it take to propagate changes throughout the entire cluster\" by calculating it on the metrics. Not ideal but I feel like just exporting the current state of SWIM as metrics is useful already.\r\n\n rodrigo-lourenco-lopes: @oleschoenburg If I understood correctly we would export the local state of all members on each node? ",
    "title": "Add metrics to SwimMembershipProtocol"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12584",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Documentation"
    },
    "gitHubIssueText": "**Description**\r\n\r\nAdd a guide on what to do when a flaky test is encountered. The guide should enable contributors to make progress when flaky tests occur in their contributions.\r\n\r\nThis was an action derived from one of the ZPA team's recent retros.\r\n\r\n\n",
    "title": "Document guidelines on how to handle flaky tests"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/12633",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4618",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4464",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4567",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4661",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4602",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4462",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4493",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4465",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4669",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4657",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4641",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4642",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4622",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4635",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4636",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4629",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4589",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4587",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4586",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4582",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4632",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4620",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4495",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4615",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4614",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4609",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4608",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4575",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4571",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4568",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4577",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4563",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4569",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4535",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4561",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4560",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4559",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4557",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4416",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4564",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4556",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4565",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4430",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/3901",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4549",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4548",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4547",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4534",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4533",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4546",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4550",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4541",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4538",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4537",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4536",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4539",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4516",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4514",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4526",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4498",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4470",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4520",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4519",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4518",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4513",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4506",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4496",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4511",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4255",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4256",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4265",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4309",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4502",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4483",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4467",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4461",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4460",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4458",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4457",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4456",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4454",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3009",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3032",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3018",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3016",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3004",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2980",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3003",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2929",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2946",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2927",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2939",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2917",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2910",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2915",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2890",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2891",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3055",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3057",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3047",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3044",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3060",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3053",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3045",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3041",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3040",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3025",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3024",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3008",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3012",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3011",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3014",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3007",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2997",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2964",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2962",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2959",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2935",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2932",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2931",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2922",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2920",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2912",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2815",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2855",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2851",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2852",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2854",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2856",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3065",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3058",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3052",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3054",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3021",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3033",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3031",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3034",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3030",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3028",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3022",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3020",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3013",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2988",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2987",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2985",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3000",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3006",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/3005",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2996",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2977",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2994",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2993",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2992",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2991",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2983",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2982",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2981",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2978",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2975",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2970",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2967",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2968",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2963",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2949",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2948",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2947",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2943",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2899",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2926",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2930",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2925",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2924",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2921",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2919",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2913",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2911",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2906",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2905",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2903",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2902",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2901",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2869",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2859",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2836",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2752",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2828",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2835",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2882",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2880",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1837",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1830",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1823",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1827",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1818",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1816",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1791",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1789",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1749",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1814",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1825",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1824",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1804",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1780",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1775",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1768",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1758",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1757",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1756",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1737",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1829",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1838",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1833",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1836",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1832",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1826",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1828",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1820",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1822",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1808",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1806",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1807",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1805",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1798",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1795",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1800",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1533",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1796",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1794",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1781",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1782",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1792",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1777",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1778",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1776",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1759",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1772",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1769",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1764",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1765",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1761",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1747",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1755",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1754",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1753",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1752",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1735",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha2",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1736",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12122",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nSometimes, I may want to specify a larger than the default (4 MB) inbound `MAX_MESSAGE_SIZE` for the client. For example, when I create a process instance with a result where the resulting variables are larger than the default 4 MB.\r\n\r\n**Describe the solution you'd like**\r\nAdd a configuration option to the client to specify an inbound max message size applied to the gRPC responses. The default should be the current hardcoded 4 MB.\r\n\r\n**Describe alternatives you've considered**\r\n- specify it for each command: this is hard to implement at this time as the channel is set up on client construction\r\n- don't allow specifying it: the current behavior throws errors in the client when the gateway responds with larger than 4MB messages (which is possible already).\r\n\r\n**Additional context**\r\n- https://github.com/camunda/zeebe/pull/11902#issuecomment-1480460031\r\n- https://github.com/camunda/zeebe/issues/12104\r\n\n",
    "title": "Configure the client's inbound max_message_size"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12538",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\n\r\nContainer elements (process, subprocess, etc.) all terminate their child instances when `onTerminate` is called. For all these elements this method calls the `BpmnStateTransitionBehavior#terminateChildInstances` method. This method needs to be changed to make use of the new `ProcessInstanceBatch` command with the `TERMINATE` intent.\r\n\r\n- Modify the `BpmnStateTransitionBehavior#terminateChildInstances `:\r\n    - Create a `ProcessInstanceBatch` record\r\n        - `batchElementInstanceKey` will be the key of the container element\r\n        - `index` will be empty as this is the first batch command\r\n    - Write the a `ProcessInstanceBatch.TERMINATE` command using the created record\n",
    "title": "Use the `ProcessInstanceBatch Command` when terminating container elements"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12537",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "## Description\r\nThe `ProcessInstanceBatch` record and intent will be used to perform certain actions (terminating/activating children) on a process instance in batches.\r\n\r\nThe record will contain the following data:\r\n- `batchElementInstanceKey` - The element instance key of the element that the batch is executed. E.g., the key of a subprocess which will terminate all its children.\r\n- ~~`childElementInstanceKeys` - The element instance keys of all elements for which the batch action needs to be performed. E.g., all child instance for which a `TERMINATE` command needs to be written.~~\r\n- `index` - The index to keep track of where we are in the batch. Depending on the Intent this index can be something different. For `TERMINATE` this will be the element instance key of the first child in the next batch.\r\n\r\nThere will be only 1 intent for now:\r\n- `TERMINATE`\n",
    "title": "Create `ProcessInstanceBatch` Record and Intent"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12416",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nIt is hard to tune the JVM if I cannot override the default `-Xms128m`. I can't override it via `JAVA_OPTS` because it is used at first, and then goes explicit `-Xms128m` (and finally applied):\r\n```\r\nexec \"$JAVACMD\" $JAVA_OPTS -Xms128m -XX:+ExitOnOutOfMemoryError -Dfile.encoding=UTF-8...\r\n```\r\n\r\n**Describe the solution you'd like**\r\nI would like to move all JVM options into the default Docker environment variable (`JAVA_OPTS`) so that users could override it easily (`-Xms` included).\r\n\r\n**Describe alternatives you've considered**\r\nWe could just remove `-Xms128m` from the default options, but I can't predict the consequences.\r\n\r\n**Additional context**\r\nSee the details [here](https://camunda-platform.slack.com/archives/C6WGNHV2A/p1681141205701259).\r\nI would like to see the changes backported in 8.0, 8.1, 8.2 as well 🙂\n\n remcowesterhoud: @megglos Do you know which team should be responsible for this one? I feel like this is more of a shared responsibility and I don't see it listed in the team split document.\n megglos: this affects the dist module (which is shared? 😅 ), the script is generated via the `appassembler-maven-plugin`, which has a feature request for exactly this open since 2016 😅 \r\nhttps://github.com/mojohaus/appassembler/issues/48\r\n\r\nJust synced with @Zelldon on this, the best would probably be to remove the Xms flag from the plugin config and stick to jvm defaults\r\nhttps://github.com/camunda/zeebe/blob/main/dist/pom.xml#L290\n remcowesterhoud: Oh right, I don't know how I missed that in the split document 🤦 \r\n\r\nI'll assign it to both our project in that case\n aivinog1: Hey @remcowesterhoud! If this is not occupied I could provide the PR to fix it:\r\n> Just synced with @Zelldon on this, the best would probably be to remove the Xms flag from the plugin config and stick to jvm defaults\n remcowesterhoud: @aivinog1 Thanks! It's not occupied so go ahead. I'll assign the issue to you 👍 ",
    "title": "Remove the default un-overridable `-Xms128m` value"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12000",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "**Is your feature request related to a problem? Please describe.**\r\nThis is part of the break-down of the following epic https://github.com/camunda/product-hub/issues/120 .\r\n\r\n**Describe the solution you'd like**\r\nIn order to add native support for oauth token authentication in Camunda 8 SM the Zeebe Gateway needs to get extended to perform oauth authentication token validation. The scope of this feature is just authentication, authorisation is to be added in a follow-up. Ideally the solution is making use of the identity-sdk for token verification.\r\n\r\n**Describe alternatives you've considered**\r\nAn alternative would be adding an auth proxy component to the Camunda 8 SM stack, however this adds more complexity to the infrastructure.\r\n\r\n**Additional context**\r\n- https://github.com/camunda-community-hub/zeebe-keycloak-interceptor Community project\r\n- Identity interceptor prototype https://github.com/npepinpe/zeebe-identity-interceptor/blob/main/src/main/java/org/camunda/community/zeebe/interceptors/identity/Interceptor.java\r\n\r\n\r\nRelates to:\r\n- https://jira.camunda.com/browse/SUPPORT-15807\n",
    "title": "OAuth Auth Token authentication support in Zeebe Gateway"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11920",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "- ## Adjust `EndEventProcessor` to broadcast a signal on activation\r\n\t- Introduce a new `SignalEndEventBehavior`\r\n\t- When Signal End Event activates:\r\n\t\t- Apply input mappings\r\n\t\t- Transition to activated\r\n\t\t- Write `Signal:Broadcast` command\r\n\t\t- Apply output mappings\r\n\t\t- Transition to complete the element\n",
    "title": "Support Broadcast signal for Signal End Events"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11919",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "gitHubIssueText": "- ## Adjust `IntermediateThrowEventProcessor` to broadcast a signal on activation\r\n\t- When Signal Intermediate Throw Event activates:\r\n\t\t- Apply input mappings\r\n\t\t- Transition to activated\r\n\t\t- Write `Signal:Broadcast` command\r\n\t\t- Apply output mappings\r\n\t\t- Transition to complete the element\r\n\r\n\n",
    "title": "Support Broadcast signal for Signal Intermediate Throw Events"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12622",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nSometime, a partition may attempt to take a backup again when a backup for the same id already exists. This case is rare, but can happen sometimes if there is a leader change while taking a backup. If this happens, list backup fails with an error message:\r\n```\r\n{\r\n    \"message\": \"Duplicate key 1 (attempted merging values BackupStatus[backupId=1, partitionId=1, status=COMPLETED, failureReason=, brokerVersion=8.2.2, createdAt=...])\"\r\n}\r\n```\r\n\r\n**Expected behavior**\r\nList backup should be able to handle duplicate backup ids for a partition.\r\n\n",
    "title": "List backup fails when a partition has same backup taken by multiple nodes"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12597",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nTrying to list all available backups will always fail without a useful error message.\r\nThe gateway distributes a list request to all brokers which then list all of their backups and try to respond with a `BackupListResponse`:\r\n\r\nhttps://github.com/camunda/zeebe/blob/4854b6606a803926ed9cadabfc2edb4aede18cb4/protocol/src/main/resources/cluster-management-protocol.xml#L64-L76\r\n\r\nThe `groupSizeEncoding` is defined by us:\r\n\r\nhttps://github.com/camunda/zeebe/blob/c861aac736376e1cc20aa558979c6d9c289b4a1f/protocol/src/main/resources/common-types.xml#L16-L19\r\n\r\nIt uses a `unit8` to represent the number of entries. When trying to write a `BackupListResponse` with more than 255 entries, the encoder rejects it:\r\n```\r\njava.lang.IllegalArgumentException: count outside allowed range: count=774\r\n\tat io.camunda.zeebe.protocol.management.BackupListResponseEncoder$BackupsEncoder.wrap(BackupListResponseEncoder.java:137) ~[zeebe-protocol-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.protocol.management.BackupListResponseEncoder.backupsCount(BackupListResponseEncoder.java:114) ~[zeebe-protocol-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.protocol.impl.encoding.BackupListResponse.write(BackupListResponse.java:100) ~[zeebe-protocol-impl-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.broker.transport.backupapi.BackupApiResponseWriter.write(BackupApiResponseWriter.java:71) ~[zeebe-broker-8.2.2.jar:8.2.2]\r\n....\r\n\r\n```\r\n**To Reproduce**\r\n\r\nTake 256 backups, then query all backups via `GET actuator/backups`.\r\n\r\n**Expected behavior**\r\n\r\n1. Zeebe supports much more backups than 255 (for example by using a `uint16`, thus supporting 65535 backups)\r\n2. The number of listed backups should be limited to a reasonable number. Querying the backup store to list, say 1000, available backups is likely to result in timeouts and makes the backup API unusable.\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.lang.IllegalArgumentException: count outside allowed range: count=774\r\n\tat io.camunda.zeebe.protocol.management.BackupListResponseEncoder$BackupsEncoder.wrap(BackupListResponseEncoder.java:137) ~[zeebe-protocol-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.protocol.management.BackupListResponseEncoder.backupsCount(BackupListResponseEncoder.java:114) ~[zeebe-protocol-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.protocol.impl.encoding.BackupListResponse.write(BackupListResponse.java:100) ~[zeebe-protocol-impl-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.broker.transport.backupapi.BackupApiResponseWriter.write(BackupApiResponseWriter.java:71) ~[zeebe-broker-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.transport.impl.ServerResponseImpl.write(ServerResponseImpl.java:50) ~[zeebe-transport-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.transport.impl.AtomixServerTransport.sendResponse(AtomixServerTransport.java:154) ~[zeebe-transport-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.broker.transport.backupapi.BackupApiResponseWriter.tryWriteResponse(BackupApiResponseWriter.java:51) ~[zeebe-broker-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.broker.transport.AsyncApiRequestHandler.lambda$handleRequest$1(AsyncApiRequestHandler.java:123) ~[zeebe-broker-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.future.FutureContinuationRunnable.run(FutureContinuationRunnable.java:28) ~[zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94) ~[zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) [zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) [zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) [zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) [zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) [zeebe-scheduler-8.2.2.jar:8.2.2]\r\n\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- Zeebe Version: >= 8.1\r\n\n",
    "title": "Listing backups fails if more than 255 backups are available"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12509",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nMessageTTL checking fails with deserialization errors.\r\n\r\n```\r\njava.lang.RuntimeException: Could not deserialize object [MessageRecord]. Deserialization stuck at offset 24 of length 69\r\n    at io.camunda.zeebe.msgpack.UnpackedObject.wrap(UnpackedObject.java:38)\r\n    at io.camunda.zeebe.engine.api.records.RecordBatchEntry.createEntry(RecordBatchEntry.java:73)\r\n    at io.camunda.zeebe.engine.api.records.RecordBatch.appendRecord(RecordBatch.java:48)\r\n    at io.camunda.zeebe.streamprocessor.BufferedTaskResultBuilder.appendCommandRecord(BufferedTaskResultBuilder.java:47)\r\n    at io.camunda.zeebe.engine.processing.message.MessageTimeToLiveChecker.lambda$execute$0(MessageTimeToLiveChecker.java:90)\r\n```\r\n\r\nThis will fail the processing actor and prevent processing on this partition. When the (experimental) feature flag `enableMessageTtlCheckerAsync` is used, a different actor fails and only prevents further MessageTTL checking but not processing.\r\n\r\nThis is a regression, introduced in https://github.com/camunda/zeebe/commit/e1a6cae69c17325fc71a8ee92022a70d969bd0da\r\nIt affects versions >= 8.1.9 and >= 8.2.0.\r\n\r\n\r\n**To Reproduce**\r\n\r\nRun a broker that is leader for multiple partitions. When messages from two different partitions are expired at the same time, there is unsafe concurrent access to the writer of a shared record value. \r\n\r\n**Expected behavior**\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.lang.RuntimeException: Could not deserialize object [MessageRecord]. Deserialization stuck at offset 24 of length 69\r\n    at io.camunda.zeebe.msgpack.UnpackedObject.wrap(UnpackedObject.java:38)\r\n    at io.camunda.zeebe.engine.api.records.RecordBatchEntry.createEntry(RecordBatchEntry.java:73)\r\n    at io.camunda.zeebe.engine.api.records.RecordBatch.appendRecord(RecordBatch.java:48)\r\n    at io.camunda.zeebe.streamprocessor.BufferedTaskResultBuilder.appendCommandRecord(BufferedTaskResultBuilder.java:47)\r\n    at io.camunda.zeebe.engine.processing.message.MessageTimeToLiveChecker.lambda$execute$0(MessageTimeToLiveChecker.java:90)\r\n    at io.camunda.zeebe.engine.state.message.DbMessageState.lambda$visitMessagesWithDeadlineBeforeTimestamp$2(DbMessageState.java:358)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.visit(TransactionalColumnFamily.java:390)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.lambda$forEachInPrefix$19(TransactionalColumnFamily.java:369)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.ColumnFamilyContext.withPrefixKey(ColumnFamilyContext.java:112)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.forEachInPrefix(TransactionalColumnFamily.java:353)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.lambda$whileTrue$8(TransactionalColumnFamily.java:174)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.lambda$ensureInOpenTransaction$18(TransactionalColumnFamily.java:308)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.DefaultTransactionContext.runInNewTransaction(DefaultTransactionContext.java:61)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.DefaultTransactionContext.runInTransaction(DefaultTransactionContext.java:33)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.ensureInOpenTransaction(TransactionalColumnFamily.java:307)\r\n    at io.camunda.zeebe.db.impl.rocksdb.transaction.TransactionalColumnFamily.whileTrue(TransactionalColumnFamily.java:174)\r\n    at io.camunda.zeebe.engine.state.message.DbMessageState.visitMessagesWithDeadlineBeforeTimestamp(DbMessageState.java:351)\r\n    at io.camunda.zeebe.engine.processing.message.MessageTimeToLiveChecker.execute(MessageTimeToLiveChecker.java:76)\r\n    at io.camunda.zeebe.streamprocessor.ProcessingScheduleServiceImpl.lambda$toRunnable$6(ProcessingScheduleServiceImpl.java:137)\r\n    at io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:94)\r\n    at io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45)\r\n    at io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119)\r\n    at io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106)\r\n    at io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87)\r\n    at io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198)\r\nCaused by: io.camunda.zeebe.msgpack.spec.MsgpackReaderException: Unable to determine string type, found unknown header byte 0x00 at reader offset 23\r\n    at io.camunda.zeebe.msgpack.spec.MsgPackReader.exceptionOnUnknownHeader(MsgPackReader.java:474)\r\n    at io.camunda.zeebe.msgpack.spec.MsgPackReader.readStringLength(MsgPackReader.java:140)\r\n    at io.camunda.zeebe.msgpack.value.StringValue.read(StringValue.java:96)\r\n    at io.camunda.zeebe.msgpack.value.ObjectValue.read(ObjectValue.java:91)\r\n    at io.camunda.zeebe.msgpack.UnpackedObject.wrap(UnpackedObject.java:32)\r\n    ... 24 more\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- OS: <!-- [e.g. Linux] -->\r\n- Zeebe Version: <!-- [e.g. 0.20.0] -->\r\n- Configuration: <!-- [e.g. exporters etc.] -->\r\n- \r\nRelates to https://jira.camunda.com/browse/SUPPORT-16711\r\n\n",
    "title": "MessageTTL checking fails with deserialization errors"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12433",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Description**\r\nWe are configuring brokers with the S3 backup properties as follows:\r\n- ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLEBACKUP: true\r\n- ZEEBE_BROKER_DATA_BACKUP_STORE: S3\r\n- ZEEBE_BROKER_DATA_BACKUP_S3_BUCKETNAME: <s3_bucket_name>\r\n- ZEEBE_BROKER_DATA_BACKUP_S3_REGION: us-east-1\r\n- ZEEBE_BROKER_DATA_BACKUP_S3_ENDPOINT: \"https://s3.us-east-1.amazonaws.com\"\r\n\r\nWe don't supply credentials through environment variables as they can be pulled from aws credentials provider chain. These properties worked on old version 8.1.9 but we got errors when switching to latest version 8.2.1 and broker cannot start due to the error:\r\n\r\n> \r\n> 2023-04-14T11:21:51.313-04:00 | org.springframework.boot.SpringApplication - Application run failed\r\n> 2023-04-14T11:21:51.313-04:00 | java.lang.IllegalStateException: Failed to execute CommandLineRunner\r\n> 2023-04-14T11:21:51.313-04:00 | at org.springframework.boot.SpringApplication.callRunner(SpringApplication.java:772) [spring-boot-3.0.5.jar:3.0.5]\r\n> 2023-04-14T11:21:51.313-04:00 | at org.springframework.boot.SpringApplication.callRunners(SpringApplication.java:753) [spring-boot-3.0.5.jar:3.0.5]\r\n> 2023-04-14T11:21:51.313-04:00 | at org.springframework.boot.SpringApplication.run(SpringApplication.java:317) [spring-boot-3.0.5.jar:3.0.5]\r\n> 2023-04-14T11:21:51.313-04:00 | at io.camunda.zeebe.broker.StandaloneBroker.main(StandaloneBroker.java:82) [camunda-zeebe-8.2.1.jar:8.2.1]\r\n> 2023-04-14T11:21:51.313-04:00 | Caused by: io.camunda.zeebe.broker.system.InvalidConfigurationException: Failed configuring backup store S3\r\n> 2023-04-14T11:21:51.313-04:00 | at io.camunda.zeebe.broker.system.SystemContext.validateBackupCfg(SystemContext.java:132) ~[zeebe-broker-8.2.1.jar:8.2.1]\r\n> 2023-04-14T11:21:51.313-04:00 | at io.camunda.zeebe.broker.system.SystemContext.validateDataConfig(SystemContext.java:116) ~[zeebe-broker-8.2.1.jar:8.2.1]\r\n> 2023-04-14T11:21:51.313-04:00 | at io.camunda.zeebe.broker.system.SystemContext.validateConfiguration(SystemContext.java:71) ~[zeebe-broker-8.2.1.jar:8.2.1]\r\n> 2023-04-14T11:21:51.313-04:00 | at io.camunda.zeebe.broker.system.SystemContext.initSystemContext(SystemContext.java:60) ~[zeebe-broker-8.2.1.jar:8.2.1]\r\n> 2023-04-14T11:21:51.313-04:00 | at io.camunda.zeebe.broker.system.SystemContext.<init>(SystemContext.java:56) ~[zeebe-broker-8.2.1.jar:8.2.1]\r\n> 2023-04-14T11:21:51.313-04:00 | at io.camunda.zeebe.broker.StandaloneBroker.run(StandaloneBroker.java:87) ~[camunda-zeebe-8.2.1.jar:8.2.1]\r\n> 2023-04-14T11:21:51.313-04:00 | at org.springframework.boot.SpringApplication.callRunner(SpringApplication.java:769) ~[spring-boot-3.0.5.jar:3.0.5]\r\n> 2023-04-14T11:21:51.313-04:00 | ... 3 more\r\n> 2023-04-14T11:21:51.313-04:00 | Caused by: java.lang.NullPointerException: Access key ID cannot be blank.\r\n\r\nIt looks like io.camunda.zeebe.backup.s3.S3BackupStore.buildClient still calls AwsBasicCredentials.create(credentials.accessKey(), credentials.secretKey() even we don't pass in any credentials. It might be related to this commit: https://github.com/camunda/zeebe/commit/dfd3b9e1034365b3fc1859d5e35353826e1222b3#diff-1b91d4d4e6875c0169ea56f9c35382f5f194a1af782db4879a26971fe32e6daeL72-L74\r\nWe also noticed that none of your unit or integration tests are checking for cases where ACCESS_KEY, SECRET_KEY are not passed. \r\n\r\nCould you please look into this issue? Thank you!\r\n\n\n oleschoenburg: Thanks for reporting @NingyuanZhang, you are right that https://github.com/camunda/zeebe/commit/dfd3b9e1034365b3fc1859d5e35353826e1222b3#diff-1b91d4d4e6875c0169ea56f9c35382f5f194a1af782db4879a26971fe32e6daeL72-L74 broke this use case.\r\n\r\nI'll provide a fix for this and see if we can test this too.",
    "title": "Broker cannot start with S3 accessKey and secretKey not supplied "
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12328",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nWith 8.2.0, it's not possible to disable the Raft flush without specifying a delay. This is mostly due to how Spring deserializes its configuration. As we use a `record` internally for the configuration, it will try to pass both properties, and the second one will be null. It's not really possible to rely on \"default\" values as we used to with simple classes.\r\n\r\n**To Reproduce**\r\n\r\nStart a 8.2.0 broker with the following environment variable: `ZEEBE_BROKER_CLUSTER_RAFT_FLUSH_ENABLED=false`\r\n\r\nThe broker will fail to start. It will however start if you specify:\r\n\r\n```yaml\r\nZEEBE_BROKER_CLUSTER_RAFT_FLUSH_ENABLED=false\r\nZEEBE_BROKER_CLUSTER_RAFT_FLUSH_DELAYTIME=0s\r\n```\r\n\r\nWith this configuration, you can pass whatever as delay time, it will simply be ignored. The behavior after is correct - it's just a matter of deserializing the configuration.\r\n\r\n**Expected behavior**\r\n\r\nI can disable the flush without having to specify a delay time.\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.2.0\r\n\n",
    "title": "Cannot disable Raft flush without specifying a delay"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12326",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "Hi Zeebe Team, please consider the following bug\r\n\r\n**Describe the bug**\r\n\r\nGiven the [following code snippet](https://github.com/camunda/connector-sdk/blob/main/runtime-util/src/main/java/io/camunda/connector/runtime/util/outbound/ConnectorJobHandler.java#L145-L150).\r\n\r\nThe `newThrowErrorCommand` cannot be handled via BPMN.\r\n\r\n**To Reproduce**\r\n\r\n1. Clone [camunda platform docker compose repo](https://github.com/camunda/camunda-platform).\r\n2. Keep `CAMUNDA_PLATFORM_VERSION` as `SNAPSHOT` or `8.2.0` in the `.env` [file](https://github.com/camunda/camunda-platform/blob/main/.env#L4).\r\n3. Run any variant, e.g. core as `docker-compose -f docker-compose-core.yaml up`.\r\n4. Deploy and start a BPMN diagram attached to this ticket: [error-handling.bpmn.txt](https://github.com/camunda/zeebe/files/11178216/error-handling.bpmn.txt).\r\n5. See an error: `Expected to throw an error event with the code '500' with message 'Got a 500', but it was not caught. No error events are available in the scope.`\r\n\r\n![Screenshot 2023-04-07 at 13 25 51](https://user-images.githubusercontent.com/108870003/230594417-d92ba764-6387-49a7-a9c7-b6773b66e863.png)\r\n\r\n![Screenshot 2023-04-07 at 13 26 02](https://user-images.githubusercontent.com/108870003/230594436-9703f853-5e96-4d4f-8676-3509789e59e9.png)\r\n\r\n\r\n**Expected behavior**\r\n\r\nDo the same as in **To Reproduce** except change the `CAMUNDA_PLATFORM_VERSION` to `8.1.10` or `8.1.9`.\r\n\r\nSee the process works correctly.\r\n\r\n![Screenshot 2023-04-07 at 13 23 19](https://user-images.githubusercontent.com/108870003/230594734-ef38e7b0-48ec-4632-bd62-4378e34a6fef.png)\r\n\r\n**Log/Stacktrace**\r\n\r\nNo valuable stacktraces.\r\n\r\n**Environment:**\r\n- OS:  MacOS 13.3 with M1 chip; Docker Engine version 20.10.23\r\n- Zeebe Version: `8.2.0`, `SNAPSHOT` - reproducible; `8.1.10`, `8.1.9` - not reproducible.\r\n- Configuration: [camunda-platform docker compose repo](https://github.com/camunda/camunda-platform/blob/main/docker-compose-core.yaml).\r\n\n\n remcowesterhoud: I had a first look. This was introduced when [adding support for FEEL expressions in error codes](https://github.com/camunda/zeebe/pull/10972). \r\n\r\nIn this PR we changed the way transform errors, by either setting the `errorCodeExpression` (not support by catch events) or by setting the `errorCode` directly. We _always_ parse the error code using the FEEL engine. In this case this will result in a static expression.\r\nWhere we're going wrong is that we make the assumption that this static expression can only be of the type `String`. In this case this is not true, it is a `Number` instead.\r\n\r\nRelevant code:\r\n\r\n```java\r\n  @Override\r\n  public void transform(final Error element, final TransformContext context) {\r\n\r\n    final var error = new ExecutableError(element.getId());\r\n    final var expressionLanguage = context.getExpressionLanguage();\r\n\r\n    // ignore error events that are not references by the process\r\n    Optional.ofNullable(element.getErrorCode())\r\n        .ifPresent(\r\n            errorCode -> {\r\n              final Expression errorCodeExpression = expressionLanguage.parseExpression(errorCode);\r\n\r\n              error.setErrorCodeExpression(errorCodeExpression);\r\n              if (errorCodeExpression.isStatic()) {\r\n                final EvaluationResult errorCodeResult =\r\n                    expressionLanguage.evaluateExpression(errorCodeExpression, variable -> null);\r\n\r\n                if (errorCodeResult.getType() == ResultType.STRING) {\r\n                  error.setErrorCode(BufferUtil.wrapString(errorCodeResult.getString()));\r\n                }\r\n              }\r\n\r\n              context.addError(error);\r\n            });\r\n  }\r\n```\r\n\r\nLooking into the `StaticExpression`  can only be of type `String` or `Number`\r\n```java\r\n  public StaticExpression(final String expression) {\r\n    this.expression = expression;\r\n\r\n    try {\r\n      treatAsNumber(expression);\r\n    } catch (final NumberFormatException e) {\r\n      treatAsString(expression);\r\n    }\r\n  }\r\n```\r\n\r\n------\r\n\r\nOptions to fix this:\r\n- Support `Number` in our transformers\r\n- Add a `getAsString` method to the `StaticExpression` which takes care of parsing the `Number` to a `String` (not sure how viable this is)\r\n\r\n------\r\n\r\nPlease note we do a similar thing in:\r\n- `EscalationTransformer`\r\n- `MessageTransformer` - Caught with deployment rejection or creating an incident. We should not change this behaviour\r\n- `SignalTransformer` - 8.2 only supports start event. Deployments with a signal as a number get rejected already\r\n- `StartEventTransformer` - Only for messages and signals which we should not change",
    "title": "The `newThrowErrorCommand` incorrectly handled in `8.2.0`"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12173",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nObserved in a test cluster in `ultrachaos`.\r\n`zeebe-2` is restarted. But it can never gets ready because it is not receiving heartbeats from the leader zeebe-0.\r\n We can see repeated logs in zeebe-2  `No heartbeat from a known leader .. Sending poll requests to all active members`\r\nHowever, in zeebe-0 we see that it is indeed sending messages to zeebe-2 and getting acknowledgments.\r\n```\r\nDEBUG 2023-03-28T11:13:29.998615141Z [resource.labels.podName: zeebe-0] [resource.labels.containerName: zeebe] RaftServer{raft-partition-partition-3} - Sending AppendRequest{term=2, leader=0, prevLogIndex=32433723, prevLogTerm=2, entries=1, commitIndex=32433722} to 2\r\nDEBUG 2023-03-28T11:13:29.998929396Z [resource.labels.podName: zeebe-0] [resource.labels.containerName: zeebe] RaftServer{raft-partition-partition-1} - Received AppendResponse{status=OK, term=1, succeeded=true, lastLogIndex=32442689, lastSnapshotIndex=32240013} from 2\r\n``` \r\nHowever, zeebe-2 is not receiving any of these messages. Confirmed from the logs and zeebe-2 journal segments.\r\n\r\nOn investigating futher, it seems zeebe-1 is receiving messages for zeebe-2. \r\n\r\nFor every single request send from zeebe-0 to zeebe-1, zeebe-1 is receiving two requests. It seems the duplicate request is for zeebe-2. (the logs that shows requests to 2 is omitted in the following logs).\r\n```\r\nDEBUG 2023-03-28T11:11:08.699639973Z [ zeebe-0] - Sending AppendRequest{term=1, leader=0, prevLogIndex=32402784, prevLogTerm=1, entries=1, commitIndex=32402784} to 1\r\n\r\nDEBUG 2023-03-28T11:11:08.701492523Z [ zeebe-1]{role=FOLLOWER} - Sending AppendResponse{status=OK, term=1, succeeded=true, lastLogIndex=32402785, lastSnapshotIndex=32240013}\r\nDEBUG 2023-03-28T11:11:08.701775314Z [ zeebe-1]{role=FOLLOWER} - Received AppendRequest{term=1, leader=0, prevLogIndex=32402784, prevLogTerm=1, entries=1, commitIndex=32402784}\r\nDEBUG 2023-03-28T11:11:08.702472045Z [ zeebe-1]{role=FOLLOWER} - Sending AppendResponse{status=OK, term=1, succeeded=true, lastLogIndex=32402785, lastSnapshotIndex=32240013}\r\nDEBUG 2023-03-28T11:11:08.702963186Z [ zeebe-0] - Received AppendResponse{status=OK, term=1, succeeded=true, lastLogIndex=32402785, lastSnapshotIndex=32240013} from 1\r\n\r\n\r\nDEBUG 2023-03-28T11:11:08.706279292Z [ zeebe-0] - Sending AppendRequest{term=1, leader=0, prevLogIndex=32402785, prevLogTerm=1, entries=1, commitIndex=32402785} to 1\r\nDEBUG 2023-03-28T11:11:08.706659278Z [ zeebe-1]{role=FOLLOWER} - Received AppendRequest{term=1, leader=0, prevLogIndex=32402785, prevLogTerm=1, entries=1, commitIndex=32402785}\r\nDEBUG 2023-03-28T11:11:08.708798386Z [ zeebe-1]{role=FOLLOWER} - Sending AppendResponse{status=OK, term=1, succeeded=true, lastLogIndex=32402786, lastSnapshotIndex=32240013}\r\nDEBUG 2023-03-28T11:11:08.709046684Z [ zeebe-1]{role=FOLLOWER} - Received AppendRequest{term=1, leader=0, prevLogIndex=32402785, prevLogTerm=1, entries=1, commitIndex=32402785}\r\nDEBUG 2023-03-28T11:11:08.709334419Z [ zeebe-1]{role=FOLLOWER} - Sending AppendResponse{status=OK, term=1, succeeded=true, lastLogIndex=32402786, lastSnapshotIndex=32240013}\r\n```\r\n\r\n**To Reproduce**\r\n\r\n\r\n**Expected behavior**\r\n\r\n\r\n\r\n**Log/Stacktrace**\r\n[logs](https://console.cloud.google.com/logs/query;cursorTimestamp=2023-03-28T11:13:29.999108007Z;pinnedLogId=2023-03-28T11:13:29.998615141Z%2Fvx3s55brpp9of4ho;query=resource.type%3D%22k8s_container%22%0Aresource.labels.project_id%3D%22camunda-cloud-240911%22%0Aresource.labels.location%3D%22europe-west1-d%22%0Aresource.labels.cluster_name%3D%22ultrachaos%22%0Aresource.labels.namespace_name%3D%22fea132ff-1e4e-4c7b-b278-8059264f9efa-zeebe%22%0Aresource.labels.container_name%3D%22zeebe%22%0Aresource.labels.pod_name%3D%22zeebe-0%22%0Atimestamp%3D%222023-03-28T11:13:29.998615141Z%22%0AinsertId%3D%22vx3s55brpp9of4ho%22;summaryFields=resource%252Flabels%252Fpod_name:false:32:beginning;timeRange=2023-03-28T10:49:15.000Z%2F2023-03-28T11:50:45.000Z?project=camunda-cloud-240911)\r\n\r\n\r\n[Heap dump of zeebe-0](https://github.com/camunda/zeebe/files/11089429/zeebe-0-13-30.zip)\r\n\r\n\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.2.0-SNAPSHOT \r\n\n\n deepthidevaki: On analyzing heap dump, the channel state in NettyMessaginService is inconsistent.\r\n\r\nAccording to state in SwimMembershipProtocol, the IP of the brokers are as follows:\r\n```\r\nzeebe-1: 10.56.46.232\r\nzeebe-2: 10.56.52.2\r\n```\r\n\r\nThere is a channel pool for `10.56.52.2`\r\n![image](https://user-images.githubusercontent.com/1997478/228247889-53407fd1-7e67-4982-861f-6fd54833f6e3.png)\r\n\r\nThe channel for `raft-partition-partition-1-append` will be at offset 4. But the channel at offset 4 has a remote connection to zeebe-1: 10.56.46.232\r\n![image](https://user-images.githubusercontent.com/1997478/228248235-fb298026-3159-4c6b-9e72-7b6b2d9b1239.png)\r\n\r\n\r\nSo it seems AppendRequest to zeebe-2 (10.56.52.2) is being sent to zeebe-1(10.56.46.232) instead.\n npepinpe: Good catch!\r\n\r\nThe first image is showing the channel pool for 10.56.52.2 on the `zeebe-0` node? Funnily enough, the key, though pointing to 10.56.52.2, has an InetAddress of `zeebe-1`...\r\n\r\nAny thoughts on solutions?\r\n\r\nJust thinking out loud:\r\n\r\n1. When a node is removed from the cluster, it's channels should be removed from the pool (not sure if this is already done)\r\n1. When a node is added to the cluster, channels for its IP should be reset in the pool (or just removed? Is it even possible to have something in the pool for that IP if the member was not in the cluster?)\r\n\r\nOr encoding the recipient concept in the protocol. Have the sender send both the member ID and the cluster ID, so the receiver can reject with a special message, forcing the sender to evict all channels for the wrong IP.\r\n\r\nBoth of these rely on the membership protocol providing the right IPs. I think this may break down with advertised addresses, since the advertised IP is not that of the receiver, and the receiver may not even be able to resolve that one. So perhaps coupling the member ID to the messaging service is necessary? :shrug: \n deepthidevaki: > The first image is showing the channel pool for 10.56.52.2 on the zeebe-0 node? Funnily enough, the key, though pointing to 10.56.52.2, has an InetAddress of zeebe-1...\r\n\r\nI also observed this. And I think this also points to one of the root cause. Here, zeebe-0 re-used old zeebe-1 IP. The channel pool use InetSocketAddress as key, which used IP address in `equals` if IP is available. So when finding channel for zeebe-0, it finds channel pool created for zeebe-1 with it's old address. However, I don't know yet how this channel pool contains a channel to new zeebe-1 address. \r\n\r\nI haven't thought about a solution. But we should see if we can combine hostname+ip to find the channels. So when IP is re-assigned we don't accidentally chose the wrong channel.\r\n\r\n> Or encoding the recipient concept in the protocol. Have the sender send both the member ID and the cluster ID, so the receiver can reject with a special message, forcing the sender to evict all channels for the wrong IP.\r\n\r\n:+1: This would be useful as a early detection/prevention of such cases.\r\n\r\n\n deepthidevaki: Summary of discussion with @npepinpe \r\n- Use address + inetAddress to find the channel pool. This will prevent using the wrong channel, in case ip is reassigned.\r\n- We could encode recipient host with all messages. On the receiver we can check and return a error response. Looks like this can be done in a backward compatible way. Server and client negotiates the protocol version during the handshake. So we can add a new protocol version with the recipient address. https://github.com/camunda/zeebe/issues/12309\r\n- We discarded the other idea to clear channelpool when membership of a node changed. This requires messaging layer to have knowledge about membership service, which is not ideal.",
    "title": "Zeebe node sends messages to wrong node"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11594",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nBasically, it is the same as #11591 but related to timer events (and not messages). In a nutshell, Zeebe checks regularly for due timer events to eventually trigger them and continue with the process flow. The corresponding checker shares the actor of the Stream Processor which will block the Stream Processor while the checker runs. Also, the checker might submit a batch of commands to trigger timer events which will be executed by the Stream Processor in a row without anything else in between.\r\n\r\n**Expected behavior**\r\n* The Stream Processor and the checker do not share an actor so the Stream Processor continues processing while the checker collects timers to trigger.\r\n* (This might be partially already the case, needs to be checked.) The checker only submits a batch with a limited number of commands. For example, when the checker runs it will collect the first 10 due timer events and submit them as a batch to the log stream. And then continues with collecting the next 10 due timer events, and so on until there are due timer events. That way, triggering the timer events would interleave with any incoming commands from users/clients. \r\n* Instead of writing 10 commands, it could write just one command containing the 10 due timer events to trigger. (Might not be easily possible in terms of rolling upgrades, an old version of Zeebe would not be able to process such a command.)\r\n\r\n**Hints**\r\n* A simple prototype to avoid sharing the actor can be found here (in case of expired messages): https://github.com/camunda/zeebe/pull/11550\r\n* Things to consider: When the checker and the Stream Processor do not share an actor, they may run concurrently. That means, the checker reads from RocksDB, and the Stream Processor (mostly) writes to RocksDB. While RocksDB itself is thread-safe, the Zeebe layer may not (like `TransactionContext`, ...).\r\n* Also, while reading the state by the checker, the state might change.\r\n* The same pattern should be applied to the job's timeline and backoff checker.\r\n\r\n**Environment:**\r\n- Zeebe Version: 8.1\r\n\r\n---\r\n\r\n* related to [SUPPORT-15902](https://jira.camunda.com/browse/SUPPORT-15902)\r\n* related to #8991 \n\n Zelldon: Related to https://github.com/camunda/zeebe/issues/8991\n megglos: related to https://github.com/camunda/zeebe/issues/11591\n megglos: Sync with @abbasadel :\r\n- @abbasadel checks in with Nico once https://github.com/camunda/zeebe/issues/11762 is completed on whether the ZPA team can continue with this right after\n korthout: Discussed this issue in the ZPA triage:\n - not urgent for the 8.2 release (other issues have priority)\n - it is an important issue and should be fixed \n - marking it is as `later` as we will first focus on the 8.2 release\n megglos: as this issue strongly relates to the mission of the ZDP team, we would take this one over to drive it forward\n megglos: @abbasadel Ole might need support at least for alignment and dicussion from a ZPA engineer",
    "title": "Triggering due timer events causes periodic latency spikes"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11414",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\n```\r\n java.util.NoSuchElementException: No value present\r\n\tat java.util.Optional.orElseThrow(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.updateInMemoryState(DbProcessState.java:180) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n```\r\n\r\n**To Reproduce**\r\nUnclear\r\n\r\n**Expected behavior**\r\n\r\nInvalid BPMN resources should be handled gracefully and not result in an unhandled `NoSuchElementException`.\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\n java.util.NoSuchElementException: No value present\r\n\tat java.util.Optional.orElseThrow(Unknown Source) ~[?:?]\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.updateInMemoryState(DbProcessState.java:180) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.lookupProcessByIdAndPersistedVersion(DbProcessState.java:314) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.state.deployment.DbProcessState.getLatestProcessVersionByProcessId(DbProcessState.java:217) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.transformProcessResource(BpmnResourceTransformer.java:138) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.lambda$transformResource$0(BpmnResourceTransformer.java:77) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.util.Either$Right.map(Either.java:355) ~[zeebe-util-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.lambda$transformResource$1(BpmnResourceTransformer.java:75) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.util.Either$Right.flatMap(Either.java:366) ~[zeebe-util-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.BpmnResourceTransformer.transformResource(BpmnResourceTransformer.java:65) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.DeploymentTransformer.transformResource(DeploymentTransformer.java:120) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.transform.DeploymentTransformer.transform(DeploymentTransformer.java:97) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.processing.deployment.DeploymentCreateProcessor.processRecord(DeploymentCreateProcessor.java:96) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.engine.Engine.process(Engine.java:127) ~[zeebe-workflow-engine-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$3(ProcessingStateMachine.java:264) ~[zeebe-stream-platform-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run(ZeebeTransaction.java:84) ~[zeebe-db-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand(ProcessingStateMachine.java:260) ~[zeebe-stream-platform-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord(ProcessingStateMachine.java:209) ~[zeebe-stream-platform-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.readNextRecord(ProcessingStateMachine.java:185) ~[zeebe-stream-platform-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) ~[zeebe-scheduler-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) ~[zeebe-scheduler-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) ~[zeebe-scheduler-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) ~[zeebe-scheduler-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) ~[zeebe-scheduler-8.2.0-alpha3.jar:8.2.0-alpha3]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) ~[zeebe-scheduler-8.2.0-alpha3.jar:8.2.0-alpha3] \r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- SaaS\r\n- Zeebe Version: 8.2.0-alpha3\r\n\r\n[Error group](https://console.cloud.google.com/errors/detail/CM_e_rP419icrgE;service=zeebe;time=P7D?project=camunda-cloud-240911)\n\n oleschoenburg: Might be related to https://github.com/camunda/zeebe/issues/11392\n korthout: We need to understand the impact before we can prioritize this. Let's investigate whether this leads to a blacklisted instance.\n remcowesterhoud: I had a look into this with @koevskinikola. We suspect that somehow during the deployment, the deployment record process metadata does not contain a process that we do have in the bpmn file. As a result we have a process in the bpmn file that we have not stored as a `PersistedProcess`, resulting in this exception.\r\n\r\nWe are unsure how this could occur and how we can reproduce it. The exception has occurred 3 times in trial clusters thus far.\n remcowesterhoud: We discussed within the team and we will have another look into this issue together during out next mob-programming hour 2 weeks from now.\n korthout: Today, the team looked again at this issue. Having no way to reproduce it nor a way to find out how it might have happened makes this hard to debug.\r\n\r\nI've had another look at several parts:\r\n- ❌ could this have been caused by #11392 by checking multiple scenarios (e.g. deploying valid processes with id's that were already used in deployments that encountered that bug)\r\n- ❓ what is this code actually doing?\r\n\r\nThis question led me down a path where I noticed something:\r\n- when we deploy a BPMN file, we transform it\r\n- then, for the duplication check, we lookup the latest deployed version \r\n- this either uses the cached version, or it will read it from the state and then cache it\r\n- if it wasn't cached already but does exist in the state, it will then proceed to transform the persisted process XML\r\n- there is no real reason why we transform it here. The latest version is only used to determine version duplicates (and in that case, return the same key, version, etc as a response)\r\n- but this is the transformation where this specific bug happened.\r\n\r\nSo we could perhaps swat two flies at once (this is a Dutch saying):\r\n- remove the code path that was part of this bug\r\n- improve the performance when a new process version is deployed, when the latest isn't cached anymore (not sure if that ever happens, though).\r\n\r\nI'm curious as to what others think about this. Is that worth it? WDYT?\r\n\r\n//cc @remcowesterhoud @koevskinikola @berkaycanbc \n koevskinikola: Hey @korthout, I would vote for the following:\r\n\r\n1. Close this issue since we don't have enough data to reproduce and qualify it.\r\n2. Create an issue for optimizing the performance of the deployment of new process versions (with a low priority).\r\n    * I'm not sure if we should work on this soon, or until we understand the bug.\r\n\r\nMy reasoning is that currently we still don't understand why the bug is happening. The code you're suggesting for removal is just the place where the issue becomes visible (through a `NoSuchElementException`). By removing that code, we might miss any new occurrences of the bug, so it will become more difficult to detect and reproduce.\r\n\r\nUPDATE: Maybe we can wrap the existing exception [here](https://github.com/camunda/zeebe/blob/main/engine/src/main/java/io/camunda/zeebe/engine/state/deployment/DbProcessState.java#L180) into a more understandable message like: \"This might indicate a bug with our deployment process, please raise an issue with our GitHub tracker\"?\n korthout: Thanks @koevskinikola \r\n\r\n1. Agreed 👍 \r\n2. I don't think the performance improvement is a good enough reason to open an issue. The improvement could be tiny. I agree with your reasoning that \"it will become more difficult to detect and reproduce\" 👍 \r\n\r\n>UPDATE: Maybe we can wrap the existing exception [here](https://github.com/camunda/zeebe/blob/main/engine/src/main/java/io/camunda/zeebe/engine/state/deployment/DbProcessState.java#L180) into a more understandable message like: \"This might indicate a bug with our deployment process, please raise an issue with our GitHub tracker\"?\r\n\r\n👍 I like this idea, but perhaps we can change the message. Likely, we read the error message in our own environment. So there is no need to ask users to report the bug if we already know it exists. Let's add some details about the current scenario instead. \r\n\r\nEDIT: I've moved this back into `READY`, so we can make an effort to root cause this\n berkaycanbc: Implemented a PR to create a detailed error message. We should re-open it when we encounter the log message:\n\n> Expected to find executable process in persisted process with key '%s', but after transformation no such executable process could be found\n\ncc: @korthout ",
    "title": "Unhandled `NoSuchElementException` when looking for executable process while deploying BPMN resource"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11355",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "gitHubIssueText": "**Describe the bug**\r\n\r\nWe got reports of crash looping Zeebe brokers on prod, it looks like the process which is running does some nesting or looping over certain activities. TODO: I will add the process model later.\r\n\r\nThe user tried to cancel the corresponding process instance but [this failed because](https://console.cloud.google.com/logs/query;query=%0AlogName:%22stdout%22%0Aresource.type%3D%22k8s_container%22%0Aresource.labels.location%3D%22us-central1%22%0Aresource.labels.namespace_name%3D%228dca781e-03c0-4a15-9b88-1832c5d60b19-zeebe%22%0Aresource.labels.cluster_name%3D%22prod-worker-3%22%0Aresource.labels.container_name%3D%22zeebe%22%0Aresource.labels.pod_name%3D%22zeebe-0%22%0Aresource.labels.project_id%3D%22camunda-cloud-240911%22;timeRange=2023-01-02T09:49:55.253Z%2F2023-01-02T10:49:55.253Z;cursorTimestamp=2023-01-02T10:19:25.169339248Z?project=camunda-cloud-240911) there were too many activities to terminate. \r\n\r\n```\r\nExpected to write one or more follow-up records for record 'LoggedEvent [type=0, version=0, streamId=2, position=299792, key=4503599627371681, timestamp=1672654759877, sourceEventPosition=297539] RecordMetadata{recordType=COMMAND, intentValue=255, intent=TERMINATE_ELEMENT, requestStreamId=-2147483648, requestId=-1, protocolVersion=3, valueType=PROCESS_INSTANCE, rejectionType=NULL_VAL, rejectionReason=, brokerVersion=8.2.0}' without errors, but exception was thrown.\r\n```\r\n\r\nError group: https://console.cloud.google.com/errors/detail/COWzpqvwz4Cg0wE;service=zeebe;time=P7D?project=camunda-cloud-240911\r\n<!-- A clear and concise description of what the bug is. -->\r\n> **Note:** Even though we replaced the dispatcher this error will still happen since we have this max message size limit.\r\n\r\nI put the severity to high since I see no workaround. BTW due to the loop and which causes the pod crash looping the cluster was in this case unusable.\r\n\r\n**To Reproduce**\r\nHave a process instance with a lot of activities active, and terminate the corresponding process instance.\r\n<!--\r\nSteps to reproduce the behavior\r\n\r\nIf possible add a minimal reproducer code sample\r\n- when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java\r\n\r\n-->\r\n\r\n**Expected behavior**\r\nTermination of instances takes into account the batch size, and terminates activities batch-wise, similar issue as to activitate multi instances.\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n**Log/Stacktrace**\r\n\r\n<!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->\r\n\r\n<details><summary>Full Stacktrace</summary>\r\n <p>\r\n\r\n```\r\njava.lang.IllegalArgumentException: Expected to claim segment of size 4481608, but can't claim more than 4194304 bytes.\r\n\tat io.camunda.zeebe.dispatcher.Dispatcher.offer(Dispatcher.java:207) ~[zeebe-dispatcher-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.dispatcher.Dispatcher.claimFragmentBatch(Dispatcher.java:164) ~[zeebe-dispatcher-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.logstreams.impl.log.LogStreamBatchWriterImpl.claimBatchForEvents(LogStreamBatchWriterImpl.java:235) ~[zeebe-logstreams-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.logstreams.impl.log.LogStreamBatchWriterImpl.tryWrite(LogStreamBatchWriterImpl.java:212) ~[zeebe-logstreams-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$writeRecords$9(ProcessingStateMachine.java:354) ~[zeebe-stream-platform-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.retry.ActorRetryMechanism.run(ActorRetryMechanism.java:28) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.retry.AbortableRetryStrategy.run(AbortableRetryStrategy.java:45) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.invoke(ActorJob.java:92) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.ActorJob.execute(ActorJob.java:45) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.ActorTask.execute(ActorTask.java:119) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask(ActorThread.java:106) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.doWork(ActorThread.java:87) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\tat io.camunda.zeebe.scheduler.ActorThread.run(ActorThread.java:198) ~[zeebe-scheduler-8.2.0-alpha2.jar:8.2.0-alpha2]\r\n\"\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n**Environment:**\r\n- OS: <!-- [e.g. Linux] -->\r\n- Zeebe Version: 8.2.0-alpha2 <!-- [e.g. 0.20.0] -->\r\n- Configuration: Production G3-S<!-- [e.g. exporters etc.] -->\r\n\r\nrelates to https://jira.camunda.com/browse/SUPPORT-16499\r\n\n\n Zelldon: Another but related error occured on PROD:\r\n\r\n```\r\nio.camunda.zeebe.stream.api.records.ExceededBatchRecordSizeException: Can't append entry: 'RecordBatchEntry[key=2251799813801783, sourceIndex=-1, recordMetadata=RecordMetadata{recordType=COMMAND, intentValue=10, intent=TERMINATE_ELEMENT, requestStreamId=-2147483648, requestId=-1, protocolVersion=3, valueType=PROCESS_INSTANCE, rejectionType=NULL_VAL, rejectionReason=, brokerVersion=8.2.0}, unifiedRecordValue={\"bpmnProcessId\":\"Process_372fbfc7-9a4a-4f0b-aee5-bd96ed3e3e5d\",\"version\":1,\"processDefinitionKey\":2251799813685320,\"processInstanceKey\":2251799813685333,\"elementId\":\"Activity_0vhm20h\",\"flowScopeKey\":2251799813685333,\"bpmnElementType\":\"USER_TASK\",\"bpmnEventType\":\"UNSPECIFIED\",\"parentProcessInstanceKey\":-1,\"parentElementInstanceKey\":-1}]' with size: 335 this would exceed the maximum batch size. [ currentBatchEntryCount: 11814, currentBatchSize: 3957709]\r\n\r\nat io.camunda.zeebe.stream.impl.records.RecordBatch.appendRecord ( [io/camunda.zeebe.stream.impl.records/RecordBatch.java:66](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl.records%2FRecordBatch.java&line=66&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.BufferedProcessingResultBuilder.appendRecordReturnEither ( [io/camunda.zeebe.stream.impl/BufferedProcessingResultBuilder.java:62](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FBufferedProcessingResultBuilder.java&line=62&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.api.ProcessingResultBuilder.appendRecord ( [io/camunda.zeebe.stream.api/ProcessingResultBuilder.java:38](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.api%2FProcessingResultBuilder.java&line=38&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.streamprocessor.writers.ResultBuilderBackedTypedCommandWriter.appendRecord ( [io/camunda.zeebe.engine.processing.streamprocessor.writers/ResultBuilderBackedTypedCommandWriter.java:37](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.streamprocessor.writers%2FResultBuilderBackedTypedCommandWriter.java&line=37&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.streamprocessor.writers.ResultBuilderBackedTypedCommandWriter.appendFollowUpCommand ( [io/camunda.zeebe.engine.processing.streamprocessor.writers/ResultBuilderBackedTypedCommandWriter.java:32](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.streamprocessor.writers%2FResultBuilderBackedTypedCommandWriter.java&line=32&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnStateTransitionBehavior.lambda$terminateChildInstances$3 ( [io/camunda.zeebe.engine.processing.bpmn.behavior/BpmnStateTransitionBehavior.java:332](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn.behavior%2FBpmnStateTransitionBehavior.java&line=332&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.behavior.BpmnStateTransitionBehavior.terminateChildInstances ( [io/camunda.zeebe.engine.processing.bpmn.behavior/BpmnStateTransitionBehavior.java:330](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn.behavior%2FBpmnStateTransitionBehavior.java&line=330&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.container.ProcessProcessor.onTerminate ( [io/camunda.zeebe.engine.processing.bpmn.container/ProcessProcessor.java:85](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn.container%2FProcessProcessor.java&line=85&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.container.ProcessProcessor.onTerminate ( [io/camunda.zeebe.engine.processing.bpmn.container/ProcessProcessor.java:27](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn.container%2FProcessProcessor.java&line=27&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.processEvent ( [io/camunda.zeebe.engine.processing.bpmn/BpmnStreamProcessor.java:122](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn%2FBpmnStreamProcessor.java&line=122&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.lambda$processRecord$0 ( [io/camunda.zeebe.engine.processing.bpmn/BpmnStreamProcessor.java:95](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn%2FBpmnStreamProcessor.java&line=95&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.util.Either$Right.ifRightOrLeft ( [io/camunda.zeebe.util/Either.java:381](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.util%2FEither.java&line=381&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.processing.bpmn.BpmnStreamProcessor.processRecord ( [io/camunda.zeebe.engine.processing.bpmn/BpmnStreamProcessor.java:92](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine.processing.bpmn%2FBpmnStreamProcessor.java&line=92&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.engine.Engine.process ( [io/camunda.zeebe.engine/Engine.java:128](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.engine%2FEngine.java&line=128&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.lambda$processCommand$3 ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:264](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=264&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.db.impl.rocksdb.transaction.ZeebeTransaction.run ( [io/camunda.zeebe.db.impl.rocksdb.transaction/ZeebeTransaction.java:84](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.db.impl.rocksdb.transaction%2FZeebeTransaction.java&line=84&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.processCommand ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:260](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=260&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.tryToReadNextRecord ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:209](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=209&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.stream.impl.ProcessingStateMachine.readNextRecord ( [io/camunda.zeebe.stream.impl/ProcessingStateMachine.java:185](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.stream.impl%2FProcessingStateMachine.java&line=185&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.scheduler.ActorJob.invoke ( [io/camunda.zeebe.scheduler/ActorJob.java:92](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorJob.java&line=92&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.scheduler.ActorJob.execute ( [io/camunda.zeebe.scheduler/ActorJob.java:45](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorJob.java&line=45&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.scheduler.ActorTask.execute ( [io/camunda.zeebe.scheduler/ActorTask.java:119](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorTask.java&line=119&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask ( [io/camunda.zeebe.scheduler/ActorThread.java:106](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorThread.java&line=106&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.scheduler.ActorThread.doWork ( [io/camunda.zeebe.scheduler/ActorThread.java:87](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorThread.java&line=87&project=camunda-cloud-240911) )\r\nat io.camunda.zeebe.scheduler.ActorThread.run ( [io/camunda.zeebe.scheduler/ActorThread.java:198](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorThread.java&line=198&project=camunda-cloud-240911) )\r\n```\r\n\r\nError group, https://console.cloud.google.com/errors/detail/CJujpJmq_NqemgE;service=zeebe;time=P7D?project=camunda-cloud-240911\n saig0: :information_source: Currently, the `cancel` command is excluded from blacklisting (see [here](https://github.com/camunda/zeebe/blob/main/protocol/src/main/java/io/camunda/zeebe/protocol/record/intent/ProcessInstanceIntent.java#L22)). As a result, the process instance continues with processing.\n Zelldon: :warning: Happened again this week, and caused another incident\r\n\r\nHappened on 8.1.3 https://console.cloud.google.com/errors/detail/CKvjvtrYm_SiuwE;service=zeebe;time=P7D?project=camunda-cloud-240911 \n Zelldon: I would request to re-evaluate the priority of this by @camunda/zeebe-process-automation \r\n\r\nIncidents shouldn't happen twice. This seems to be an issue that people seem to run into easily, and there is no good way to resolve it.\n korthout: Triage summary:\r\n- Create an EPIC to tackle this problem correctly: support cancelling instances with many tokens (@aleksander-dytko )\r\n- Provide a quick and dirty solution to avoid this producing further incidents.\r\n\r\nLet's continue working on this issue by providing this quick and dirty solution\n aleksander-dytko: @korthout could you please check if I have summarized all the details in https://github.com/camunda/product-hub/issues/1067 ? \r\nThanks!\n korthout: @aleksander-dytko Thanks for creating the EPIC. I think you cover all the details.\n npepinpe: This happened again, except this time the number of child element instances is so great it causes the nodes to first slow down to a crawl due to very high GC times, then be killed due to OOM.\r\n\r\nIncident link: https://camunda.slack.com/archives/C051HA4V63D\r\nData link (incl. heap dump, process BPMN, and the complete node state): https://drive.google.com/drive/folders/1VkseQsD8Czi33dQi_kE_vV-YnfOTuJgu?usp=share_link\r\n\r\nIn case of investigation with this data, the key of the command is `4503599643148887` and its position is `93582578`. It is a `ProcessInstance.TERMINATE_ELEMENT` command.\r\n\r\nAffected version is 8.1.9, though I imagine most versions are affected.\r\n\r\nFrom the heap dump:\r\n\r\n![image](https://user-images.githubusercontent.com/43373/229531128-3fdd7686-3840-4c26-a42b-002a51142bfe.png)\r\n\r\n> The thread io.camunda.zeebe.scheduler.ActorThread @ 0xab7760e8 Broker-2-zb-actors-2 keeps local variables with total size 1.90 GB (98.54%) bytes.\r\nThe memory is accumulated in one instance of java.lang.Object[], loaded by <system class loader>, which occupies 1.90 GB (98.52%) bytes.\r\nThe stacktrace of this Thread is available. See stacktrace. See stacktrace with involved local variables.\r\n>\r\n> Keywords\r\n> - java.lang.Object[]\r\n> - io.camunda.zeebe.engine.state.instance.DbElementInstanceState.lambda$getChildren$2(Ljava/util/List;Lio/camunda/zeebe/db/impl/DbCompositeKey;Lio/camunda/zeebe/db/impl/DbNil;)V\r\nDbElementInstanceState.java:258\r\n> - io.camunda.zeebe.engine.state.instance.DbElementInstanceState.getChildren(J)Ljava/util/List;\r\n> - DbElementInstanceState.java:254\r\n\r\nMemory metrics:\r\n\r\n![image](https://user-images.githubusercontent.com/43373/229530936-c7b62201-eae5-4091-8051-09ab681e5bae.png)\r\n\r\n\r\nIn our case, the cluster was also unusable, and likely the only way to recover it is to give it [ludicrous](https://www.youtube.com/watch?v=oApAdwuqtn8) amounts of memory.\n npepinpe: Relevant support issue: https://jira.camunda.com/browse/SUPPORT-16499\r\n\r\nAnd clusters which run into this are likely to be affected by https://github.com/camunda/zeebe/issues/12239 as well (relevant support issue: https://jira.camunda.com/browse/SUPPORT-16394).\r\n\r\nPlease update the support team once these issues are fixed with a patch ETA :pray: \n remcowesterhoud: I've renamed this issue as the descriptions are not related to deep-nesting. They are related to a process instance which contains many active elements instances.\r\n\r\nFor the deep-nesting we have another issue: \r\n- https://github.com/camunda/zeebe/issues/8955\r\n\r\nI've created an epic to do a proper task breakdown https://github.com/camunda/zeebe/issues/12485",
    "title": "Not possible to cancel process instance with many active element instances"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12577",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nFor main, stable/8.1, probably also stable/8.2 renovate seems to have issues updating the digest.\r\n\r\n```\r\nDEBUG: Digest is not updated(packageFile=\"Dockerfile\", branch=\"renovate/stable/8.1-eclipse-temurin-17.x\")\r\n{\r\n  \"baseBranch\": \"stable/8.1\",\r\n  \"manager\": \"dockerfile\",\r\n  \"expectedValue\": \"sha256:22f133769ce2b956d150ab749cd4630b3e7fbac2b37049911aa0973a1283047c\",\r\n  \"foundValue\": \"sha256:b10df4660e02cf944260b13182e4815fc3e577ba510de7f4abccc797e93d9106\"\r\n}\r\nWARN: Error updating branch: update failure(branch=\"renovate/stable/8.1-eclipse-temurin-17.x\")\r\n{\r\n  \"baseBranch\": \"stable/8.1\"\r\n}\r\n```\r\n\r\nSee further logs at https://app.renovatebot.com/dashboard#github/camunda/zeebe/\r\n\n\n megglos: turns out the recent introduction of JVM and JAVA_VERSION broke the updates\r\n```\r\n          {\r\n            \"autoReplaceStringTemplate\": \"{{depName}}{{#if newValue}}:{{newValue}}{{/if}}{{#if newDigest}}@{{newDigest}}{{/if}}\",\r\n            \"datasource\": \"docker\",\r\n            \"depType\": \"stage\",\r\n            \"replaceString\": \"${JVM}:${JAVA_VERSION}-jre-focal@sha256:54f64f1cf8e9b984a92d06d3ad5c10fbbb9e9869144f1f45decdf530d64a4163\",\r\n            \"skipReason\": \"invalid-name\",\r\n            \"updates\": []\r\n          },\r\n```\n koevskinikola: ZPA triage:\n- @megglos it looks like ZDP is already working on this issue. Is there any reason why the issue is on the ZPA board as well?\n- ZPA will remove it from the team board. If there is anything we need to do, please re-add it to the ZPA board.\n megglos: changes are applied, I'm monitoring it till the next updates are applied to all branches",
    "title": "Renovate fails updating docker digest"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12469",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "Happened twice now:\r\n- https://github.com/camunda/zeebe/actions/runs/4731056097/jobs/8395540393\r\n- https://github.com/camunda/zeebe/actions/runs/4730193543/jobs/8393592546\r\n\r\nFor smoke tests we set additional JVM options:\r\nhttps://github.com/camunda/zeebe/blob/c51d6ab7a0f4d2e6250602f90f8b76e4c59baa2a/.github/workflows/ci.yml#L172\r\n\r\nWe should re-evaluate those. I suspect that we can simply remove them.\n",
    "title": "Smoke tests fail due to full code cache"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12387",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nIn order for the job push' `ClientStreamService` to detect brokers being added/removed, we need a way to get this information via the topology. We could use a plain membership listener, but that one is unaware of whether a node is a broker or a gateway, whereas the `BrokerTopologyManager` can already provide this information.\r\n\r\nWe should add the following capabilities:\r\n\r\n- [ ] Add a new listener which gets notified when a broker is added or removed\r\n- [ ] When the listener is initially added, it is also initialized the current list of known brokers, to avoid race conditions\r\n- [ ] The listener can be removed via its identity\r\n\n",
    "title": "Allow listening for updates in BrokerTopologyListener"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12386",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "## Description\r\nThe current error handling strategy in the job push pipeline is to yield the job back to partition if there are any observed errors until it reaches the client.\r\n\r\nThis can have a significant performance impact, since it means writing a new command - this command has to be written, replicated, committed, then processed, and the events must be committed, and then the job is activated and sent out.\r\n\r\nTo speed things up, if there is another logically equivalent stream (whether on the gateway or broker), we should try pushing to that one instead.\r\n\r\nFor the first phase, we can do this naively, ignoring any flow control or optimum work distribution.\n\n npepinpe: @deepthidevaki - this makes me think, maybe the gateway should never yield the job back directly, and instead the broker will always be the one doing this. \r\n\r\nSince a retry strategy could be:\r\n\r\nBroker pushes job to gateway 1/stream A -> gateway 1 fails to forward job, tries with stream B -> gateway 1 fails to forward job, sends it back to the broker -> broker pushes to gateway 2/stream A' -> success\n deepthidevaki: > this makes me think, maybe the gateway should never yield the job back directly, and instead the broker will always be the one doing this.\r\n> \r\n> Since a retry strategy could be:\r\n> \r\n> Broker pushes job to gateway 1/stream A -> gateway 1 fails to forward job, tries with stream B -> gateway 1 fails to forward job, sends it back to the broker -> broker pushes to gateway 2/stream A' -> success\r\n\r\nMakes sense. We could start with this approach.\r\n\r\n",
    "title": "Try pushing activated job to next logical stream on failure before yielding"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12384",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nIn order to get more visibility, we should add the following metrics in the gateway components:\r\n\r\n- [ ] The number of registered client streams in the `ClientStreamManager`\r\n- [ ] The rate of error/failures in the job receiver\r\n\r\nDuring kick off, decide if we want to add more dimensions here (e.g. failure code, registered stream state, etc.), and how this will look like.\n",
    "title": "Additional job push metrics"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12083",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "## Introduction\r\n\r\nBroadcasting to notify listeners when new jobs become available is done in the `DbJobState`. This doesn't work well with the new push-based job activation mechanism.\r\n\r\n## AC\r\nBroadcasting to notify listeners when new jobs become available is done as a side-effect (see [SideEffectProducer](https://github.com/camunda/zeebe/blob/6dac668956e2a7a79df9df31bf0f8238e87eb7c3/stream-platform/src/main/java/io/camunda/zeebe/stream/api/SideEffectProducer.java)) in the following processors:\r\n\r\n- [ ] [JobWorkerTaskProcessor](https://github.com/camunda/zeebe/blob/319813a684325c97556fb59013eb8b88ea27b3f2/engine/src/main/java/io/camunda/zeebe/engine/processing/bpmn/task/JobWorkerTaskProcessor.java#L54)\r\n- [ ] [JobTimeOutProcessor](https://github.com/camunda/zeebe/blob/ad1d5c92a3d4d6009da3a9d968238b83b9dd5c5c/engine/src/main/java/io/camunda/zeebe/engine/processing/job/JobTimeOutProcessor.java#L20)\r\n- [ ] [JobFailProcessor](https://github.com/camunda/zeebe/blob/813c86f780f54496fcd089810704c1a4e4958141/engine/src/main/java/io/camunda/zeebe/engine/processing/job/JobFailProcessor.java)\r\n- [ ] [JobRecurProcessor](https://github.com/camunda/zeebe/blob/813c86f780f54496fcd089810704c1a4e4958141/engine/src/main/java/io/camunda/zeebe/engine/processing/job/JobRecurProcessor.java)\r\n- [ ] [ResolveIncidentProcessor](https://github.com/camunda/zeebe/blob/18dd3c5e8df9bd2164e4d0fc73f5429c9d38b05c/engine/src/main/java/io/camunda/zeebe/engine/processing/incident/ResolveIncidentProcessor.java)\r\n\r\n## Blocked by\r\n* #12082\n",
    "title": "Broadcasting for available jobs is done as a processor side-effect"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12082",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "## Introduction\r\nThe `JobStreamer` API is available to the engine processors.\r\n\r\n## AC\r\nIt needs to be passed to the following job processors:\r\n- [ ] [JobWorkerTaskProcessor](https://github.com/camunda/zeebe/blob/319813a684325c97556fb59013eb8b88ea27b3f2/engine/src/main/java/io/camunda/zeebe/engine/processing/bpmn/task/JobWorkerTaskProcessor.java#L54) / [BpmnJobBehavior](https://github.com/camunda/zeebe/blob/4d46a4947e6d3ac72cb4e0af324f4c978b591989/engine/src/main/java/io/camunda/zeebe/engine/processing/bpmn/behavior/BpmnJobBehavior.java#L81-L92)\r\n- [ ] [JobTimeOutProcessor](https://github.com/camunda/zeebe/blob/ad1d5c92a3d4d6009da3a9d968238b83b9dd5c5c/engine/src/main/java/io/camunda/zeebe/engine/processing/job/JobTimeOutProcessor.java#L20)\r\n- [ ] [JobFailProcessor](https://github.com/camunda/zeebe/blob/813c86f780f54496fcd089810704c1a4e4958141/engine/src/main/java/io/camunda/zeebe/engine/processing/job/JobFailProcessor.java)\r\n- [ ] [JobRecurProcessor](https://github.com/camunda/zeebe/blob/813c86f780f54496fcd089810704c1a4e4958141/engine/src/main/java/io/camunda/zeebe/engine/processing/job/JobRecurProcessor.java)\r\n- [ ] [ResolveIncidentProcessor](https://github.com/camunda/zeebe/blob/18dd3c5e8df9bd2164e4d0fc73f5429c9d38b05c/engine/src/main/java/io/camunda/zeebe/engine/processing/incident/ResolveIncidentProcessor.java)\r\n\r\nA \"behavior\" class can be wrapped around the `JobStreamer` API to ease working with it through multiple processors.\r\n\r\n## Additional context\r\nThe refactoring should be partially done by #12067\n",
    "title": "The JobStreamer API is available to the engine processors"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12041",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "<!--\r\nIn case you have questions about our software we encourage everyone to participate in our community via the\r\n- Camunda Platform community forum https://forum.camunda.io/ or \r\n- Slack https://camunda-cloud.slack.com/ (For invite: https://camunda-slack-invite.herokuapp.com/)\r\n\r\nThere you can exchange ideas with other Zeebe and Camunda Platform 8 users, as well as the product developers, and use the search to find answer to similar questions.\r\n\r\nThis issue template is used by the Zeebe engineers to create general tasks.\r\n-->\r\n\r\n**Description**\r\n\r\n_Related to https://github.com/camunda/zeebe/issues/12035_\r\n_Related to https://github.com/camunda/zeebe/issues/12033_\r\n_Came up in https://github.com/camunda/zeebe/issues/11813_\r\n\r\n\r\n> _Blacklist checks_\r\n> \r\n> With (almost) every command, Zeebe will check if the given process instance is blacklisted, even if the process instance is just created. Also, this check happens all the time during batch processing:\r\n> \r\n> https://github.com/camunda/zeebe/blob/ae6dbfc5439c889ad8b8fd24d7431b46110c7900/engine/src/main/java/io/camunda/zeebe/engine/Engine.java#L130-L134\r\n> \r\n> Just some a thoughts on how this could be improved:\r\n> * Cache the information that there is no blacklisted process instance so that Zeebe avoids the lookup in RocksDB.\r\n> * Avoid doing the check with every (follow-up) command during batch processing.\r\n> * When creating a new process instance, the check is unnecessary.\r\n> * TBD: If there are blacklisted process instances, how to make the lookup efficient.\r\n> \r\n\n\n megglos: https://github.com/camunda/zeebe/pull/12306\r\ncovers:\r\n\r\n- [x] Cache the information that there is no blacklisted process instance so that Zeebe avoids the lookup in RocksDB.\r\n- [x] When creating a new process instance, the check is unnecessary.\n Zelldon: This is done via https://github.com/camunda/zeebe/pull/12306",
    "title": "Unnecessary Blacklist checks"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12028",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "<!--\r\nIn case you have questions about our software we encourage everyone to participate in our community via the\r\n- Camunda Platform community forum https://forum.camunda.io/ or \r\n- Slack https://camunda-cloud.slack.com/ (For invite: https://camunda-slack-invite.herokuapp.com/)\r\n\r\nThere you can exchange ideas with other Zeebe and Camunda Platform 8 users, as well as the product developers, and use the search to find answer to similar questions.\r\n\r\nThis issue template is used by the Zeebe engineers to create general tasks.\r\n-->\r\n\r\n### Issue Description\r\n\r\nThe current compilation time with maven `mvn clean install -DskipTests -T1C` is for me locally ~1.5 min, but this highly depends on the machine. The execution of all the tests is way too long and I never do it locally (which is bad). If I do changes I normally just run the test in the affected module.\r\n\r\nThe CI pipeline takes ~33 min, to build, test and deploy snapshots. See this [33m 28s](https://github.com/camunda/zeebe/actions/runs/4416915694/usage).\r\n\r\nIf we do a calculation of how many engineers we are working on this project ~9 and say everyone is pushing at least once to a branch we run 33 * 9 min = 297 min per day. If we say we have around 200 working days in a year, we have 59400 min CI time, only for our work. \r\n\r\nThis doesn't include daily builds, dependabot (which are a lot), stable branches, etc.\r\n\r\nIf we are reducing the test and compile time, this will have a major effect on costs (since we using self-managed runners) but also gives us a boost in effectivity, since we getting faster feedback.\r\n\r\n\r\n### Potential Idea:\r\n\r\nI think there are several ideas to tackle this issue, improving the tests, reducing integration tests and increasing unit test coverage, running partial builds, etc.\r\n\r\nBut there is also another thing that might be interesting for us to investigate. We have several modules we haven't touched for years. There are also modules that we touch only from time to time, maybe every quarter once.\r\n\r\nI would propose we release one version of each of these modules and pin them in our dependency. This would allow us to skip the compiling and running tests of these modules on every build. For example, it is not necessary to compile and run tests every time for `Zeebe Msgpack Core`, `Zeebe Msgpack Value`, or others.\r\n\r\nI think it is important that if we change one of these modules we directly release a new version of it. IMHO that is ok since these modules are and should only be used internally. As long we keep our public release intact and release `dist` and `clients` with the correct release number this should be fine. This would decrease the time of the general CI pipeline by a lot.\r\n\r\nBTW When we started with Zeebe (or TNGP) we had each module its own REPO, but we realized that since our APIs were so fragile we had to change a lot on multi modules so it didn't make much sense to have them in several repos and release them every time (or use snapshot). But I think this has changed since a lot of modules are more mature now.\r\n\r\n### First iteration scope\r\nOptimize overhead of test jobs => target max time 10m\r\n\r\n```[tasklist]\n### Tasks\n- [x] https://github.com/camunda/zeebe/pull/12406 - will optimize the order of tests module tests always offset qa integration tests\n- [x] https://github.com/camunda/zeebe/pull/12497 - will optimize duration on test success (most impact on unit test job)\n- [ ] https://github.com/camunda/zeebe/issues/12417 - single long running test classes limit the yield of forks\n- [x] https://github.com/camunda/zeebe/pull/12424 - a considerable amount of time is spend downloading deps, we use cahces in some but not all jobs with this PR caching will be used by default\n```\r\n\r\n\n\n korthout: >I would propose we release one version of each of these modules and pin them in our dependency. This would allow us to skip the compiling and running tests of these modules on every build. For example, it is not necessary to compile and run tests every time for Zeebe Msgpack Core, Zeebe Msgpack Value, or others\r\n\r\nThis makes sense to me. Especially as you say: \r\n>But I think this has changed since a lot of modules are more mature now.\r\n\r\nBefore choosing one solution, I want to propose an alternative:\r\n- only run ci on modules with changes (or changes in dependencies) (can be easily checked using git, but there are also maven extensions for this)\n Zelldon: Yep this is what I meant with `running partial builds` but the last time I checked was the maven support not that optimal (at least the plugin was no longer maintained etc.) \r\n\r\n\n korthout: ZPA triage summary:\n- Let's discuss with infra to see the actual CI costs, so we can help quantify these improvements\n- @abbasadel Please discuss with @megglos to coordinate efforts\n megglos: ZDP planning:\n- we will look into a reasonable scope for the next iteration\n megglos: Given with recent improvements we brought the CI stage down to about 10m I would conclude the first iteration scope complete. See e.g. https://github.com/camunda/zeebe/actions/runs/5068424932?pr=12850\r\n\r\nI would still consider https://github.com/camunda/zeebe/issues/12417 worth doing anytime soon to ensure the forking of IT jobs is efficient.\r\n\r\nLet's check in on how to proceed.\n megglos: ZDP-Triage:\n- right now flaky tests seem to be a bigger annoyance\n- would be worth revisiting while\n- moving back to backlog pausing for for now",
    "title": "Build and test pipeline takes too long"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11713",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nWhen a client sends a job stream call (as defined in #11708), the gateway should keep track of these in an in-memory register.\r\n\r\nThis will be a long living stream observer. It's important that the gateway properly keep track of it, including when it is closed (gracefully or not, including cancellations), and should only be available to receive jobs when it is ready. \r\n\r\nSee [the prototype for an example](https://github.com/camunda/zeebe/blob/56873bbda3456df153629f915703384667a0a1f3/gateway/src/main/java/io/camunda/zeebe/gateway/GatewayGrpcService.java#L71-L95).\r\n\r\nClients which are logically equivalent based on their activation properties should be grouped together using some unique ID. This unique ID will be used to communicate with the broker when it comes to adding/removing workers, and pushing jobs.\r\n\r\nWhen a new logical client is added, then gateway should send a `AddWorker` request to all brokers. If the broker returns that the request is invalid, then the error should be propagated to the client. Any other errors should be retried.\r\n\r\n> **Note** We may want to limit this of course, but in theory if a broker never replies successfully but the request is correct, likely the broker is dead and will be removed from the topology soon.\r\n\r\nIf there are no brokers, the client is not cancelled or closed.\r\n\r\nWhen a new broker is added to the topology, an `AddWorker` request should be sent for all workers.\r\n\r\n> **Note** We can look into batching this as an optimization in a further iteration.\r\n\r\nWhen a broker is removed from the topology, nothing is to be done.\r\n\r\nWhen the gateway is shutting down, it should send a `RemoveWorkers` with its own member ID to all brokers. Requests are not retried here, of course, it's just a best-of effort to be polite.\n\n deepthidevaki: A first version for stream-management support for gateway is available in PR #12116. For easy tracking, I'm listing pending tasks here:\r\n\r\n- [ ] Aggregate multiple client streams with same metadata and streamType to a single stream. This should be done in transport, and transparently to the consumers of the API `ClientStreamer#add(streamType, metadata, consumer)`. https://github.com/camunda/zeebe/issues/12253 \r\n- [x] ~Re-visit how to handle node restarts. Currently, it fully relies on MembershipService to notify when a node is added or removed. However, there are some edge-cases which we have to handle. For example, if the broker restarts too fast, gateway may not detect the broker is dead, and as a result will not notify when the broker is restarted.~ \r\n   - This is not a problem. [see comment](https://github.com/camunda/zeebe/issues/11713#issuecomment-1496108623)\r\n- [ ] Optimal handling of retries. We might have to stop retrying sending add/remove request to a server at some point. Currently, it is retried indefinitely. It might also be good to retry with a backoff.\r\n- [ ] Integrate transport stream-management with gateway so that implementation of grpc api https://github.com/camunda/zeebe/issues/11708 can use them.\n npepinpe: Can we add these points to the open questions in the epic issue? We should evaluate the cost of not doing these now, and the benefits of adding them in the second phase. If we're not sure about the benefits, then identify scenarios which will help us evaluate these.\r\n\r\nExcept the last one of course, that we just have to do :smile: \n deepthidevaki: > Can we add these points to the open questions in the epic issue? \r\n\r\nI expect that most of these will be done in the scope of this issue. Let's first discuss what is necessary and what can be postponed, and then move them to open questions. I will not close this issue until then. So we won't loose track of it.\n deepthidevaki: Another edge case to consider: If a client stream is removed, while there is an ongoing \"AddRequest\" retry, it is possible that the stream is added to the broker after it has been removed. \r\n\r\n- [ ] Allow cancelling ongoing retries\r\n\n deepthidevaki: > * Re-visit how to handle node restarts. Currently, it fully relies on MembershipService to notify when a node is added or removed. However, there are some edge-cases which we have to handle. For example, if the broker restarts too fast, gateway may not detect the broker is dead, and as a result will not notify when the broker is restarted.\r\n\r\nThis is not a problem. When a node is restarted, it's incarnation number will be chaged. Swim detects this and post and MEMBER_REMOVED, and MEMBER_ADDED event. So we will always get notified of node restarts, and we will re-send AddStream requests. \r\n\r\nhttps://github.com/camunda/zeebe/blob/c055d58fc86afa675e27f7c8b7e6ce6110a29592/atomix/cluster/src/main/java/io/atomix/cluster/protocol/SwimMembershipProtocol.java#L251   \r\n``` java\r\n    // If the term has been increased, update the member and record a gossip event.\r\n    else if (member.incarnationNumber() > swimMember.getIncarnationNumber()) {\r\n      // If the member's version has changed, remove the old member and add the new member.\r\n      if (!Objects.equals(member.version(), swimMember.version())) {\r\n        members.remove(member.id());\r\n        post(new GroupMembershipEvent(GroupMembershipEvent.Type.MEMBER_REMOVED, swimMember.copy()));\r\n        ....\r\n        LOGGER.debug(\"{} - Evicted member for new version {}\", localMember.id(), swimMember);\r\n        post(new GroupMembershipEvent(GroupMembershipEvent.Type.MEMBER_ADDED, swimMember.copy()));\r\n        recordUpdate(swimMember.copy());\r\n      }\r\n\r\n```",
    "title": "Add gRPC stream API gateway implementation"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11712",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Maintenance"
    },
    "gitHubIssueText": "**Description**\r\n\r\nIn order to push jobs from the gateway to the client, the gateway has to be able to receive them from the broker.\r\n\r\nTo do this, it should set up a receiving endpoint (i.e. subscribe to an Atomix topic). This endpoint will only receive one type of request, which will contain the job key, the job record, and the stream worker unique ID for which the job was activated.\r\n\r\nIf the gateway has no clients for this unique ID, it should make the job available again to the broker by sending a fail command (but keeping the same number of retries), much like we do in the long polling handler.\r\n\r\n> **Note** We may want to also send a `RemoveWorker` request to be safe, in case there was some bug.\r\n\r\nIf there are some clients for the unique ID (as registered in #11713), then we should pick one of them and push the job to it.\r\n\r\nIf an error occurs when pushing (and the error indicates the job was not sent to the client), we can pick the next one, and push it to that one. If we cannot decide if the job was sent or not, we should assume it was.\r\n\r\n> **Note** Please challenge this! Let's see what makes for the best UX here.\r\n\n\n deepthidevaki: This issue can be split into two:\r\n\r\n- [x] Adding support for receiving and handling payloads received via a stream. This will be implemented in transport. Stream management in transport receives payload from a broker and handles it over to a registered client stream.\r\n- [ ] Sending the activated jobs to a client. This will be implemented in gateway, which will process the payload handled over from the transport. The gateway know if the client is available or not, and should implement proper error handling such as closing the stream, marking the job re-activatable etc.\n npepinpe: I've extracted the remaining point to a separate issue which the ZPA team will take over. So we can close this for now.",
    "title": "Introduce job receiver in the gateway"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/12624",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/12621",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/12610",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/12604",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/12534",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/12402",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/12263",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/12174",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/pull/12170",
      "component": "release/8.3.0-alpha1",
      "subcomponent": "Misc",
      "context": "Merged Pull Requests"
    }
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4450",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4323",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4406",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4367",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4314",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4342",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4337",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4325",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4333",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4455",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4453",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4351",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4444",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4424",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4423",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4422",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4419",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4420",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4385",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4361",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4476",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4478",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4466",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4452",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4451",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4449",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4438",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4437",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4435",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4447",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4443",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4446",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4433",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4426",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4412",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4408",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4407",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4405",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4404",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4432",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4417",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4402",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4401",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4414",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4396",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4390",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4287",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4178",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4391",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4389",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4384",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4365",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4181",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4394",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4393",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4352",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4364",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4379",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/operate/issues/4376",
      "component": "Operate",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2872",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2833",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2840",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2802",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2739",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2825",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2817",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2807",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2810",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2724",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2709",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2755",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2794",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2793",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2796",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2781",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2754",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2763",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2761",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2760",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2722",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2875",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2878",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2868",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2858",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2850",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2841",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2843",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2829",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2830",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2465",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2442",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2827",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2819",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2822",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2818",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2814",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2805",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2559",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2809",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2790",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2803",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2798",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2784",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2767",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2785",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2782",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2738",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2772",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2753",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2756",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2766",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2725",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda/tasklist/issues/2714",
      "component": "Tasklist",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1789",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1749",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🚀 New Features"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1780",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1775",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1768",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1758",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1757",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1756",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1737",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "💊 Bugfixes"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1792",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1777",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1778",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1776",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1759",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1772",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1769",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1764",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1765",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1761",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1747",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1755",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1754",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1753",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1752",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1735",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  },
  {
    "version": "8.3.0-alpha1",
    "note": {
      "githubUrl": "https://github.com/camunda-cloud/identity/issues/1736",
      "component": "Identity",
      "subcomponent": "Misc",
      "context": "🧹 Chore"
    },
    "gitHubIssueText": "Error: Not Found"
  }
]