[
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14019",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Enhancements"
    },
    "title": "Create `DbTenantAwareKey`",
    "releaseNoteText": " Multi-tenancy was not properly supported, resulting in data inconsistencies for different tenants. \n The lack of a tenant id in the state caused confusion and made it difficult to differentiate between different tenants' data.\n Introduce a new object called `DbTenantAwareKey` to store the tenant id in the state. This key will implement the `DbKey` interface and always contain a `DbString` to store the tenantId. It can also wrap any other key as needed.\n With the implementation of `DbTenantAwareKey`, multi-tenancy is now properly supported. Each tenant's data can be appropriately identified and managed, ensuring data consistency and preventing mix-ups between different tenants."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13989",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Enhancements"
    },
    "title": "Add auth data to Zeebe records",
    "releaseNoteText": " Previously, the Zeebe system did not include authentication data in the Zeebe records. This meant that user permissions and access lists were not accounted for, limiting the ability to enforce security measures and conduct proper access control.\n The lack of authentication data in Zeebe records was due to the absence of a mechanism to include this information. The current record structure did not provide a space to store such data, resulting in a lack of extendability for future updates.\n In order to address this issue, authentication data has been added to the `RecordMetadata`. This ensures that the data is easily accessible and allows for future extensions without the need for significant changes to the codebase. The auth data is encoded within the `RecordMetadata` structure and includes a flag to indicate the mechanism used for encoding and decoding.\n With the implementation of this fix, Zeebe now includes auth data in all types of Zeebe records. The auth data is stored in the `RecordMetadata`, allowing for easy access and extending support for user permissions in the future. This update enhances the security and access control capabilities of Zeebe, ensuring a more robust and flexible system for users."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13988",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Enhancements"
    },
    "title": "Provide an API for data interchange of Zeebe auth data",
    "releaseNoteText": "\nZeebe authentication data, such as the user's tenant access list, was not easily sent from the gateway to the broker in a way that allowed for future extensions. This limitation had the potential to hinder the support of user permissions in the future.\nThe data interchange protocol/mechanism for Zeebe authentication data was not pluggable, making it difficult to adapt to different protocols in the future. Additionally, the use of unsigned JWT tokens was chosen as a temporary solution, which posed limitations on the validation and confidence in the authentication data.\nA general API for encoding and decoding auth data was implemented, allowing for easier and extensible interchange of Zeebe authentication data. The API supports encoding auth data into JWT token strings.\nWith this fix, Zeebe now has a more flexible and future-proof solution for exchanging authentication data. The general API provides the ability to encode auth data into JWT token strings, allowing for wider compatibility and easier integration with other components. This improvement allows for the possibility of extending Zeebe's support for user permissions in the future without significant changes to the codebase."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13987",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Enhancements"
    },
    "title": "Provide a `TenantAccessChecker` API for multi-tenancy",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/8263",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Enhancements"
    },
    "title": "Show blacklisting in the Grafana Dashboard",
    "releaseNoteText": " The Grafana Dashboard does not currently show any indication of blacklisted instances or the rate of blacklisting. This makes it difficult for users to quickly identify if instances are blacklisted or not, resulting in a loss of visibility and potential delays in detecting and resolving issues with blacklisting.\n The metrics for blacklisting were added previously, but they were never incorporated into the Grafana Dashboard.\n The metric for blacklisted instances will now be displayed on the Grafana Dashboard, providing users with a clear indication of whether instances are blacklisted or not. Additionally, the metric will be refilled on restarts to ensure continuous visibility.\n With this fix, users will have improved visibility into blacklisted instances through the Grafana Dashboard. They will be able to easily identify if instances are blacklisted or not, enabling them to take appropriate action and resolve any issues in a timely manner."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14497",
      "component": "Zeebe",
      "subcomponent": "Gateway",
      "context": "Enhancements"
    },
    "title": "Allow using the default tenant even when no tenant associated to requester",
    "releaseNoteText": " Previously, users were unable to use the default tenant when they were not associated with any tenants in Identity. This limited the ability for all clients to utilize the default tenant, even if they were not explicitly associated with any tenants.\n The underlying cause of this issue was the lack of support for using the default tenant when no tenant was associated with the requester.\n We have implemented a fix to allow the usage of the default tenant in cases where the requester is not associated with any tenants in Identity.\n With this fix in place, all clients, regardless of their association with tenants in Identity, can now utilize the default tenant. This enhances the flexibility and accessibility of the system, allowing for smoother onboarding experiences and ensuring that data from different teams remains segregated appropriately."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14278",
      "component": "Zeebe",
      "subcomponent": "Gateway",
      "context": "Enhancements"
    },
    "title": "Gateway supports multi-tenancy in EvaluateDecision RPC",
    "releaseNoteText": " Previously, the Gateway did not support multi-tenancy in the EvaluateDecision RPC, causing limitations in the system's ability to handle multiple tenants simultaneously.\n This issue was caused by a missing implementation in the Gateway, which prevented seamless integration of multi-tenancy in the EvaluateDecision RPC.\n The Gateway has been updated to include support for multi-tenancy in the EvaluateDecision RPC, enabling the system to efficiently handle requests from multiple tenants concurrently.\n With this fix, the Gateway now fully supports multi-tenancy in the EvaluateDecision RPC. Users will experience improved performance and scalability when making decision evaluations for multiple tenants simultaneously."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14276",
      "component": "Zeebe",
      "subcomponent": "Gateway",
      "context": "Enhancements"
    },
    "title": "Gateway supports multi-tenancy in PublishMessage RPCs",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14254",
      "component": "Zeebe",
      "subcomponent": "Gateway",
      "context": "Enhancements"
    },
    "title": "Job polling/pushing in the Gateway supports multi-tenancy",
    "releaseNoteText": " When multi-tenancy is enabled, the Gateway may return a PERMISSION_DENIED error code (7) if a user attempts to poll or stream jobs for a tenant they are not authorized for. Similarly, the Gateway may return an INVALID_ARGUMENT error code (3) when a non-default tenant ID is provided and multi-tenancy is disabled, or when an invalid tenant ID is provided and multi-tenancy is enabled.\n The issue is caused by a lack of proper authorization checks in the Gateway when processing ActivateJobs and StreamActivatedJobs RPC calls with a list of tenant IDs. It also stems from the Gateway not verifying the validity of the tenant ID when multi-tenancy is enabled.\n The Gateway has been fixed to perform comprehensive authorization checks when processing ActivateJobs and StreamActivatedJobs RPC calls with a list of tenant IDs. It now properly validates the tenant ID format when multi-tenancy is enabled.\n After applying the fix, users will no longer receive a PERMISSION_DENIED error code when attempting to poll or stream jobs for unauthorized tenants. The Gateway will also return an INVALID_ARGUMENT error code only when appropriate, based on the multi-tenancy configuration and the validity of the provided tenant ID."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14211",
      "component": "Zeebe",
      "subcomponent": "Gateway",
      "context": "Enhancements"
    },
    "title": "Gateway supports multi-tenancy in CreateProcessInstance* RPCs",
    "releaseNoteText": " Users can now create process instances with a specific tenant ID in the Gateway.\n The Gateway was not previously able to receive and forward `CreateProcessInstance*` RPC calls with a `tenantId` property.\n The Gateway now supports receiving and forwarding `CreateProcessInstance*` RPC calls with a `tenantId` property. The `createProcessInstance(...)` and `createProcessInstanceWithResult(...)` Gateway endpoints now pass the `tenantId` property to the corresponding `BrokerCreateProcessInstanceRequest` and `BrokerCreateProcessInstanceWithResultRequest`. If multi-tenancy is disabled, the `tenantId` is set to `<default>`. If multi-tenancy is enabled and the `tenantId` is `null`, the request is rejected.\n Users can now successfully create process instances with a specific tenant ID using the Gateway. The `CreateProcessInstanceRequest`, `CreateProcessInstanceWithResultRequest`, and `CreateProcessInstanceResponse` gRPC messages now contain a new `tenantId` property, and the `tenantId` is correctly passed to the `BrokerCreateProcessInstanceRequest` and `BrokerCreateProcessInstanceWithResultRequest`."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14041",
      "component": "Zeebe",
      "subcomponent": "Gateway",
      "context": "Enhancements"
    },
    "title": "I can use a Gateway configuration flag for multi-tenancy",
    "releaseNoteText": " The lack of a gateway configuration flag for multi-tenancy impacted the ability to enable multi-tenancy for the Zeebe cluster. Users were unable to toggle multi-tenancy and had to work with the default setting, which was disabled.\n The absence of a gateway configuration flag prevented users from enabling multi-tenancy for the Zeebe cluster. The system did not have the necessary engineering to support this feature.\n The development team implemented a new gateway configuration flag, `zeebe.gateway.multiTenancy.enabled`, with a default value of `FALSE` to toggle multi-tenancy for the Zeebe cluster. This fix ensures that users have the option to enable or disable multi-tenancy through the configuration flag.\n With the new gateway configuration flag for multi-tenancy, users can now enable multi-tenancy for the Zeebe cluster by setting the `zeebe.gateway.multiTenancy.enabled` flag to `TRUE`. This provides users with greater flexibility and control over the multi-tenancy feature, improving their overall experience with the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13989",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Enhancements"
    },
    "title": "Add auth data to Zeebe records",
    "releaseNoteText": " Previously, the Zeebe system did not include authentication data in the Zeebe records. This meant that user permissions and access lists were not accounted for, limiting the ability to enforce security measures and conduct proper access control.\n The lack of authentication data in Zeebe records was due to the absence of a mechanism to include this information. The current record structure did not provide a space to store such data, resulting in a lack of extendability for future updates.\n In order to address this issue, authentication data has been added to the `RecordMetadata`. This ensures that the data is easily accessible and allows for future extensions without the need for significant changes to the codebase. The auth data is encoded within the `RecordMetadata` structure and includes a flag to indicate the mechanism used for encoding and decoding.\n With the implementation of this fix, Zeebe now includes auth data in all types of Zeebe records. The auth data is stored in the `RecordMetadata`, allowing for easy access and extending support for user permissions in the future. This update enhances the security and access control capabilities of Zeebe, ensuring a more robust and flexible system for users."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13237",
      "component": "Zeebe",
      "subcomponent": "Gateway",
      "context": "Enhancements"
    },
    "title": "Support multi-tenancy in the Gateway",
    "releaseNoteText": " \nThe Zeebe Gateway did not support multi-tenancy, which limited the ability to provide data on user's authorized tenants and forward this data to the Zeebe Broker.\n \nThe underlying cause of the issue was that the Zeebe Gateway did not have built-in support for multi-tenancy.\n \nThe Zeebe Gateway now provides support for multi-tenancy with the following additions:\n- A new configuration flag `zeebe.gateway.multiTenancy.enabled` has been introduced to toggle multi-tenancy for the Zeebe cluster.\n- The `IdentityInterceptor` class has been expanded to provide a list of tenant IDs the user has access to. If multi-tenancy is disabled, the list will only contain the `<default>` tenant ID.\n \nWith this fix, the Zeebe Gateway now fully supports multi-tenancy. Users can now retrieve data on their authorized tenants and forward it to the Zeebe Broker. The Gateway configuration flag allows easy toggling of multi-tenancy for the Zeebe cluster."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14255",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "title": "The JobWorker API of the java client support multi-tenancy",
    "releaseNoteText": " The JobWorker API of the Java client did not support multi-tenancy, resulting in job workers not being able to handle jobs specific to a particular tenant.\n This issue was caused by the lack of methods in the JobWorker API to specify single or multiple tenant identifiers for job polling/streaming, as well as the failure to pick up tenant IDs from the `defaultJobWorkerTenantIds` client configuration property if no tenant IDs were defined.\n The JobWorker API has been updated to include methods for specifying single or multiple tenant identifiers. Additionally, the API now properly retrieves and utilizes tenant IDs from the `defaultJobWorkerTenantIds` client configuration property when no tenant IDs are explicitly defined.\n With this fix applied, the JobWorker API now fully supports multi-tenancy. Job workers are now able to handle jobs specific to a particular tenant by specifying tenant identifiers during job polling/streaming. The API also seamlessly picks up tenant IDs from the `defaultJobWorkerTenantIds` configuration property, providing a convenient default behavior for handling jobs."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13752",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "title": "job.getVariable ergonomic method in Java client",
    "releaseNoteText": "\nUsers experienced the need to repeatedly write `job.getVariablesAsMap().get(\"name\")` when using the Java client with variables.\nThe absence of a shorthand method like `job.getVariable(\"name\")` caused users to resort to using `job.getVariablesAsMap().get(\"name\")` multiple times.\nA new method `job.getVariable()` was added, which internally caches the map from `job.getVariablesAsMap()` and retrieves the desired variable using `.get(\"name\")`.\nUsers can now use the `job.getVariable(\"name\")` method to obtain the value of a variable with reduced boilerplate code. This improvement enhances the first-time experience with the platform, making it more convenient for users."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13560",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "title": "Java client supports multi-tenancy for the ActivateJobs/StreamJobs commands",
    "releaseNoteText": " Users of the Java client for Zeebe were unable to utilize multi-tenancy when using the `ActivateJobsCommandImpl` and `StreamJobsCommandImpl` commands. This meant that they could not specify a single or multiple tenant IDs for which the client would poll/stream jobs from the Zeebe Gateway/Broker.\n The Java client did not have support for multi-tenancy in the `ActivateJobsCommandImpl` and `StreamJobsCommandImpl` commands. There was no option to provide a `tenantIds` property or method, preventing users from specifying the desired tenant IDs.\n The Java client has been updated to support multi-tenancy in the `ActivateJobsCommandImpl` and `StreamJobsCommandImpl` commands. A new `tenantIds` method has been added to both commands, allowing users to specify the desired tenant IDs for polling/streaming jobs from the Zeebe Gateway/Broker.\n With this fix, users of the Java client can now utilize multi-tenancy in the `ActivateJobsCommandImpl` and `StreamJobsCommandImpl` commands. They can specify one or multiple tenant IDs, enabling them to poll/stream jobs from the Zeebe Gateway/Broker for specific tenants."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13559",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "title": "Java client supports multi-tenancy for PublishMessage RPC",
    "releaseNoteText": "\nThe Java client had a limitation in supporting multi-tenancy for the PublishMessage RPC. This resulted in users only being able to use the PublishMessage RPC for a single tenant at a time.\nThe limitation was caused by a missing implementation in the Java client code that didn't allow for multi-tenancy support in the PublishMessage RPC. This was an oversight in the engineering of the product and led to the restricted functionality.\nThe missing implementation in the Java client code for multi-tenancy support in the PublishMessage RPC has now been added. This fix enables users to use the PublishMessage RPC for multiple tenants simultaneously.\nWith this fix, users can now leverage the multi-tenancy support in the Java client for the PublishMessage RPC. This means that they can perform PublishMessage operations for different tenants without any restrictions, providing a more flexible and efficient experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13557",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "title": "Java client supports multi-tenancy for EvaluateDecision RPC",
    "releaseNoteText": " The Java client did not support multi-tenancy for EvaluateDecision RPC, resulting in limited functionality for users who required this feature.\n The underlying cause of this issue was that the tasks in the umbrella issue were not converted to specific issues, leaving the implementation of multi-tenancy incomplete.\n The issue was resolved by adding the necessary functionality to enable multi-tenancy for EvaluateDecision RPC in the Java client.\n Users now have the ability to utilize multi-tenancy for EvaluateDecision RPC in the Java client, allowing for improved flexibility and efficiency in their decision-making processes."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13536",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "title": "Java client supports multi-tenancy for CreateProcessInstance* RPCs",
    "releaseNoteText": " The Java client did not support multi-tenancy for CreateProcessInstance* RPCs, resulting in limited functionality for managing tenant-aware process instances in Zeebe. Users were unable to start process instances for specific tenants or encountered errors when attempting to do so.\n The Java client lacked support for multi-tenancy, meaning it did not provide the necessary property/method for specifying a tenant ID when creating process instances in Zeebe.\n The Java client has been updated to include support for multi-tenancy in CreateProcessInstance* RPCs. The `CreateProcessInstanceCommand` and `CreateProcessInstanceWithResultCommand` now expose an optional `tenantId` property/method, allowing users to specify the desired tenant for each process instance.\n With this fix, users can now successfully create tenant-aware process instances in Zeebe using the Java client. They can specify the appropriate tenant ID when starting process instances, ensuring that the instances are associated with the correct tenant. This enhances the functionality and flexibility of managing process instances in a multi-tenant environment."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13473",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "title": "Stream jobs using job worker",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13460",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "title": "newModifyProcessInstanceCommand: complete command with single variable",
    "releaseNoteText": "\nThe `newModifyProcessInstanceCommand` in the Java Client did not have an option to complete the command with a single variable.\nThe lack of a single variable completion option in `newModifyProcessInstanceCommand` was due to the absence of a method that could accept a single variable.\nAdded the `variable(String name, Object value)` method to the `newModifyProcessInstanceCommand` in the Java Client, allowing the command to be completed with a single variable.\nUsers can now use the `client.newModifyProcessInstanceCommand(job).variable(\"name\", value)` syntax to complete the command with a single variable, providing a more convenient and concise way to modify process instances."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13458",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "title": "newThrowErrorCommand: complete command with single variable",
    "releaseNoteText": "\nThe incomplete `newThrowErrorCommand()` method in the Java Client was causing inconvenience for users as they had to use the alternative method `client.newThrowErrorCommand(job).variables(Map.of(\"name\", value))` when they needed to pass only a single variable. \nThe inability of the `newThrowErrorCommand()` method to accept a single variable was due to the lack of support for passing a single variable directly.\nThe `newThrowErrorCommand()` method has been updated to now support a single variable. Users can now complete the command by using `client.newThrowErrorCommand(job).variable(\"name\", value)`.\nAs a result of this fix, users can now conveniently complete the `newThrowErrorCommand()` by passing a single variable directly. This will enhance the user experience and provide a more intuitive way to work with the Java Client. Users will no longer need to rely on the alternative method when they need to pass only one variable."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13456",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "title": "newFailCommand: complete command with single variable",
    "releaseNoteText": "\nPreviously, when using the Java Client, the `.newFailCommand()` method required multiple variables to be completed. This imposed extra complexity and verbosity on the user.\nThe underlying cause of this issue was the design of the `.newFailCommand()` method, which did not support completing the command with a single variable.\nIn this release, we have made a fix to the Java Client by enhancing the `.newFailCommand()` method. Users can now use a single variable to complete the command by calling `.variable(\"name\", value)`.\nAs a result of this fix, users of the Java Client can now complete the `.newFailCommand()` method with a single variable, simplifying their code and reducing verbosity."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13451",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "title": "newBroadcastingSignalCommand: complete command with single variable",
    "releaseNoteText": "\nPreviously, when using the Java Client's `newBroadcastingSignalCommand()` method, users had to specify multiple variables using a `Map` object. This was inconvenient for users who were not using Java 9 or above, as the `Map.of()` method was not available. \nThe underlying cause of this issue was that the `newBroadcastingSignalCommand()` method did not provide an option for users to specify variables using a single variable.\nIn this release, we have made improvements to the `newBroadcastingSignalCommand()` method. Users can now complete the command by using only a single variable, making it more convenient and compatible with different versions of Java.\nAfter applying this fix, users can use the `newBroadcastingSignalCommand(job).variable(\"name\", value)` syntax to easily specify variables for the broadcasting signal command. This simplifies the code and improves the overall user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13449",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "title": "newEvaluateDecisionCommand: complete command with single variable",
    "releaseNoteText": " The `.newEvaluateDecisionCommand()` method did not allow users to complete the command with a single variable.\n The implementation of the method did not support passing a single variable as an argument, which limited the usability and flexibility of the feature.\n The `.newEvaluateDecisionCommand()` method has been updated to accept a single variable as an argument, allowing users to easily complete the command with a single variable.\n Users can now conveniently use the `.newEvaluateDecisionCommand()` method by providing a single variable, enhancing the usability and flexibility of the feature."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13447",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "title": "newPublishMessageCommand: complete command with single variable",
    "releaseNoteText": " The `newPublishMessageCommand` method did not support completing the command with a single variable in the Java Client. This meant that users had to use the `variables` method with a map or upgrade to Java 9 or above to use the `Map.of()` method.\n The Java Client did not provide a way to complete the `newPublishMessageCommand` with a single variable parameter, only allowing users to pass a map.\n We have added support for completing the `newPublishMessageCommand` method with a single variable by introducing the `variable` method. This allows users to provide a name-value pair directly as parameters to the method.\n Users can now complete the `newPublishMessageCommand` method using a single variable by using the `variable` method, simplifying the code and making it more intuitive."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13443",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "title": "newCreateInstanceCommand: complete command with single variable",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13428",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "title": "Allow custom job worker executors",
    "releaseNoteText": "\nPreviously, users were unable to customize the job worker executor and measure the waiting time for jobs before processing. The only configurable option was the number of threads in the job worker's pool, which was always global per client.\nThe limitation in customizing the job worker executor was due to the Java client's compatibility with Java 8.\nIn this release, the Java client has been updated to allow custom job worker executors. This update includes the implementation of a thread-per-task execution model, using virtual threads for execution. Users can now provide their own executor and easily instrument it.\nAs a result of this fix, users can now customize the job worker executor and measure the waiting time for jobs before processing. This improvement enhances performance and provides a better user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13321",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "title": "Java client supports multi-tenancy for DeployResource RPC",
    "releaseNoteText": " Previously, the Java client did not support multi-tenancy for the `DeployResource` RPC. This meant that users were not able to deploy resources to specific tenants in Zeebe. This resulted in confusion and limitations for users who required multi-tenancy support.\n The lack of support for multi-tenancy in the Java client was due to the absence of an optional `tenantId` property or method in the `DeployResourceCommand`. This prevented users from specifying a tenant for their deployments.\n To address this issue, the Java client has been updated with multi-tenancy support for the `DeployResource` RPC. The `ZeebeClientBuilderImpl` class now includes a `defaultTenantId` property where users can set a default tenant ID value. The `DeployResourceCommand` has also been enhanced with a new `tenantId(String tenantId)` method, which allows users to specify the desired tenant ID for their deployments.\n With this fix, users can now take advantage of multi-tenancy support in the Java client for the `DeployResource` RPC. They can specify a tenant ID for their deployments, enabling them to properly manage and access data for specific tenants. This greatly enhances the flexibility and functionality of the Java client for users in multi-tenant environments."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12122",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "title": "Configure the client's inbound max_message_size",
    "releaseNoteText": "\nUsers were unable to specify a larger inbound max message size for the client, resulting in errors when the gateway responded with messages larger than the default 4 MB.\nThe client did not have a configuration option for specifying the inbound max message size, and the channel setup on client construction made it difficult to implement this feature.\nA configuration option has been added to the client to allow users to specify an inbound max message size for gRPC responses. The default value for this option is set to the current hardcoded limit of 4 MB.\nAfter applying the fix, users can now configure the client's inbound max message size. This allows for handling larger-than-default messages from the gateway without encountering errors."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/4700",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "title": "Introduce JobWorker metrics for the Java client",
    "releaseNoteText": "\nPreviously, there were no metrics available for monitoring Zeebe workers in the Java client. This made it difficult for users to track the number of jobs scheduled by a worker.\nThe lack of metrics was due to the absence of a metrics facade in the job worker of the Java client.\nA metrics facade has been added to the job worker in the Java client. This facade allows users to monitor the number of jobs currently enqueued by a worker.\nWith this fix, users can now easily monitor their Zeebe workers in the Java client. They have access to metrics such as the count of jobs activated and handled, and the number of queued jobs. This provides better visibility into the status and performance of the workers."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13024",
      "component": "Zeebe",
      "subcomponent": "zbctl",
      "context": "Enhancements"
    },
    "title": "Remove zbctl from 8.3 images going forward",
    "releaseNoteText": "\nPreviously, the docker images for version 8.3 of the product included the `zbctl` tool. However, this tool has caused several recently reported CVEs related to golang. These CVEs have impacted the security of the docker images and have raised concerns about the overall safety of the product. \nThe underlying cause of the issue was the inclusion of the `zbctl` tool in the docker images. Although the tool was intended for debugging and troubleshooting purposes, it has posed a significant security risk, leading to the reported CVEs. \nTo address this issue, the `zbctl` tool has been removed from the docker images in version 8.3 going forward. This decision eliminates the potential vulnerabilities associated with the tool and enhances the overall security of the product.\nAs a result of this fix, users will observe that the docker images for version 8.3 and onwards no longer include the `zbctl` tool. By removing the tool, potential security risks are mitigated, ensuring a safer environment for the users."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14555",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Support Multi-tenant Form deployments",
    "releaseNoteText": " Previously, forms were not implemented in a tenant-aware manner for multi-tenant deployments. This resulted in forms not being stored in the state by tenant, causing a lack of proper segregation and management.\n Forms were built in parallel without considering multi-tenancy requirements. This led to the oversight of not implementing forms in a tenant-aware manner.\n The issue has been resolved by implementing the necessary changes to make forms tenant-aware. Forms are now properly stored in the state based on the respective tenant, ensuring proper segregation and management.\n With this fix, multi-tenant form deployments are now supported. Forms are stored in the state based on tenants, allowing for improved segregation and management of forms within a multi-tenant environment. Users can now deploy and manage forms effectively across different tenants."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14302",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Add tests to verify Incident behaviour when form not found during user task activation",
    "releaseNoteText": " Users were experiencing issues when activating user tasks with a formId that was not yet deployed. This resulted in an Incident being raised.\n The system did not have a mechanism to handle user task activation with a formId that was not yet deployed. This caused a break in the workflow and led to the creation of Incidents.\n Added tests to verify the behaviour of Incidents when a formId is not found during user task activation. If a form is not deployed, an Incident is now raised. Additionally, if a form with the same formId is subsequently deployed, the previously raised Incident is automatically resolved. The fix ensures that no Incidents are raised if the form is already deployed before user task activation.\n With this fix, users can now activate user tasks with confidence, knowing that if a form is not yet deployed, an Incident will be raised. Additionally, if a form with the same formId is later deployed, the previous Incident will be automatically resolved. This improvement in the system ensures a smoother workflow and a more seamless user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14270",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Receive tenantId in the Form deployment response",
    "releaseNoteText": " The tenantId was not being received in the Form deployment response, resulting in incomplete information for the user.\n The Java client module did not have the necessary fields to support the retrieval of the Form tenantId in the DeployResource gRPC.\n The `tenantId` field has been added to both the `Form` and `FormImpl` classes in the Java client module.\n After this fix, the user will be able to receive the tenantId in the Form deployment response, providing complete and accurate information for their forms."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14269",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Add tenantId to the Form deployment gRPC response",
    "releaseNoteText": "\nThe tenantId was missing from the Form deployment gRPC response, which resulted in the inability to map the tenantId from the FormRecord to the gRPC response. As a result, new Form-related Record/RecordValues did not have a `tenantId` property.\nThe issue was caused by the absence of the `tenantId` property in the Form deployment gRPC response. This oversight prevented the mapping of the tenantId from the FormRecord.\nThe `tenantId` property was added to the Form deployment gRPC response. This ensures that the tenantId from the FormRecord can now be successfully mapped to the gRPC response. Additionally, new Form-related Record/RecordValues now have the required `tenantId` property.\nWith the fix applied, the Form deployment gRPC response includes the `tenantId` property, allowing for successful mapping from the FormRecord. New Form-related Record/RecordValues will now include the necessary `tenantId` property, ensuring proper functionality and accurate association with the respective tenant."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14268",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Support versioning of the Forms in the state",
    "releaseNoteText": " Users were unable to version Forms, resulting in potential migration issues in the future if versioning support was added for clients. Upon activation of a user task, only the latest version of the Form was exported.\n There was no existing support for versioning Forms within the system. The lack of a version manager class similar to `ProcessVersionManager` and a version info class similar to `ProcessVersionInfo` along with the absence of a dedicated column family for retrieving keys with `formId` and `version` values prevented the implementation of versioning.\n A version manager class, similar to `ProcessVersionManager`, was implemented to support versioning of Forms. Additionally, a version info class, similar to `ProcessVersionInfo`, was created. A new column family was established to allow retrieval of keys with `formId` and `version` values. The version manager was integrated into the `DbFormState` class.\n The system now supports versioning of Forms. Upon activation of a user task, the appropriate version of the Form is exported, preventing any potential migration issues. The implementation of the version manager and related components ensures smooth version control for Forms within the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14248",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Export Deployment Record with form metadata",
    "releaseNoteText": " Exporting the deployment record did not include the form metadata information, causing the exported record to be incomplete.\n The `zeebe-record-deployment-template.json` was not updated to include the form metadata properties, which resulted in the missing information in the exported record.\n The `zeebe-record-deployment-template.json` has been updated to include the necessary form metadata properties.\n After applying this fix, when exporting the deployment record, the form metadata information will also be included, ensuring that the exported record is complete and contains all relevant information."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14222",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Export Form Deployment record to Opensearch",
    "releaseNoteText": "\nExporting Form Deployment record to Opensearch was not supported, resulting in other teams not being able to display deployed forms.\nThe system did not have the capability to export forms to Opensearch. There was no index template created for form fields and the ValueType for forms was not added.\nA new index template called `zeebe-record-form-template.json` was created with all the necessary form fields. The `indexPattern` property was set to `zeebe-record_form_*`. Additionally, the ValueType for forms was added to the system.\nAfter the fix, forms can now be exported to Opensearch. The new index template and ValueType enable the creation of the `form` index, allowing other teams to easily display deployed forms."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14187",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "zeebe running with relatively high idle CPU load",
    "releaseNoteText": " Zeebe has been observed to have a relatively high CPU load when idle, even with no processes or clients running. In a test installation, it was noted that the CPU usage reached 10%. This excessive CPU usage is mainly attributed to the usage of four actor threads, each consuming approximately 2.5% of CPU. This issue leads to wastage of system resources, especially considering that the application runs very few processes per day.\n The high CPU load when idle is primarily caused by the initialization of the `ActorTaskRunnerIdleStrategy` with a low polling interval of 1ms in the `ActorThread.java` file. This frequent polling for work by the ActorThread leads to unnecessary CPU usage even when there is no work to be done.\n The fix for this issue involves increasing the default polling interval in the `BackoffIdleStrategy` used by the `ActorThread`. The default value is currently set to 1ms. However, after performing tests, it was found that increasing this value to 10ms or even 20ms resulted in no noticeable performance difference in terms of throughput, while reducing CPU usage by 50%. Therefore, the fix involves increasing the default polling interval to a more appropriate value.\n With this fix, users will observe a significant reduction in CPU usage when Zeebe is idle. The default polling interval of the ActorThread is increased, resulting in a reduced CPU load and improved resource utilization. This change will particularly benefit users who run Zeebe instances with the default configuration, as it will help save power consumption and improve the overall performance of the system when idle. Additionally, the fix allows for the default polling interval to be easily adjusted to meet different use cases, providing more flexibility to users."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14139",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Add support to Java client for returning Form metadata in DeployResource gRPC response",
    "releaseNoteText": "\nThe Java client does not currently support returning Form metadata in the DeployResource gRPC response. As a result, users are unable to access metadata related to forms in the client side.\nThe underlying cause of this issue is that the `DeploymentEventImpl` class constructor is not properly mapping Form metadata. Additionally, there is a missing `Form` class that should be created to hold the Form metadata in the client side.\nThe `DeploymentEventImpl` class constructor has been updated to correctly map the Form metadata. Furthermore, the missing `Form` class has been implemented to properly hold the Form metadata in the client side.\nWith this fix, the Java client now supports returning Form metadata in the DeployResource gRPC response. Users can now access and utilize the metadata related to forms in the client side, allowing for improved functionality and flexibility."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14138",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Export form key of the latest form on user task activation",
    "releaseNoteText": " The latest version of the Form key was not fetched from the state upon activation of a user task that referenced a Form by formId. As a result, the form key was not added to the custom headers of the Job record. When a form was not found by the given formId on user task activation, an `Either.left<Failure>` was not returned from the `BpmnJobBehaviour.createNewJob` method, and therefore, an incident was not raised.\n The missing functionality was due to the absence of the `formId` expression in `JobWorkerProperties`, the lack of a method in `UserTaskTransformer` for transforming `formId`, and the oversight in updating `BpmnJobBehaviour.encodeHeaders` to include the transformed `formId` in the job custom headers.\n The `formId` expression was added to `JobWorkerProperties`, and a method was added to `UserTaskTransformer` to transform the `formId`. Additionally, `BpmnJobBehaviour.encodeHeaders` was updated to include the transformed `formId` in the job custom headers. The `BpmnJobBehaviour.createNewJob` method was also modified to return an `Either.left<Failure>` if a form was not found by the given `formId` on user task activation, resulting in the raising of an incident. The incident can later be resolved using the `ResolveIncident` gRPC.\n With this fix, the latest version of the Form key will be fetched from the state when a user task is activated, and the fetched form key will be added to the custom headers of the Job record. If a form is not found by the given formId on user task activation, an incident will be raised. This allows for the deployment of a form with the given formId and the subsequent resolution of the incident using the `ResolveIncident` gRPC."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14137",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Implement query to retrieve the latest form by formId from the state",
    "releaseNoteText": " Users were unable to retrieve the latest form by formId from the state.\n The `DbFormState` class did not have a method to retrieve the latest form by form id.\n Added a new method to the `DbFormState` class that allows retrieval of the latest form by form id.\n Users can now retrieve the latest form by formId from the state using the new method in the `DbFormState` class."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14136",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Export Form Deployment record to Elasticsearch",
    "releaseNoteText": " The deployed forms cannot be displayed by other teams as the export to Elasticsearch functionality is missing.\n The export to Elasticsearch functionality for deployed forms was not implemented in the Zeebe system.\n A new index template called `zeebe-record-form-template.json` was created with all the necessary form fields. The `indexPattern` property was set to `zeebe-record_form_*`, and `FORM` was added as a new index `ValueType`. Additionally, the creation of the new `form` index was enabled.\n After applying the fix, the Zeebe system now exports deployed forms to Elasticsearch. This allows other teams to easily display and access the deployed forms."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14135",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Allow binding forms to start events by formId",
    "releaseNoteText": " Previously, it was not possible to link a `Start Event` to a deployed form using the `formId` field. This caused limitations in the configuration of forms for start events.\n The absence of the `formId` attribute in the `ZeebeFormDefinition` prevented the linkage of forms to start events. Additionally, the `ZeebeElementValidator.validate()` method did not have the capability to validate groups of fields, resulting in the inability to determine the validity of start events with linked forms.\n The `formId` attribute has been added to the `ZeebeFormDefinition` to allow the linkage of forms to start events. The `ZeebeElementValidator.validate()` method has also been updated to validate groups of fields, making it possible to determine the validity of start events with either the `formKey` or `formId` present.\n With this fix, users can now link a `Start Event` to a deployed form using the new `formId` field. This provides flexibility in the configuration of forms for start events and allows for validation of start events with linked forms based on the presence of either the `formKey` or `formId`."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14134",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Allow binding forms to user tasks by formId",
    "releaseNoteText": " The user task binding to a deployed form was not possible by using the formId field. \n The formId attribute was not present in the ZeebeFormDefinition, which prevented the user from linking a user task to a deployed form using the formId. Additionally, the ZeebeElementValidator.validate() method did not have the capability to validate a group of fields, causing issues when validating a user task with a linked form.\n The formId attribute has been added to the ZeebeFormDefinition, allowing users to bind a user task to a deployed form using the formId field. The ZeebeElementValidator.validate() method has been updated to validate either the formKey or the formId field, ensuring that a user task with a linked form is valid if at least one of these fields is present.\n Users can now link a user task to a deployed form by using the formId field. The ZeebeElementValidator.validate() method properly validates user tasks with linked forms, allowing for smooth and error-free form bindings."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14133",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Save Form to the state",
    "releaseNoteText": " Saving the Form to the state was not possible, causing user data to not be persisted.\n The Form DB classes, Form column family definitions, and the necessary methods to store and retrieve the Form were not implemented.\n Implemented the Form DB classes, created Form column family definitions, and added methods to store and retrieve the Form.\n Users can now successfully save the Form to the state, ensuring that their data is persisted and can be retrieved later when needed."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14132",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Handle Form Deployment in the Engine",
    "releaseNoteText": "  Users were unable to deploy forms in the engine, resulting in a lack of functionality for creating and using forms within the system. \n The processing of form deployment was not implemented in the engine, causing the inability to create forms and include form metadata in the deployment records. \n Several components were added to handle form deployment, including a form intent, a form transformer, and a form applier. The deployed form was also transformed in the `DeploymentCreateProcessor` class. \n After this fix, users can now deploy forms in the engine. Form creation and usage are now fully functional, allowing for a more comprehensive and efficient system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14131",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Implement Form Deployment gRPC endpoint in the Gateway",
    "releaseNoteText": "\nThe `DeployResource` gRPC endpoint did not support Form deployments, causing a limitation in the system's ability to deploy forms.\nThe `DeployResource` gRPC endpoint did not include the necessary Form metadata response definition in the `gateway.proto` file, and the Form metadata was not filled in the gateway `ResponseMapper` class.\nThe `DeployResource` gRPC endpoint was updated to include the Form metadata response definition in the `gateway.proto` file. Additionally, the Form metadata was filled in the gateway `ResponseMapper` class.\nWith this fix, the system can now support Form deployments through the `DeployResource` gRPC endpoint. The necessary Form metadata will be included in the response, providing accurate information about the deployed forms."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13516",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Add integration tests for Job Push",
    "releaseNoteText": "\nIntegration tests for Job Push were missing, leaving a gap in the coverage of the happy path scenario. This made it difficult to ensure that the system was working correctly end-to-end.\nThe absence of integration tests for Job Push was due to an oversight during the development process. These tests were not implemented, resulting in a gap in the test coverage.\nIntegration tests were added for Job Push to cover the happy path scenario. This involved writing test cases that simulated the entire process, ensuring that all steps were functioning correctly.\nWith the addition of integration tests for Job Push, the system now has improved test coverage for the happy path scenario. This helps to ensure that the functionality is working correctly and allows for easier detection and resolution of any issues that may arise."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13465",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Validate user input before registering a worker to the job stream",
    "releaseNoteText": "\nUser input was not being validated before registering a worker to the job stream. This resulted in several issues. Firstly, the system was unable to provide accurate validation error messages to the client. Additionally, workers were able to register with incorrect job types, leading to a lack of job pushes in the future.\nThe underlying cause of this issue was the lack of validation for user input when registering a worker to the job stream. This validation was necessary in order to ensure the accuracy and integrity of the system.\nTo address this issue, the team implemented validation checks for user input before registering a worker to the job stream. Specifically, they added checks for blank or null values in the type field and a minimum timeout value of 1.\nAs a result of this fix, user input is now properly validated before registering a worker to the job stream. This ensures that validation error messages can be accurately returned to the client, and workers with incorrect job types are prevented from registering. The system now operates with improved accuracy and integrity in handling worker registrations."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13429",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Implement ZeebeClient wrapper for the StreamJobs RPC",
    "releaseNoteText": " The lack of a ZeebeClient wrapper for the StreamJobs RPC prevented users from easily streaming jobs from a Zeebe cluster. This meant that users had to manually handle the gRPC call and integrate it into their client.\n The lack of a ZeebeClient command and API for the StreamJobs RPC was the underlying cause of this issue. The existing ActivateJobsCommand was not suitable for this long-lasting streaming functionality.\n A new command, StreamJobsCommand, and a new API, ZeebeClient#newStreamJobsCommand, were introduced to wrap the underlying gRPC call and provide integration into the client. The StreamJobsCommand has three steps: setting the job type, setting a job consumer, and setting metadata (optional).\n With this fix, users can now easily stream jobs from a Zeebe cluster using the ZeebeClient wrapper for the StreamJobs RPC. The StreamJobsCommand allows users to set the job type, a job consumer, and metadata. Additionally, the integration ensures that the stream is long-lived and does not apply the default request timeout. Users can still provide a request timeout if desired."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13426",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "A ResolveIncident request/command supports tenant validation",
    "releaseNoteText": " Users were able to resolve incidents regardless of whether they had access to the tenant that owns the incident. \n The ResolveIncidentProcessor did not perform any validation to check if the user making the request had access to the tenant that owns the incident. \n The ResolveIncidentProcessor was updated to include a validation step that checks if the user making the request has access to the tenant that owns the incident. \n Users will now be able to only resolve incidents if they have access to the tenant that owns the incident. This ensures better security and prevents unauthorized access to incidents."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13425",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "A ThrowError request/command supports tenant validation",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13388",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Support multi-tenancy for setting variables on a process instance scope",
    "releaseNoteText": " Users were unable to set variables on a process instance scope in a multi-tenant environment.\n The system did not have support for multi-tenancy when setting variables on a specific scope, such as a process instance or flow element instance.\n The `VariableDocumentRecord` and `VariableRecord` classes were updated to include a new `tenantId` property. The `VariableState` now supports storing and fetching variables by `tenantId`. The Elasticsearch/Opensearch `variable-template.json` and `variable-document-template.json` were also updated to include the `tenantId` property. The `UpdateVariableDocumentProcessor` was modified to update variables only if the tenant that owns the scope is accessible by the user making the request. The processor now rejects a command if the tenant that owns the scope is not accessible. The `VariableBehavior` class was updated to append the `tenantId` from the scope to all `VariableRecord` instances. Follow-up events from `VariableDocumentRecord` and `VariableRecord` now inherit the `tenantId` from the updated scope. The user's tenant id list is now passed in the `RecordMetadata`.\n Users can now set variables on a process instance scope in a multi-tenant environment. The system properly handles multi-tenancy by ensuring that users can only update variables on scopes accessible to them."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13354",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Delete and recreate signal subscription of previous version",
    "releaseNoteText": " The previous version of a process was not being properly unsubscribed, resulting in unexpected behavior with signal subscriptions. This could lead to incorrect process instance creation and signals not being received in certain scenarios. \n The issue was caused by not properly handling the deletion and recreation of signal subscriptions when a previous version of a process becomes the new latest version. This led to the signal subscription not being properly created and linked to the process instances.\n The code was updated to properly handle the deletion and recreation of signal subscriptions when a previous version becomes the new latest version. The process now correctly unsubscribes the signal from the previous version and creates the signal subscription for the new latest version.\n With this fix, when a previous version of a process becomes the new latest version, the signal subscription is correctly created, ensuring that the process instances are started and that signals are received as expected."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13349",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Add and handle Process Deleting event",
    "releaseNoteText": " The process deleting event was not being handled, resulting in the process definition not being marked as pending deletion.\n The cause of this issue was the absence of the `DELETING` intent in the `ProcessIntent` and a missing `EventApplier` to handle the `DELETING` intent. Additionally, there was no method on the `DbProcessState` to change the state to `PENDING_DELETION`, and the state was not updated in the `ColumnFamily` or the cache.\n The fix involved adding the `DELETING` intent to the `ProcessIntent` and creating an `EventApplier` to handle this intent. The `EventApplier` was responsible for changing the state of the `PersistedProcess` to `PENDING_DELETION` and updating the state in the `ColumnFamily` and the cache. A new method was also created on the `DbProcessState` to support the state change, and tests were included to ensure its correctness.\n With this fix, the process deleting event is now properly handled. When this fix is applied, the process definition will be correctly marked as pending deletion, ensuring that it can be effectively deleted in the future."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13348",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Add a `state` to `PersistedProcess`",
    "releaseNoteText": " The system did not have a way to track process definitions that were pending deletion.\n The `PersistedProcess` did not have a `state` property to indicate the status of the process.\n A `state` property was added to the `PersistedProcess` class as a new enum called `PersistedProcessState`. The `state` property can have two values: `ACTIVE` and `PENDING_DELETION`.\n Users can now easily mark a process for deletion by setting its `state` to `PENDING_DELETION`. This allows for better organization and management of process definitions in the system. Additionally, in the future, more states can be added to the `PersistedProcessState` enum to accommodate other statuses, such as `SUSPENDED`."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13347",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "An UpdateJobRetries request/command supports tenant validation",
    "releaseNoteText": " Users were able to update job retries for jobs that they did not have access to.\n The `JobUpdateRetriesProcessor` did not include tenant validation when updating job retries.\n The `JobUpdateRetriesProcessor` now checks if the user making the request has access to the tenant that owns the job before updating the job retries.\n Users can only update job retries for jobs that they have access to."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13346",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "A FailJob request/command supports tenant validation",
    "releaseNoteText": " The FailJob command does not have support for tenant validation when a job is failed. This means that any user can fail a job, regardless of whether they have access to the tenant that owns the job.\n The JobFailProcessor does not check the accessibility of the tenant that owns the job when processing a FailJob command. This lack of tenant validation allows users to fail jobs belonging to tenants they do not have access to.\n The JobFailProcessor now checks the accessibility of the tenant that owns the job when processing a FailJob command. If the tenant is not accessible by the user making the request, the command is rejected.\n With the fix in place, when a user tries to fail a job, the JobFailProcessor will validate whether the user has access to the tenant that owns the job. If the user does not have access, the command will be rejected, preventing unauthorized access to job failure. This ensures better security and control over the management of job failures."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13343",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Migrate running process instance into `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily",
    "releaseNoteText": " Running process instances were not being migrated into the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily.\n The ColumnFamily did not contain all running process instances, leading to incomplete migration.\n A migration script was created to populate the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily with all running process instances. Tests were included to ensure the integrity of the migration process.\n After applying this fix, the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily now contains all running process instances, ensuring complete migration and accurate representation of process data."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13342",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Remove from `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily",
    "releaseNoteText": "\nThe `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily was not being updated correctly when a process reached an end state. This resulted in outdated and unnecessary data in the ColumnFamily.\nThe underlying cause of this issue was the lack of a method to remove data from the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily. This omission prevented the system from properly cleaning up after a process reached an end state.\nA method has been added to remove data from the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily. This method has been thoroughly tested to ensure its correctness.\nAdditionally, the `ElementInstanceStateTest#shouldNotLeakMemoryOnRemoval` has been updated to change the generated processInstanceRecord to be of element type PROCESS.\nNow, when a process reaches an end state, it is correctly removed from the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily. This ensures that the system does not retain outdated and unnecessary data, leading to improved performance and reliability."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13341",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Insert into the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily",
    "releaseNoteText": " The `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily is empty, resulting in missing data when starting a new process instance or calling a CallActivity.\n The missing data in the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily is caused by the lack of a method to insert data into this ColumnFamily.\n A new method has been added to insert data into the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily. Tests have also been included to ensure the correctness of the insertion process.\n With this fix, the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily will now be populated with the necessary data. This means that starting a new process instance and calling a CallActivity will properly insert data into the ColumnFamily, ensuring the availability of the required information."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13340",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Add `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily",
    "releaseNoteText": " Users were unable to find out if there were any process instances for a definition key, which impacted the ability to check if there were other process instances still running when a process instance is terminated/completed. This caused pending deployments to not be fully deleted.\n The `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily was not created in `ZbColumnFamilies` and the creation of this ColumnFamily was missing in the `DbElementInstanceState`.\n The `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily has been created in `ZbColumnFamilies` and has been added to `DbElementInstanceState` to ensure its creation.\n Users can now check if there are any process instances for a definition key. This allows for proper handling of terminated/completed process instances and ensures that pending deployments are fully deleted when there are no other process instances running."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13337",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Support multi-tenancy for BPMN timer events",
    "releaseNoteText": " Timer events in BPMN did not support multi-tenancy, causing issues for non-default tenants. As a result, the feature was disabled for non-default tenants in the release candidate.\n The absence of a `tenantId` property and implementation of `TimerInstanceState` prevented timer events from being multi-tenant aware.\n The `TimerRecord` and `TimerInstance` classes were updated to include a `tenantId` property. Additionally, the Elasticsearch/Opensearch record templates were also updated to include a `tenantId` property.\n With the fix applied, timer start events and timer boundary/intermediate events now support multi-tenancy. Timers are processed in the engine and inherit the `tenantId` from the process definition or process instance. This ensures proper functionality and behavior of timer events for all tenants in the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13335",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Add Resource Deleting intent",
    "releaseNoteText": " The system did not have a specific intent for resource deletion, resulting in inconsistent behavior and a lack of clarity for users. The system only had a `DELETED` event written to the log and sent as a response to the client, without logging the `DELETING` event or updating the response message.\n The `ResourceDeletionIntent` class did not have a designated `DELETING` intent, causing a gap in the processing flow. The processor responsible for resource deletion in the system only handled the `DELETED` event and did not log the `DELETING` event or update the response message.\n A new `Intent` named `DELETING` was added to the `ResourceDeletionIntent` class. The `ResourceDeletionProcessor` was modified to write the `DELETING` event to the log before sending the response to the client. The response message was also changed to reflect the `DELETING` event. Finally, the `DELETED` event is still logged after the response is sent.\n With this fix, when a resource is being deleted, the system now properly recognizes the `DELETING` intent. The `DELETING` event is logged before the response is sent to the client, providing a clear indication of the ongoing deletion process. The response message is also updated to reflect the `DELETING` event, ensuring consistent and informative communication with the user. Finally, the `DELETED` event continues to be logged after the response, allowing for complete tracking and auditing of the deletion process."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13320",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Engine can process multi-tenant DeploymentRecord commands",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13319",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Gateway supports multi-tenancy in deployment RPCs",
    "releaseNoteText": " The Gateway now has the capability to support multi-tenancy in deployment RPCs. When receiving and forwarding DeployResource RPC calls, the Gateway now includes a `tenantId` property.\n The issue was caused by the need to support multi-tenancy in the Gateway's deployment RPCs. This required adding the `tenantId` property to various gRPC messages, such as `DeployResourceRequest`, `ProcessMetadata`, `DecisionMetadata`, and `DecisionRequirementsMetadata`. \n The fix involved modifying the `deployResource(...)` Gateway endpoint to pass the `DeployResourceRequest#tenantId` property to the `BrokerDeployResourceRequest`. Depending on the multi-tenancy configuration, the `BrokerDeployResourceRequest#tenantId` was set to either `<default>` or the provided `DeployResourceRequest#tenantId`. \n As a result of this fix, the Gateway is now able to receive and forward DeployResource RPC calls with the appropriate `tenantId` property. This allows for proper multi-tenancy support in the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13318",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "`JobBatchRecord` and `JobActivationProperties` provide tenant information",
    "releaseNoteText": " Prior to this fix, the `JobBatchRecord` and `JobActivationProperties` classes did not provide any information about the tenant. This limitation prevented the filtering of jobs based on both the `jobType` and `tenantId`, making it difficult to support multi-tenancy when polling or pushing jobs. \n The missing tenant information in the `JobBatchRecord` and `JobActivationProperties` classes was due to the absence of a `tenantIds` property. Without this property, it was not possible to include the list of tenant IDs in instances of these classes.\n This fix introduced a `tenantIds` property to the `JobBatchRecord` and `JobActivationProperties` classes. This property allows for the inclusion of a list of tenant IDs, enabling the filtering of jobs based on both the `jobType` and `tenantId`.\n With this fix applied, users are now able to filter jobs by `jobType` **AND** `tenantId`. The addition of the `tenantIds` property in the `JobBatchRecord` and `JobActivationProperties` classes ensures that multi-tenancy is supported when polling or pushing jobs, allowing for more efficient and targeted job management across multiple tenants."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13317",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Jobs are pushed/polled only for requested tenants",
    "releaseNoteText": " Jobs were being pushed or polled without considering the requested `tenantId`, resulting in potential data leakage between tenants.\n The job polling and pushing logic did not include filtering by `jobType` and `tenantId`, causing all jobs to be activated regardless of the requested tenant.\n Adjusted the `JobBatchCollector` to query jobs by `jobType` and `tenantIds`, and added a new method `streamFor(final DirectBuffer jobType, final DirectBuffer tenantId)` to the `JobStreamer` API, which returns a `JobStream` for a given `jobType` and `tenantId`.\n Jobs are now only pushed or polled for the requested tenants, ensuring data isolation and preventing potential data leakage between tenants. This fix enhances the security and privacy of the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13316",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "A CompleteJob request/command supports tenant validation",
    "releaseNoteText": " The CompleteJob request/command did not support tenant validation, which meant that any user could complete a job regardless of their access to the tenant owning the job.\n The JobCompleteProcessor did not check if the user making the request had access to the tenant that owns the job.\n The JobCompleteProcessor was updated to perform tenant validation by checking if the user making the request had access to the tenant that owns the job.\n With this fix, when a CompleteJob request/command is made, the JobCompleteProcessor will now ensure that the user making the request has access to the tenant owning the job. This ensures that only users with the proper access rights can complete jobs."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13288",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Support multi-tenancy for process instance operations (modify, cancel, set variables)",
    "releaseNoteText": " Users were unable to perform process instance operations such as modification, cancellation, and setting variables on multi-tenant environments.\n The process instance operations did not support multi-tenancy. The validation of tenant access was not implemented in the appropriate processors, and the Gateway did not propagate tenant data to the Broker. \n The engine now validates tenant access in the appropriate processors. The Gateway has been updated to propagate tenant data to the Broker. No client changes are required for these operations. The tenant access is validated by comparing the tenant ownership acquired from the State and the list of tenants the user is assigned to acquired from the Identity SDK in the Gateway.\n Users can now perform process instance operations (modify, cancel, set variables) on multi-tenant environments. The operations are only possible if the user/token has access to the tenant owning the process instance."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13279",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Support multi-tenancy for core BPMN process operations",
    "releaseNoteText": " The core operations of BPMN processes in Zeebe did not support multi-tenancy. Starting a process instance, starting a process instance with results, starting a CallActivitiy, evaluating a Decision through a Business Rule task, and child instance creation did not include a `tenantId`. \n The lack of multi-tenancy support was due to a missing implementation in the core engine of Zeebe.\n The engine was updated to process multi-tenant ProcessInstanceCreationRecord commands with a tenant id, propagate the tenant id to process instance elements, and start a CallActivity by tenant id.\n With the fix implemented, the core operations of BPMN processes in Zeebe now support multi-tenancy. Process instances are owned by a tenant, child instances are owned by the same tenant as the process instance, and the CallActivity and Decision evaluations are done by checking ownership against the tenant of the parent process instance."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13253",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Notify Zeebe Team in Slack when an issue is labelled as critical",
    "releaseNoteText": " Previously, critical issues in the Zeebe system would go unnoticed unless someone manually notified the team. This could lead to delays in addressing these urgent problems.\n The lack of an automatic notification system for critical issues meant that the team relied on manual reminders to be aware of these urgent problems.\n A workflow has been implemented that checks if an issue is labelled as critical. If a critical label is added, a notification is automatically sent to the Zeebe Slack channel, alerting the team to the need for immediate attention.\n With this fix in place, the Zeebe team will now receive immediate notifications in Slack when an issue is labelled as critical. This will ensure that urgent problems are promptly addressed and prevent any delays in resolving critical issues."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13238",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Support multi-tenancy for deployments",
    "releaseNoteText": "\nMulti-tenancy support has been added for the Deployment functionality of Zeebe. This means that deployments can now be associated with specific tenants.\nPreviously, Zeebe did not support multi-tenancy for deployments. All deployments and their resources were mapped to the default tenant.\nWith this fix, if multi-tenancy is enabled, the following behavior is supported:\n- If no tenant id is provided, the deployment is rejected. This is because Zeebe will support shared deployments in the future that might be mapped to this call.\n- If a tenant id is provided, the resources are owned by a single tenant.\n- Resource versioning is separated by tenant, meaning that process definitions/decisions owned by different tenants with the same processId have separate version counts in each tenant.\nNow, with multi-tenancy support for deployments in place, users can specify a tenant id when performing a deployment. This allows for better tracking and organization of resources, as well as ensuring that deployments are associated with the correct tenants. Additionally, resource versioning is now separated by tenant, providing a more accurate reflection of the status of process definitions and decisions for each individual tenant."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13040",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "As a Zeebe Java user, I want to complete a Job with a single variable",
    "releaseNoteText": "\nCompleting a job in Zeebe Java required wrapping the variable value in a Map before passing it to the `newCompleteCommand` method. This led to unnecessary overhead and additional code complexity for developers.\nThe underlying cause of this issue was the limited functionality of the `newCompleteCommand` method, which only offered a `.variables()` parameter that required a Map input.\nThe fix for this issue involved introducing a new feature that allows developers to complete a job with a single variable value. The `newCompleteCommand` method now accepts a `variable(key, value)` parameter, allowing developers to directly pass the variable value without the need for a Map wrapper.\nAfter applying this fix, Zeebe Java users can now complete a job using the simplified syntax `client.newCompleteCommand(job).variable(\"name\", value)`. This removes the overhead of wrapping the value in a Map and provides a more intuitive and streamlined approach for completing jobs. Additionally, by promoting this feature and providing examples in the documentation, the Zeebe team aims to ensure that all developers, including those not familiar with newer Java versions, are aware of this shortcut."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12975",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Export number of buffered messages as metrics",
    "releaseNoteText": "\nThe user was unable to obtain the number of buffered messages as a metric, making it difficult to assess the health of the system and identify any potential issues.\nThe product did not have a feature in place to export the number of buffered messages as metrics, limiting the user's visibility into the system's performance.\nA fix was implemented to export additional message-related metrics, including the number of buffered messages. These metrics are now accurately reported even after broker restarts, providing a comprehensive view of the system's health.\nWith this fix, users now have access to the number of buffered messages as a metric, allowing them to monitor the system's performance more effectively. This enables them to identify any issues related to message accumulation and adjust the TTL checker configuration accordingly."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12942",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Support S3 backup httpclient custom configuration.",
    "releaseNoteText": " The backup API fails when attempting to upload multiple log segments in parallel. This results in a `java.util.concurrent.CompletionException` and the backup process is marked as \"FAILED\". \n The issue arises due to the lack of limiting parallel uploads when there are a large number of log segments. In this specific case, with 66 log segments, some uploads wait for longer than the default connection acquisition timeout of 45 seconds, leading to backup failure.\n The fix involves introducing a limit on the number of concurrent uploads to prevent the issue of long wait times for available connections. The limit will be configurable, allowing users to adjust according to their specific requirements.\n After applying the fix, the backup API will successfully upload log segments without exceeding the connection acquisition timeout. The number of concurrent uploads can be controlled by adjusting the configuration, ensuring smooth and efficient backup operations."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12878",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Add message definition extension elements",
    "releaseNoteText": "\nIn the BPMN model API, the `send task` or `message throw event` did not have the ability to reference a message definition. This hindered the ability to model processes where a Send Task or Message Throw Event sends a message.\nThe underlying cause of this issue was that there was no extension element available to specify the details of the message to be published, such as the correlation key, time to live, and message ID.\nA new extension element, `zeebe:publishMessage`, was added to the `bpmn:messageEventDefinition` of Intermediate Throw Event and End Event, as well as to the `bpmn:sendTask`. This allowed specifying the details of the message to be published.\nWith this fix, users can now add the `zeebe:publishMessage` extension element to the relevant BPMN elements and specify the correlation key, time to live, and message ID of the message to be published. This enables the modeling of processes where Send Tasks and Message Throw Events send messages, and allows for more flexibility in specifying different details for the same message in different elements."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12796",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Provide Error Handler implementation for Job Streamer",
    "releaseNoteText": " When pushing a job failed, the `JobYieldProcessor` was not triggered, which resulted in the related job not being available for long polling.\n The error handler was not implemented for the Job Streamer, specifically the `RemoteJobStreamErrorHandlerService` inside `JobStreamServiceStep`.\n The error handler has been implemented and registered to the `RemoteJobStreamErrorHandlerService` inside `JobStreamServiceStep`. Now, when a job push fails, the `JobIntent.YIELD` command is appended.\n When a job push fails, the `JobYieldProcessor` is now triggered, making the related job available for long polling."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12793",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Allow configuring request timeout for InstallRequest",
    "releaseNoteText": " Increased timeout for InstallRequest to prevent timeout exceptions during snapshot replication, especially on networks with higher latency between brokers.\n Previously, the default request timeout for all raft requests was being used for InstallRequest as well. However, since InstallRequest sends larger snapshot files, it sometimes took longer to send the request and get a response, leading to timeout exceptions.\n Exposed a new configuration option to set a separate request timeout specifically for InstallRequest. This allows users to increase the timeout only for InstallRequest without affecting the timeout for other raft requests.\n Users can now configure a higher request timeout for InstallRequest, reducing the chances of timeout exceptions during snapshot replication. This improves the reliability and efficiency of the snapshot replication process, especially on networks with higher latency between brokers."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12696",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Forcefully terminate a process instance",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12655",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Automatically add `support` label to support related issues",
    "releaseNoteText": " Issues related to support were not being labeled automatically, leading to manual effort in identifying support-related issues for release notes.\n The previous system did not have a mechanism in place to automatically add the `support` label to support-related issues.\n Introduced a GitHub action that checks new issues and comments for the text `SUPPORT-XXXX`. If found, the action automatically adds the `support` label to the respective issue.\n Support-related issues are now automatically labeled with the `support` label, reducing the manual effort required to identify and track these issues. This ensures that the list of fixed issues related to support tickets can be easily generated after each release."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12575",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Improve the traversing of snapshot files",
    "releaseNoteText": "\nThe traversal of snapshot files in the system was inefficient as it involved collecting the files in a list, leading to performance degradation.\nThe inefficient traversal of snapshot files was caused by the usage of the `.forEachOrdered` method in the code, which accumulated the snapshot files in a list.\nThe code has been updated to use the `Stream.forEachOrdered` method instead, eliminating the need to collect the snapshot files in a list and improving performance.\nAs a result of this fix, the traversal of snapshot files has been optimized and the system now has improved performance in creating snapshots. Users will experience faster processing times, especially in load test environments. This enhancement will also prevent potential degradation as the number of snapshot files increases over time."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12548",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Provide Grafana Dashboards for the Actor metrics",
    "releaseNoteText": "\nPreviously, there was no out-of-the-box Grafana dashboard for the Actor metrics in the experimental phase.\nThe absence of a separate Grafana dashboard for the Actor metrics was due to the experimental nature of these metrics and the need to keep them separate from the existing dashboard.\nA new file called \"actor.json\" has been added to the Grafana dashboards folder to provide a separate dashboard for the Actor metrics.\nUsers can now access a dedicated Grafana dashboard for the Actor metrics, providing better visibility and convenience for monitoring the experimental phase of these metrics. Users who do not enable these metrics can choose to not use the dashboard, ensuring flexibility in configuration."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12541",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Jobs are pushed from relevant processors",
    "releaseNoteText": "\nJobs were not being pushed from relevant processors, resulting in a lack of activation of jobs for clients.\nThe `Processor` classes responsible for pushing activated jobs were not properly configured to utilize the `JobStreamer` API and perform the necessary steps for job activation.\nThe `BpmnJobActivationBehavior` class was modified to incorporate the use of the `JobStreamer` API for pushing jobs. The following steps were implemented:\n- Obtaining `JobActivationProperties` for an available `JobStream`\n- Setting the `deadline` for the `JobRecord` using `JobActivationProperties`\n- Setting the `variables` for the `JobRecord` using `JobActivationProperties`\n- Setting the `worker` for the `JobRecord` using `JobActivationProperties`\n- Activating the job using a `JobBatchRecord` with the intent `JobBatchIntent.ACTIVATE`\n- Pushing the `JobRecord` onto the `JobStream` through a `SideEffectProducer`\nAfter the fix, the relevant processors are now able to properly push the activated jobs to the clients. This ensures that the jobs are correctly activated and processed within the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12539",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Create `ProcessInstanceBatch.TERMINATE` processor",
    "releaseNoteText": "\nPreviously, there was no processor available to handle the `ProcessInstanceBatch.TERMINATE` commands. As a result, these commands could not be processed, causing a delay in terminating process instances.\nThe absence of a `ProcessInstanceBatch.TERMINATE` processor was the underlying cause of this issue. The BpmnStateBehavior did not have a method to retrieve a specific number of child elements starting from a given key, which prevented the creation of this processor.\nThis issue has been resolved by introducing a new processor called `ProcessInstanceBatch.TERMINATE`. To enable this, a method has been added to the BpmnStateBehavior that allows retrieving a specified number of child elements starting from a specific key. Additionally, a new `ProcessInstanceBatch` record is created when the `index` is available, and a `ProcessInstanceBatch.TERMINATE` command is written for each element instance key in the list.\nWith this fix in place, the `ProcessInstanceBatch.TERMINATE` commands can now be properly processed by the system. When the `index` is available, the processor retrieves the next `BATCH_SIZE` + 1 child instances of the `batchElementInstanceKey`, creates a new `ProcessInstanceBatch` record, and writes a corresponding `ProcessInstanceBatch.TERMINATE` command. In the absence of an `index`, the system continues as usual with no additional actions required."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12538",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Use the `ProcessInstanceBatch Command` when terminating container elements",
    "releaseNoteText": " The system did not correctly terminate container elements when the `onTerminate` method was called. This resulted in child instances of these elements not being properly terminated.\n The issue was caused by the usage of the outdated `BpmnStateTransitionBehavior#terminateChildInstances` method. This method did not utilize the new `ProcessInstanceBatch` command with the `TERMINATE` intent.\n The `BpmnStateTransitionBehavior#terminateChildInstances` method has been modified. It now creates a `ProcessInstanceBatch` record for the container element and writes a `ProcessInstanceBatch.TERMINATE` command using the created record.\n With the fix in place, container elements now correctly terminate their child instances when the `onTerminate` method is called. The new `ProcessInstanceBatch` command with the `TERMINATE` intent ensures that child instances are properly terminated, improving the overall behavior and reliability of the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12537",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Create `ProcessInstanceBatch` Record and Intent",
    "releaseNoteText": " The system lacked the ability to perform batch actions on process instances, such as terminating or activating children. This limitation affected the efficiency and flexibility of managing large sets of process instances. \n The absence of a `ProcessInstanceBatch` record and intent prevented users from executing batch actions on process instances. \n The `ProcessInstanceBatch` record and intent have been implemented to enable users to perform batch actions on process instances. The record now includes the `batchElementInstanceKey`, which represents the element instance key of the element that the batch is executed on. In addition, the `index` field has been added to keep track of the current position in the batch.\n With this fix, users can now utilize the `ProcessInstanceBatch` record and intent to efficiently perform batch actions on process instances. They have the option to terminate or activate batches of child instances, enhancing their ability to manage and control process execution."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12416",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Remove the default un-overridable `-Xms128m` value",
    "releaseNoteText": " Users were unable to override the default `-Xms128m` value when tuning the JVM. This caused difficulty in customizing the JVM options for optimal performance.\n The default `-Xms128m` value was hardcoded in the script generated by the `appassembler-maven-plugin`. Attempts to override it via `JAVA_OPTS` were ineffective as the explicit `-Xms128m` flag was applied after.\n The `-Xms` flag was removed from the plugin configuration, allowing the JVM to use its default value. This change was implemented by @aivinog1.\n Users can now easily override the default `-Xms` value by modifying the `JAVA_OPTS` environment variable. The JVM will use the appropriate value specified, providing greater flexibility in tuning the system for optimal performance."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12382",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Docker: Run the zeebe process with an unprivileged user by default",
    "releaseNoteText": " The zeebe process in the Docker image is currently run by the root user. This poses a security risk according to the OWASP recommendation.\n The zeebe user with UID 1000, which is already set up in the Docker image, is not used by default.\n The zeebe image now runs with an unprivileged user by default. The Dockerfile has been updated to utilize the existing zeebe user.\n With this fix, the zeebe process will be run with an unprivileged user by default, improving the security of the Docker image."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12283",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Zeebe BPMN Model should provide reasonable defaults for definition attributes",
    "releaseNoteText": "\nPreviously, when using the `zeebe-bpmn-model` to model processes, users had to manually set multiple attributes by hand in order to open the processes in the Desktop Modeler as a C8 process. This process was cumbersome and time-consuming.\nThe lack of reasonable defaults for the definition attributes in the `zeebe-bpmn-model` caused users to manually set the necessary attributes. This was a technical limitation of the product.\nIn this release, we have added reasonable defaults for the definition attributes in the `zeebe-bpmn-model`. This allows users to avoid manually setting these attributes, saving time and effort.\nWith this fix applied, users will no longer have to manually set the attributes when modeling processes with the `zeebe-bpmn-model`. The default values will be automatically applied, making the process more efficient and user-friendly."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12085",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Job Yield Processor is implemented to be used for Job Push fallback",
    "releaseNoteText": " Previously, when a job pushed to the `JobStreamer` API failed to be handed over to the client due to client failure, there was no fallback mechanism in place. \n The lack of a fallback mechanism in the `JobStreamer` API caused jobs to be permanently stuck in a failed state when a client failure occurred.\n A new `JobYield` processor has been added to handle the fallback scenario when a job fails to be handed over to the client. This processor performs similar logic to the existing `JobFailProcessor`, allowing the job to become activatable again.\n With this fix, jobs that fail to be handed over to the client due to client failure will now be processed by the new `JobYield` processor, which sets the job to an `ACTIVATABLE` state. This enables the job to be retried or processed by other fallback mechanisms, ensuring a smoother and more reliable job processing experience. The implementation of this fix should also be used in the job push `ErrorHandler` implementation for comprehensive error handling."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12000",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "OAuth Auth Token authentication support in Zeebe Gateway",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11920",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Support Broadcast signal for Signal End Events",
    "releaseNoteText": " The broadcast signal for Signal End Events was not supported, resulting in the inability to trigger a signal when the Signal End Event activates.\n The `EndEventProcessor` did not have the capability to broadcast a signal on activation.\n To address this issue, a new feature called `SignalEndEventBehavior` was introduced. When a Signal End Event activates, the system now applies input mappings, transitions to the activated state, writes a `Signal:Broadcast` command, applies output mappings, and finally transitions to complete the element.\n With this fix, users can now utilize Signal End Events to trigger signals and achieve the expected behavior in the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11919",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Support Broadcast signal for Signal Intermediate Throw Events",
    "releaseNoteText": " The signal intermediate throw events were not broadcasting a signal when activated, resulting in an incomplete communication flow.\n The `IntermediateThrowEventProcessor` did not have the functionality to broadcast a signal upon activation.\n We made adjustments to the `IntermediateThrowEventProcessor` to incorporate broadcasting of a signal when the signal intermediate throw event is activated. This includes applying input mappings, transitioning to the activated state, writing a `Signal:Broadcast` command, applying output mappings, and transitioning to complete the element.\n With this fix, the signal intermediate throw events can now properly broadcast a signal when activated. This ensures a seamless communication flow and allows for effective synchronization between different parts of the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11708",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Add gRPC job stream API",
    "releaseNoteText": " The system did not have a gRPC API for registering job streams to the gateway, which resulted in a limited functionality for job management. Users were not able to have a long-living stream for job activation properties.\n The absence of a gRPC API for job streams was due to the lack of implementation in the system. \n A new gRPC API for job streams was added to support registering job streams to the gateway. The API was designed as a unidirectional stream from the server to the client, accepting the same activation properties as the job worker.\n With the addition of the new gRPC job stream API, users can now register job streams to the gateway and receive a long-living stream of single `ActivatedJob` objects. The system's job management functionality is enhanced, allowing for improved end-to-end pipeline performance."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/10031",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Support BPMN Converging Inclusive Gateway",
    "releaseNoteText": "\nBPMN converging inclusive gateway behavior is missing from the supported features. This means that users have been unable to utilize the converging behavior of inclusive gateways.\nThe validation in the system restricted the number of incoming sequence flows to an inclusive gateway to a maximum of one. This limitation prevented the implementation of converging inclusive gateway behavior.\nThe fix involved removing the validation that restricted the number of incoming sequence flows to an inclusive gateway. Additionally, the fix introduced conditions for activating the inclusive gateway, which included checking for active children of the flow scope instance and ensuring that all incoming sequence flows were taken at least once. However, there was still a limitation in checking if no path could be found from any active child to the inclusive gateway.\nWith this fix, users can now utilize the converging behavior of inclusive gateways. The system no longer restricts the number of incoming sequence flows to an inclusive gateway, allowing for more flexible and comprehensive handling of process flows. This enhancement provides users with a more powerful and versatile tool for designing their BPMN workflows."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14509",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "title": "Potential inconsistency in raft after restoring from a backup",
    "releaseNoteText": "\nAfter restoring from a backup, some partitions encountered an error where the system attempted to delete an index that was lower than the commit index, resulting in potential inconsistencies. This caused the leader to be unable to commit any new entries.\nThe issue occurred because, after restoring from a backup, the raft metastore was empty, causing the system to restart the term from 1. When the new leader committed its initial entry, the previous leader started a new election because it did not receive any heartbeats from the current leader. This led to an inconsistency where the follower accepted a poll request from the candidate with a higher lastLogTerm.\nThe code has been updated to handle the situation after restoring from a backup. The system now ensures that the lastLogTerm is properly initialized, preventing inconsistencies in the leader election process.\nAfter the fix, the system is able to properly handle the situation after restoring from a backup. The leader election process works correctly, ensuring that the leader can commit new entries without encountering errors. This prevents potential data inconsistencies and ensures the stability of the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14486",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "title": "Failure to write snapshot checksum is silently ignored",
    "releaseNoteText": " The system silently ignores any `IOException` that occurs when writing to the checksum file of a snapshot. This means that if there is a failure in writing the checksum, it is not reported or handled as a failed attempt to commit a snapshot.\n The issue arises from the use of `PrintWriter#flush` which does not throw `IOException`s but only sets an internal error flag. The recommended approach is to use `PrintWriter#checkError` which flushes and checks for errors simultaneously.\n The fix involves replacing the usage of `PrintWriter#flush` with `PrintWriter#checkError` in order to correctly handle `IOException`s when writing the checksum file.\n With this fix, any `IOException` that occurs when writing to the checksum file will be properly thrown and handled as a failed attempt to commit a snapshot. This ensures that any IO failure during the snapshot process is correctly reported and addressed."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14406",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "title": "Migration could result in inconsistent data on rolling update",
    "releaseNoteText": " The rolling update process can result in inconsistent data snapshots across brokers. This occurs when a broker running an older version replays events and does not have a specific column family that is present in the new version. As a result, the snapshot state of the broker does not align with other brokers, leading to a corrupted state if the affected broker becomes the leader.\n The issue arises from the fact that during the rolling update, already executed migration tasks are marked as finished in the runtime state to prevent them from being rerun. However, when a broker running an older version replays events without the necessary column family, the state of the snapshot becomes inconsistent.\n The team has decided to remove the feature that marked already executed migration tasks as finished in order to ensure consistent snapshot states during rolling updates. Now, if an old broker receives a snapshot from a new broker, it will replay the events and write them to the necessary column family. When the updated broker migrates, it will also migrate these records to the new column family.\n With this fix, the rolling update process will now ensure that all brokers have a consistent snapshot state, even when running on different versions. This prevents the occurrence of corrupted states and ensures the integrity of the system's data."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14367",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "title": "Duplicate snapshot replication causing to fail raft follower",
    "releaseNoteText": "\nThe issue caused the duplicate replication of snapshots, leading to the failure of the raft follower. This resulted in the node transitioning to an inactive state and no longer participating in the corresponding partition, impacting the system's functionality.\nThe issue was caused by a race condition in the code. The snapshot listener was invoked asynchronously after persisting, but the snapshot reference of the follower may not have been updated when the leader pushed out an append request. This led to the leader trying to send the same snapshot again.\nThe fix addressed the race condition by ensuring that the snapshot reference was updated before processing the next leader request. This prevented the leader from sending duplicate snapshot install requests.\nAfter applying the fix, the system no longer experiences duplicate snapshot replication. The follower correctly handles the install request and avoids transitioning to an inactive state. The issue of the raft follower failure is resolved, maintaining the system's stability and functionality."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14309",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "title": "Cannot process record due to process deployed version being null.",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14275",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "title": "IndexedRaftLogEntry are kept in memory until committed, which can lead to OOM",
    "releaseNoteText": "\nIndexedRaftLogEntry were kept in memory until committed, leading to Out of Memory (OOM) issues. This was observed when the system encountered longer periods of commits being not possible, such as due to network issues or disk problems. As a result, the system accumulated more futures and records, especially when there was a high internal load with multiple scheduled timers triggering simultaneously and repeatedly. This caused the Broker to eventually run out of memory.\nThe issue occurred because the appendFutures and async lambda completion listeners in the LeaderRole were referencing the IndexedRaftLogEntry throughout their entire lifetime. As a result, when commits were not possible, the system continued to accumulate more and more futures and records, leading to memory exhaustion.\nThe fix involved modifying the LeaderRole to no longer keep the reference to the instances of IndexedRaftLogEntry. The system no longer needed all the information in the listeners, and only required the commit indexes and other relevant information. This prevented the accumulation of unnecessary data and reduced the memory consumption.\nWith this fix, the system no longer keeps references to IndexedRaftLogEntry instances in the appendFutures and async lambda completion listeners. As a result, when commits are not possible for longer periods, the system no longer accumulates excessive futures and records, preventing Out of Memory (OOM) issues. Users will experience improved memory management and stability in scenarios with high internal load and prolonged commit unavailability."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14055",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "title": "No executable process found while processing Deployment:Create command",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14044",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "title": "Backpressure queue is not reset when back-to-back role transitions",
    "releaseNoteText": "\nThe backpressure queue was not properly reset when there were back-to-back role transitions in the system. This resulted in 100% backpressure on one partition, preventing it from processing tasks.\nThe issue was caused by a bug in which the command API was not properly notified when a node transitioned from leader to follower. This bug was introduced in a previous fix that resulted in the reuse of the limiter from the previous leader role, even when the transition was cancelled. As a result, the backpressure queue was not reset correctly.\nIn order to fix the issue, the code was modified to ensure that the command API is properly notified when a role transition occurs. Specifically, the code in the `PartitionAwareRequestLimiter` class was updated to remove the limiter when a transition is cancelled.\nWith this fix, the backpressure queue is now correctly reset when there are back-to-back role transitions. This ensures that the system can process tasks in a timely manner and prevents any unnecessary backpressure on the partitions."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13936",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "title": "Condition ignored on Inclusive Gateway with singular outgoing Sequence Flow",
    "releaseNoteText": "\nAn Inclusive Gateway with a single outgoing Sequence Flow was ignoring the Condition. The expected behavior, as specified in the BPMN spec, was for a runtime exception to occur if none of the conditional expressions evaluated to true. However, in this case, the Sequence Flow was being taken without raising an incident.\nThe underlying cause of this issue was the improper handling of the Condition in the Inclusive Gateway logic. The system was not correctly evaluating the Condition expression and was allowing the Sequence Flow to be taken even when the Condition evaluated to false.\nThe fix for this issue involved updating the logic of the Inclusive Gateway to properly evaluate the Condition expression. The system now correctly checks the Condition and only allows the Sequence Flow to be taken if the Condition evaluates to true. If none of the conditional expressions evaluate to true, a runtime exception is raised, as expected.\nWith this fix, an Inclusive Gateway with a single outgoing Sequence Flow correctly considers the Condition. If the Condition evaluates to false, a runtime exception is raised, indicating that none of the conditional expressions evaluated to true. This ensures that the system behaves according to the BPMN spec, providing a more accurate and reliable process flow."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13233",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "title": "Regression in deploying large payloads",
    "releaseNoteText": " Deploying large payloads on multi-partition clusters introduced a regression that impacted the maximum payload size of deployments. Before the regression, a deployed resource was written in two follow-up events (`Deployment:CREATED` and `Process:CREATED` or `DecisionRequirements:CREATED`). However, with the regression, a new event (`CommandDistribution:STARTED`) was introduced to store the command for distribution, leading to a reduction in the maximum payload size. This regression lowered the maximum payload size of deployments from approximately 2MB to around 1.4MB. The regression did not affect single partition clusters as they do not require the distribution of the deployment.\n The regression was caused by the introduction of the `CommandDistribution:STARTED` event, which stored the entire deployment including the resource for distribution. This event was only appended on multi-partition clusters, and it further reduced the maximum payload size due to the `MAX_BATCH_SIZE` restriction.\n To address this issue, the fix involved not writing the resource in the `Deployment:CREATED` event but only in the `Process:CREATED` and `DecisionRequirements:CREATED` events.\n After applying the fix, the maximum payload size of deployments on multi-partition clusters is restored to its original size of approximately 2MB. The resource is now available in the `Process:CREATED` and `DecisionRequirements:CREATED` events, allowing for proper distribution without impacting the payload size. It is important to note that this fix requires alignment with Operate and Optimize to ensure that they consume the resource from the correct events. The documentation has been updated to inform users that the resource is no longer written in the `Deployment:CREATED` event and is available in the `Process:CREATED` and `DecisionRequirements:CREATED` events instead."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13093",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "title": "`RandomizedRaftTest.consistencyTestWithSnapshot` fails with unexpected exception",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12780",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "title": "Failing to write to logstream during stepdown is logged as error",
    "releaseNoteText": "\nPreviously, during a leader transition in the system, if the logstream closed before the leader could write a user request to it, an error was logged. This error message was introduced in a recent update, causing confusion for users. The error message was noisy and unnecessary, creating additional log entries.\nThe error was occurring due to a change introduced in a recent pull request. Previously, this error was ignored, but the update caused the error message to be logged. The logstream closed before the leader could complete the write operation, leading to the error.\nThe log level has been reduced to warn/debug to reduce the noise in the logs. Additionally, the `logstream#tryWrite` function has been updated to return specific error codes for better error handling. Now, when a leader transition occurs and the logstream is closed, the system recognizes this situation and returns a `PARTITION_LEADER_MISMATCH` code to the gateway, allowing it to retry the command with the new leader.\nWith this fix, the unnecessary error message is no longer logged during leader transitions when the logstream is closed. The log level has been reduced to warn/debug, reducing noise in the logs. The system now handles the situation of a leader transition and closed logstream correctly, returning the appropriate error code to the gateway for retrying the command with the new leader. This improves the overall behavior and reliability of the system during leader transitions."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/7855",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "title": "Messaging service is not running",
    "releaseNoteText": " The messaging service was not running, causing an error when trying to send a response and closing the Broker.\n The issue occurred due to the incorrect order of resource closure. The messaging service was closed concurrently while still accepting or attempting to send a response, resulting in an error during the closure of the Broker.\n The `AtomixClientTransportAdapter` was not closed by the `BrokerClientImpl`. The fix involved closing the `AtomixClientTransportAdapter` when closing the `BrokerClientImpl` to ensure that no requests are sent after it is closed.\n After the fix, the messaging service is running correctly, ensuring that requests are sent and responses are received without errors during the closure of the Broker."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/5209",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "title": "Startup failure of one raft partition server affects the availability of other healthy partition",
    "releaseNoteText": " The availability of healthy partitions is affected by the startup failure of one raft partition server. This issue arises because the system currently waits for all partitions to successfully start their raft servers before starting the zeebe partition services. As a result, even though there are leaders for all partitions, the service may be unavailable or only partially available.\n The issue is caused by the current logic of waiting for all partitions to start before installing stream processors and other leader services. This logic is based on waiting for the atomix start to be completed.\n The fix involves removing the complete atomix wrapper and bootstrap logic. Instead, independent starters for each partition are implemented. The bootstrapping logic is improved by starting the atomix transport, membership, topology, monitoring, and disk space first. Then, the partition starts are split up into sub-steps for installing processors and other services.\n After the fix is applied, the system will no longer wait for all partitions to start before installing stream processors and leader services. Each partition will be able to start independently, allowing for parallel startup of the partitions. This will improve the availability of healthy partitions and prevent the system from being partially or fully unavailable due to startup failures."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/2890",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "title": "I can spawn inner instances for a large input collection",
    "releaseNoteText": "\nThe broker fails to spawn inner instances when the input collection for a multi-instance activity contains a large number of elements. This requires manual adjustment of the collection variable to fix the issue.\nThe current behavior of the multi-instance activity is to spawn all instances at once, which can overload the system when dealing with large input collections.\nThe instances are now spawned step-wise, allowing for a controlled creation process. For example, a set number of instances are spawned first, followed by the spawning of the next set of instances after a specific event or terminate command.\nUsers can now spawn as many instances as defined by the input collection, even for large collections. The instances are spawned in a controlled manner, ensuring the system can handle the process without overloading."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14236",
      "component": "Zeebe",
      "subcomponent": "Gateway",
      "context": "Bug Fixes"
    },
    "title": "Prevent concurrent StreamObserver calls on the same instance",
    "releaseNoteText": " When calling `StreamObserver` methods concurrently, such as `onNext`, the underlying stream would send garbled messages and result in errors like `java.lang.IllegalStateException: sendHeaders has already been called` or `io.grpc.StatusRuntimeException: INTERNAL: Connection closed after GOAWAY. HTTP/2 error code: INTERNAL_ERROR, debug data: Stream 7 sent too many headers EOS: false`.\n The issue was caused by the `ClientStreamAdapter` calling `onNext` using a thread pool executor, which allowed for concurrent calls to `StreamObserver` methods.\n The team implemented a fix by using gRPC's `SerializingExecutor` to wrap around the thread pool, ensuring that calls to `StreamObserver` are serialized.\n With the fix in place, calls to `StreamObserver` methods are now properly serialized, preventing garbled messages and errors. This ensures the correct and reliable functioning of the underlying stream."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14176",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Bug Fixes"
    },
    "title": "ActivateJobsCommandImpl throws NullPointerException when ZeebeClientProperties.getDefaultJobWorkerName is null",
    "releaseNoteText": "\nThe constructor of the `ActivateJobsCommandImpl` throws a `NullPointerException` when `ZeebeClientProperties.getDefaultJobWorkerName` is null. This issue occurs because the builder used in the constructor does not support a null value for the worker name.\nThe issue is caused by a recent change where the builder used in the constructor was updated to not allow null values for the worker name. This change was introduced in a commit (https://github.com/camunda/zeebe/commit/359a402b5bbee7247749385a458c5b3f3aba7e78) and affects the behavior of setting the worker name during construction.\nThe issue was fixed by modifying the code to apply defaults to the worker name only in the build method, rather than in the constructor.\nAfter the fix, the builder in the `ActivateJobsCommandImpl` will fallback to the default worker name only if it was not set by a client when constructing the final command. This ensures that a `NullPointerException` will no longer occur when `ZeebeClientProperties.getDefaultJobWorkerName` is null."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14496",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Restore with fewer brokers fails to find all backups",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14418",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Inconsitency detected in PROCESS_SUBSCRIPTION_BY_KEY",
    "releaseNoteText": "\nA inconsistency was detected in the `PROCESS_SUBSCRIPTION_BY_KEY` column family, resulting in a key `DbCompositeKey{first=DbLong{2251799813994826}, second=optimus_entity_created_message}` already existing. This led to an `io.camunda.zeebe.db.ZeebeDbInconsistentException` being thrown.\nThe inconsistency was caused by the recreation of a message subscription and process message subscription during the activation of a call activity where the called process was not found. When the incident was resolved and the call activity was activated again, the message subscription and process message subscription were recreated, resulting in the duplication of the key.\nThe fix for this issue involved modifying the handling of the call activity activation to prevent the recreation of the message subscription and process message subscription that caused the inconsistency.\nAfter applying the fix, the call activity activation no longer recreates the message subscription and process message subscription, eliminating the cause of the inconsistency. Users will no longer encounter the `io.camunda.zeebe.db.ZeebeDbInconsistentException` and the system behavior will be consistent when activating call activities."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14366",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Cancel command fails because process is null (NPE)",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14146",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Remove DRG from cache upon deletion",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14047",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Snapshot version of  benchmark application (starter/worker) doesn't work with SaaS",
    "releaseNoteText": " The current benchmark starter and worker applications do not work with SaaS clusters. Users reported issues when trying to benchmark cluster plans and had to resort to using an old version (8.1.8) to overcome the problem. This created inconvenience and confusion for users who rely on these applications for benchmarking purposes, including upcoming game days.\n The issue stems from the client builder being fixed to use plaintext instead of establishing proper TLS connections with SaaS. The benchmark applications were not properly configured to enable TLS automatically when credentials were provided, resulting in connection failures.\n The benchmark applications have been updated to properly establish TLS connections with SaaS clusters. The client builder has been modified to use the correct configuration for enabling TLS when credentials are provided.\n With this fix, users will be able to seamlessly connect the benchmark starter and worker applications to SaaS clusters. The applications will establish TLS connections, ensuring secure communication between the benchmarking components and the clusters. Users can now effectively benchmark cluster plans and generate datasets without any issues."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14028",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Should not activate link catch event in subprocess",
    "releaseNoteText": "\nThe link catch event inside the (event) subprocess is activated when a link throw event occurs, even if they are not in the same scope. This behavior is contrary to the BPMN specification and the Camunda documentation.\nThe activation of a link catch event in a subprocess is a result of a technical issue in the product. The system was not properly checking the scope of the link events before activating them.\nThe issue has been fixed by implementing a check to ensure that a link throw event can only activate a link catch event within the same scope. The validation logic now correctly denies the activation of a link catch event in a different scope.\nAfter applying the fix, the link catch event inside the (event) subprocess will no longer be activated when a link throw event occurs. The deployment of the process will be rejected if there is no link catch event in the same scope, ensuring compliance with the BPMN specification and the Camunda documentation."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13881",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Upgrading leaves deadline entries without jobs",
    "releaseNoteText": "\nUpgrading Zeebe from version 8.2.8 or earlier to a newer version may result in entries in the job deadline column family being left without corresponding jobs. This can lead to repeated error logs indicating that the job cannot be found.\nThe issue was caused by a change in behavior where the system expects no duplicate deadline entries and does not clean up duplicated or orphaned entries during the upgrade process.\nA fix has been implemented that includes a migration process to clean up orphaned entries in the job deadline column family. This ensures that these entries are removed from the system's state and no longer cause error logs.\nAfter applying the fix, upgrading Zeebe to a version beyond 8.2.8 will no longer result in orphaned entries in the job deadline column family. The migration process will clean up any existing orphaned entries, preventing them from causing error logs indefinitely."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13867",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "ConcurrentModificationException when clearing obsolete job activation requests",
    "releaseNoteText": " Users experienced a `ConcurrentModificationException` when attempting to clear obsolete job activation requests. This exception occurred due to the non-thread-safe nature of the `activeRequests` LinkedList.\n The issue originated from the attempt to clear the `activeRequests` LinkedList, which was not designed to be accessed concurrently.\n The non-thread-safe behavior of the `activeRequests` LinkedList was addressed by implementing thread-safe modifications.\n With the fix applied, users will no longer encounter the `ConcurrentModificationException` when clearing obsolete job activation requests. The system now properly handles concurrent modifications to the `activeRequests` LinkedList, ensuring smooth execution without any exceptions."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13814",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Job streaming may trigger unnecessary polling",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13796",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "IllegalStateArgument when removing job stream",
    "releaseNoteText": " When attempting to remove a job stream in the gateway, an exception was thrown resulting in the stream not being removed from the gateway or the broker.\n The issue was caused by the future completing without an executor, causing it to complete within the actor context and then mistakenly calling `Actor.call` instead.\n To ensure that an executor is used, the fix updated the code to correctly utilize an executor when completing the future.\n After applying this fix, the job stream is successfully removed from the gateway without any errors being thrown."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13787",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Release: `Repo dispatch Benchmark` fails on 8.0/8.1",
    "releaseNoteText": " The `Repo dispatch Benchmark` fails on versions 8.0/8.1. When building the starter and worker, the Maven wrapper is not found on version 8.1, resulting in an error indicating that the file or directory does not exist. Additionally, on version 8.0, the build of Zeebe fails due to the absence of the `DIST=build` setup in the Dockerfile.\n The issue was caused by the absence of the Maven wrapper on version 8.1 and the missing `DIST=build` setup in the Dockerfile on version 8.0.\n The fix for this issue involved backporting the Maven wrapper to versions 8.0 and 8.1 to prevent workflow merge conflicts when moving from the main branch to stable. Additionally, a `benchmark.yaml` workflow was added to each stable branch to maintain a stable setup. The trigger for these workflows was modified to use workflow dispatch, referencing the release branch. Finally, the `dispatch-benchmark.yaml` workflow was removed from the main branch.\n After applying the fix, the `Repo dispatch Benchmark` issue no longer occurs on versions 8.0/8.1. The starter and worker can be built successfully without encountering errors related to the missing Maven wrapper. Additionally, the build of Zeebe on version 8.0 now includes the necessary `DIST=build` setup in the Dockerfile."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13715",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Release Dry fails because of unrecognized argument",
    "releaseNoteText": " The release dry run fails due to an unrecognized argument.\n The issue was caused by an error in the called workflow, specifically in line 329 where an unrecognized named-value 'env' was used.\n A fix was implemented to correct the error by removing the unrecognized argument 'env.RELEASE_BRANCH'.\n After applying the fix, the release dry run now executes successfully without any unrecognized argument errors."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13650",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "`ZeebePartitionHealth` repeatedly reports change of health status",
    "releaseNoteText": "\n`ZeebePartitionHealth` repeatedly called listeners and logged a change of health status even when the health had not actually changed. This resulted in unnecessary notifications and log entries for the users.\nThe issue occurred because the `ZeebePartitionHealth` class was incorrectly comparing the health reports using identity check instead of comparing the health status itself.\nThe issue has been resolved by modifying the comparison logic in the `ZeebePartitionHealth` class. The health reports are now properly compared to determine if there has been an actual change in health status.\nAfter the fix, `ZeebePartitionHealth` will no longer falsely report a change in health status when there has been no actual change. Users will no longer receive unnecessary notifications or see misleading log entries regarding the health status of the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13521",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Process instances are banned when trying to activate a call activity with large variables",
    "releaseNoteText": "\nWhen trying to activate a call activity with large variables, the complete process instance is banned and left in limbo. This can lead to data loss and severe consequences for the user. \nThe issue is caused by the processing exceeding the batch size, which results in the ban of the process instance. The current error handling in the BpmnStreamProcessor does not raise an incident for every process instance element type, which prevents the proper handling of errors.\nTo resolve this issue, the error handling in the BpmnStreamProcessor has been modified. Errors are now properly handled in the respective BpmnElementProcessor, such as the CallActivityProcessor. This allows for the raising of incidents and better handling of process instances.\nWith this fix, incidents will be raised when activating a call activity with large variables exceeds the batch size. This allows users to modify their variables and save the process instance, thus preventing data loss and ensuring the proper execution of the workflow."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13471",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "PartitionListeners are notified even if the transition is cancelled causing NPE",
    "releaseNoteText": "\nPartitionListeners were being notified even if the transition was cancelled, leading to a NullPointerException (NPE). This caused the partition to become inactive.\nThe issue occurred because the `PartitionStartupAndTransitionContextImpl.lambda$notifyListenersOfBecomingLeader` method was still invoked even when the transition to leader was cancelled. Since the services were not installed, the logstream was null, resulting in an NPE in the listener.\nThe issue was fixed by modifying the logic in the `PartitionTransitionProcess` class. The transition is now only considered complete and the listeners are only invoked if the transition completes successfully.\nAfter the fix, partition listeners will only be invoked if the transition to leader is successfully completed. This prevents the occurrence of NullPointerException and ensures that the partition operates as expected."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13431",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Gateway readiness fails when tls enabled",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13254",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "When the deployment of a process definition failed, clients can still start new process instances of the actually non-existing process definition",
    "releaseNoteText": "\nWhen the deployment of a process definition failed, clients were still able to start new process instances of the non-existing process definition. This caused issues with replaying events for followers, lead to failures during failover, and resulted in process definitions being available on some partitions but not others.\nThe issue was caused by the cached process definitions remaining in memory even after the deployment failed. The process definitions were stored in a map for caching purposes, and the failure to write the follow-up event did not remove them from the cache.\nThe fix involved clearing the cached process definitions when a deployment failed, ensuring that only successfully deployed process definitions were stored in memory.\nAfter applying the fix, clients can no longer start process instances of non-existing process definitions. The cached process definitions are correctly cleared when a deployment fails, preventing issues with event replay, failover, and partition consistency."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13164",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Segfault on enabling async scheduled task",
    "releaseNoteText": "\nA segfault could occur when enabling one experimental feature flag and disabling the other (`ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLEMESSAGETTLCHECKERASYNC` and `ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLETIMERDUEDATECHECKERASYNC`). This issue could also occur when both feature flags were enabled. The segfault would result in unexpected behavior for the user.\nThe segfault was caused by a specific situation where the stream processor had its own transaction context and the scheduled tasks had their own shared transaction context. The issue arose when two scheduled tasks ran on different actors while reusing the shared transaction context.\nThe fix for this issue involved addressing the transactional context conflict. The shared transaction context is now properly managed to prevent the segfault from occurring.\nAfter applying the fix, the system no longer experiences a segfault when enabling or disabling the experimental feature flags. Users will observe consistent and expected behavior without any unexpected crashes. This fix has been implemented in the latest release, `8.3.0-alpha3`, ensuring a more stable and reliable system for all users."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13123",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "NPE when processing Job.TIME_OUT command",
    "releaseNoteText": "\nA NullPointerException occurs when processing the Job.TIME_OUT command. This results in the process instance related to the job becoming banned and unrecoverable.\nThe JobTimeOutProcessor incorrectly assumes that the job to be timed out is present in the state. However, there is a possibility that the job has already been removed from the state due to previous commands.\nThe fix involves updating the JobTimeOutProcessor to handle cases where the job being timed out no longer exists in the state. This prevents the NullPointerException from occurring.\nAfter applying the fix, the Job.TIME_OUT command will be rejected when the job being timed out does not exist. This ensures that the process instance remains in a valid state and can be recovered if needed."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13061",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Cancel on-going remote stream registration on stream removal",
    "releaseNoteText": " There was a potential race condition where a remote stream could exist server-side even after the client stream was removed. This resulted in additional latency during a push or unnecessary job activation if it was the last stream of its type. However, the stream would eventually be removed appropriately.\n The registration of remote streams was not properly sequenced, leading to interleaving with asynchronous removal requests. This race condition occurred because remote stream registration was asynchronous and could be submitted before or after a remove request, causing the stream to exist server-side.\n To address this issue, a `ClientStreamRegistration` state machine was associated with each stream. This state machine managed the current state of the remote registration, transitioning between states such as `Initial`, `Adding`, `Added`, `Removing`, and `Removed`. Additionally, the fix included canceling in-flight registration requests and sending remove requests to all known brokers.\n After applying the fix, the registration and removal of remote streams are properly sequenced. Remove requests now cancel in-flight registration attempts and queue the removal after any ongoing requests are finished. This eliminates the race condition and ensures that the client and server-side states are synchronized. As a result, users will experience improved performance with reduced latency and the avoidance of unnecessary job activation for removed streams."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13046",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Error message truncation may be too strict",
    "releaseNoteText": " Error messages were being truncated at 500 characters, which was too strict. This limited the visibility of the full error message, especially for stack traces that exceeded the character limit. This resulted in a poor user experience, requiring users to check the worker logs for the complete error message.\n The truncation of error messages was implemented in order to prevent them from exceeding the maximum message size. However, the limit of 500 characters proved to be too strict for practical use.\n The limit on error message truncation has been increased, allowing for a higher character limit of 10,000. This will provide a better user experience by displaying the full error message instead of truncating it.\n Users will now be able to see the complete error message, even for stack traces that exceed the previous 500 character limit. This improvement enhances the usability of the system by eliminating the need to check worker logs for error details."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13041",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Don't mutate state through `JobBackoffChecker`",
    "releaseNoteText": " Mutating state through `JobBackoffChecker` resulted in unintended side effects for the user. This caused issues similar to the one reported in ticket #12797. \n The `JobBackoffChecker` was not designed to handle state mutations, resulting in unexpected behavior when attempting to cleanup backoffs.\n The issue has been resolved by updating the `JobBackoffChecker` to prevent state mutations. The code in `DbJobState.java` has been modified (lines 308-317) to ensure that state mutations are handled correctly.\n As a result of this fix, the `JobBackoffChecker` now functions as intended and no longer causes unintended side effects or issues related to state mutations. Users can now safely use `JobBackoffChecker` without worrying about unexpected behavior."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13038",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Handle stream consumers changes during push",
    "releaseNoteText": " Previously, when attempting a push from the broker side, if all stream consumers were removed during a retry, an error would occur because the system would try to generate a random index from 0 to 0.\n The issue was caused by attempting to pick the next random stream consumer to push to without considering the possibility that all consumers may have been removed during a retry.\n The `AggregatedRemoteStream` was made immutable by copying the `AggregatedRemoteStream` record when selecting the target. This ensures that the system can handle the scenario of streams with no consumers during retries.\n With this fix in place, the system is now able to cope with streams having no consumers, even during retries. The process of picking a random stream consumer has been modified to handle this scenario and bail out early, preventing errors from occurring."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13036",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Endless client job stream registration",
    "releaseNoteText": "\nClients registering a new job stream from the gateway to any broker were experiencing an issue where the brokers were successfully registering the streams but not responding to the gateway. This resulted in the gateway continuously retrying the registration process, leading to excessive noise and giving the impression that the streams were not successfully registered.\nThe underlying cause of this issue was the utilization of the `ClusterCommunicationService#subscribe(String, Function<byte[], M>, BiConsumer<MemberId, M>, Executor executor)`. In this case, any subscriber that acted as a consumer would not send a response back to the gateway. However, the client expected a response, resulting in the endless loop when registering a job stream.\nThe issue was resolved by modifying the registration process for client job streams. The modifications addressed the issue with the `ClusterCommunicationService#subscribe` method, ensuring that the clients received the expected response upon successfully registering a job stream.\nAfter the fix, clients can now register job streams successfully on both ends. The registration process no longer enters an endless loop, and the brokers respond to the gateway as expected. This improves the overall user experience by reducing noise and providing the correct impression that the job streams have been successfully registered."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12957",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Straight-through processing loop may cause problems",
    "releaseNoteText": "\nSince version 8.2, deploying processes with undefined tasks, which are processed as straight-through activities, can lead to problems. When placed in a loop without a wait state, the workflow engine may process faster than the exporters can export records. This may cause the log to grow, increasing disk space usage and potentially affecting the availability of Zeebe Brokers, Operate, and Elasticsearch.\nThe cause of this issue is the ability to deploy processes with undefined tasks and straight-through processing loops. When the workflow engine processes faster than the exporters can export records, it can lead to resource problems and unavailability of Zeebe.\nTo mitigate this issue, a quick fix has been implemented. A delayed task has been introduced for undefined and manual tasks, introducing a wait state that offsets resource issues caused by straight-through processing loops. This allows for the completion of the tasks after a delay.\nWith the quick fix in place, the impact of straight-through processing loops causing problems in Zeebe has been mitigated. The delayed task for undefined and manual tasks introduces a wait state that prevents resource issues and ensures the completion of these tasks. This improves the overall stability and availability of Zeebe."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12933",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Failing jobs with job push enabled breaks the job lifecycle",
    "releaseNoteText": "\nWith job push enabled, when a job is failed with remaining retries and no backoff, the job is immediately activated again and marked as failed, resulting in pushing out jobs that are in a failed state. This breaks the job lifecycle and may lead to data inconsistencies.\nThe issue was caused by a confusing interplay between the `JobFailProcessor` (a `CommandProcessor`), the `CommandControl`, and the `BpmnJobActivationBehavior`. The code correctly transitioned the job to the `FAILED` state and then handed it over to the `jobActivationBehavior` to be activated. However, the `CommandControl` did not write the follow-up event immediately, leading to incorrect state transitions.\nTwo possible fixes were proposed for this issue. The first fix was to move the job push to the `CommandProcessor#afterAccept` method. The second fix, which was considered a nicer solution, involved refactoring the processors to use the `TypedRecordProcessor` interface, which would make the event writes more clear and readable.\nThe fix was implemented by refactoring the processors to use the `TypedRecordProcessor` interface, ensuring that the job lifecycle is followed correctly. Now, when a job is failed with remaining retries and no backoff, the job transitions to the `FAILED` state and is then activated again. The log now shows a clear sequence of job events: ACTIVATED -> FAIL -> FAILED -> ACTIVATED. This eliminates confusion and provides a more accurate representation of the job's state transitions."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12915",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Allow extending journal record format with out breaking compatibility",
    "releaseNoteText": " Previously, it was not possible to change the journal record schema without breaking backward compatibility. This meant that a broker on a newer version could not receive events via raft replication if the event was written by a leader on an older version. Additionally, a broker on an older version could not receive events via raft replication if the event was written by a leader on a newer version. This limitation made it difficult to upgrade a running system to a new version and caused potential unavailability during rolling updates.\n The inability to extend the journal record schema without breaking compatibility was a result of the journal record schema being moved to SBE (Simple Binary Encoding). While this move was intended to allow for easier extension, it did not fully achieve that goal.\n The fix involved changing the mechanism for sending the serialized journal record in the AppendRequest. This required modifications to the journal API and the raft replication handling. A new concept of raft-protocol-version was introduced, allowing for interpretation of requests based on the version of the sender. When a raft follower on a new version receives a request from an older leader, it writes the record using the old SBE version and calculates a checksum that matches the original. If the request is from a new leader, the new logic is used to handle the serialized journal record.\n With this fix, it is now possible to extend the journal and raft record schema without breaking compatibility. Brokers on newer versions can work with records written with older versions, ensuring backward compatibility. While brokers on older versions may not be able to read records from new versions, this does not cause any inconsistency. This fix allows for smoother rolling updates and recreating with new versions, as followers on newer versions can receive events from leaders on older versions."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12886",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Zeebe should only log as JSON, if configured to do so",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12875",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "`ModifyProcessInstanceRejectionTest#shouldRejectActivationWhenAncestorScopeIsNotFlowScope` test fails on Windows",
    "releaseNoteText": "\nThe `ModifyProcessInstanceRejectionTest#shouldRejectActivationWhenAncestorScopeIsNotFlowScope` test fails on Windows 10. The test expects to be able to activate a parallel gateway 'join', but not all sequence flows have been taken, resulting in an INVALID_STATE error. Additionally, the test attempts to modify an instance of the process 'process', but it contains activate instructions with ancestor scope keys that are not ancestors of the element to activate, resulting in an INVALID_ARGUMENT error.\nThe test failure is caused by line breaks in the error message, which are not handled correctly on the Windows platform.\nThe code has been updated to handle line breaks in the error message on Windows.\nAfter the fix, the `ModifyProcessInstanceRejectionTest#shouldRejectActivationWhenAncestorScopeIsNotFlowScope` test no longer fails on Windows. The error message is now displayed correctly, allowing the test to pass successfully."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12837",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Catch all error events by using empty 'errorCode' does not work",
    "releaseNoteText": "\nCatching error events using an empty `errorCode` did not work. This resulted in incidents with `errorType=UNHANDLED_ERROR_EVENT`. In addition, only error events with a non-empty errorCode were reported, causing confusion for users.\nThe issue was caused by the error catch event (boundary or start event in Subprocess) not correctly handling the empty `errorCode`, leading to incidents being created instead of the error being caught.\nThe fix involved updating the error catch event to properly handle the scenario of an empty `errorCode`. This allowed the error to be caught instead of creating an incident.\nAfter the fix, error events with an empty `errorCode` are now properly caught, and no incidents are created. Users can catch all error events without needing to remove the `errorEventDefinition` as a workaround."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12833",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Cannot resolve output mapping incident on a none end event",
    "releaseNoteText": " Users were unable to resolve incidents on a none end event, leading to the process being stuck on that event.\n The issue was caused by an incident in one of the chaos test models, where a none end event had an output mapping referencing a non-existent variable.\n The incident was resolved by adding a dummy variable and removing the incorrect mapping.\n With this fix, users can now successfully resolve incidents on a none end event, allowing the process to continue as expected."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12754",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Journal reset resulting in an intermediate state which is detected as corruption",
    "releaseNoteText": " A bug was causing the system to enter an invalid intermediate state, which was detected as corruption. This resulted in a startup failure with the error message \"Expected to find a snapshot at index >= log's first index X, but found snapshot Y. A previous snapshot is most likely corrupted.\" The issue arose when a follower received a snapshot, but before committing it, the segments were reset. If the node was shutdown during the segment deletion process and then restarted, it would have the old snapshot and partially deleted segments, leading to the observed corruption.\n The reset or snapshot commit process was not handling the deletion of segments correctly. The reset and segment deletion were not atomic operations, leading to the possibility of an invalid intermediate state.\n The segments were modified to be deleted in reverse order during the reset process. This ensured that there were no gaps in the logs or snapshots, effectively preventing any corruption. However, this fix only addressed the specific case and did not solve the underlying atomicity issue in the installation operation on the follower.\n With the fix in place, the reset or snapshot commit process no longer results in an invalid intermediate state being detected as corruption. The system now correctly handles the deletion of segments, ensuring that the logs and snapshots are always in a valid state. This prevents any startup failures due to corruption and improves the overall stability and reliability of the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12699",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Do not write the entire command for rejection",
    "releaseNoteText": " When the engine rejected a deployment request because it exceeded the maxMessageSize, it was unable to write the rejection record due to attempting to write the entire deployment. This caused a loop in the StreamProcessor, resulting in the partition being fully blocked and making no progress.\n The issue originated from the engine's attempt to write the entire deployment as the rejection record, which was larger than the maxMessageSize. This led to the failure to write the rejection record and the subsequent loop in the StreamProcessor.\n The fix for this issue involved rejecting the request in the CommandAPI before writing to the logstream. Additionally, measures were taken to ensure that rejection records are not too large to prevent similar accidental cases.\n Rejection records can now be written reliably, even if the original command is rejected due to ExceededBatchSize. The fix ensures that rejection records only contain a reference to the original command, instead of the entire command. As a result, the system can accurately handle rejection cases and prevent the blocking of partitions."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12623",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "After restoring from backup, a partition re-takes the backup",
    "releaseNoteText": " After restoring from a backup, a partition would unnecessarily re-take the backup, resulting in a duplicate backup of the partition with the same backup ID. This would waste resources without any impact on the functionality as the new backup would be logically equivalent to the old backup.\n The issue occurred when the leader who originally took the backup was no longer the leader of the partition after the restore. This caused the partition to go through the backup process again, resulting in a duplicate backup.\n The fix involved modifying the partition logic to prevent it from re-taking a backup after a restore. This change ensured that the partition does not unnecessarily repeat the backup process.\n After applying the fix, when restoring from a backup, the partition no longer re-takes the backup unnecessarily. This improves resource utilization and does not have any impact on the functionality as the new backup is logically equivalent to the old one."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12622",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "List backup fails when a partition has same backup taken by multiple nodes",
    "releaseNoteText": "\nWhen a partition has the same backup taken by multiple nodes, the list backup fails with an error message indicating a duplicate key.\nThis issue occurs when a partition attempts to take a backup again while a backup for the same ID already exists. It is a rare case that can occur if there is a leader change during the backup process.\nThe fix for this issue includes handling duplicate backup IDs for a partition in the list backup functionality.\nAfter applying the fix, the list backup will be able to handle partitions with duplicate backup IDs. Users will no longer encounter the error message indicating a duplicate key when trying to list backups."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12597",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Listing backups fails if more than 255 backups are available",
    "releaseNoteText": "\nAttempting to list all available backups fails with an error message stating that the count is outside the allowed range, even though more than 255 backups are present. This issue prevents the proper functioning of the backup API and makes it unusable for querying a large number of backups.\nThe issue is caused by the definition of the `groupSizeEncoding` in the `common-types.xml` file, which uses a `unit8` to represent the number of entries. This encoding mechanism limits the number of backups that can be listed to a maximum of 255.\nTo address this issue, the `groupSizeEncoding` has been modified to use a `uint16` encoding instead. This change will support up to 65535 backups.\nAfter applying the fix, Zeebe will be able to support a much larger number of backups, up to a maximum of 65535. The backup API will now function properly, and the number of listed backups will be limited to a reasonable number, avoiding potential timeouts and making the backup API usable for querying backups."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12591",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "8.2.3 Degradation: Creating an oversized BPMN causes unrecoverable failure ",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12509",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "MessageTTL checking fails with deserialization errors",
    "releaseNoteText": "\nMessageTTL checking failed with deserialization errors. This resulted in a `RuntimeException` when trying to deserialize the `MessageRecord` object, causing the processing actor to fail. As a consequence, processing on the affected partition was also prevented. This issue impacted versions 8.1.9 and 8.2.0 onwards.\nThe issue was caused by unsafe concurrent access to the writer of a shared record value when messages from different partitions expired at the same time. This concurrent access led to deserialization errors and the failure of the processing actor.\nThe fix for this issue was implemented in commit [e1a6cae69c17325fc71a8ee92022a70d969bd0da](https://github.com/camunda/zeebe/commit/e1a6cae69c17325fc71a8ee92022a70d969bd0da). With this fix, the issue of deserialization errors during MessageTTL checking was resolved. \nAfter applying the fix, MessageTTL checking is now performed without encountering any deserialization errors. The processing actor no longer fails and processing on the affected partition can proceed as expected. This fix is available in versions after 8.1.9 and 8.2.0."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12433",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Broker cannot start with S3 accessKey and secretKey not supplied ",
    "releaseNoteText": "\nThe broker cannot start when S3 accessKey and secretKey are not supplied. This results in an error during startup and the broker fails to execute.\nThe issue stemmed from a recent commit that did not handle the case where ACCESS_KEY and SECRET_KEY were not passed. The method `io.camunda.zeebe.backup.s3.S3BackupStore.buildClient` still called `AwsBasicCredentials.create(credentials.accessKey(), credentials.secretKey())` even when no credentials were provided. Additionally, none of the unit or integration tests checked for this scenario.\nA fix has been implemented to handle the case where accessKey and secretKey are not supplied. The method `io.camunda.zeebe.backup.s3.S3BackupStore.buildClient` now verifies if the credentials are present before calling `AwsBasicCredentials.create()`.\nWith the fix in place, the broker now starts successfully even when S3 accessKey and secretKey are not provided. Users can configure the S3 backup properties without needing to supply the credentials through environment variables."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12374",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "CorruptedJournalException: Fail to read version byte from segment",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12328",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Cannot disable Raft flush without specifying a delay",
    "releaseNoteText": "\nIn version 8.2.0, users were unable to disable the Raft flush without specifying a delay. Attempting to start the broker with the `ZEEBE_BROKER_CLUSTER_RAFT_FLUSH_ENABLED` set to `false` resulted in the broker failing to start.\nThis issue was caused by how Spring deserialized the configuration. When using a `record` internally for the configuration, the deserialization process attempted to pass both properties, with the second one being `null`. This prevented relying on \"default\" values as previously done with simple classes.\nThe fix for this issue involved updating the deserialization logic of the Spring configuration. The code was modified to properly handle the scenario where the second property is null, allowing the disabling of Raft flush without specifying a delay.\nWith this fix applied, users can now disable the Raft flush without the need to specify a delay time. Starting the broker with `ZEEBE_BROKER_CLUSTER_RAFT_FLUSH_ENABLED` set to `false` will result in the expected behavior, where the flush is disabled and the broker starts successfully."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12326",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "The `newThrowErrorCommand` incorrectly handled in `8.2.0`",
    "releaseNoteText": " In version 8.2.0 of Zeebe, the `newThrowErrorCommand` was not correctly handled when used with BPMN, causing an error to occur when attempting to throw an error event with a specific code and message.\n The issue was caused by a change in the way errors are transformed in the code. When parsing the error code using the FEEL engine, the assumption was made that the static expression would always be of type `String`, whereas in this case it was a `Number`.\n The issue was addressed by adding support for `Number` in the transformers responsible for handling errors.\n With the fix in place, the `newThrowErrorCommand` is now correctly handled when used with BPMN, allowing the error event to be thrown with the specified code and message without any errors occurring."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12173",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Zeebe node sends messages to wrong node",
    "releaseNoteText": " The Zeebe node was sending messages to the wrong node.\n The channel state in the NettyMessagingService was found to be inconsistent. While analyzing the heap dump, it was discovered that the channel pool for a specific IP address was pointing to the wrong node.\n The fix involved using both the address and inetAddress to find the channel pool. This ensured that the correct channel was used, even if the IP address was reassigned.\n After the fix, the Zeebe node successfully sends messages to the intended node, resolving the issue of sending messages to the wrong node."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12007",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "ExceededBatchRecordSizeException: Can't append entry",
    "releaseNoteText": " Users were unable to activate jobs due to the `ExceededBatchRecordSizeException`. The partition was marked as unhealthy, causing the stream processor to not make progress.\n The issue was caused by the append entry check not accurately determining if a record of large size could be appended to the batch. Additionally, the expected event length calculation was not fully accurate, leading to records being appended even if it exceeded the batch size.\n A fix was implemented to add an extra buffer to the expected event length calculation. This buffer allows for a small margin of error and ensures that records that exceed the batch size are not appended.\n With the fix, users can activate jobs without encountering the `ExceededBatchRecordSizeException`. The partition is healthy, and the stream processor can successfully make progress."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11594",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Triggering due timer events causes periodic latency spikes",
    "releaseNoteText": " Triggering due timer events caused periodic latency spikes in the system. This impact was observed in the blocking of the Stream Processor while the timer event checker ran, resulting in delays in the overall process flow. Additionally, the execution of a batch of commands to trigger timer events without any interleaving with incoming commands further contributed to the latency spikes.\n The issue stemmed from the shared actor between the Stream Processor and the timer event checker. This caused the Stream Processor to be blocked while the checker ran, leading to concurrency issues as reading from RocksDB by the checker could result in changes to the state. Similarly, the job's timeline and backoff checker also exhibited the same pattern.\n To address this issue, the Stream Processor and the checker were decoupled to ensure parallel execution. The checker now collects timers to trigger without blocking the Stream Processor, allowing the process flow to continue uninterrupted. Furthermore, a batch with a limited number of commands (e.g., 10 due timer events) is now submitted by the checker, providing interleaving with incoming commands."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11578",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Multi-Instance with messageevent-based subprocess that uses inputElement as correlationKey fails",
    "releaseNoteText": " Multi-Instance fails to start when it contains a Message Event-based Subprocess that uses the inputElement as correlationKey. Instead, an incident is created, and the expression 'myObject.myId' fails to evaluate due to the variable 'myObject' not being found.\n The issue is caused by a bug in the system where the Multi-Instance process is unable to handle a Message Event-based Subprocess that uses the inputElement as correlationKey. This results in the creation of an incident and the failure of the expression evaluation.\n The bug has been fixed in the latest patch release (version 8.0.15, 8.1.13, 8.2.6, and 8.3.0). The fix addresses the issue with the Multi-Instance process and ensures that it can properly handle the Message Event-based Subprocess with the inputElement as correlationKey.\n With the fix applied, the Multi-Instance process will start as expected and process the necessary tasks. It will also be able to listen to any messages on the event-based subprocess without any incidents or expression evaluation failures."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11414",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Unhandled `NoSuchElementException` when looking for executable process while deploying BPMN resource",
    "releaseNoteText": "\nUsers experienced an unhandled `NoSuchElementException` when attempting to deploy BPMN resources. This issue caused invalid BPMN resources to be handled without any grace and resulted in the exception being thrown.\nThe underlying cause of this issue was that during the deployment, the deployment record process metadata did not contain a process that was present in the BPMN file. This led to a process in the BPMN file that had not been stored as a `PersistedProcess`, causing the `NoSuchElementException` to occur.\nTo address this issue, the code path that was part of the bug was removed. This involved removing the transformation step for the persisted process XML that triggered the bug. Additionally, the code was optimized to improve the performance when deploying new process versions.\nWith this fix applied, users will no longer encounter the unhandled `NoSuchElementException` when attempting to deploy invalid BPMN resources. The deployment process will now handle invalid resources gracefully and provide a more understandable error message."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11355",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Not possible to cancel process instance with many active element instances",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13058",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Documentation"
    },
    "title": "Document new Deployment Distribution logic",
    "releaseNoteText": " The deployment distribution logic was not documented, causing confusion and making it difficult for users to understand how the distribution works.\n The lack of documentation on the deployment distribution logic stemmed from the decision not to include a section on resource deployments. This decision was made based on the belief that distribution is generic and should work the same for all commands.\n The documentation has been updated to include a comprehensive description of the deployment distribution logic, including a section on resource deployments. This section provides clear and detailed information on how the distribution works in the context of deploying resources.\n Users now have access to complete and accurate documentation that explains the deployment distribution logic and provides specific details on resource deployments. This will enhance user understanding and enable them to effectively utilize the distribution functionality in the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12584",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Documentation"
    },
    "title": "Document guidelines on how to handle flaky tests",
    "releaseNoteText": " Flaky tests were causing disruptions in the testing process and hindering contributors' progress.\n Flaky tests were occurring due to unpredictability in the test environment, timing issues, or race conditions.\n A guide was added to provide contributors with clear instructions on how to handle flaky tests. The guide includes best practices and troubleshooting steps to identify and resolve flakiness issues.\n Contributors now have a comprehensive guide to refer to when encountering flaky tests. This empowers them to effectively troubleshoot and resolve flakiness, resulting in smoother testing processes and improved progress in their contributions."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13989",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Enhancements"
    },
    "title": "Add auth data to Zeebe records",
    "releaseNoteText": " Previously, the Zeebe system did not include authentication data in the Zeebe records. This meant that user permissions and access lists were not accounted for, limiting the ability to enforce security measures and conduct proper access control.\n The lack of authentication data in Zeebe records was due to the absence of a mechanism to include this information. The current record structure did not provide a space to store such data, resulting in a lack of extendability for future updates.\n In order to address this issue, authentication data has been added to the `RecordMetadata`. This ensures that the data is easily accessible and allows for future extensions without the need for significant changes to the codebase. The auth data is encoded within the `RecordMetadata` structure and includes a flag to indicate the mechanism used for encoding and decoding.\n With the implementation of this fix, Zeebe now includes auth data in all types of Zeebe records. The auth data is stored in the `RecordMetadata`, allowing for easy access and extending support for user permissions in the future. This update enhances the security and access control capabilities of Zeebe, ensuring a more robust and flexible system for users."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13989",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Enhancements"
    },
    "title": "Add auth data to Zeebe records",
    "releaseNoteText": " Previously, the Zeebe system did not include authentication data in the Zeebe records. This meant that user permissions and access lists were not accounted for, limiting the ability to enforce security measures and conduct proper access control.\n The lack of authentication data in Zeebe records was due to the absence of a mechanism to include this information. The current record structure did not provide a space to store such data, resulting in a lack of extendability for future updates.\n In order to address this issue, authentication data has been added to the `RecordMetadata`. This ensures that the data is easily accessible and allows for future extensions without the need for significant changes to the codebase. The auth data is encoded within the `RecordMetadata` structure and includes a flag to indicate the mechanism used for encoding and decoding.\n With the implementation of this fix, Zeebe now includes auth data in all types of Zeebe records. The auth data is stored in the `RecordMetadata`, allowing for easy access and extending support for user permissions in the future. This update enhances the security and access control capabilities of Zeebe, ensuring a more robust and flexible system for users."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13752",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "title": "job.getVariable ergonomic method in Java client",
    "releaseNoteText": "\nUsers experienced the need to repeatedly write `job.getVariablesAsMap().get(\"name\")` when using the Java client with variables.\nThe absence of a shorthand method like `job.getVariable(\"name\")` caused users to resort to using `job.getVariablesAsMap().get(\"name\")` multiple times.\nA new method `job.getVariable()` was added, which internally caches the map from `job.getVariablesAsMap()` and retrieves the desired variable using `.get(\"name\")`.\nUsers can now use the `job.getVariable(\"name\")` method to obtain the value of a variable with reduced boilerplate code. This improvement enhances the first-time experience with the platform, making it more convenient for users."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/4700",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "title": "Introduce JobWorker metrics for the Java client",
    "releaseNoteText": "\nPreviously, there were no metrics available for monitoring Zeebe workers in the Java client. This made it difficult for users to track the number of jobs scheduled by a worker.\nThe lack of metrics was due to the absence of a metrics facade in the job worker of the Java client.\nA metrics facade has been added to the job worker in the Java client. This facade allows users to monitor the number of jobs currently enqueued by a worker.\nWith this fix, users can now easily monitor their Zeebe workers in the Java client. They have access to metrics such as the count of jobs activated and handled, and the number of queued jobs. This provides better visibility into the status and performance of the workers."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13321",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "title": "Java client supports multi-tenancy for DeployResource RPC",
    "releaseNoteText": " Previously, the Java client did not support multi-tenancy for the `DeployResource` RPC. This meant that users were not able to deploy resources to specific tenants in Zeebe. This resulted in confusion and limitations for users who required multi-tenancy support.\n The lack of support for multi-tenancy in the Java client was due to the absence of an optional `tenantId` property or method in the `DeployResourceCommand`. This prevented users from specifying a tenant for their deployments.\n To address this issue, the Java client has been updated with multi-tenancy support for the `DeployResource` RPC. The `ZeebeClientBuilderImpl` class now includes a `defaultTenantId` property where users can set a default tenant ID value. The `DeployResourceCommand` has also been enhanced with a new `tenantId(String tenantId)` method, which allows users to specify the desired tenant ID for their deployments.\n With this fix, users can now take advantage of multi-tenancy support in the Java client for the `DeployResource` RPC. They can specify a tenant ID for their deployments, enabling them to properly manage and access data for specific tenants. This greatly enhances the flexibility and functionality of the Java client for users in multi-tenant environments."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14044",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "title": "Backpressure queue is not reset when back-to-back role transitions",
    "releaseNoteText": "\nThe backpressure queue was not properly reset when there were back-to-back role transitions in the system. This resulted in 100% backpressure on one partition, preventing it from processing tasks.\nThe issue was caused by a bug in which the command API was not properly notified when a node transitioned from leader to follower. This bug was introduced in a previous fix that resulted in the reuse of the limiter from the previous leader role, even when the transition was cancelled. As a result, the backpressure queue was not reset correctly.\nIn order to fix the issue, the code was modified to ensure that the command API is properly notified when a role transition occurs. Specifically, the code in the `PartitionAwareRequestLimiter` class was updated to remove the limiter when a transition is cancelled.\nWith this fix, the backpressure queue is now correctly reset when there are back-to-back role transitions. This ensures that the system can process tasks in a timely manner and prevents any unnecessary backpressure on the partitions."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13936",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "title": "Condition ignored on Inclusive Gateway with singular outgoing Sequence Flow",
    "releaseNoteText": "\nAn Inclusive Gateway with a single outgoing Sequence Flow was ignoring the Condition. The expected behavior, as specified in the BPMN spec, was for a runtime exception to occur if none of the conditional expressions evaluated to true. However, in this case, the Sequence Flow was being taken without raising an incident.\nThe underlying cause of this issue was the improper handling of the Condition in the Inclusive Gateway logic. The system was not correctly evaluating the Condition expression and was allowing the Sequence Flow to be taken even when the Condition evaluated to false.\nThe fix for this issue involved updating the logic of the Inclusive Gateway to properly evaluate the Condition expression. The system now correctly checks the Condition and only allows the Sequence Flow to be taken if the Condition evaluates to true. If none of the conditional expressions evaluate to true, a runtime exception is raised, as expected.\nWith this fix, an Inclusive Gateway with a single outgoing Sequence Flow correctly considers the Condition. If the Condition evaluates to false, a runtime exception is raised, indicating that none of the conditional expressions evaluated to true. This ensures that the system behaves according to the BPMN spec, providing a more accurate and reliable process flow."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13093",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "title": "`RandomizedRaftTest.consistencyTestWithSnapshot` fails with unexpected exception",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/7855",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "title": "Messaging service is not running",
    "releaseNoteText": " The messaging service was not running, causing an error when trying to send a response and closing the Broker.\n The issue occurred due to the incorrect order of resource closure. The messaging service was closed concurrently while still accepting or attempting to send a response, resulting in an error during the closure of the Broker.\n The `AtomixClientTransportAdapter` was not closed by the `BrokerClientImpl`. The fix involved closing the `AtomixClientTransportAdapter` when closing the `BrokerClientImpl` to ensure that no requests are sent after it is closed.\n After the fix, the messaging service is running correctly, ensuring that requests are sent and responses are received without errors during the closure of the Broker."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/5209",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "title": "Startup failure of one raft partition server affects the availability of other healthy partition",
    "releaseNoteText": " The availability of healthy partitions is affected by the startup failure of one raft partition server. This issue arises because the system currently waits for all partitions to successfully start their raft servers before starting the zeebe partition services. As a result, even though there are leaders for all partitions, the service may be unavailable or only partially available.\n The issue is caused by the current logic of waiting for all partitions to start before installing stream processors and other leader services. This logic is based on waiting for the atomix start to be completed.\n The fix involves removing the complete atomix wrapper and bootstrap logic. Instead, independent starters for each partition are implemented. The bootstrapping logic is improved by starting the atomix transport, membership, topology, monitoring, and disk space first. Then, the partition starts are split up into sub-steps for installing processors and other services.\n After the fix is applied, the system will no longer wait for all partitions to start before installing stream processors and leader services. Each partition will be able to start independently, allowing for parallel startup of the partitions. This will improve the availability of healthy partitions and prevent the system from being partially or fully unavailable due to startup failures."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14146",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Remove DRG from cache upon deletion",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14028",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Should not activate link catch event in subprocess",
    "releaseNoteText": "\nThe link catch event inside the (event) subprocess is activated when a link throw event occurs, even if they are not in the same scope. This behavior is contrary to the BPMN specification and the Camunda documentation.\nThe activation of a link catch event in a subprocess is a result of a technical issue in the product. The system was not properly checking the scope of the link events before activating them.\nThe issue has been fixed by implementing a check to ensure that a link throw event can only activate a link catch event within the same scope. The validation logic now correctly denies the activation of a link catch event in a different scope.\nAfter applying the fix, the link catch event inside the (event) subprocess will no longer be activated when a link throw event occurs. The deployment of the process will be rejected if there is no link catch event in the same scope, ensuring compliance with the BPMN specification and the Camunda documentation."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13881",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Upgrading leaves deadline entries without jobs",
    "releaseNoteText": "\nUpgrading Zeebe from version 8.2.8 or earlier to a newer version may result in entries in the job deadline column family being left without corresponding jobs. This can lead to repeated error logs indicating that the job cannot be found.\nThe issue was caused by a change in behavior where the system expects no duplicate deadline entries and does not clean up duplicated or orphaned entries during the upgrade process.\nA fix has been implemented that includes a migration process to clean up orphaned entries in the job deadline column family. This ensures that these entries are removed from the system's state and no longer cause error logs.\nAfter applying the fix, upgrading Zeebe to a version beyond 8.2.8 will no longer result in orphaned entries in the job deadline column family. The migration process will clean up any existing orphaned entries, preventing them from causing error logs indefinitely."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13867",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "ConcurrentModificationException when clearing obsolete job activation requests",
    "releaseNoteText": " Users experienced a `ConcurrentModificationException` when attempting to clear obsolete job activation requests. This exception occurred due to the non-thread-safe nature of the `activeRequests` LinkedList.\n The issue originated from the attempt to clear the `activeRequests` LinkedList, which was not designed to be accessed concurrently.\n The non-thread-safe behavior of the `activeRequests` LinkedList was addressed by implementing thread-safe modifications.\n With the fix applied, users will no longer encounter the `ConcurrentModificationException` when clearing obsolete job activation requests. The system now properly handles concurrent modifications to the `activeRequests` LinkedList, ensuring smooth execution without any exceptions."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13814",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Job streaming may trigger unnecessary polling",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13796",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "IllegalStateArgument when removing job stream",
    "releaseNoteText": " When attempting to remove a job stream in the gateway, an exception was thrown resulting in the stream not being removed from the gateway or the broker.\n The issue was caused by the future completing without an executor, causing it to complete within the actor context and then mistakenly calling `Actor.call` instead.\n To ensure that an executor is used, the fix updated the code to correctly utilize an executor when completing the future.\n After applying this fix, the job stream is successfully removed from the gateway without any errors being thrown."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13787",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Release: `Repo dispatch Benchmark` fails on 8.0/8.1",
    "releaseNoteText": " The `Repo dispatch Benchmark` fails on versions 8.0/8.1. When building the starter and worker, the Maven wrapper is not found on version 8.1, resulting in an error indicating that the file or directory does not exist. Additionally, on version 8.0, the build of Zeebe fails due to the absence of the `DIST=build` setup in the Dockerfile.\n The issue was caused by the absence of the Maven wrapper on version 8.1 and the missing `DIST=build` setup in the Dockerfile on version 8.0.\n The fix for this issue involved backporting the Maven wrapper to versions 8.0 and 8.1 to prevent workflow merge conflicts when moving from the main branch to stable. Additionally, a `benchmark.yaml` workflow was added to each stable branch to maintain a stable setup. The trigger for these workflows was modified to use workflow dispatch, referencing the release branch. Finally, the `dispatch-benchmark.yaml` workflow was removed from the main branch.\n After applying the fix, the `Repo dispatch Benchmark` issue no longer occurs on versions 8.0/8.1. The starter and worker can be built successfully without encountering errors related to the missing Maven wrapper. Additionally, the build of Zeebe on version 8.0 now includes the necessary `DIST=build` setup in the Dockerfile."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13521",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Process instances are banned when trying to activate a call activity with large variables",
    "releaseNoteText": "\nWhen trying to activate a call activity with large variables, the complete process instance is banned and left in limbo. This can lead to data loss and severe consequences for the user. \nThe issue is caused by the processing exceeding the batch size, which results in the ban of the process instance. The current error handling in the BpmnStreamProcessor does not raise an incident for every process instance element type, which prevents the proper handling of errors.\nTo resolve this issue, the error handling in the BpmnStreamProcessor has been modified. Errors are now properly handled in the respective BpmnElementProcessor, such as the CallActivityProcessor. This allows for the raising of incidents and better handling of process instances.\nWith this fix, incidents will be raised when activating a call activity with large variables exceeds the batch size. This allows users to modify their variables and save the process instance, thus preventing data loss and ensuring the proper execution of the workflow."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12699",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Do not write the entire command for rejection",
    "releaseNoteText": " When the engine rejected a deployment request because it exceeded the maxMessageSize, it was unable to write the rejection record due to attempting to write the entire deployment. This caused a loop in the StreamProcessor, resulting in the partition being fully blocked and making no progress.\n The issue originated from the engine's attempt to write the entire deployment as the rejection record, which was larger than the maxMessageSize. This led to the failure to write the rejection record and the subsequent loop in the StreamProcessor.\n The fix for this issue involved rejecting the request in the CommandAPI before writing to the logstream. Additionally, measures were taken to ensure that rejection records are not too large to prevent similar accidental cases.\n Rejection records can now be written reliably, even if the original command is rejected due to ExceededBatchSize. The fix ensures that rejection records only contain a reference to the original command, instead of the entire command. As a result, the system can accurately handle rejection cases and prevent the blocking of partitions."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13516",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Add integration tests for Job Push",
    "releaseNoteText": "\nIntegration tests for Job Push were missing, leaving a gap in the coverage of the happy path scenario. This made it difficult to ensure that the system was working correctly end-to-end.\nThe absence of integration tests for Job Push was due to an oversight during the development process. These tests were not implemented, resulting in a gap in the test coverage.\nIntegration tests were added for Job Push to cover the happy path scenario. This involved writing test cases that simulated the entire process, ensuring that all steps were functioning correctly.\nWith the addition of integration tests for Job Push, the system now has improved test coverage for the happy path scenario. This helps to ensure that the functionality is working correctly and allows for easier detection and resolution of any issues that may arise."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13354",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Delete and recreate signal subscription of previous version",
    "releaseNoteText": " The previous version of a process was not being properly unsubscribed, resulting in unexpected behavior with signal subscriptions. This could lead to incorrect process instance creation and signals not being received in certain scenarios. \n The issue was caused by not properly handling the deletion and recreation of signal subscriptions when a previous version of a process becomes the new latest version. This led to the signal subscription not being properly created and linked to the process instances.\n The code was updated to properly handle the deletion and recreation of signal subscriptions when a previous version becomes the new latest version. The process now correctly unsubscribes the signal from the previous version and creates the signal subscription for the new latest version.\n With this fix, when a previous version of a process becomes the new latest version, the signal subscription is correctly created, ensuring that the process instances are started and that signals are received as expected."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13343",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Migrate running process instance into `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily",
    "releaseNoteText": " Running process instances were not being migrated into the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily.\n The ColumnFamily did not contain all running process instances, leading to incomplete migration.\n A migration script was created to populate the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily with all running process instances. Tests were included to ensure the integrity of the migration process.\n After applying this fix, the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily now contains all running process instances, ensuring complete migration and accurate representation of process data."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13253",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Notify Zeebe Team in Slack when an issue is labelled as critical",
    "releaseNoteText": " Previously, critical issues in the Zeebe system would go unnoticed unless someone manually notified the team. This could lead to delays in addressing these urgent problems.\n The lack of an automatic notification system for critical issues meant that the team relied on manual reminders to be aware of these urgent problems.\n A workflow has been implemented that checks if an issue is labelled as critical. If a critical label is added, a notification is automatically sent to the Zeebe Slack channel, alerting the team to the need for immediate attention.\n With this fix in place, the Zeebe team will now receive immediate notifications in Slack when an issue is labelled as critical. This will ensure that urgent problems are promptly addressed and prevent any delays in resolving critical issues."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12942",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Support S3 backup httpclient custom configuration.",
    "releaseNoteText": " The backup API fails when attempting to upload multiple log segments in parallel. This results in a `java.util.concurrent.CompletionException` and the backup process is marked as \"FAILED\". \n The issue arises due to the lack of limiting parallel uploads when there are a large number of log segments. In this specific case, with 66 log segments, some uploads wait for longer than the default connection acquisition timeout of 45 seconds, leading to backup failure.\n The fix involves introducing a limit on the number of concurrent uploads to prevent the issue of long wait times for available connections. The limit will be configurable, allowing users to adjust according to their specific requirements.\n After applying the fix, the backup API will successfully upload log segments without exceeding the connection acquisition timeout. The number of concurrent uploads can be controlled by adjusting the configuration, ensuring smooth and efficient backup operations."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12283",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Zeebe BPMN Model should provide reasonable defaults for definition attributes",
    "releaseNoteText": "\nPreviously, when using the `zeebe-bpmn-model` to model processes, users had to manually set multiple attributes by hand in order to open the processes in the Desktop Modeler as a C8 process. This process was cumbersome and time-consuming.\nThe lack of reasonable defaults for the definition attributes in the `zeebe-bpmn-model` caused users to manually set the necessary attributes. This was a technical limitation of the product.\nIn this release, we have added reasonable defaults for the definition attributes in the `zeebe-bpmn-model`. This allows users to avoid manually setting these attributes, saving time and effort.\nWith this fix applied, users will no longer have to manually set the attributes when modeling processes with the `zeebe-bpmn-model`. The default values will be automatically applied, making the process more efficient and user-friendly."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14137",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Implement query to retrieve the latest form by formId from the state",
    "releaseNoteText": " Users were unable to retrieve the latest form by formId from the state.\n The `DbFormState` class did not have a method to retrieve the latest form by form id.\n Added a new method to the `DbFormState` class that allows retrieval of the latest form by form id.\n Users can now retrieve the latest form by formId from the state using the new method in the `DbFormState` class."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14135",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Allow binding forms to start events by formId",
    "releaseNoteText": " Previously, it was not possible to link a `Start Event` to a deployed form using the `formId` field. This caused limitations in the configuration of forms for start events.\n The absence of the `formId` attribute in the `ZeebeFormDefinition` prevented the linkage of forms to start events. Additionally, the `ZeebeElementValidator.validate()` method did not have the capability to validate groups of fields, resulting in the inability to determine the validity of start events with linked forms.\n The `formId` attribute has been added to the `ZeebeFormDefinition` to allow the linkage of forms to start events. The `ZeebeElementValidator.validate()` method has also been updated to validate groups of fields, making it possible to determine the validity of start events with either the `formKey` or `formId` present.\n With this fix, users can now link a `Start Event` to a deployed form using the new `formId` field. This provides flexibility in the configuration of forms for start events and allows for validation of start events with linked forms based on the presence of either the `formKey` or `formId`."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14134",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Allow binding forms to user tasks by formId",
    "releaseNoteText": " The user task binding to a deployed form was not possible by using the formId field. \n The formId attribute was not present in the ZeebeFormDefinition, which prevented the user from linking a user task to a deployed form using the formId. Additionally, the ZeebeElementValidator.validate() method did not have the capability to validate a group of fields, causing issues when validating a user task with a linked form.\n The formId attribute has been added to the ZeebeFormDefinition, allowing users to bind a user task to a deployed form using the formId field. The ZeebeElementValidator.validate() method has been updated to validate either the formKey or the formId field, ensuring that a user task with a linked form is valid if at least one of these fields is present.\n Users can now link a user task to a deployed form by using the formId field. The ZeebeElementValidator.validate() method properly validates user tasks with linked forms, allowing for smooth and error-free form bindings."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/14133",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Save Form to the state",
    "releaseNoteText": " Saving the Form to the state was not possible, causing user data to not be persisted.\n The Form DB classes, Form column family definitions, and the necessary methods to store and retrieve the Form were not implemented.\n Implemented the Form DB classes, created Form column family definitions, and added methods to store and retrieve the Form.\n Users can now successfully save the Form to the state, ensuring that their data is persisted and can be retrieved later when needed."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13319",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Gateway supports multi-tenancy in deployment RPCs",
    "releaseNoteText": " The Gateway now has the capability to support multi-tenancy in deployment RPCs. When receiving and forwarding DeployResource RPC calls, the Gateway now includes a `tenantId` property.\n The issue was caused by the need to support multi-tenancy in the Gateway's deployment RPCs. This required adding the `tenantId` property to various gRPC messages, such as `DeployResourceRequest`, `ProcessMetadata`, `DecisionMetadata`, and `DecisionRequirementsMetadata`. \n The fix involved modifying the `deployResource(...)` Gateway endpoint to pass the `DeployResourceRequest#tenantId` property to the `BrokerDeployResourceRequest`. Depending on the multi-tenancy configuration, the `BrokerDeployResourceRequest#tenantId` was set to either `<default>` or the provided `DeployResourceRequest#tenantId`. \n As a result of this fix, the Gateway is now able to receive and forward DeployResource RPC calls with the appropriate `tenantId` property. This allows for proper multi-tenancy support in the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13473",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "title": "Stream jobs using job worker",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13460",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "title": "newModifyProcessInstanceCommand: complete command with single variable",
    "releaseNoteText": "\nThe `newModifyProcessInstanceCommand` in the Java Client did not have an option to complete the command with a single variable.\nThe lack of a single variable completion option in `newModifyProcessInstanceCommand` was due to the absence of a method that could accept a single variable.\nAdded the `variable(String name, Object value)` method to the `newModifyProcessInstanceCommand` in the Java Client, allowing the command to be completed with a single variable.\nUsers can now use the `client.newModifyProcessInstanceCommand(job).variable(\"name\", value)` syntax to complete the command with a single variable, providing a more convenient and concise way to modify process instances."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13458",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "title": "newThrowErrorCommand: complete command with single variable",
    "releaseNoteText": "\nThe incomplete `newThrowErrorCommand()` method in the Java Client was causing inconvenience for users as they had to use the alternative method `client.newThrowErrorCommand(job).variables(Map.of(\"name\", value))` when they needed to pass only a single variable. \nThe inability of the `newThrowErrorCommand()` method to accept a single variable was due to the lack of support for passing a single variable directly.\nThe `newThrowErrorCommand()` method has been updated to now support a single variable. Users can now complete the command by using `client.newThrowErrorCommand(job).variable(\"name\", value)`.\nAs a result of this fix, users can now conveniently complete the `newThrowErrorCommand()` by passing a single variable directly. This will enhance the user experience and provide a more intuitive way to work with the Java Client. Users will no longer need to rely on the alternative method when they need to pass only one variable."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13456",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "title": "newFailCommand: complete command with single variable",
    "releaseNoteText": "\nPreviously, when using the Java Client, the `.newFailCommand()` method required multiple variables to be completed. This imposed extra complexity and verbosity on the user.\nThe underlying cause of this issue was the design of the `.newFailCommand()` method, which did not support completing the command with a single variable.\nIn this release, we have made a fix to the Java Client by enhancing the `.newFailCommand()` method. Users can now use a single variable to complete the command by calling `.variable(\"name\", value)`.\nAs a result of this fix, users of the Java Client can now complete the `.newFailCommand()` method with a single variable, simplifying their code and reducing verbosity."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13451",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "title": "newBroadcastingSignalCommand: complete command with single variable",
    "releaseNoteText": "\nPreviously, when using the Java Client's `newBroadcastingSignalCommand()` method, users had to specify multiple variables using a `Map` object. This was inconvenient for users who were not using Java 9 or above, as the `Map.of()` method was not available. \nThe underlying cause of this issue was that the `newBroadcastingSignalCommand()` method did not provide an option for users to specify variables using a single variable.\nIn this release, we have made improvements to the `newBroadcastingSignalCommand()` method. Users can now complete the command by using only a single variable, making it more convenient and compatible with different versions of Java.\nAfter applying this fix, users can use the `newBroadcastingSignalCommand(job).variable(\"name\", value)` syntax to easily specify variables for the broadcasting signal command. This simplifies the code and improves the overall user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13449",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "title": "newEvaluateDecisionCommand: complete command with single variable",
    "releaseNoteText": " The `.newEvaluateDecisionCommand()` method did not allow users to complete the command with a single variable.\n The implementation of the method did not support passing a single variable as an argument, which limited the usability and flexibility of the feature.\n The `.newEvaluateDecisionCommand()` method has been updated to accept a single variable as an argument, allowing users to easily complete the command with a single variable.\n Users can now conveniently use the `.newEvaluateDecisionCommand()` method by providing a single variable, enhancing the usability and flexibility of the feature."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13447",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "title": "newPublishMessageCommand: complete command with single variable",
    "releaseNoteText": " The `newPublishMessageCommand` method did not support completing the command with a single variable in the Java Client. This meant that users had to use the `variables` method with a map or upgrade to Java 9 or above to use the `Map.of()` method.\n The Java Client did not provide a way to complete the `newPublishMessageCommand` with a single variable parameter, only allowing users to pass a map.\n We have added support for completing the `newPublishMessageCommand` method with a single variable by introducing the `variable` method. This allows users to provide a name-value pair directly as parameters to the method.\n Users can now complete the `newPublishMessageCommand` method using a single variable by using the `variable` method, simplifying the code and making it more intuitive."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13443",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "title": "newCreateInstanceCommand: complete command with single variable",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13428",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "title": "Allow custom job worker executors",
    "releaseNoteText": "\nPreviously, users were unable to customize the job worker executor and measure the waiting time for jobs before processing. The only configurable option was the number of threads in the job worker's pool, which was always global per client.\nThe limitation in customizing the job worker executor was due to the Java client's compatibility with Java 8.\nIn this release, the Java client has been updated to allow custom job worker executors. This update includes the implementation of a thread-per-task execution model, using virtual threads for execution. Users can now provide their own executor and easily instrument it.\nAs a result of this fix, users can now customize the job worker executor and measure the waiting time for jobs before processing. This improvement enhances performance and provides a better user experience."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13024",
      "component": "Zeebe",
      "subcomponent": "zbctl",
      "context": "Enhancements"
    },
    "title": "Remove zbctl from 8.3 images going forward",
    "releaseNoteText": "\nPreviously, the docker images for version 8.3 of the product included the `zbctl` tool. However, this tool has caused several recently reported CVEs related to golang. These CVEs have impacted the security of the docker images and have raised concerns about the overall safety of the product. \nThe underlying cause of the issue was the inclusion of the `zbctl` tool in the docker images. Although the tool was intended for debugging and troubleshooting purposes, it has posed a significant security risk, leading to the reported CVEs. \nTo address this issue, the `zbctl` tool has been removed from the docker images in version 8.3 going forward. This decision eliminates the potential vulnerabilities associated with the tool and enhances the overall security of the product.\nAs a result of this fix, users will observe that the docker images for version 8.3 and onwards no longer include the `zbctl` tool. By removing the tool, potential security risks are mitigated, ensuring a safer environment for the users."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13465",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Validate user input before registering a worker to the job stream",
    "releaseNoteText": "\nUser input was not being validated before registering a worker to the job stream. This resulted in several issues. Firstly, the system was unable to provide accurate validation error messages to the client. Additionally, workers were able to register with incorrect job types, leading to a lack of job pushes in the future.\nThe underlying cause of this issue was the lack of validation for user input when registering a worker to the job stream. This validation was necessary in order to ensure the accuracy and integrity of the system.\nTo address this issue, the team implemented validation checks for user input before registering a worker to the job stream. Specifically, they added checks for blank or null values in the type field and a minimum timeout value of 1.\nAs a result of this fix, user input is now properly validated before registering a worker to the job stream. This ensures that validation error messages can be accurately returned to the client, and workers with incorrect job types are prevented from registering. The system now operates with improved accuracy and integrity in handling worker registrations."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13429",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Implement ZeebeClient wrapper for the StreamJobs RPC",
    "releaseNoteText": " The lack of a ZeebeClient wrapper for the StreamJobs RPC prevented users from easily streaming jobs from a Zeebe cluster. This meant that users had to manually handle the gRPC call and integrate it into their client.\n The lack of a ZeebeClient command and API for the StreamJobs RPC was the underlying cause of this issue. The existing ActivateJobsCommand was not suitable for this long-lasting streaming functionality.\n A new command, StreamJobsCommand, and a new API, ZeebeClient#newStreamJobsCommand, were introduced to wrap the underlying gRPC call and provide integration into the client. The StreamJobsCommand has three steps: setting the job type, setting a job consumer, and setting metadata (optional).\n With this fix, users can now easily stream jobs from a Zeebe cluster using the ZeebeClient wrapper for the StreamJobs RPC. The StreamJobsCommand allows users to set the job type, a job consumer, and metadata. Additionally, the integration ensures that the stream is long-lived and does not apply the default request timeout. Users can still provide a request timeout if desired."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13349",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Add and handle Process Deleting event",
    "releaseNoteText": " The process deleting event was not being handled, resulting in the process definition not being marked as pending deletion.\n The cause of this issue was the absence of the `DELETING` intent in the `ProcessIntent` and a missing `EventApplier` to handle the `DELETING` intent. Additionally, there was no method on the `DbProcessState` to change the state to `PENDING_DELETION`, and the state was not updated in the `ColumnFamily` or the cache.\n The fix involved adding the `DELETING` intent to the `ProcessIntent` and creating an `EventApplier` to handle this intent. The `EventApplier` was responsible for changing the state of the `PersistedProcess` to `PENDING_DELETION` and updating the state in the `ColumnFamily` and the cache. A new method was also created on the `DbProcessState` to support the state change, and tests were included to ensure its correctness.\n With this fix, the process deleting event is now properly handled. When this fix is applied, the process definition will be correctly marked as pending deletion, ensuring that it can be effectively deleted in the future."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13348",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Add a `state` to `PersistedProcess`",
    "releaseNoteText": " The system did not have a way to track process definitions that were pending deletion.\n The `PersistedProcess` did not have a `state` property to indicate the status of the process.\n A `state` property was added to the `PersistedProcess` class as a new enum called `PersistedProcessState`. The `state` property can have two values: `ACTIVE` and `PENDING_DELETION`.\n Users can now easily mark a process for deletion by setting its `state` to `PENDING_DELETION`. This allows for better organization and management of process definitions in the system. Additionally, in the future, more states can be added to the `PersistedProcessState` enum to accommodate other statuses, such as `SUSPENDED`."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13342",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Remove from `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily",
    "releaseNoteText": "\nThe `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily was not being updated correctly when a process reached an end state. This resulted in outdated and unnecessary data in the ColumnFamily.\nThe underlying cause of this issue was the lack of a method to remove data from the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily. This omission prevented the system from properly cleaning up after a process reached an end state.\nA method has been added to remove data from the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily. This method has been thoroughly tested to ensure its correctness.\nAdditionally, the `ElementInstanceStateTest#shouldNotLeakMemoryOnRemoval` has been updated to change the generated processInstanceRecord to be of element type PROCESS.\nNow, when a process reaches an end state, it is correctly removed from the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily. This ensures that the system does not retain outdated and unnecessary data, leading to improved performance and reliability."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13341",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Insert into the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily",
    "releaseNoteText": " The `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily is empty, resulting in missing data when starting a new process instance or calling a CallActivity.\n The missing data in the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily is caused by the lack of a method to insert data into this ColumnFamily.\n A new method has been added to insert data into the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily. Tests have also been included to ensure the correctness of the insertion process.\n With this fix, the `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily will now be populated with the necessary data. This means that starting a new process instance and calling a CallActivity will properly insert data into the ColumnFamily, ensuring the availability of the required information."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13340",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Add `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily",
    "releaseNoteText": " Users were unable to find out if there were any process instances for a definition key, which impacted the ability to check if there were other process instances still running when a process instance is terminated/completed. This caused pending deployments to not be fully deleted.\n The `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily was not created in `ZbColumnFamilies` and the creation of this ColumnFamily was missing in the `DbElementInstanceState`.\n The `PROCESS_INSTANCE_KEYS_BY_DEFINITION_KEY` ColumnFamily has been created in `ZbColumnFamilies` and has been added to `DbElementInstanceState` to ensure its creation.\n Users can now check if there are any process instances for a definition key. This allows for proper handling of terminated/completed process instances and ensures that pending deployments are fully deleted when there are no other process instances running."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13335",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Add Resource Deleting intent",
    "releaseNoteText": " The system did not have a specific intent for resource deletion, resulting in inconsistent behavior and a lack of clarity for users. The system only had a `DELETED` event written to the log and sent as a response to the client, without logging the `DELETING` event or updating the response message.\n The `ResourceDeletionIntent` class did not have a designated `DELETING` intent, causing a gap in the processing flow. The processor responsible for resource deletion in the system only handled the `DELETED` event and did not log the `DELETING` event or update the response message.\n A new `Intent` named `DELETING` was added to the `ResourceDeletionIntent` class. The `ResourceDeletionProcessor` was modified to write the `DELETING` event to the log before sending the response to the client. The response message was also changed to reflect the `DELETING` event. Finally, the `DELETED` event is still logged after the response is sent.\n With this fix, when a resource is being deleted, the system now properly recognizes the `DELETING` intent. The `DELETING` event is logged before the response is sent to the client, providing a clear indication of the ongoing deletion process. The response message is also updated to reflect the `DELETING` event, ensuring consistent and informative communication with the user. Finally, the `DELETED` event continues to be logged after the response, allowing for complete tracking and auditing of the deletion process."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13040",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "As a Zeebe Java user, I want to complete a Job with a single variable",
    "releaseNoteText": "\nCompleting a job in Zeebe Java required wrapping the variable value in a Map before passing it to the `newCompleteCommand` method. This led to unnecessary overhead and additional code complexity for developers.\nThe underlying cause of this issue was the limited functionality of the `newCompleteCommand` method, which only offered a `.variables()` parameter that required a Map input.\nThe fix for this issue involved introducing a new feature that allows developers to complete a job with a single variable value. The `newCompleteCommand` method now accepts a `variable(key, value)` parameter, allowing developers to directly pass the variable value without the need for a Map wrapper.\nAfter applying this fix, Zeebe Java users can now complete a job using the simplified syntax `client.newCompleteCommand(job).variable(\"name\", value)`. This removes the overhead of wrapping the value in a Map and provides a more intuitive and streamlined approach for completing jobs. Additionally, by promoting this feature and providing examples in the documentation, the Zeebe team aims to ensure that all developers, including those not familiar with newer Java versions, are aware of this shortcut."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12975",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Export number of buffered messages as metrics",
    "releaseNoteText": "\nThe user was unable to obtain the number of buffered messages as a metric, making it difficult to assess the health of the system and identify any potential issues.\nThe product did not have a feature in place to export the number of buffered messages as metrics, limiting the user's visibility into the system's performance.\nA fix was implemented to export additional message-related metrics, including the number of buffered messages. These metrics are now accurately reported even after broker restarts, providing a comprehensive view of the system's health.\nWith this fix, users now have access to the number of buffered messages as a metric, allowing them to monitor the system's performance more effectively. This enables them to identify any issues related to message accumulation and adjust the TTL checker configuration accordingly."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12878",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Add message definition extension elements",
    "releaseNoteText": "\nIn the BPMN model API, the `send task` or `message throw event` did not have the ability to reference a message definition. This hindered the ability to model processes where a Send Task or Message Throw Event sends a message.\nThe underlying cause of this issue was that there was no extension element available to specify the details of the message to be published, such as the correlation key, time to live, and message ID.\nA new extension element, `zeebe:publishMessage`, was added to the `bpmn:messageEventDefinition` of Intermediate Throw Event and End Event, as well as to the `bpmn:sendTask`. This allowed specifying the details of the message to be published.\nWith this fix, users can now add the `zeebe:publishMessage` extension element to the relevant BPMN elements and specify the correlation key, time to live, and message ID of the message to be published. This enables the modeling of processes where Send Tasks and Message Throw Events send messages, and allows for more flexibility in specifying different details for the same message in different elements."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12696",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Forcefully terminate a process instance",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12382",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Docker: Run the zeebe process with an unprivileged user by default",
    "releaseNoteText": " The zeebe process in the Docker image is currently run by the root user. This poses a security risk according to the OWASP recommendation.\n The zeebe user with UID 1000, which is already set up in the Docker image, is not used by default.\n The zeebe image now runs with an unprivileged user by default. The Dockerfile has been updated to utilize the existing zeebe user.\n With this fix, the zeebe process will be run with an unprivileged user by default, improving the security of the Docker image."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11708",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Add gRPC job stream API",
    "releaseNoteText": " The system did not have a gRPC API for registering job streams to the gateway, which resulted in a limited functionality for job management. Users were not able to have a long-living stream for job activation properties.\n The absence of a gRPC API for job streams was due to the lack of implementation in the system. \n A new gRPC API for job streams was added to support registering job streams to the gateway. The API was designed as a unidirectional stream from the server to the client, accepting the same activation properties as the job worker.\n With the addition of the new gRPC job stream API, users can now register job streams to the gateway and receive a long-living stream of single `ActivatedJob` objects. The system's job management functionality is enhanced, allowing for improved end-to-end pipeline performance."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13233",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "title": "Regression in deploying large payloads",
    "releaseNoteText": " Deploying large payloads on multi-partition clusters introduced a regression that impacted the maximum payload size of deployments. Before the regression, a deployed resource was written in two follow-up events (`Deployment:CREATED` and `Process:CREATED` or `DecisionRequirements:CREATED`). However, with the regression, a new event (`CommandDistribution:STARTED`) was introduced to store the command for distribution, leading to a reduction in the maximum payload size. This regression lowered the maximum payload size of deployments from approximately 2MB to around 1.4MB. The regression did not affect single partition clusters as they do not require the distribution of the deployment.\n The regression was caused by the introduction of the `CommandDistribution:STARTED` event, which stored the entire deployment including the resource for distribution. This event was only appended on multi-partition clusters, and it further reduced the maximum payload size due to the `MAX_BATCH_SIZE` restriction.\n To address this issue, the fix involved not writing the resource in the `Deployment:CREATED` event but only in the `Process:CREATED` and `DecisionRequirements:CREATED` events.\n After applying the fix, the maximum payload size of deployments on multi-partition clusters is restored to its original size of approximately 2MB. The resource is now available in the `Process:CREATED` and `DecisionRequirements:CREATED` events, allowing for proper distribution without impacting the payload size. It is important to note that this fix requires alignment with Operate and Optimize to ensure that they consume the resource from the correct events. The documentation has been updated to inform users that the resource is no longer written in the `Deployment:CREATED` event and is available in the `Process:CREATED` and `DecisionRequirements:CREATED` events instead."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13715",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Release Dry fails because of unrecognized argument",
    "releaseNoteText": " The release dry run fails due to an unrecognized argument.\n The issue was caused by an error in the called workflow, specifically in line 329 where an unrecognized named-value 'env' was used.\n A fix was implemented to correct the error by removing the unrecognized argument 'env.RELEASE_BRANCH'.\n After applying the fix, the release dry run now executes successfully without any unrecognized argument errors."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13650",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "`ZeebePartitionHealth` repeatedly reports change of health status",
    "releaseNoteText": "\n`ZeebePartitionHealth` repeatedly called listeners and logged a change of health status even when the health had not actually changed. This resulted in unnecessary notifications and log entries for the users.\nThe issue occurred because the `ZeebePartitionHealth` class was incorrectly comparing the health reports using identity check instead of comparing the health status itself.\nThe issue has been resolved by modifying the comparison logic in the `ZeebePartitionHealth` class. The health reports are now properly compared to determine if there has been an actual change in health status.\nAfter the fix, `ZeebePartitionHealth` will no longer falsely report a change in health status when there has been no actual change. Users will no longer receive unnecessary notifications or see misleading log entries regarding the health status of the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13471",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "PartitionListeners are notified even if the transition is cancelled causing NPE",
    "releaseNoteText": "\nPartitionListeners were being notified even if the transition was cancelled, leading to a NullPointerException (NPE). This caused the partition to become inactive.\nThe issue occurred because the `PartitionStartupAndTransitionContextImpl.lambda$notifyListenersOfBecomingLeader` method was still invoked even when the transition to leader was cancelled. Since the services were not installed, the logstream was null, resulting in an NPE in the listener.\nThe issue was fixed by modifying the logic in the `PartitionTransitionProcess` class. The transition is now only considered complete and the listeners are only invoked if the transition completes successfully.\nAfter the fix, partition listeners will only be invoked if the transition to leader is successfully completed. This prevents the occurrence of NullPointerException and ensures that the partition operates as expected."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13431",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Gateway readiness fails when tls enabled",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13061",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Cancel on-going remote stream registration on stream removal",
    "releaseNoteText": " There was a potential race condition where a remote stream could exist server-side even after the client stream was removed. This resulted in additional latency during a push or unnecessary job activation if it was the last stream of its type. However, the stream would eventually be removed appropriately.\n The registration of remote streams was not properly sequenced, leading to interleaving with asynchronous removal requests. This race condition occurred because remote stream registration was asynchronous and could be submitted before or after a remove request, causing the stream to exist server-side.\n To address this issue, a `ClientStreamRegistration` state machine was associated with each stream. This state machine managed the current state of the remote registration, transitioning between states such as `Initial`, `Adding`, `Added`, `Removing`, and `Removed`. Additionally, the fix included canceling in-flight registration requests and sending remove requests to all known brokers.\n After applying the fix, the registration and removal of remote streams are properly sequenced. Remove requests now cancel in-flight registration attempts and queue the removal after any ongoing requests are finished. This eliminates the race condition and ensures that the client and server-side states are synchronized. As a result, users will experience improved performance with reduced latency and the avoidance of unnecessary job activation for removed streams."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13046",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Error message truncation may be too strict",
    "releaseNoteText": " Error messages were being truncated at 500 characters, which was too strict. This limited the visibility of the full error message, especially for stack traces that exceeded the character limit. This resulted in a poor user experience, requiring users to check the worker logs for the complete error message.\n The truncation of error messages was implemented in order to prevent them from exceeding the maximum message size. However, the limit of 500 characters proved to be too strict for practical use.\n The limit on error message truncation has been increased, allowing for a higher character limit of 10,000. This will provide a better user experience by displaying the full error message instead of truncating it.\n Users will now be able to see the complete error message, even for stack traces that exceed the previous 500 character limit. This improvement enhances the usability of the system by eliminating the need to check worker logs for error details."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13041",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Don't mutate state through `JobBackoffChecker`",
    "releaseNoteText": " Mutating state through `JobBackoffChecker` resulted in unintended side effects for the user. This caused issues similar to the one reported in ticket #12797. \n The `JobBackoffChecker` was not designed to handle state mutations, resulting in unexpected behavior when attempting to cleanup backoffs.\n The issue has been resolved by updating the `JobBackoffChecker` to prevent state mutations. The code in `DbJobState.java` has been modified (lines 308-317) to ensure that state mutations are handled correctly.\n As a result of this fix, the `JobBackoffChecker` now functions as intended and no longer causes unintended side effects or issues related to state mutations. Users can now safely use `JobBackoffChecker` without worrying about unexpected behavior."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12886",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Zeebe should only log as JSON, if configured to do so",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12007",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "ExceededBatchRecordSizeException: Can't append entry",
    "releaseNoteText": " Users were unable to activate jobs due to the `ExceededBatchRecordSizeException`. The partition was marked as unhealthy, causing the stream processor to not make progress.\n The issue was caused by the append entry check not accurately determining if a record of large size could be appended to the batch. Additionally, the expected event length calculation was not fully accurate, leading to records being appended even if it exceeded the batch size.\n A fix was implemented to add an extra buffer to the expected event length calculation. This buffer allows for a small margin of error and ensures that records that exceed the batch size are not appended.\n With the fix, users can activate jobs without encountering the `ExceededBatchRecordSizeException`. The partition is healthy, and the stream processor can successfully make progress."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12655",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Automatically add `support` label to support related issues",
    "releaseNoteText": " Issues related to support were not being labeled automatically, leading to manual effort in identifying support-related issues for release notes.\n The previous system did not have a mechanism in place to automatically add the `support` label to support-related issues.\n Introduced a GitHub action that checks new issues and comments for the text `SUPPORT-XXXX`. If found, the action automatically adds the `support` label to the respective issue.\n Support-related issues are now automatically labeled with the `support` label, reducing the manual effort required to identify and track these issues. This ensures that the list of fixed issues related to support tickets can be easily generated after each release."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13254",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "When the deployment of a process definition failed, clients can still start new process instances of the actually non-existing process definition",
    "releaseNoteText": "\nWhen the deployment of a process definition failed, clients were still able to start new process instances of the non-existing process definition. This caused issues with replaying events for followers, lead to failures during failover, and resulted in process definitions being available on some partitions but not others.\nThe issue was caused by the cached process definitions remaining in memory even after the deployment failed. The process definitions were stored in a map for caching purposes, and the failure to write the follow-up event did not remove them from the cache.\nThe fix involved clearing the cached process definitions when a deployment failed, ensuring that only successfully deployed process definitions were stored in memory.\nAfter applying the fix, clients can no longer start process instances of non-existing process definitions. The cached process definitions are correctly cleared when a deployment fails, preventing issues with event replay, failover, and partition consistency."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13164",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Segfault on enabling async scheduled task",
    "releaseNoteText": "\nA segfault could occur when enabling one experimental feature flag and disabling the other (`ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLEMESSAGETTLCHECKERASYNC` and `ZEEBE_BROKER_EXPERIMENTAL_FEATURES_ENABLETIMERDUEDATECHECKERASYNC`). This issue could also occur when both feature flags were enabled. The segfault would result in unexpected behavior for the user.\nThe segfault was caused by a specific situation where the stream processor had its own transaction context and the scheduled tasks had their own shared transaction context. The issue arose when two scheduled tasks ran on different actors while reusing the shared transaction context.\nThe fix for this issue involved addressing the transactional context conflict. The shared transaction context is now properly managed to prevent the segfault from occurring.\nAfter applying the fix, the system no longer experiences a segfault when enabling or disabling the experimental feature flags. Users will observe consistent and expected behavior without any unexpected crashes. This fix has been implemented in the latest release, `8.3.0-alpha3`, ensuring a more stable and reliable system for all users."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13123",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "NPE when processing Job.TIME_OUT command",
    "releaseNoteText": "\nA NullPointerException occurs when processing the Job.TIME_OUT command. This results in the process instance related to the job becoming banned and unrecoverable.\nThe JobTimeOutProcessor incorrectly assumes that the job to be timed out is present in the state. However, there is a possibility that the job has already been removed from the state due to previous commands.\nThe fix involves updating the JobTimeOutProcessor to handle cases where the job being timed out no longer exists in the state. This prevents the NullPointerException from occurring.\nAfter applying the fix, the Job.TIME_OUT command will be rejected when the job being timed out does not exist. This ensures that the process instance remains in a valid state and can be recovered if needed."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13038",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Handle stream consumers changes during push",
    "releaseNoteText": " Previously, when attempting a push from the broker side, if all stream consumers were removed during a retry, an error would occur because the system would try to generate a random index from 0 to 0.\n The issue was caused by attempting to pick the next random stream consumer to push to without considering the possibility that all consumers may have been removed during a retry.\n The `AggregatedRemoteStream` was made immutable by copying the `AggregatedRemoteStream` record when selecting the target. This ensures that the system can handle the scenario of streams with no consumers during retries.\n With this fix in place, the system is now able to cope with streams having no consumers, even during retries. The process of picking a random stream consumer has been modified to handle this scenario and bail out early, preventing errors from occurring."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13036",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Endless client job stream registration",
    "releaseNoteText": "\nClients registering a new job stream from the gateway to any broker were experiencing an issue where the brokers were successfully registering the streams but not responding to the gateway. This resulted in the gateway continuously retrying the registration process, leading to excessive noise and giving the impression that the streams were not successfully registered.\nThe underlying cause of this issue was the utilization of the `ClusterCommunicationService#subscribe(String, Function<byte[], M>, BiConsumer<MemberId, M>, Executor executor)`. In this case, any subscriber that acted as a consumer would not send a response back to the gateway. However, the client expected a response, resulting in the endless loop when registering a job stream.\nThe issue was resolved by modifying the registration process for client job streams. The modifications addressed the issue with the `ClusterCommunicationService#subscribe` method, ensuring that the clients received the expected response upon successfully registering a job stream.\nAfter the fix, clients can now register job streams successfully on both ends. The registration process no longer enters an endless loop, and the brokers respond to the gateway as expected. This improves the overall user experience by reducing noise and providing the correct impression that the job streams have been successfully registered."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12957",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Straight-through processing loop may cause problems",
    "releaseNoteText": "\nSince version 8.2, deploying processes with undefined tasks, which are processed as straight-through activities, can lead to problems. When placed in a loop without a wait state, the workflow engine may process faster than the exporters can export records. This may cause the log to grow, increasing disk space usage and potentially affecting the availability of Zeebe Brokers, Operate, and Elasticsearch.\nThe cause of this issue is the ability to deploy processes with undefined tasks and straight-through processing loops. When the workflow engine processes faster than the exporters can export records, it can lead to resource problems and unavailability of Zeebe.\nTo mitigate this issue, a quick fix has been implemented. A delayed task has been introduced for undefined and manual tasks, introducing a wait state that offsets resource issues caused by straight-through processing loops. This allows for the completion of the tasks after a delay.\nWith the quick fix in place, the impact of straight-through processing loops causing problems in Zeebe has been mitigated. The delayed task for undefined and manual tasks introduces a wait state that prevents resource issues and ensures the completion of these tasks. This improves the overall stability and availability of Zeebe."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12933",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Failing jobs with job push enabled breaks the job lifecycle",
    "releaseNoteText": "\nWith job push enabled, when a job is failed with remaining retries and no backoff, the job is immediately activated again and marked as failed, resulting in pushing out jobs that are in a failed state. This breaks the job lifecycle and may lead to data inconsistencies.\nThe issue was caused by a confusing interplay between the `JobFailProcessor` (a `CommandProcessor`), the `CommandControl`, and the `BpmnJobActivationBehavior`. The code correctly transitioned the job to the `FAILED` state and then handed it over to the `jobActivationBehavior` to be activated. However, the `CommandControl` did not write the follow-up event immediately, leading to incorrect state transitions.\nTwo possible fixes were proposed for this issue. The first fix was to move the job push to the `CommandProcessor#afterAccept` method. The second fix, which was considered a nicer solution, involved refactoring the processors to use the `TypedRecordProcessor` interface, which would make the event writes more clear and readable.\nThe fix was implemented by refactoring the processors to use the `TypedRecordProcessor` interface, ensuring that the job lifecycle is followed correctly. Now, when a job is failed with remaining retries and no backoff, the job transitions to the `FAILED` state and is then activated again. The log now shows a clear sequence of job events: ACTIVATED -> FAIL -> FAILED -> ACTIVATED. This eliminates confusion and provides a more accurate representation of the job's state transitions."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12915",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Allow extending journal record format with out breaking compatibility",
    "releaseNoteText": " Previously, it was not possible to change the journal record schema without breaking backward compatibility. This meant that a broker on a newer version could not receive events via raft replication if the event was written by a leader on an older version. Additionally, a broker on an older version could not receive events via raft replication if the event was written by a leader on a newer version. This limitation made it difficult to upgrade a running system to a new version and caused potential unavailability during rolling updates.\n The inability to extend the journal record schema without breaking compatibility was a result of the journal record schema being moved to SBE (Simple Binary Encoding). While this move was intended to allow for easier extension, it did not fully achieve that goal.\n The fix involved changing the mechanism for sending the serialized journal record in the AppendRequest. This required modifications to the journal API and the raft replication handling. A new concept of raft-protocol-version was introduced, allowing for interpretation of requests based on the version of the sender. When a raft follower on a new version receives a request from an older leader, it writes the record using the old SBE version and calculates a checksum that matches the original. If the request is from a new leader, the new logic is used to handle the serialized journal record.\n With this fix, it is now possible to extend the journal and raft record schema without breaking compatibility. Brokers on newer versions can work with records written with older versions, ensuring backward compatibility. While brokers on older versions may not be able to read records from new versions, this does not cause any inconsistency. This fix allows for smoother rolling updates and recreating with new versions, as followers on newer versions can receive events from leaders on older versions."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12875",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "`ModifyProcessInstanceRejectionTest#shouldRejectActivationWhenAncestorScopeIsNotFlowScope` test fails on Windows",
    "releaseNoteText": "\nThe `ModifyProcessInstanceRejectionTest#shouldRejectActivationWhenAncestorScopeIsNotFlowScope` test fails on Windows 10. The test expects to be able to activate a parallel gateway 'join', but not all sequence flows have been taken, resulting in an INVALID_STATE error. Additionally, the test attempts to modify an instance of the process 'process', but it contains activate instructions with ancestor scope keys that are not ancestors of the element to activate, resulting in an INVALID_ARGUMENT error.\nThe test failure is caused by line breaks in the error message, which are not handled correctly on the Windows platform.\nThe code has been updated to handle line breaks in the error message on Windows.\nAfter the fix, the `ModifyProcessInstanceRejectionTest#shouldRejectActivationWhenAncestorScopeIsNotFlowScope` test no longer fails on Windows. The error message is now displayed correctly, allowing the test to pass successfully."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12837",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Catch all error events by using empty 'errorCode' does not work",
    "releaseNoteText": "\nCatching error events using an empty `errorCode` did not work. This resulted in incidents with `errorType=UNHANDLED_ERROR_EVENT`. In addition, only error events with a non-empty errorCode were reported, causing confusion for users.\nThe issue was caused by the error catch event (boundary or start event in Subprocess) not correctly handling the empty `errorCode`, leading to incidents being created instead of the error being caught.\nThe fix involved updating the error catch event to properly handle the scenario of an empty `errorCode`. This allowed the error to be caught instead of creating an incident.\nAfter the fix, error events with an empty `errorCode` are now properly caught, and no incidents are created. Users can catch all error events without needing to remove the `errorEventDefinition` as a workaround."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12833",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Cannot resolve output mapping incident on a none end event",
    "releaseNoteText": " Users were unable to resolve incidents on a none end event, leading to the process being stuck on that event.\n The issue was caused by an incident in one of the chaos test models, where a none end event had an output mapping referencing a non-existent variable.\n The incident was resolved by adding a dummy variable and removing the incorrect mapping.\n With this fix, users can now successfully resolve incidents on a none end event, allowing the process to continue as expected."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/13058",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Documentation"
    },
    "title": "Document new Deployment Distribution logic",
    "releaseNoteText": " The deployment distribution logic was not documented, causing confusion and making it difficult for users to understand how the distribution works.\n The lack of documentation on the deployment distribution logic stemmed from the decision not to include a section on resource deployments. This decision was made based on the belief that distribution is generic and should work the same for all commands.\n The documentation has been updated to include a comprehensive description of the deployment distribution logic, including a section on resource deployments. This section provides clear and detailed information on how the distribution works in the context of deploying resources.\n Users now have access to complete and accurate documentation that explains the deployment distribution logic and provides specific details on resource deployments. This will enhance user understanding and enable them to effectively utilize the distribution functionality in the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/8263",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Enhancements"
    },
    "title": "Show blacklisting in the Grafana Dashboard",
    "releaseNoteText": " The Grafana Dashboard does not currently show any indication of blacklisted instances or the rate of blacklisting. This makes it difficult for users to quickly identify if instances are blacklisted or not, resulting in a loss of visibility and potential delays in detecting and resolving issues with blacklisting.\n The metrics for blacklisting were added previously, but they were never incorporated into the Grafana Dashboard.\n The metric for blacklisted instances will now be displayed on the Grafana Dashboard, providing users with a clear indication of whether instances are blacklisted or not. Additionally, the metric will be refilled on restarts to ensure continuous visibility.\n With this fix, users will have improved visibility into blacklisted instances through the Grafana Dashboard. They will be able to easily identify if instances are blacklisted or not, enabling them to take appropriate action and resolve any issues in a timely manner."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12796",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Provide Error Handler implementation for Job Streamer",
    "releaseNoteText": " When pushing a job failed, the `JobYieldProcessor` was not triggered, which resulted in the related job not being available for long polling.\n The error handler was not implemented for the Job Streamer, specifically the `RemoteJobStreamErrorHandlerService` inside `JobStreamServiceStep`.\n The error handler has been implemented and registered to the `RemoteJobStreamErrorHandlerService` inside `JobStreamServiceStep`. Now, when a job push fails, the `JobIntent.YIELD` command is appended.\n When a job push fails, the `JobYieldProcessor` is now triggered, making the related job available for long polling."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12793",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Allow configuring request timeout for InstallRequest",
    "releaseNoteText": " Increased timeout for InstallRequest to prevent timeout exceptions during snapshot replication, especially on networks with higher latency between brokers.\n Previously, the default request timeout for all raft requests was being used for InstallRequest as well. However, since InstallRequest sends larger snapshot files, it sometimes took longer to send the request and get a response, leading to timeout exceptions.\n Exposed a new configuration option to set a separate request timeout specifically for InstallRequest. This allows users to increase the timeout only for InstallRequest without affecting the timeout for other raft requests.\n Users can now configure a higher request timeout for InstallRequest, reducing the chances of timeout exceptions during snapshot replication. This improves the reliability and efficiency of the snapshot replication process, especially on networks with higher latency between brokers."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12575",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Improve the traversing of snapshot files",
    "releaseNoteText": "\nThe traversal of snapshot files in the system was inefficient as it involved collecting the files in a list, leading to performance degradation.\nThe inefficient traversal of snapshot files was caused by the usage of the `.forEachOrdered` method in the code, which accumulated the snapshot files in a list.\nThe code has been updated to use the `Stream.forEachOrdered` method instead, eliminating the need to collect the snapshot files in a list and improving performance.\nAs a result of this fix, the traversal of snapshot files has been optimized and the system now has improved performance in creating snapshots. Users will experience faster processing times, especially in load test environments. This enhancement will also prevent potential degradation as the number of snapshot files increases over time."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12548",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Provide Grafana Dashboards for the Actor metrics",
    "releaseNoteText": "\nPreviously, there was no out-of-the-box Grafana dashboard for the Actor metrics in the experimental phase.\nThe absence of a separate Grafana dashboard for the Actor metrics was due to the experimental nature of these metrics and the need to keep them separate from the existing dashboard.\nA new file called \"actor.json\" has been added to the Grafana dashboards folder to provide a separate dashboard for the Actor metrics.\nUsers can now access a dedicated Grafana dashboard for the Actor metrics, providing better visibility and convenience for monitoring the experimental phase of these metrics. Users who do not enable these metrics can choose to not use the dashboard, ensuring flexibility in configuration."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12541",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Jobs are pushed from relevant processors",
    "releaseNoteText": "\nJobs were not being pushed from relevant processors, resulting in a lack of activation of jobs for clients.\nThe `Processor` classes responsible for pushing activated jobs were not properly configured to utilize the `JobStreamer` API and perform the necessary steps for job activation.\nThe `BpmnJobActivationBehavior` class was modified to incorporate the use of the `JobStreamer` API for pushing jobs. The following steps were implemented:\n- Obtaining `JobActivationProperties` for an available `JobStream`\n- Setting the `deadline` for the `JobRecord` using `JobActivationProperties`\n- Setting the `variables` for the `JobRecord` using `JobActivationProperties`\n- Setting the `worker` for the `JobRecord` using `JobActivationProperties`\n- Activating the job using a `JobBatchRecord` with the intent `JobBatchIntent.ACTIVATE`\n- Pushing the `JobRecord` onto the `JobStream` through a `SideEffectProducer`\nAfter the fix, the relevant processors are now able to properly push the activated jobs to the clients. This ensures that the jobs are correctly activated and processed within the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12539",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Create `ProcessInstanceBatch.TERMINATE` processor",
    "releaseNoteText": "\nPreviously, there was no processor available to handle the `ProcessInstanceBatch.TERMINATE` commands. As a result, these commands could not be processed, causing a delay in terminating process instances.\nThe absence of a `ProcessInstanceBatch.TERMINATE` processor was the underlying cause of this issue. The BpmnStateBehavior did not have a method to retrieve a specific number of child elements starting from a given key, which prevented the creation of this processor.\nThis issue has been resolved by introducing a new processor called `ProcessInstanceBatch.TERMINATE`. To enable this, a method has been added to the BpmnStateBehavior that allows retrieving a specified number of child elements starting from a specific key. Additionally, a new `ProcessInstanceBatch` record is created when the `index` is available, and a `ProcessInstanceBatch.TERMINATE` command is written for each element instance key in the list.\nWith this fix in place, the `ProcessInstanceBatch.TERMINATE` commands can now be properly processed by the system. When the `index` is available, the processor retrieves the next `BATCH_SIZE` + 1 child instances of the `batchElementInstanceKey`, creates a new `ProcessInstanceBatch` record, and writes a corresponding `ProcessInstanceBatch.TERMINATE` command. In the absence of an `index`, the system continues as usual with no additional actions required."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12538",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Use the `ProcessInstanceBatch Command` when terminating container elements",
    "releaseNoteText": " The system did not correctly terminate container elements when the `onTerminate` method was called. This resulted in child instances of these elements not being properly terminated.\n The issue was caused by the usage of the outdated `BpmnStateTransitionBehavior#terminateChildInstances` method. This method did not utilize the new `ProcessInstanceBatch` command with the `TERMINATE` intent.\n The `BpmnStateTransitionBehavior#terminateChildInstances` method has been modified. It now creates a `ProcessInstanceBatch` record for the container element and writes a `ProcessInstanceBatch.TERMINATE` command using the created record.\n With the fix in place, container elements now correctly terminate their child instances when the `onTerminate` method is called. The new `ProcessInstanceBatch` command with the `TERMINATE` intent ensures that child instances are properly terminated, improving the overall behavior and reliability of the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12085",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Job Yield Processor is implemented to be used for Job Push fallback",
    "releaseNoteText": " Previously, when a job pushed to the `JobStreamer` API failed to be handed over to the client due to client failure, there was no fallback mechanism in place. \n The lack of a fallback mechanism in the `JobStreamer` API caused jobs to be permanently stuck in a failed state when a client failure occurred.\n A new `JobYield` processor has been added to handle the fallback scenario when a job fails to be handed over to the client. This processor performs similar logic to the existing `JobFailProcessor`, allowing the job to become activatable again.\n With this fix, jobs that fail to be handed over to the client due to client failure will now be processed by the new `JobYield` processor, which sets the job to an `ACTIVATABLE` state. This enables the job to be retried or processed by other fallback mechanisms, ensuring a smoother and more reliable job processing experience. The implementation of this fix should also be used in the job push `ErrorHandler` implementation for comprehensive error handling."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/10031",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Support BPMN Converging Inclusive Gateway",
    "releaseNoteText": "\nBPMN converging inclusive gateway behavior is missing from the supported features. This means that users have been unable to utilize the converging behavior of inclusive gateways.\nThe validation in the system restricted the number of incoming sequence flows to an inclusive gateway to a maximum of one. This limitation prevented the implementation of converging inclusive gateway behavior.\nThe fix involved removing the validation that restricted the number of incoming sequence flows to an inclusive gateway. Additionally, the fix introduced conditions for activating the inclusive gateway, which included checking for active children of the flow scope instance and ensuring that all incoming sequence flows were taken at least once. However, there was still a limitation in checking if no path could be found from any active child to the inclusive gateway.\nWith this fix, users can now utilize the converging behavior of inclusive gateways. The system no longer restricts the number of incoming sequence flows to an inclusive gateway, allowing for more flexible and comprehensive handling of process flows. This enhancement provides users with a more powerful and versatile tool for designing their BPMN workflows."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/2890",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "title": "I can spawn inner instances for a large input collection",
    "releaseNoteText": "\nThe broker fails to spawn inner instances when the input collection for a multi-instance activity contains a large number of elements. This requires manual adjustment of the collection variable to fix the issue.\nThe current behavior of the multi-instance activity is to spawn all instances at once, which can overload the system when dealing with large input collections.\nThe instances are now spawned step-wise, allowing for a controlled creation process. For example, a set number of instances are spawned first, followed by the spawning of the next set of instances after a specific event or terminate command.\nUsers can now spawn as many instances as defined by the input collection, even for large collections. The instances are spawned in a controlled manner, ensuring the system can handle the process without overloading."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12780",
      "component": "Zeebe",
      "subcomponent": "Broker",
      "context": "Bug Fixes"
    },
    "title": "Failing to write to logstream during stepdown is logged as error",
    "releaseNoteText": "\nPreviously, during a leader transition in the system, if the logstream closed before the leader could write a user request to it, an error was logged. This error message was introduced in a recent update, causing confusion for users. The error message was noisy and unnecessary, creating additional log entries.\nThe error was occurring due to a change introduced in a recent pull request. Previously, this error was ignored, but the update caused the error message to be logged. The logstream closed before the leader could complete the write operation, leading to the error.\nThe log level has been reduced to warn/debug to reduce the noise in the logs. Additionally, the `logstream#tryWrite` function has been updated to return specific error codes for better error handling. Now, when a leader transition occurs and the logstream is closed, the system recognizes this situation and returns a `PARTITION_LEADER_MISMATCH` code to the gateway, allowing it to retry the command with the new leader.\nWith this fix, the unnecessary error message is no longer logged during leader transitions when the logstream is closed. The log level has been reduced to warn/debug, reducing noise in the logs. The system now handles the situation of a leader transition and closed logstream correctly, returning the appropriate error code to the gateway for retrying the command with the new leader. This improves the overall behavior and reliability of the system during leader transitions."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12754",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Journal reset resulting in an intermediate state which is detected as corruption",
    "releaseNoteText": " A bug was causing the system to enter an invalid intermediate state, which was detected as corruption. This resulted in a startup failure with the error message \"Expected to find a snapshot at index >= log's first index X, but found snapshot Y. A previous snapshot is most likely corrupted.\" The issue arose when a follower received a snapshot, but before committing it, the segments were reset. If the node was shutdown during the segment deletion process and then restarted, it would have the old snapshot and partially deleted segments, leading to the observed corruption.\n The reset or snapshot commit process was not handling the deletion of segments correctly. The reset and segment deletion were not atomic operations, leading to the possibility of an invalid intermediate state.\n The segments were modified to be deleted in reverse order during the reset process. This ensured that there were no gaps in the logs or snapshots, effectively preventing any corruption. However, this fix only addressed the specific case and did not solve the underlying atomicity issue in the installation operation on the follower.\n With the fix in place, the reset or snapshot commit process no longer results in an invalid intermediate state being detected as corruption. The system now correctly handles the deletion of segments, ensuring that the logs and snapshots are always in a valid state. This prevents any startup failures due to corruption and improves the overall stability and reliability of the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12623",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "After restoring from backup, a partition re-takes the backup",
    "releaseNoteText": " After restoring from a backup, a partition would unnecessarily re-take the backup, resulting in a duplicate backup of the partition with the same backup ID. This would waste resources without any impact on the functionality as the new backup would be logically equivalent to the old backup.\n The issue occurred when the leader who originally took the backup was no longer the leader of the partition after the restore. This caused the partition to go through the backup process again, resulting in a duplicate backup.\n The fix involved modifying the partition logic to prevent it from re-taking a backup after a restore. This change ensured that the partition does not unnecessarily repeat the backup process.\n After applying the fix, when restoring from a backup, the partition no longer re-takes the backup unnecessarily. This improves resource utilization and does not have any impact on the functionality as the new backup is logically equivalent to the old one."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12622",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "List backup fails when a partition has same backup taken by multiple nodes",
    "releaseNoteText": "\nWhen a partition has the same backup taken by multiple nodes, the list backup fails with an error message indicating a duplicate key.\nThis issue occurs when a partition attempts to take a backup again while a backup for the same ID already exists. It is a rare case that can occur if there is a leader change during the backup process.\nThe fix for this issue includes handling duplicate backup IDs for a partition in the list backup functionality.\nAfter applying the fix, the list backup will be able to handle partitions with duplicate backup IDs. Users will no longer encounter the error message indicating a duplicate key when trying to list backups."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12597",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Listing backups fails if more than 255 backups are available",
    "releaseNoteText": "\nAttempting to list all available backups fails with an error message stating that the count is outside the allowed range, even though more than 255 backups are present. This issue prevents the proper functioning of the backup API and makes it unusable for querying a large number of backups.\nThe issue is caused by the definition of the `groupSizeEncoding` in the `common-types.xml` file, which uses a `unit8` to represent the number of entries. This encoding mechanism limits the number of backups that can be listed to a maximum of 255.\nTo address this issue, the `groupSizeEncoding` has been modified to use a `uint16` encoding instead. This change will support up to 65535 backups.\nAfter applying the fix, Zeebe will be able to support a much larger number of backups, up to a maximum of 65535. The backup API will now function properly, and the number of listed backups will be limited to a reasonable number, avoiding potential timeouts and making the backup API usable for querying backups."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12591",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "8.2.3 Degradation: Creating an oversized BPMN causes unrecoverable failure ",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12374",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "CorruptedJournalException: Fail to read version byte from segment",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11578",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Multi-Instance with messageevent-based subprocess that uses inputElement as correlationKey fails",
    "releaseNoteText": " Multi-Instance fails to start when it contains a Message Event-based Subprocess that uses the inputElement as correlationKey. Instead, an incident is created, and the expression 'myObject.myId' fails to evaluate due to the variable 'myObject' not being found.\n The issue is caused by a bug in the system where the Multi-Instance process is unable to handle a Message Event-based Subprocess that uses the inputElement as correlationKey. This results in the creation of an incident and the failure of the expression evaluation.\n The bug has been fixed in the latest patch release (version 8.0.15, 8.1.13, 8.2.6, and 8.3.0). The fix addresses the issue with the Multi-Instance process and ensures that it can properly handle the Message Event-based Subprocess with the inputElement as correlationKey.\n With the fix applied, the Multi-Instance process will start as expected and process the necessary tasks. It will also be able to listen to any messages on the event-based subprocess without any incidents or expression evaluation failures."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11355",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Not possible to cancel process instance with many active element instances",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12584",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Documentation"
    },
    "title": "Document guidelines on how to handle flaky tests",
    "releaseNoteText": " Flaky tests were causing disruptions in the testing process and hindering contributors' progress.\n Flaky tests were occurring due to unpredictability in the test environment, timing issues, or race conditions.\n A guide was added to provide contributors with clear instructions on how to handle flaky tests. The guide includes best practices and troubleshooting steps to identify and resolve flakiness issues.\n Contributors now have a comprehensive guide to refer to when encountering flaky tests. This empowers them to effectively troubleshoot and resolve flakiness, resulting in smoother testing processes and improved progress in their contributions."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12122",
      "component": "Zeebe",
      "subcomponent": "Java Client",
      "context": "Enhancements"
    },
    "title": "Configure the client's inbound max_message_size",
    "releaseNoteText": "\nUsers were unable to specify a larger inbound max message size for the client, resulting in errors when the gateway responded with messages larger than the default 4 MB.\nThe client did not have a configuration option for specifying the inbound max message size, and the channel setup on client construction made it difficult to implement this feature.\nA configuration option has been added to the client to allow users to specify an inbound max message size for gRPC responses. The default value for this option is set to the current hardcoded limit of 4 MB.\nAfter applying the fix, users can now configure the client's inbound max message size. This allows for handling larger-than-default messages from the gateway without encountering errors."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12538",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Use the `ProcessInstanceBatch Command` when terminating container elements",
    "releaseNoteText": " The system did not correctly terminate container elements when the `onTerminate` method was called. This resulted in child instances of these elements not being properly terminated.\n The issue was caused by the usage of the outdated `BpmnStateTransitionBehavior#terminateChildInstances` method. This method did not utilize the new `ProcessInstanceBatch` command with the `TERMINATE` intent.\n The `BpmnStateTransitionBehavior#terminateChildInstances` method has been modified. It now creates a `ProcessInstanceBatch` record for the container element and writes a `ProcessInstanceBatch.TERMINATE` command using the created record.\n With the fix in place, container elements now correctly terminate their child instances when the `onTerminate` method is called. The new `ProcessInstanceBatch` command with the `TERMINATE` intent ensures that child instances are properly terminated, improving the overall behavior and reliability of the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12537",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Create `ProcessInstanceBatch` Record and Intent",
    "releaseNoteText": " The system lacked the ability to perform batch actions on process instances, such as terminating or activating children. This limitation affected the efficiency and flexibility of managing large sets of process instances. \n The absence of a `ProcessInstanceBatch` record and intent prevented users from executing batch actions on process instances. \n The `ProcessInstanceBatch` record and intent have been implemented to enable users to perform batch actions on process instances. The record now includes the `batchElementInstanceKey`, which represents the element instance key of the element that the batch is executed on. In addition, the `index` field has been added to keep track of the current position in the batch.\n With this fix, users can now utilize the `ProcessInstanceBatch` record and intent to efficiently perform batch actions on process instances. They have the option to terminate or activate batches of child instances, enhancing their ability to manage and control process execution."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12416",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Remove the default un-overridable `-Xms128m` value",
    "releaseNoteText": " Users were unable to override the default `-Xms128m` value when tuning the JVM. This caused difficulty in customizing the JVM options for optimal performance.\n The default `-Xms128m` value was hardcoded in the script generated by the `appassembler-maven-plugin`. Attempts to override it via `JAVA_OPTS` were ineffective as the explicit `-Xms128m` flag was applied after.\n The `-Xms` flag was removed from the plugin configuration, allowing the JVM to use its default value. This change was implemented by @aivinog1.\n Users can now easily override the default `-Xms` value by modifying the `JAVA_OPTS` environment variable. The JVM will use the appropriate value specified, providing greater flexibility in tuning the system for optimal performance."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12000",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "OAuth Auth Token authentication support in Zeebe Gateway",
    "releaseNoteText": "An error occurred while generating a response."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11920",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Support Broadcast signal for Signal End Events",
    "releaseNoteText": " The broadcast signal for Signal End Events was not supported, resulting in the inability to trigger a signal when the Signal End Event activates.\n The `EndEventProcessor` did not have the capability to broadcast a signal on activation.\n To address this issue, a new feature called `SignalEndEventBehavior` was introduced. When a Signal End Event activates, the system now applies input mappings, transitions to the activated state, writes a `Signal:Broadcast` command, applies output mappings, and finally transitions to complete the element.\n With this fix, users can now utilize Signal End Events to trigger signals and achieve the expected behavior in the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11919",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Enhancements"
    },
    "title": "Support Broadcast signal for Signal Intermediate Throw Events",
    "releaseNoteText": " The signal intermediate throw events were not broadcasting a signal when activated, resulting in an incomplete communication flow.\n The `IntermediateThrowEventProcessor` did not have the functionality to broadcast a signal upon activation.\n We made adjustments to the `IntermediateThrowEventProcessor` to incorporate broadcasting of a signal when the signal intermediate throw event is activated. This includes applying input mappings, transitioning to the activated state, writing a `Signal:Broadcast` command, applying output mappings, and transitioning to complete the element.\n With this fix, the signal intermediate throw events can now properly broadcast a signal when activated. This ensures a seamless communication flow and allows for effective synchronization between different parts of the system."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12622",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "List backup fails when a partition has same backup taken by multiple nodes",
    "releaseNoteText": "\nWhen a partition has the same backup taken by multiple nodes, the list backup fails with an error message indicating a duplicate key.\nThis issue occurs when a partition attempts to take a backup again while a backup for the same ID already exists. It is a rare case that can occur if there is a leader change during the backup process.\nThe fix for this issue includes handling duplicate backup IDs for a partition in the list backup functionality.\nAfter applying the fix, the list backup will be able to handle partitions with duplicate backup IDs. Users will no longer encounter the error message indicating a duplicate key when trying to list backups."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12597",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Listing backups fails if more than 255 backups are available",
    "releaseNoteText": "\nAttempting to list all available backups fails with an error message stating that the count is outside the allowed range, even though more than 255 backups are present. This issue prevents the proper functioning of the backup API and makes it unusable for querying a large number of backups.\nThe issue is caused by the definition of the `groupSizeEncoding` in the `common-types.xml` file, which uses a `unit8` to represent the number of entries. This encoding mechanism limits the number of backups that can be listed to a maximum of 255.\nTo address this issue, the `groupSizeEncoding` has been modified to use a `uint16` encoding instead. This change will support up to 65535 backups.\nAfter applying the fix, Zeebe will be able to support a much larger number of backups, up to a maximum of 65535. The backup API will now function properly, and the number of listed backups will be limited to a reasonable number, avoiding potential timeouts and making the backup API usable for querying backups."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12509",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "MessageTTL checking fails with deserialization errors",
    "releaseNoteText": "\nMessageTTL checking failed with deserialization errors. This resulted in a `RuntimeException` when trying to deserialize the `MessageRecord` object, causing the processing actor to fail. As a consequence, processing on the affected partition was also prevented. This issue impacted versions 8.1.9 and 8.2.0 onwards.\nThe issue was caused by unsafe concurrent access to the writer of a shared record value when messages from different partitions expired at the same time. This concurrent access led to deserialization errors and the failure of the processing actor.\nThe fix for this issue was implemented in commit [e1a6cae69c17325fc71a8ee92022a70d969bd0da](https://github.com/camunda/zeebe/commit/e1a6cae69c17325fc71a8ee92022a70d969bd0da). With this fix, the issue of deserialization errors during MessageTTL checking was resolved. \nAfter applying the fix, MessageTTL checking is now performed without encountering any deserialization errors. The processing actor no longer fails and processing on the affected partition can proceed as expected. This fix is available in versions after 8.1.9 and 8.2.0."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12433",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Broker cannot start with S3 accessKey and secretKey not supplied ",
    "releaseNoteText": "\nThe broker cannot start when S3 accessKey and secretKey are not supplied. This results in an error during startup and the broker fails to execute.\nThe issue stemmed from a recent commit that did not handle the case where ACCESS_KEY and SECRET_KEY were not passed. The method `io.camunda.zeebe.backup.s3.S3BackupStore.buildClient` still called `AwsBasicCredentials.create(credentials.accessKey(), credentials.secretKey())` even when no credentials were provided. Additionally, none of the unit or integration tests checked for this scenario.\nA fix has been implemented to handle the case where accessKey and secretKey are not supplied. The method `io.camunda.zeebe.backup.s3.S3BackupStore.buildClient` now verifies if the credentials are present before calling `AwsBasicCredentials.create()`.\nWith the fix in place, the broker now starts successfully even when S3 accessKey and secretKey are not provided. Users can configure the S3 backup properties without needing to supply the credentials through environment variables."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12328",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Cannot disable Raft flush without specifying a delay",
    "releaseNoteText": "\nIn version 8.2.0, users were unable to disable the Raft flush without specifying a delay. Attempting to start the broker with the `ZEEBE_BROKER_CLUSTER_RAFT_FLUSH_ENABLED` set to `false` resulted in the broker failing to start.\nThis issue was caused by how Spring deserialized the configuration. When using a `record` internally for the configuration, the deserialization process attempted to pass both properties, with the second one being `null`. This prevented relying on \"default\" values as previously done with simple classes.\nThe fix for this issue involved updating the deserialization logic of the Spring configuration. The code was modified to properly handle the scenario where the second property is null, allowing the disabling of Raft flush without specifying a delay.\nWith this fix applied, users can now disable the Raft flush without the need to specify a delay time. Starting the broker with `ZEEBE_BROKER_CLUSTER_RAFT_FLUSH_ENABLED` set to `false` will result in the expected behavior, where the flush is disabled and the broker starts successfully."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12326",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "The `newThrowErrorCommand` incorrectly handled in `8.2.0`",
    "releaseNoteText": " In version 8.2.0 of Zeebe, the `newThrowErrorCommand` was not correctly handled when used with BPMN, causing an error to occur when attempting to throw an error event with a specific code and message.\n The issue was caused by a change in the way errors are transformed in the code. When parsing the error code using the FEEL engine, the assumption was made that the static expression would always be of type `String`, whereas in this case it was a `Number`.\n The issue was addressed by adding support for `Number` in the transformers responsible for handling errors.\n With the fix in place, the `newThrowErrorCommand` is now correctly handled when used with BPMN, allowing the error event to be thrown with the specified code and message without any errors occurring."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/12173",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Zeebe node sends messages to wrong node",
    "releaseNoteText": " The Zeebe node was sending messages to the wrong node.\n The channel state in the NettyMessagingService was found to be inconsistent. While analyzing the heap dump, it was discovered that the channel pool for a specific IP address was pointing to the wrong node.\n The fix involved using both the address and inetAddress to find the channel pool. This ensured that the correct channel was used, even if the IP address was reassigned.\n After the fix, the Zeebe node successfully sends messages to the intended node, resolving the issue of sending messages to the wrong node."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11594",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Triggering due timer events causes periodic latency spikes",
    "releaseNoteText": " Triggering due timer events caused periodic latency spikes in the system. This impact was observed in the blocking of the Stream Processor while the timer event checker ran, resulting in delays in the overall process flow. Additionally, the execution of a batch of commands to trigger timer events without any interleaving with incoming commands further contributed to the latency spikes.\n The issue stemmed from the shared actor between the Stream Processor and the timer event checker. This caused the Stream Processor to be blocked while the checker ran, leading to concurrency issues as reading from RocksDB by the checker could result in changes to the state. Similarly, the job's timeline and backoff checker also exhibited the same pattern.\n To address this issue, the Stream Processor and the checker were decoupled to ensure parallel execution. The checker now collects timers to trigger without blocking the Stream Processor, allowing the process flow to continue uninterrupted. Furthermore, a batch with a limited number of commands (e.g., 10 due timer events) is now submitted by the checker, providing interleaving with incoming commands."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11414",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Unhandled `NoSuchElementException` when looking for executable process while deploying BPMN resource",
    "releaseNoteText": "\nUsers experienced an unhandled `NoSuchElementException` when attempting to deploy BPMN resources. This issue caused invalid BPMN resources to be handled without any grace and resulted in the exception being thrown.\nThe underlying cause of this issue was that during the deployment, the deployment record process metadata did not contain a process that was present in the BPMN file. This led to a process in the BPMN file that had not been stored as a `PersistedProcess`, causing the `NoSuchElementException` to occur.\nTo address this issue, the code path that was part of the bug was removed. This involved removing the transformation step for the persisted process XML that triggered the bug. Additionally, the code was optimized to improve the performance when deploying new process versions.\nWith this fix applied, users will no longer encounter the unhandled `NoSuchElementException` when attempting to deploy invalid BPMN resources. The deployment process will now handle invalid resources gracefully and provide a more understandable error message."
  },
  {
    "version": "8.3.0",
    "note": {
      "githubUrl": "https://github.com/camunda/zeebe/issues/11355",
      "component": "Zeebe",
      "subcomponent": "Misc",
      "context": "Bug Fixes"
    },
    "title": "Not possible to cancel process instance with many active element instances",
    "releaseNoteText": "An error occurred while generating a response."
  }
]